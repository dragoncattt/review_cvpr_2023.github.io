<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
  </div>
  </div>

  <div id="sortoptions">
    To search the paper by title:
    <div class="pagination_p"> 
      <a href="search.html" >search</a> 
    </div>
    </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic1</h2>
      <b>Topic words : &ensp;</b>video, &ensp;motion, &ensp;temporal, &ensp;videos, &ensp;frame, &ensp;action, &ensp;frames, &ensp;visual</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1.MoLo: Motion-Augmented Long-Short Contrastive Learning for Few-Shot Action Recognition</span><br>
                <span class="as">Wang, XiangandZhang, ShiweiandQing, ZhiwuandGao, ChangxinandZhang, YingyaandZhao, DeliandSang, Nong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MoLo_Motion-Augmented_Long-Short_Contrastive_Learning_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18011-18021.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的少数镜头动作识别方法通过在已学习的视频特征上进行帧级别匹配来达到良好的性能，但它们通常存在两个限制：一是局部帧之间的匹配过程由于缺乏长范围时间感知的指导而往往不准确；二是显式运动学习通常被忽视，导致部分信息丢失。<br>
                    动机：为了解决这些问题，我们开发了一种名为“运动增强的长短期对比学习”（MoLo）的方法，该方法包含两个关键组件，即长短期对比目标和运动自解码器。<br>
                    方法：具体来说，长短期对比目标是通过最大化属于同一类别的视频的全局标记与局部帧特征的一致性，使局部帧特征具有长形式的时间感知能力。运动自解码器是一种轻量级架构，用于从差分特征重建像素运动，从而明确地将网络嵌入到运动动态中。<br>
                    效果：通过这种方法，MoLo可以同时学习长范围的时间上下文和运动线索，以进行全面的少数镜头匹配。我们在五个标准基准上评估了MoLo的效果，结果显示MoLo优于最近先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current state-of-the-art approaches for few-shot action recognition achieve promising performance by conducting frame-level matching on learned visual features. However, they generally suffer from two limitations: i) the matching procedure between local frames tends to be inaccurate due to the lack of guidance to force long-range temporal perception; ii) explicit motion learning is usually ignored, leading to partial information loss. To address these issues, we develop a Motion-augmented Long-short Contrastive Learning (MoLo) method that contains two crucial components, including a long-short contrastive objective and a motion autodecoder. Specifically, the long-short contrastive objective is to endow local frame features with long-form temporal awareness by maximizing their agreement with the global token of videos belonging to the same class. The motion autodecoder is a lightweight architecture to reconstruct pixel motions from the differential features, which explicitly embeds the network with motion dynamics. By this means, MoLo can simultaneously learn long-range temporal context and motion cues for comprehensive few-shot matching. To demonstrate the effectiveness, we evaluate MoLo on five standard benchmarks, and the results show that MoLo favorably outperforms recent advanced methods. The source code is available at https://github.com/alibaba-mmai-research/MoLo.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2.Video Event Restoration Based on Keyframes for Video Anomaly Detection</span><br>
                <span class="as">Yang, ZhiweiandLiu, JingandWu, ZhaoyangandWu, PengandLiu, Xiaotao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Video_Event_Restoration_Based_on_Keyframes_for_Video_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14592-14601.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频异常检测（VAD）是一个重要的计算机视觉问题。<br>
                    动机：现有的基于深度神经网络的VAD方法主要通过帧重建或帧预测进行，但缺乏对视频中更高级别的视觉特征和时间上下文关系的挖掘和学习，这限制了这两种方法的进一步性能提升。<br>
                    方法：受视频编解码理论的启发，我们引入了一种全新的VAD范式来突破这些限制：首先，我们提出了一种基于关键帧的视频事件恢复的新任务。鼓励深度神经网络根据视频关键帧推断缺失的多帧以恢复视频事件，从而更有效地激发深度神经网络挖掘和学习视频中的潜在的更高级别的视觉特征和全面的时序上下文关系。为此，我们提出了一种新的具有双跳跃连接的U形Swin Transformer网络（USTN-DSC）用于视频事件恢复，其中引入了一个交叉注意力和一个时间上采样的残差跳跃连接，以进一步帮助恢复视频中的复杂静态和动态运动对象特征。此外，我们还提出了一种简单而有效的相邻帧差异损失来约束视频序列的运动一致性。<br>
                    效果：在基准测试上的大量实验表明，USTN-DSC优于大多数现有方法，验证了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video anomaly detection (VAD) is a significant computer vision problem. Existing deep neural network (DNN) based VAD methods mostly follow the route of frame reconstruction or frame prediction. However, the lack of mining and learning of higher-level visual features and temporal context relationships in videos limits the further performance of these two approaches. Inspired by video codec theory, we introduce a brand-new VAD paradigm to break through these limitations: First, we propose a new task of video event restoration based on keyframes. Encouraging DNN to infer missing multiple frames based on video keyframes so as to restore a video event, which can more effectively motivate DNN to mine and learn potential higher-level visual features and comprehensive temporal context relationships in the video. To this end, we propose a novel U-shaped Swin Transformer Network with Dual Skip Connections (USTN-DSC) for video event restoration, where a cross-attention and a temporal upsampling residual skip connection are introduced to further assist in restoring complex static and dynamic motion object features in the video. In addition, we propose a simple and effective adjacent frame difference loss to constrain the motion consistency of the video sequence. Extensive experiments on benchmarks demonstrate that USTN-DSC outperforms most existing methods, validating the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">3.Query-Dependent Video Representation for Moment Retrieval and Highlight Detection</span><br>
                <span class="as">Moon, WonJunandHyun, SangeekandPark, SangUkandPark, DongchanandHeo, Jae-Pil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Moon_Query-Dependent_Video_Representation_for_Moment_Retrieval_and_Highlight_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23023-23033.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频瞬间检索和精彩片段检测（MR/HD）在视频理解需求急剧增加的背景下受到关注，其主要目标是对给定的文本查询进行时刻定位并估计片段级别的一致性水平，即显著性分数。<br>
                    动机：尽管最近的基于变压器的模型带来了一些进步，但我们发现这些方法并没有充分利用给定查询的信息。例如，在预测时刻和其显著性时，有时会忽视文本查询与视频内容之间的相关性。<br>
                    方法：我们引入了Query-Dependent DETR（QD-DETR），一种专为MR/HD设计的检测变压器。我们的编码模块从交叉注意力层开始，明确地将文本查询的上下文注入到视频表示中。然后，为了提高模型利用查询信息的能力，我们处理视频-查询对以产生无关对。这种负（无关）的视频-查询对被训练以产生低显著性分数，从而鼓励模型精确估计查询-视频对之间的一致性。最后，我们提出了一种输入自适应显著性预测器，该预测器为给定的视频-查询对自适应地定义显著性分数的标准。<br>
                    效果：我们广泛的研究表明，对于MR/HD来说，建立依赖于查询的表示是重要的。具体来说，QD-DETR在QVHighlights、TVSum和Charades-STA数据集上超越了最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, video moment retrieval and highlight detection (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation. Then, to enhance the model's capability of exploiting the query information, we manipulate the video-query pairs to produce irrelevant pairs. Such negative (irrelevant) video-query pairs are trained to yield low saliency scores, which in turn, encourages the model to estimate precise accordance between query-video pairs. Lastly, we present an input-adaptive saliency predictor which adaptively defines the criterion of saliency scores for the given video-query pairs. Our extensive studies verify the importance of building the query-dependent representation for MR/HD. Specifically, QD-DETR outperforms state-of-the-art methods on QVHighlights, TVSum, and Charades-STA datasets. Codes are available at github.com/wjun0830/QD-DETR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">4.Adaptive Global Decay Process for Event Cameras</span><br>
                <span class="as">Nunes, UrbanoMiguelandBenosman, RyadandIeng, Sio-Hoi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Adaptive_Global_Decay_Process_for_Event_Cameras_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9771-9780.png><br>
            
            <span class="tt"><span class="t0">研究问题：在几乎所有基于事件的视觉问题中，都需要选择最近的事件，这些事件被认为承载了最相关的信息内容。<br>
                    动机：现有的策略存在至少一个主要限制，因此提出了一种新的事件相机的衰减过程，该过程适应全局场景动态，并且其延迟在纳秒级别。<br>
                    方法：构建了一个自适应量来编码全局场景动态，称为事件活动。<br>
                    效果：该方法在多个基于事件的视觉问题和数据集上进行了评估，始终能提高相应基线方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In virtually all event-based vision problems, there is the need to select the most recent events, which are assumed to carry the most relevant information content. To achieve this, at least one of three main strategies is applied, namely: 1) constant temporal decay or fixed time window, 2) constant number of events, and 3) flow-based lifetime of events. However, these strategies suffer from at least one major limitation each. We instead propose a novel decay process for event cameras that adapts to the global scene dynamics and whose latency is in the order of nanoseconds. The main idea is to construct an adaptive quantity that encodes the global scene dynamics, denoted by event activity. The proposed method is evaluated in several event-based vision problems and datasets, consistently improving the corresponding baseline methods' performance. We thus believe it can have a significant widespread impact on event-based research. Code available: https://github.com/neuromorphic-paris/event_batch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">5.ScanDMM: A Deep Markov Model of Scanpath Prediction for 360deg Images</span><br>
                <span class="as">Sui, XiangjieandFang, YumingandZhu, HanweiandWang, ShiqiandWang, Zhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sui_ScanDMM_A_Deep_Markov_Model_of_Scanpath_Prediction_for_360deg_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6989-6999.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决360度图像的扫描路径预测问题，即如何根据人类的视觉感知机制产生动态的注视行为。<br>
                    动机：现有的360度图像扫描路径预测方法在预测人类扫描路径时没有完全考虑时间依赖性，导致性能较差和泛化能力不足。<br>
                    方法：本文提出了一种名为ScanDMM的新型深度马尔可夫模型架构来进行360度图像的扫描路径预测。我们设计了一个语义引导的转换函数来学习时间依赖的注意力景观的非线性动力学，并提出了一种状态初始化策略，通过考虑观察的起始点使模型能够以正确的"发射器"开始学习动态。<br>
                    效果：实验结果表明，我们的模型在四个360度图像数据库上达到了最先进的性能，并通过将扫描路径预测模型应用于其他视觉任务（如显著性检测和图像质量评估）展示了其泛化能力，期望为这些领域提供深刻的洞察。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scanpath prediction for 360deg images aims to produce dynamic gaze behaviors based on the human visual perception mechanism. Most existing scanpath prediction methods for 360deg images do not give a complete treatment of the time-dependency when predicting human scanpath, resulting in inferior performance and poor generalizability. In this paper, we present a scanpath prediction method for 360deg images by designing a novel Deep Markov Model (DMM) architecture, namely ScanDMM. We propose a semantics-guided transition function to learn the nonlinear dynamics of time-dependent attentional landscape. Moreover, a state initialization strategy is proposed by considering the starting point of viewing, enabling the model to learn the dynamics with the correct "launcher". We further demonstrate that our model achieves state-of-the-art performance on four 360deg image databases, and exhibit its generalizability by presenting two applications of applying scanpath prediction models to other visual tasks - saliency detection and image quality assessment, expecting to provide profound insights into these fields.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">6.A Light Weight Model for Active Speaker Detection</span><br>
                <span class="as">Liao, JunhuaandDuan, HaihanandFeng, KanghuiandZhao, WanbingandYang, YanbingandChen, Liangyin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_A_Light_Weight_Model_for_Active_Speaker_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22932-22941.png><br>
            
            <span class="tt"><span class="t0">研究问题：在音频-视觉场景中，如何有效地检测出正在说话的人。<br>
                    动机：现有的方法虽然能够提高性能，但需要大量的计算资源和内存，不适用于资源有限的环境。<br>
                    方法：通过减少输入候选者的数量，将2D和3D卷积分开用于音频-视觉特征提取，并应用低计算复杂度的门控循环单元进行跨模态建模，构建了一个轻量级的主动说话人检测架构。<br>
                    效果：实验结果表明，该方法在AVA-ActiveSpeaker数据集上取得了具有竞争力的mAP性能（94.1% vs. 94.2%），同时资源消耗显著低于现有方法，特别是在模型参数（1.0M vs. 22.5M, 大约23倍）和浮点运算次数（0.6G vs. 2.6G, 大约4倍）方面。此外，该方法在哥伦比亚数据集上也表现良好，显示出良好的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Active speaker detection is a challenging task in audio-visual scenarios, with the aim to detect who is speaking in one or more speaker scenarios. This task has received considerable attention because it is crucial in many applications. Existing studies have attempted to improve the performance by inputting multiple candidate information and designing complex models. Although these methods have achieved excellent performance, their high memory and computational power consumption render their application to resource-limited scenarios difficult. Therefore, in this study, a lightweight active speaker detection architecture is constructed by reducing the number of input candidates, splitting 2D and 3D convolutions for audio-visual feature extraction, and applying gated recurrent units with low computational complexity for cross-modal modeling. Experimental results on the AVA-ActiveSpeaker dataset reveal that the proposed framework achieves competitive mAP performance (94.1% vs. 94.2%), while the resource costs are significantly lower than the state-of-the-art method, particularly in model parameters (1.0M vs. 22.5M, approximately 23x) and FLOPs (0.6G vs. 2.6G, approximately 4x). Additionally, the proposed framework also performs well on the Columbia dataset, thus demonstrating good robustness. The code and model weights are available at https://github.com/Junhua-Liao/Light-ASD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">7.Self-Supervised Video Forensics by Audio-Visual Anomaly Detection</span><br>
                <span class="as">Feng, ChaoandChen, ZiyangandOwens, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10491-10503.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过异常检测识别视频中的视听信号不一致，并训练一个仅使用真实无标签数据的视频取证方法。<br>
                    动机：处理被篡改的视频，找出其视听信号的微妙不一致。<br>
                    方法：训练一个自回归模型生成视听特征序列，捕捉视频帧和声音之间的时间同步性。在测试时，标记模型分配概率低的视频。<br>
                    效果：尽管完全在真实视频上进行训练，但该模型在检测被篡改的语音视频任务上表现强劲。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Manipulated videos often contain subtle inconsistencies between their visual and audio signals. We propose a video forensics method, based on anomaly detection, that can identify these inconsistencies, and that can be trained solely using real, unlabeled data. We train an autoregressive model to generate sequences of audio-visual features, using feature sets that capture the temporal synchronization between video frames and sound. At test time, we then flag videos that the model assigns low probability. Despite being trained entirely on real videos, our model obtains strong performance on the task of detecting manipulated speech videos. Project site: https://cfeng16.github.io/audio-visual-forensics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">8.LOGO: A Long-Form Video Dataset for Group Action Quality Assessment</span><br>
                <span class="as">Zhang, ShiyiandDai, WenxunandWang, SujiaandShen, XiangweiandLu, JiwenandZhou, JieandTang, Yansong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LOGO_A_Long-Form_Video_Dataset_for_Group_Action_Quality_Assessment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2405-2414.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有动作质量评估（AQA）方法主要关注单人短序列场景，难以应对复杂情况的问题。<br>
                    动机：为了扩大AQA的应用范围，我们构建了一个名为LOGO的多人物长视频数据集，以应对更复杂的场景。<br>
                    方法：我们设计了一种简单而有效的方法来模拟运动员之间的关系，并推理长视频中的潜在时间逻辑。具体来说，我们设计了一个组感知注意力模块，可以很容易地插入到现有的AQA方法中，根据上下文群体信息丰富片段表示。<br>
                    效果：实验结果表明，我们的方法在LOGO数据集上取得了最先进的效果。同时，我们的数据集和代码将在GitHub上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Action quality assessment (AQA) has become an emerging topic since it can be extensively applied in numerous scenarios. However, most existing methods and datasets focus on single-person short-sequence scenes, hindering the application of AQA in more complex situations. To address this issue, we construct a new multi-person long-form video dataset for action quality assessment named LOGO. Distinguished in scenario complexity, our dataset contains 200 videos from 26 artistic swimming events with 8 athletes in each sample along with an average duration of 204.2 seconds. As for richness in annotations, LOGO includes formation labels to depict group information of multiple athletes and detailed annotations on action procedures. Furthermore, we propose a simple yet effective method to model relations among athletes and reason about the potential temporal logic in long-form videos. Specifically, we design a group-aware attention module, which can be easily plugged into existing AQA methods, to enrich the clip-wise representations based on contextual group information. To benchmark LOGO, we systematically conduct investigations on the performance of several popular methods in AQA and action segmentation. The results reveal the challenges our dataset brings. Extensive experiments also show that our approach achieves state-of-the-art on the LOGO dataset. The dataset and code will be released at https://github.com/shiyi-zh0408/LOGO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">9.Learning To Detect Mirrors From Videos via Dual Correspondences</span><br>
                <span class="as">Lin, JiayingandTan, XinandLau, RynsonW.H.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Learning_To_Detect_Mirrors_From_Videos_via_Dual_Correspondences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9109-9118.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从动态场景中检测镜子，由于缺乏高质量的数据集和有效的方法，视频镜子检测（VMD）仍然是一个未充分探索的领域。<br>
                    动机：作者观察到镜子内外的内容之间通常存在对应关系，但这些对应关系可能不会在每一帧都出现，例如由于摄像机姿态的变化。这启发了作者提出一个能够容忍空间上缺失对应关系的视频镜子检测方法。<br>
                    方法：作者提出了一种名为VMD-Net的视频镜子检测方法，该方法通过一个双对应模块在帧内和帧间考虑镜像对应关系，以寻找相关联的对应关系。此外，作者还提出了第一个大规模的VMD数据集（名为VMD-D），包含来自269个视频的14,987个图像帧和相应的手动标注掩码。<br>
                    效果：实验结果表明，该方法优于相关领域的最新技术。为了实现实时VMD，该方法有效地利用了骨干特征，消除了现有方法中常用的多级模块设计和输出映射后处理，使其非常高效且适用于实时视频应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detecting mirrors from static images has received significant research interest recently. However, detecting mirrors over dynamic scenes is still under-explored due to the lack of a high-quality dataset and an effective method for video mirror detection (VMD). To the best of our knowledge, this is the first work to address the VMD problem from a deep-learning-based perspective. Our observation is that there are often correspondences between the contents inside (reflected) and outside (real) of a mirror, but such correspondences may not always appear in every frame, e.g., due to the change of camera pose. This inspires us to propose a video mirror detection method, named VMD-Net, that can tolerate spatially missing correspondences by considering the mirror correspondences at both the intra-frame level as well as inter-frame level via a dual correspondence module that looks over multiple frames spatially and temporally for correlating correspondences. We further propose a first large-scale dataset for VMD (named VMD-D), which contains 14,987 image frames from 269 videos with corresponding manually annotated masks. Experimental results show that the proposed method outperforms SOTA methods from relevant fields. To enable real-time VMD, our method efficiently utilizes the backbone features by removing the redundant multi-level module design and gets rid of post-processing of the output maps commonly used in existing methods, making it very efficient and practical for real-time video-based applications. Code, dataset, and models are available at https://jiaying.link/cvpr2023-vmd/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">10.Towards Scalable Neural Representation for Diverse Videos</span><br>
                <span class="as">He, BoandYang, XitongandWang, HanyuandWu, ZuxuanandChen, HaoandHuang, ShuaiyiandRen, YixuanandLim, Ser-NamandShrivastava, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Towards_Scalable_Neural_Representation_for_Diverse_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6132-6142.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对大量多样化的视频进行编码。<br>
                    动机：现有的隐式神经表示（INR）方法在处理少量重复视频时表现良好，但在处理大量多样化视频时存在局限性。<br>
                    方法：提出了一种新的神经网络表示框架D-NeRV，通过将视频分解为运动信息和视觉内容，引入时间推理，并使用任务导向流作为中间输出来减少空间冗余。<br>
                    效果：实验结果表明，D-NeRV在视频压缩任务上大大超过了NeRV和传统的视频压缩技术，同时在相同的压缩比下，D-NeRV在动作识别任务上的准确率也比NeRV高3%-10%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representations (INR) have gained increasing attention in representing 3D scenes and images, and have been recently applied to encode videos (e.g., NeRV, E-NeRV). While achieving promising results, existing INR-based methods are limited to encoding a handful of short videos (e.g., seven 5-second videos in the UVG dataset) with redundant visual content, leading to a model design that fits individual video frames independently and is not efficiently scalable to a large number of diverse videos. This paper focuses on developing neural representations for a more practical setup -- encoding long and/or a large number of videos with diverse visual content. We first show that instead of dividing videos into small subsets and encoding them with separate models, encoding long and diverse videos jointly with a unified model achieves better compression results. Based on this observation, we propose D-NeRV, a novel neural representation framework designed to encode diverse videos by (i) decoupling clip-specific visual content from motion information, (ii) introducing temporal reasoning into the implicit neural network, and (iii) employing the task-oriented flow as intermediate output to reduce spatial redundancies. Our new model largely surpasses NeRV and traditional video compression techniques on UCF101 and UVG datasets on the video compression task. Moreover, when used as an efficient data-loader, D-NeRV achieves 3%-10% higher accuracy than NeRV on action recognition tasks on the UCF101 dataset under the same compression ratios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">11.Language-Guided Audio-Visual Source Separation via Trimodal Consistency</span><br>
                <span class="as">Tan, ReubenandRay, ArijitandBurns, AndreaandPlummer, BryanA.andSalamon, JustinandNieto, OriolandRussell, BryanandSaenko, Kate</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Language-Guided_Audio-Visual_Source_Separation_via_Trimodal_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10575-10584.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种基于自然语言查询的自监督学习方法，用于学习在视频中执行音频源分离。<br>
                    动机：该任务的主要挑战在于学习将发出声音的对象的语言描述与其视觉特征和相应的音频波形成分关联起来，并且在训练过程中无法访问注释信息。<br>
                    方法：通过两种新的损失函数，使现成的视觉-语言基础模型提供伪目标监督，并鼓励音频、视觉和自然语言模态之间更强的对齐。<br>
                    效果：在MUSIC、SOLOS和AudioSet三个音频-视觉分离数据集上，该方法的效果超过了最先进的有监督方法，尽管在训练过程中没有使用对象检测器或文本标签。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Finally, we also include samples of our separated audios in the supplemental for reference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">12.CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition With Variational Alignment</span><br>
                <span class="as">Zheng, JiangbinandWang, YileandTan, ChengandLi, SiyuanandWang, GeandXia, JunandChen, YidongandLi, StanZ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23141-23150.png><br>
            
            <span class="tt"><span class="t0">研究问题：手语识别（SLR）是一个弱监督任务，需要将手语视频标注为文本解释。由于缺乏大规模可用的手语数据集，训练不足成为SLR的主要瓶颈。<br>
                    动机：目前的手语识别工作主要采用预训练的视觉模块，并发展出两种主流解决方案：多流架构和单流架构。多流架构扩展了多线索视觉特征，取得了当前最先进的性能，但设计复杂且可能引入噪音。相比之下，使用视觉和文本模态之间显式跨模态对齐的高级单流SLR框架简单有效，可能与多流框架竞争。<br>
                    方法：我们提出了一种新的对比性视觉-文本转换模型CVT-SLR，以充分探索视觉和语言模态的预训练知识。基于单流跨模态对齐框架，我们提出了一种变分自编码器（VAE）用于预训练上下文知识，同时引入完整的预训练语言模块。VAE隐式地对齐视觉和文本模态，同时从预训练的上下文知识中受益，就像传统的上下文模块一样。同时，设计了一种对比性跨模态对齐算法，以明确增强一致性约束。<br>
                    效果：我们在公共数据集（PHOENIX-2014和PHOENIX-2014T）上进行了大量的实验，结果表明我们的CVT-SLR模型始终优于现有的单流方法，甚至超过了最先进的多流方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign datasets becomes the main bottleneck for SLR. Most SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variational autoencoder (VAE) for pretrained contextual knowledge while introducing the complete pretrained language module. The VAE implicitly aligns visual and textual modalities while benefiting from pretrained contextual knowledge as the traditional contextual module. Meanwhile, a contrastive cross-modal alignment algorithm is designed to explicitly enhance the consistency constraints. Extensive experiments on public datasets (PHOENIX-2014 and PHOENIX-2014T) demonstrate that our proposed CVT-SLR consistently outperforms existing single-cue methods and even outperforms SOTA multi-cue methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">13.Ego-Body Pose Estimation via Ego-Head Pose Estimation</span><br>
                <span class="as">Li, JiamanandLiu, KarenandWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17142-17151.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从第一人称视角的视频序列中估计3D人体运动，以理解人类行为并在VR/AR中应用。<br>
                    动机：由于用户的身体通常被放置在用户头部的前向相机所遮挡，直接学习第一人称视频和人体运动之间的映射是具有挑战性的。此外，收集大规模高质量的配对第一人称视频和3D人体运动数据集需要精确的运动捕捉设备，这往往限制了视频场景的多样性，使其仅限于类似实验室的环境。<br>
                    方法：我们提出了一种新的方法，名为Ego-Body Pose Estimation via Ego-Head Pose Estimation（EgoEgo），该方法将问题分解为两个阶段，并通过中间表示的头部运动进行连接。首先，EgoEgo通过集成SLAM和学习方法来估计准确的头部运动。然后，利用估计的头部姿态作为输入，EgoEgo使用条件扩散生成多个可能的全身运动。这种头部和身体姿态的解耦消除了训练数据集需要配对的第一人称视频和3D人体运动的需求，使我们能够分别利用大规模的第一人称视频数据集和运动捕捉数据集。<br>
                    效果：我们在ARES和真实数据上进行的系统基准测试表明，我们的EgoEgo模型在这两种数据集上都显著优于当前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Estimating 3D human motion from an egocentric video sequence plays a critical role in human behavior understanding and has various applications in VR/AR. However, naively learning a mapping between egocentric videos and human motions is challenging, because the user's body is often unobserved by the front-facing camera placed on the head of the user. In addition, collecting large-scale, high-quality datasets with paired egocentric videos and 3D human motions requires accurate motion capture devices, which often limit the variety of scenes in the videos to lab-like environments. To eliminate the need for paired egocentric video and human motions, we propose a new method, Ego-Body Pose Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem into two stages, connected by the head motion as an intermediate representation. EgoEgo first integrates SLAM and a learning approach to estimate accurate head motion. Subsequently, leveraging the estimated head pose as input, EgoEgo utilizes conditional diffusion to generate multiple plausible full-body motions. This disentanglement of head and body pose eliminates the need for training datasets with paired egocentric videos and 3D human motion, enabling us to leverage large-scale egocentric video datasets and motion capture datasets separately. Moreover, for systematic benchmarking, we develop a synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric videos and human motion. On both ARES and real data, our EgoEgo model performs significantly better than the current state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">14.Hierarchical Video-Moment Retrieval and Step-Captioning</span><br>
                <span class="as">Zala, AbhayandCho, JaeminandKottur, SatwikandChen, XilunandOguz, BarlasandMehdad, YasharandBansal, Mohit</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zala_Hierarchical_Video-Moment_Retrieval_and_Step-Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23056-23065.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从大规模视频语料库中进行信息搜索的问题，以及如何联合搜索视频语料库并生成摘要。<br>
                    动机：目前的研究大多将文本视频检索、时刻检索、视频摘要和视频字幕等任务分开研究，缺乏一个可以从视频语料库中进行联合搜索和生成摘要的端到端设置。<br>
                    方法：作者提出了HiREST（分层检索和步进字幕）数据集，并设计了一个新的基准测试，该测试覆盖了从教学视频语料库中的分层信息检索和视觉/文本逐步总结的任务。<br>
                    效果：实验结果表明，虽然基线模型显示出一些有希望的结果，但仍有很大的改进空间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of each moment into key instruction steps with caption and timestamps (totaling 8.6K step captions). Our hierarchical benchmark consists of video retrieval, moment retrieval, and two novel moment segmentation and step captioning tasks. In moment segmentation, models break down a video moment into instruction steps and identify start-end boundaries. In step captioning, models generate a textual summary for each step. We also present starting point task-specific and end-to-end joint baseline models for our new benchmark. While the baseline models show some promising results, there still exists large room for future improvement by the community.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">15.EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction</span><br>
                <span class="as">Erbach, JuliusandTulyakov, StepanandVitoria, PatriciaandBochicchio, AlfredoandLi, Yuanyou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Erbach_EvShutter_Transforming_Events_for_Unconstrained_Rolling_Shutter_Correction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13904-13913.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单张RGB图像和高时间分辨率的事件信息，对具有运动模糊和RS失真的图像进行去扭曲处理。<br>
                    动机：现有的基于恒定速度假设的RS失真校正算法需要多帧图像来预测密集位移场，而新提出的Eventful Shutter（EvShutter）方法通过使用事件信息和单张RGB图像，可以在不依赖恒定速度假设的情况下进行校正。<br>
                    方法：EvShutter首先使用新颖的基于流的去模糊模块去除模糊，然后使用双编码器hourglass网络进行RS补偿。与以往的方法不同，它不依赖于恒定速度假设，并使用简单的架构，这得益于一种专门针对RS的称为Filter and Flip（FnF）的事件转换，该转换将输入事件编码为仅编码GS和RS图像之间的变化。<br>
                    效果：在第一个包含真实事件和高质量可选模糊的RS-ERGB数据集上进行的评估表明，该方法在峰值信噪比（PSNR）方面比最先进的图像和事件基方法分别提高了9.16 dB和0.75 dB，在LPIPS方面提高了23%和21%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Widely used Rolling Shutter (RS) CMOS sensors capture high resolution images at the expense of introducing distortions and artifacts in the presence of motion. In such situations, RS distortion correction algorithms are critical. Recent methods rely on a constant velocity assumption and require multiple frames to predict the dense displacement field. In this work, we introduce a new method, called Eventful Shutter (EvShutter), that corrects RS artifacts using a single RGB image and event information with high temporal resolution. The method firstly removes blur using a novel flow-based deblurring module and then compensates RS using a double encoder hourglass network. In contrast to previous methods, it does not rely on a constant velocity assumption and uses a simple architecture thanks to an event transformation dedicated to RS, called Filter and Flip (FnF), that transforms input events to encode only the changes between GS and RS images. To evaluate the proposed method and facilitate future research, we collect the first dataset with real events and high-quality RS images with optional blur, called RS-ERGB. We generate the RS images from GS images using a newly proposed simulator based on adaptive interpolation. The simulator permits the use of inexpensive cameras with long exposure to capture high-quality GS images. We show that on this realistic dataset the proposed method outperforms the state-of-the-art image- and event-based methods by 9.16 dB and 0.75 dB respectively in terms of PSNR and an improvement of 23% and 21% in LPIPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">16.Hierarchical Neural Memory Network for Low Latency Event Processing</span><br>
                <span class="as">Hamaguchi, RyuheiandFurukawa, YasutakaandOnishi, MasakiandSakurada, Ken</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hamaguchi_Hierarchical_Neural_Memory_Network_for_Low_Latency_Event_Processing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22867-22876.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种低延迟神经网络架构，用于基于事件的密集预测任务。<br>
                    动机：传统的架构会以固定的速率编码整个场景内容，而不考虑其时间特性。<br>
                    方法：通过构建不同速率的堆叠潜在记忆来创建时间层次结构，实现根据运动速度在适当的时间尺度上编码内容。<br>
                    效果：该架构不仅减少了传统架构的冗余，而且利用了长期依赖性。在三个基于事件的密集预测任务上进行广泛评估，该方法在准确性和延迟方面优于现有方法，同时展示了有效的事件和图像融合能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">17.Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video</span><br>
                <span class="as">Feng, RunyangandGao, YixingandMa, XueqingandTse, TzeHoEldenandChang, HyungJin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Mutual_Information-Based_Temporal_Difference_Learning_for_Human_Pose_Estimation_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17131-17141.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行多帧人体姿态估计。<br>
                    动机：现有的方法直接使用光流或可变形卷积预测全谱运动场，可能会引入许多无关的线索，如附近的人或背景，导致结果不理想。<br>
                    方法：本文提出了一种新的多帧人体姿态估计框架，利用时间差编码动态上下文，并客观地参与互信息目标以促进有用的运动信息解耦。具体来说，设计了一个多阶段的时序差编码器，通过多阶段特征差序列的条件增量级联学习来获取有意义的运动表示。进一步从互信息的角度提出了一个表示解耦模块，通过明确定义原始运动特征的有用和噪声成分并最小化它们的互信息，可以抓取区分性的任务相关运动信号。<br>
                    效果：在复杂事件挑战中的Crowd Pose Estimation任务中排名第一，并在三个基准测试集PoseTrack2017、PoseTrack2018和PoseTrack21上实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal modeling is crucial for multi-frame human pose estimation. Most existing methods directly employ optical flow or deformable convolution to predict full-spectrum motion fields, which might incur numerous irrelevant cues, such as a nearby person or background. Without further efforts to excavate meaningful motion priors, their results are suboptimal, especially in complicated spatiotemporal interactions. On the other hand, the temporal difference has the ability to encode representative motion information which can potentially be valuable for pose estimation but has not been fully exploited. In this paper, we present a novel multi-frame human pose estimation framework, which employs temporal differences across frames to model dynamic contexts and engages mutual information objectively to facilitate useful motion information disentanglement. To be specific, we design a multi-stage Temporal Difference Encoder that performs incremental cascaded learning conditioned on multi-stage feature difference sequences to derive informative motion representation. We further propose a Representation Disentanglement module from the mutual information perspective, which can grasp discriminative task-relevant motion signals by explicitly defining useful and noisy constituents of the raw motion features and minimizing their mutual information. These place us to rank No.1 in the Crowd Pose Estimation in Complex Events Challenge on benchmark dataset HiEve, and achieve state-of-the-art performance on three benchmarks PoseTrack2017, PoseTrack2018, and PoseTrack21.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">18.SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision</span><br>
                <span class="as">Liu, XuboandLakomkin, EgorandVougioukas, KonstantinosandMa, PingchuanandChen, HonglieandXie, RuimingandDoulaty, MorrieandMoritz, NikoandKolar, JachymandPetridis, StavrosandPantic, MajaandFuegen, Christian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SynthVSR_Scaling_Up_Visual_Speech_Recognition_With_Synthetic_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18806-18815.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用合成视觉数据提升视觉语音识别（VSR）的性能。<br>
                    动机：公开的转录视频数据集规模有限，而最新的VSR技术结果却需要大量的视频数据。<br>
                    方法：首次提出一种名为SynthVSR的方法，通过使用基于语音驱动的唇部动画模型生成唇部运动来改善VSR系统的性能。该模型在未标记的视听数据集上进行训练，并在有标签视频可用时进一步优化预训练的VSR模型。<br>
                    效果：在最大的公共VSR基准测试集——Lip Reading Sentences 3（LRS3）上评估，SynthVSR仅使用30小时的真实标记数据就实现了43.3%的WER，优于使用数千小时视频的现成方法。当使用LRS3中的所有438小时标记数据时，WER进一步降低到27.9%，与最先进的自我监督AV-HuBERT方法相当。此外，当与大规模的伪标记视听数据结合使用时，SynthVSR仅使用公开可用的数据就实现了16.9%的新VSR WER，超过了最近使用非公开机器转录视频数据（90,000小时）训练的最新方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently reported state-of-the-art results in visual speech recognition (VSR) often rely on increasingly large amounts of video data, while the publicly available transcribed video datasets are limited in size. In this paper, for the first time, we study the potential of leveraging synthetic visual data for VSR. Our method, termed SynthVSR, substantially improves the performance of VSR systems with synthetic lip movements. The key idea behind SynthVSR is to leverage a speech-driven lip animation model that generates lip movements conditioned on the input speech. The speech-driven lip animation model is trained on an unlabeled audio-visual dataset and could be further optimized towards a pre-trained VSR model when labeled videos are available. As plenty of transcribed acoustic data and face images are available, we are able to generate large-scale synthetic data using the proposed lip animation model for semi-supervised VSR training. We evaluate the performance of our approach on the largest public VSR benchmark - Lip Reading Sentences 3 (LRS3). SynthVSR achieves a WER of 43.3% with only 30 hours of real labeled data, outperforming off-the-shelf approaches using thousands of hours of video. The WER is further reduced to 27.9% when using all 438 hours of labeled data from LRS3, which is on par with the state-of-the-art self-supervised AV-HuBERT method. Furthermore, when combined with large-scale pseudo-labeled audio-visual data SynthVSR yields a new state-of-the-art VSR WER of 16.9% using publicly available data only, surpassing the recent state-of-the-art approaches trained with 29 times more non-public machine-transcribed video data (90,000 hours). Finally, we perform extensive ablation studies to understand the effect of each component in our proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">19.Search-Map-Search: A Frame Selection Paradigm for Action Recognition</span><br>
                <span class="as">Zhao, MingjunandYu, YakunandWang, XiaoliandYang, LeiandNiu, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Search-Map-Search_A_Frame_Selection_Paradigm_for_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10627-10636.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管深度学习在视频理解任务中取得了成功，但处理视频的每一帧在计算上是昂贵的，并且在实时应用中通常是不必要的。<br>
                    动机：现有的帧选择方法要么根据每帧的重要性预测单独采样帧，没有考虑帧之间的交互，要么采用强化学习代理来连续找到代表性的帧，这既昂贵又可能导致潜在的稳定性问题。<br>
                    方法：我们提出了一种搜索-映射-搜索的学习范式，该范式结合了启发式搜索和监督学习的优点，从视频中选择最佳的帧组合作为一个实体。通过将搜索与学习相结合，所提出的方法可以更好地捕获帧之间的交互，同时产生低推理开销。<br>
                    效果：大量的实验表明，我们的帧选择方法有效地提高了动作识别模型的性能，并显著优于许多竞争性基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the success of deep learning in video understanding tasks, processing every frame in a video is computationally expensive and often unnecessary in real-time applications. Frame selection aims to extract the most informative and representative frames to help a model better understand video content. Existing frame selection methods either individually sample frames based on per-frame importance prediction, without considering interaction among frames, or adopt reinforcement learning agents to find representative frames in succession, which are costly to train and may lead to potential stability issues. To overcome the limitations of existing methods, we propose a Search-Map-Search learning paradigm which combines the advantages of heuristic search and supervised learning to select the best combination of frames from a video as one entity. By combining search with learning, the proposed method can better capture frame interactions while incurring a low inference overhead. Specifically, we first propose a hierarchical search method conducted on each training video to search for the optimal combination of frames with the lowest error on the downstream task. A feature mapping function is then learned to map the frames of a video to the representation of its target optimal frame combination. During inference, another search is performed on an unseen video to select a combination of frames whose feature representation is close to the projected feature representation. Extensive experiments based on several action recognition benchmarks demonstrate that our frame selection method effectively improves performance of action recognition models, and significantly outperforms a number of competitive baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">20.Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction</span><br>
                <span class="as">Xu, YiandBazarjani, ArminandChi, Hyung-gunandChoi, ChihoandFu, Yun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Uncovering_the_Missing_Pattern_Unified_Framework_Towards_Trajectory_Imputation_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9632-9643.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前轨迹预测方法常假设观察序列完整，忽视了由于物体遮挡、范围限制、传感器故障等原因导致的缺失值，这限制了轨迹预测的准确性。<br>
                    动机：为了解决这一问题，本文提出了一种统一的框架——基于图的条件变分循环神经网络（GC-VRNN），可以同时进行轨迹填充和预测。<br>
                    方法：我们引入了一种新颖的多空间图神经网络（MS-GNN），可以从不完整的观察中提取空间特征并利用缺失模式。此外，我们还采用了带有特定设计的时态衰减（TD）模块的条件变分循环神经网络来捕捉不完整轨迹中的时序依赖性和时序缺失模式。<br>
                    效果：通过广泛的实验验证了我们提出的方法的卓越性能。据我们所知，这是首次以统一的方式解决轨迹填充和预测问题的缺乏基准和技巧的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Trajectory prediction is a crucial undertaking in understanding entity movement or human behavior from observed sequences. However, current methods often assume that the observed sequences are complete while ignoring the potential for missing values caused by object occlusion, scope limitation, sensor failure, etc. This limitation inevitably hinders the accuracy of trajectory prediction. To address this issue, our paper presents a unified framework, the Graph-based Conditional Variational Recurrent Neural Network (GC-VRNN), which can perform trajectory imputation and prediction simultaneously. Specifically, we introduce a novel Multi-Space Graph Neural Network (MS-GNN) that can extract spatial features from incomplete observations and leverage missing patterns. Additionally, we employ a Conditional VRNN with a specifically designed Temporal Decay (TD) module to capture temporal dependencies and temporal missing patterns in incomplete trajectories. The inclusion of the TD module allows for valuable information to be conveyed through the temporal flow. We also curate and benchmark three practical datasets for the joint problem of trajectory imputation and prediction. Extensive experiments verify the exceptional performance of our proposed method. As far as we know, this is the first work to address the lack of benchmarks and techniques for trajectory imputation and prediction in a unified manner.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">21.3D Video Loops From Asynchronous Input</span><br>
                <span class="as">Ma, LiandLi, XiaoyuandLiao, JingandSander, PedroV.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_3D_Video_Loops_From_Asynchronous_Input_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/310-320.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现动态3D循环场景的沉浸式体验。<br>
                    动机：现有的方法大多局限于2D表示，我们提出一种新颖的方法来处理异步输入的每个视图的循环条件，同时保持3D表示的视图一致性。<br>
                    方法：我们提出了一种新的稀疏3D视频表示方法，即多图视频（MTV），它不仅提供了一致的视图先验，而且大大减少了内存使用，使得4D体积的优化变得可行。然后，我们引入了一个两阶段管道，从完全异步的多视图视频中构建3D循环MTV，这些视频没有时间重叠。在优化过程中，我们采用了基于视频时间重定向算法的新型循环损失来循环3D场景。<br>
                    效果：我们的框架实验已显示出在实时生成和渲染逼真的3D循环视频方面具有潜力，即使在移动设备上也能实现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Looping videos are short video clips that can be looped endlessly without visible seams or artifacts. They provide a very attractive way to capture the dynamism of natural scenes. Existing methods have been mostly limited to 2D representations. In this paper, we take a step forward and propose a practical solution that enables an immersive experience on dynamic 3D looping scenes. The key challenge is to consider the per-view looping conditions from asynchronous input while maintaining view consistency for the 3D representation. We propose a novel sparse 3D video representation, namely Multi-Tile Video (MTV), which not only provides a view-consistent prior, but also greatly reduces memory usage, making the optimization of a 4D volume tractable. Then, we introduce a two-stage pipeline to construct the 3D looping MTV from completely asynchronous multi-view videos with no time overlap. A novel looping loss based on video temporal retargeting algorithms is adopted during the optimization to loop the 3D scene. Experiments of our framework have shown promise in successfully generating and rendering photorealistic 3D looping videos in real time even on mobile devices. The code, dataset, and live demos are available in https://limacv.github.io/VideoLoop3D_web/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">22.Frame Interpolation Transformer and Uncertainty Guidance</span><br>
                <span class="as">Plack, MarkusandBriedis, KarlisMartinsandDjelouah, AbdelazizandHullin, MatthiasB.andGross, MarkusandSchroers, Christopher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Plack_Frame_Interpolation_Transformer_and_Uncertainty_Guidance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9811-9821.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，视频帧插值技术取得了重要进展，但在复杂光照或大运动等更具挑战性的条件下仍存在问题。<br>
                    动机：为了解决这些问题，我们提出了一种新的基于变换器的插值网络架构，能够估计预期误差和插值帧。<br>
                    方法：我们的方法通过直接预测或转换器来探索替代方案，并利用改进的光学流方法和改善的喷射策略或来自深度的额外线索。<br>
                    效果：实验结果表明，我们的方法在多个数据集上提高了视觉质量，并通过用户研究进一步证明了质量的提升。此外，我们的方法还能估计插值帧的错误图，这对于标记有问题的帧的长视频序列的实际应用至关重要。最后，对于渲染内容，我们可以使用预计算的错误指导中间帧的部分渲染过程来生成质量更高的新帧。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video frame interpolation has seen important progress in recent years, thanks to developments in several directions. Some works leverage better optical flow methods with improved splatting strategies or additional cues from depth, while others have investigated alternative approaches through direct predictions or transformers. Still, the problem remains unsolved in more challenging conditions such as complex lighting or large motion. In this work, we are bridging the gap towards video production with a novel transformer-based interpolation network architecture capable of estimating the expected error together with the interpolated frame. This offers several advantages that are of key importance for frame interpolation usage: First, we obtained improved visual quality over several datasets. The improvement in terms of quality is also clearly demonstrated through a user study. Second, our method estimates error maps for the interpolated frame, which are essential for real-life applications on longer video sequences where problematic frames need to be flagged. Finally, for rendered content a partial rendering pass of the intermediate frame, guided by the predicted error, can be utilized during the interpolation to generate a new frame of superior quality. Through this error estimation, our method can produce even higher-quality intermediate frames using only a fraction of the time compared to a full rendering.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">23.QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation</span><br>
                <span class="as">Yang, SichengandWu, ZhiyongandLi, MingleiandZhang, ZhensongandHao, LeiandBao, WeihongandZhuang, Haolin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_QPGesture_Quantization-Based_and_Phase-Guided_Motion_Matching_for_Natural_Speech-Driven_Gesture_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2321-2330.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地解决由人类运动随机抖动和语音与手势之间固有的异步关系引起的语音驱动手势生成的挑战。<br>
                    动机：为了解决这些挑战，我们提出了一种新颖的基于量化和相位引导的运动匹配框架。<br>
                    方法：我们首先提出了一个手势VQ-VAE模块，用于学习一个码本以总结有意义的手势单元。然后，我们使用Levenshtein距离来对齐不同的手势和语音。此外，我们还引入了相位，以根据音频的上下文语义或节奏指导最优的手势匹配。<br>
                    效果：广泛的实验表明，我们的方法在语音驱动的手势生成方面优于最近的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Speech-driven gesture generation is highly challenging due to the random jitters of human motion. In addition, there is an inherent asynchronous relationship between human speech and gestures. To tackle these challenges, we introduce a novel quantization-based and phase-guided motion matching framework. Specifically, we first present a gesture VQ-VAE module to learn a codebook to summarize meaningful gesture units. With each code representing a unique gesture, random jittering problems are alleviated effectively. We then use Levenshtein distance to align diverse gestures with different speech. Levenshtein distance based on audio quantization as a similarity metric of corresponding speech of gestures helps match more appropriate gestures with speech, and solves the alignment problem of speech and gestures well. Moreover, we introduce phase to guide the optimal gesture matching based on the semantics of context or rhythm of audio. Phase guides when text-based or speech-based gestures should be performed to make the generated gestures more natural. Extensive experiments show that our method outperforms recent approaches on speech-driven gesture generation. Our code, database, pre-trained models and demos are available at https://github.com/YoungSeng/QPGesture.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">24.On the Benefits of 3D Pose and Tracking for Human Action Recognition</span><br>
                <span class="as">Rajasegaran, JathushanandPavlakos, GeorgiosandKanazawa, AngjooandFeichtenhofer, ChristophandMalik, Jitendra</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rajasegaran_On_the_Benefits_of_3D_Pose_and_Tracking_for_Human_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/640-649.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究使用追踪和3D姿态进行动作识别的好处。<br>
                    动机：通过在人类运动轨迹上分析动作，而非在空间的固定点上，可以更好地预测人们的动作。<br>
                    方法：首先，我们展示了使用3D姿态推断动作的好处，并研究了人与人之间的互动。然后，我们提出了一个拉格朗日动作识别模型，该模型通过融合3D姿态和上下文化的外观信息来识别动作。<br>
                    效果：在AVA v2.2数据集上，我们的方法在仅使用姿态设置和标准基准设置上都取得了最先进的性能。仅使用姿态线索推理动作时，我们的姿态模型比相应的最新技术提高了+10.0 mAP，而我们的融合模型比最佳最新技术提高了+2.8 mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work we study the benefits of using tracking and 3D poses for action recognition. To achieve this, we take the Lagrangian view on analysing actions over a trajectory of human motion rather than at a fixed point in space. Taking this stand allows us to use the tracklets of people to predict their actions. In this spirit, first we show the benefits of using 3D pose to infer actions, and study person-person interactions. Subsequently, we propose a Lagrangian Action Recognition model by fusing 3D pose and contextualized appearance over tracklets. To this end, our method achieves state-of-the-art performance on the AVA v2.2 dataset on both pose only settings and on standard benchmark settings. When reasoning about the action using only pose cues, our pose model achieves +10.0 mAP gain over the corresponding state-of-the-art while our fused model has a gain of +2.8 mAP over the best state-of-the-art model. Code and results are available at: https://brjathu.github.io/LART</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">25.Continuous Sign Language Recognition With Correlation Network</span><br>
                <span class="as">Hu, LianyuandGao, LiqingandLiu, ZekangandFeng, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2529-2539.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前连续手语识别（CSLR）方法通常独立处理帧以捕获帧特征，无法有效识别手势。<br>
                    动机：人体轨迹是视频中识别动作的重要线索，主要通过连续帧中的手和脸来传达。<br>
                    方法：提出相关性网络（CorrNet），明确利用跨帧的人体轨迹进行手势识别。具体包括强调每帧中表达手势的有益信息的识别模块，以及动态计算当前帧与相邻邻居之间的相关性图以捕捉跨帧轨迹的关联模块。<br>
                    效果：由于对体轨的关注，CorrNet在四个大规模数据集PHOENIX14、PHOENIX14-T、CSL-Daily和CSL上实现了新的最先进准确性。与先前的空间-时间推理方法的全面比较验证了其有效性。可视化展示了CorrNet强调相邻帧之间人体轨迹的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human body trajectories are a salient cue to identify actions in video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition(CSLR) usually process frames independently to capture frame-wise features, thus failing to capture cross-frame trajectories to effectively identify a sign. To handle this limitation, we propose correlation network (CorrNet) to explicitly leverage body trajectories across frames to identify signs. In specific, an identification module is first presented to emphasize informative regions in each frame that are beneficial in expressing a sign. A correlation module is then proposed to dynamically compute correlation maps between current frame and adjacent neighbors to capture cross-frame trajectories. As a result, the generated features are able to gain an overview of local temporal movements to identify a sign. Thanks to its special attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A comprehensive comparison between CorrNet and previous spatial-temporal reasoning methods verifies its effectiveness. Visualizations are given to demonstrate the effects of CorrNet on emphasizing human body trajectories across adjacent frames.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">26.An Actor-Centric Causality Graph for Asynchronous Temporal Inference in Group Activity</span><br>
                <span class="as">Xie, ZhaoandGao, TianandWu, KeweiandChang, Jiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_An_Actor-Centric_Causality_Graph_for_Asynchronous_Temporal_Inference_in_Group_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6652-6661.png><br>
            
            <span class="tt"><span class="t0">研究问题：组活动识别中因果关系建模仍然是一个挑战。<br>
                    动机：现有的图模型主要关注学习演员关系的同步时序特征，这对于处理具有异步时序特征的因果关系来说是不够的。<br>
                    方法：本文提出了一种以演员为中心的因果关系图模型，该模型通过三个模块学习异步时序的因果关系，即异步时序因果关系检测模块、因果关系特征融合模块和因果关系关系图推理模块。<br>
                    效果：大量实验表明，该方法在排球数据集和集体活动数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The causality relation modeling remains a challenging task for group activity recognition. The causality relations describe the influence of some actors (cause actors) on other actors (effect actors). Most existing graph models focus on learning the actor relation with synchronous temporal features, which is insufficient to deal with the causality relation with asynchronous temporal features. In this paper, we propose an Actor-Centric Causality Graph Model, which learns the asynchronous temporal causality relation with three modules, i.e., an asynchronous temporal causality relation detection module, a causality feature fusion module, and a causality relation graph inference module. First, given a centric actor and correlative actor, we analyze their influences to detect causality relation. We estimate the self influence of the centric actor with self regression. We estimate the correlative influence from the correlative actor to the centric actor with correlative regression, which uses asynchronous features at different timestamps. Second, we synchronize the two action features by estimating the temporal delay between the cause action and the effect action. The synchronized features are used to enhance the feature of the effect action with a channel-wise fusion. Third, we describe the nodes (actors) with causality features and learn the edges by fusing the causality relation with the appearance relation and distance relation. The causality relation graph inference provides crucial features of effect action, which are complementary to the base model using synchronous relation inference. The two relation inferences are aggregated to enhance group relation learning. Extensive experiments show that our method achieves state-of-the-art performance on the Volleyball dataset and Collective Activity dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">27.How You Feelin&#x27;? Learning Emotions and Mental States in Movie Scenes</span><br>
                <span class="as">Srivastava, DhruvandSingh, AdityaKumarandTapaswi, Makarand</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Srivastava_How_You_Feelin_Learning_Emotions_and_Mental_States_in_Movie_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2517-2528.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过理解角色的情绪和心理状态来分析电影故事。<br>
                    动机：现有的方法无法全面预测电影场景中每个角色的多种情绪和心理状态。<br>
                    方法：提出EmoTx，一种基于Transformer的多模态架构，通过整合视频、多个角色和对话语句进行联合预测。<br>
                    效果：实验证明EmoTx在预测经典情绪和其他心理状态上有效，且优于其他最先进的情感识别方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Movie story analysis requires understanding characters' emotions and mental states. Towards this goal, we formulate emotion understanding as predicting a diverse and multi-label set of emotions at the level of a movie scene and for each character. We propose EmoTx, a multimodal Transformer-based architecture that ingests videos, multiple characters, and dialog utterances to make joint predictions. By leveraging annotations from the MovieGraphs dataset, we aim to predict classic emotions (e.g. happy, angry) and other mental states (e.g. honest, helpful). We conduct experiments on the most frequently occurring 10 and 25 labels, and a mapping that clusters 181 labels to 26. Ablation studies and comparison against adapted state-of-the-art emotion recognition approaches shows the effectiveness of EmoTx. Analyzing EmoTx's self-attention scores reveals that expressive emotions often look at character tokens while other mental states rely on video and dialog cues.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">28.Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification</span><br>
                <span class="as">Zhang, YukangandWang, Hanzi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2153-2162.png><br>
            
            <span class="tt"><span class="t0">研究问题：可见光-红外人脸再识别（VIReID）任务中，主要挑战是可见光（VIS）和红外（IR）图像之间的模态差距。<br>
                    动机：由于训练样本通常有限，而模态差距过大，导致现有方法无法有效挖掘多样的跨模态线索。<br>
                    方法：提出一种新颖的嵌入空间中的增强网络，称为多样化嵌入扩展网络（DEEN）。DEEN可以有效地生成多样化的嵌入，学习信息丰富的特征表示，并减小VIS和IR图像之间的模态差异。<br>
                    效果：在SYSU-MM01、RegDB和LLCM数据集上的大量实验表明，提出的DEEN优于其他几种最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For the visible-infrared person re-identification (VIReID) task, one of the major challenges is the modality gaps between visible (VIS) and infrared (IR) images. However, the training samples are usually limited, while the modality gaps are too large, which leads that the existing methods cannot effectively mine diverse cross-modality clues. To handle this limitation, we propose a novel augmentation network in the embedding space, called diverse embedding expansion network (DEEN). The proposed DEEN can effectively generate diverse embeddings to learn the informative feature representations and reduce the modality discrepancy between the VIS and IR images. Moreover, the VIReID model may be seriously affected by drastic illumination changes, while all the existing VIReID datasets are captured under sufficient illumination without significant light changes. Thus, we provide a low-light cross-modality (LLCM) dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9 RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets show the superiority of the proposed DEEN over several other state-of-the-art methods. The code and dataset are released at: https://github.com/ZYK100/LLCM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">29.Weakly Supervised Video Representation Learning With Unaligned Text for Sequential Videos</span><br>
                <span class="as">Dong, SixunandHu, HuazhangandLian, DongzeandLuo, WeixinandQian, YichengandGao, Shenghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Weakly_Supervised_Video_Representation_Learning_With_Unaligned_Text_for_Sequential_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2437-2447.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督的序列视频理解问题，即在没有精确的时间戳级别文本-视频对齐的情况下进行视频理解。<br>
                    动机：由于其目标导向的特性，新兴的序列视频理解任务引起了研究者的广泛关注。<br>
                    方法：本文借鉴了CLIP的思想，使用转换器聚合帧级特征以表示视频，并使用预训练的文本编码器分别对每个动作和整个视频的文本进行编码。为了建立文本和视频之间的对应关系，提出了多粒度损失，其中包括视频-段落对比损失（强制整个视频与完整脚本匹配）和细粒度的帧-句子对比损失（强制每个动作与其描述匹配）。由于帧-句子对应关系不可用，本文提出利用视频动作在时间域中的顺序发生这一事实生成伪帧-句子对应关系，并用伪标签监督网络训练。<br>
                    效果：在视频序列验证和文本到视频匹配等任务上的大量实验表明，该方法比基线方法有大幅度的提升，验证了所提出方法的有效性。代码可在https://github.com/svip-lab/WeakSVR获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sequential video understanding, as an emerging video understanding task, has driven lots of researchers' attention because of its goal-oriented nature. This paper studies weakly supervised sequential video understanding where the accurate time-stamp level text-video alignment is not provided. We solve this task by borrowing ideas from CLIP. Specifically, we use a transformer to aggregate frame-level features for video representation and use a pre-trained text encoder to encode the texts corresponding to each action and the whole video, respectively. To model the correspondence between text and video, we propose a multiple granularity loss, where the video-paragraph contrastive loss enforces matching between the whole video and the complete script, and a fine-grained frame-sentence contrastive loss enforces the matching between each action and its description. As the frame-sentence correspondence is not available, we propose to use the fact that video actions happen sequentially in the temporal domain to generate pseudo frame-sentence correspondence and supervise the network training with the pseudo labels. Extensive experiments on video sequence verification and text-to-video matching show that our method outperforms baselines by a large margin, which validates the effectiveness of our proposed approach. Code is available at https://github.com/svip-lab/WeakSVR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">30.Chat2Map: Efficient Scene Mapping From Multi-Ego Conversations</span><br>
                <span class="as">Majumder, SagnikandJiang, HaoandMoulon, PierreandHenderson, EthanandCalamia, PaulandGrauman, KristenandIthapu, VamsiKrishna</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Majumder_Chat2Map_Efficient_Scene_Mapping_From_Multi-Ego_Conversations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10554-10564.png><br>
            
            <span class="tt"><span class="t0">研究问题：能否通过从多个自我中心视角捕捉到的对话视频，以低成本的方式揭示场景的地图？<br>
                    动机：我们希望通过让多个人在场景中移动并进行对话，利用他们接收到的丰富的视听线索来揭示场景中未被看到的区域。<br>
                    方法：我们提出了一种新的问题解决方法，即通过利用参与者在自然对话中的自我中心音频视觉观察中的共享信息，高效地构建以前未见过的环境的3D地图。<br>
                    效果：我们的模型优于先前最先进的映射方法，并实现了优秀的成本-准确性权衡。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Can conversational videos captured from multiple egocentric viewpoints reveal the map of a scene in a cost-efficient way? We seek to answer this question by proposing a new problem: efficiently building the map of a previously unseen 3D environment by exploiting shared information in the egocentric audio-visual observations of participants in a natural conversation. Our hypothesis is that as multiple people ("egos") move in a scene and talk among themselves, they receive rich audio-visual cues that can help uncover the unseen areas of the scene. Given the high cost of continuously processing egocentric visual streams, we further explore how to actively coordinate the sampling of visual information, so as to minimize redundancy and reduce power use. To that end, we present an audio-visual deep reinforcement learning approach that works with our shared scene mapper to selectively turn on the camera to efficiently chart out the space. We evaluate the approach using a state-of-the-art audio-visual simulator for 3D scenes as well as real-world video. Our model outperforms previous state-of-the-art mapping methods, and achieves an excellent cost-accuracy tradeoff. Project: https://vision.cs.utexas.edu/projects/chat2map.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">31.Executing Your Commands via Motion Diffusion in Latent Space</span><br>
                <span class="as">Chen, XinandJiang, BiaoandLiu, WenandHuang, ZilongandFu, BinandChen, TaoandYu, Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18000-18010.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何根据各种条件输入（如动作类别或文本描述符）生成合理的人体运动序列。<br>
                    动机：由于人体运动具有高度多样性，其分布与条件模态（如自然语言的文本描述符）差异较大，因此难以学习从所需条件模态到人体运动序列的概率映射。此外，来自运动捕捉系统的原始运动数据可能在序列中存在冗余并包含噪声；直接对原始运动序列和条件模态建立联合分布需要大量的计算开销，并可能导致由捕获的噪声引入的伪像。<br>
                    方法：设计了一个强大的变分自编码器（VAE），为人体运动序列得到一个代表性和低维的潜在代码。然后，在运动潜在空间上执行扩散过程，而不是使用扩散模型来建立原始运动序列和条件输入之间的连接。<br>
                    效果：所提出的运动潜在基扩散模型（MLD）能够生成符合给定条件输入的生动运动序列，并在训练和推理阶段大幅减少计算开销。在各种人体运动生成任务上的大量实验表明，我们的MLD在广泛的人体运动生成任务上取得了显著优于现有方法的效果，比之前的原始运动序列扩散模型快两个数量级。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study a challenging task, conditional human motion generation, which produces plausible human motion sequences according to various conditional inputs, such as action classes or textual descriptors. Since human motions are highly diverse and have a property of quite different distribution from conditional modalities, such as textual descriptors in natural languages, it is hard to learn a probabilistic mapping from the desired conditional modality to the human motion sequences. Besides, the raw motion data from the motion capture system might be redundant in sequences and contain noises; directly modeling the joint distribution over the raw motion sequences and conditional modalities would need a heavy computational overhead and might result in artifacts introduced by the captured noises. To learn a better representation of the various human motion sequences, we first design a powerful Variational AutoEncoder (VAE) and arrive at a representative and low-dimensional latent code for a human motion sequence. Then, instead of using a diffusion model to establish the connections between the raw motion sequences and the conditional inputs, we perform a diffusion process on the motion latent space. Our proposed Motion Latent-based Diffusion model (MLD) could produce vivid motion sequences conforming to the given conditional inputs and substantially reduce the computational overhead in both the training and inference stages. Extensive experiments on various human motion generation tasks demonstrate that our MLD achieves significant improvements over the state-of-the-art methods among extensive human motion generation tasks, with two orders of magnitude faster than previous diffusion models on raw motion sequences.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">32.Adaptive Human Matting for Dynamic Videos</span><br>
                <span class="as">Lin, Chung-ChingandWang, JiangandLuo, KunandLin, KevinandLi, LinjieandWang, LijuanandLiu, Zicheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Adaptive_Human_Matting_for_Dynamic_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10229-10238.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频抠图技术中，由于需要人工标注的trimap成本高且不适用于实时应用，因此如何消除trimap依赖性是一个重要的问题。<br>
                    动机：尽管最新的无trimap方法在实验中表现出了良好的效果，但在处理高度多样化和非结构化的视频时，其性能往往会下降。<br>
                    方法：我们提出了一种名为AdaM的自适应动态视频抠图框架，该框架同时进行前景和背景的区分，并捕捉前景人物的alpha通道细节。我们采用了两个相互连接的网络设计来实现这一目标：（1）一个编码器-解码器网络，用于生成alpha通道和中间遮罩，这些遮罩被用来指导transformer自适应地解码前景和背景；（2）一个transformer网络，其中的长短期注意力结合，以保留空间和时间上下文，有助于解码前景细节。<br>
                    效果：我们在新近推出的数据集上对模型进行了基准测试和研究，结果显示，我们的模型显著提高了复杂真实世界视频的抠图真实性和时间连贯性，并在一般化能力上取得了新的最优结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The most recent efforts in video matting have focused on eliminating trimap dependency since trimap annotations are expensive and trimap-based methods are less adaptable for real-time applications. Despite the latest tripmap-free methods showing promising results, their performance often degrades when dealing with highly diverse and unstructured videos. We address this limitation by introducing Adaptive Matting for Dynamic Videos, termed AdaM, which is a framework designed for simultaneously differentiating foregrounds from backgrounds and capturing alpha matte details of human subjects in the foreground. Two interconnected network designs are employed to achieve this goal: (1) an encoder-decoder network that produces alpha mattes and intermediate masks which are used to guide the transformer in adaptively decoding foregrounds and backgrounds, and (2) a transformer network in which long- and short-term attention combine to retain spatial and temporal contexts, facilitating the decoding of foreground details. We benchmark and study our methods on recently introduced datasets, showing that our model notably improves matting realism and temporal coherence in complex real-world videos and achieves new best-in-class generalizability. Further details and examples are available at https://github.com/microsoft/AdaM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">33.UDE: A Unified Driving Engine for Human Motion Generation</span><br>
                <span class="as">Zhou, ZixiangandWang, Baoyuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_UDE_A_Unified_Driving_Engine_for_Human_Motion_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5632-5641.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成可控、可编辑的三维人形运动序列。<br>
                    动机：虽然学习基础的方法已经被开发和应用，但生成和动画化人类运动仍然是一项劳动密集型的任务，且这些方法通常是任务特定的或模态特定的。<br>
                    方法：本文提出了“UDE”，这是第一个能够从自然语言或音频序列生成人类运动序列的统一驱动引擎。它包括基于VQVAE的运动量化模块（将连续运动序列表示为离散潜在代码）、模态无关的变压器编码器（学习将模态感知的驱动信号映射到联合空间）以及统一的令牌变压器网络（以自回归方式预测量化潜在代码索引）。<br>
                    效果：在HumanML3D和AIST++基准测试中进行评估，实验结果表明该方法实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating controllable and editable human motion sequences is a key challenge in 3D Avatar generation. It has been labor-intensive to generate and animate human motion for a long time until learning-based approaches have been developed and applied recently. However, these approaches are still task-specific or modality-specific. In this paper, we propose "UDE", the first unified driving engine that enables generating human motion sequences from natural language or audio sequences (see Fig. 1). Specifically, UDE consists of the following key components: 1) a motion quantization module based on VQVAE that represents continuous motion sequence as discrete latent code, 2) a modality-agnostic transformer encoder that learns to map modality-aware driving signals to a joint space, and 3) a unified token transformer (GPT-like) network to predict the quantized latent code index in an auto-regressive manner. 4) a diffusion motion decoder that takes as input the motion tokens and decodes them into motion sequences with high diversity. We evaluate our method on HumanML3D and AIST++ benchmarks, and the experiment results demonstrate our method achieves state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">34.PivoTAL: Prior-Driven Supervision for Weakly-Supervised Temporal Action Localization</span><br>
                <span class="as">Rizve, MamshadNayeemandMittal, GauravandYu, YeandHall, MatthewandSajeev, SandraandShah, MubarakandChen, Mei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rizve_PivoTAL_Prior-Driven_Supervision_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22992-23002.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用弱监督在未修剪的视频中定位动作？<br>
                    动机：现有的方法主要从分类的角度进行定位，缺乏对动作边界的明确理解，导致动作定位不完整。<br>
                    方法：提出PivoTAL模型，通过学习直接定位动作片段来直接进行定位，利用视频中的时空规律和可学习的高斯先验进行监督训练。<br>
                    效果：在THUMOS-14和ActivitNet-v1.3等基准数据集上，PivoTAL相比现有方法有至少3%的平均mAP提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-supervised Temporal Action Localization (WTAL) attempts to localize the actions in untrimmed videos using only video-level supervision. Most recent works approach WTAL from a localization-by-classification perspective where these methods try to classify each video frame followed by a manually-designed post-processing pipeline to aggregate these per-frame action predictions into action snippets. Due to this perspective, the model lacks any explicit understanding of action boundaries and tends to focus only on the most discriminative parts of the video resulting in incomplete action localization. To address this, we present PivoTAL, Prior-driven Supervision for Weakly-supervised Temporal Action Localization, to approach WTAL from a localization-by-localization perspective by learning to localize the action snippets directly. To this end, PivoTAL leverages the underlying spatio-temporal regularities in videos in the form of action-specific scene prior, action snippet generation prior, and learnable Gaussian prior to supervise the localization-based training. PivoTAL shows significant improvement (of at least 3% avg mAP) over all existing methods on the benchmark datasets, THUMOS-14 and ActivitNet-v1.3.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">35.The Wisdom of Crowds: Temporal Progressive Attention for Early Action Prediction</span><br>
                <span class="as">Stergiou, AlexandrosandDamen, Dima</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Stergiou_The_Wisdom_of_Crowds_Temporal_Progressive_Attention_for_Early_Action_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14709-14719.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从部分观察的视频中预测正在进行的动作的问题。<br>
                    动机：早期的行动预测需要从视频的开始阶段就推断出正在进行的动作，这在许多应用中都非常重要。<br>
                    方法：提出了一种基于瓶颈的注意力模型，通过在细到粗的不同尺度上进行渐进采样来捕捉动作的演变。该模型由多个注意力塔组成，每个尺度对应一个塔。预测的动作标签是基于这些塔的集体协议和置信度来确定的。<br>
                    效果：通过对四个视频数据集的大量实验，证明了TemPr模型在早期动作预测任务上的优越性能，并在各种编码器架构上都表现出色。通过详细的消融实验，展示了TemPr模型的有效性和一致性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Early action prediction deals with inferring the ongoing action from partially-observed videos, typically at the outset of the video. We propose a bottleneck-based attention model that captures the evolution of the action, through progressive sampling over fine-to-coarse scales. Our proposed Temporal Progressive (TemPr) model is composed of multiple attention towers, one for each scale. The predicted action label is based on the collective agreement considering confidences of these towers. Extensive experiments over four video datasets showcase state-of-the-art performance on the task of Early Action Prediction across a range of encoder architectures. We demonstrate the effectiveness and consistency of TemPr through detailed ablations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">36.StepFormer: Self-Supervised Step Discovery and Localization in Instructional Videos</span><br>
                <span class="as">Dvornik, NikitaandHadji, IsmaandZhang, RanandDerpanis, KonstantinosG.andWildes, RichardP.andJepson, AllanD.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dvornik_StepFormer_Self-Supervised_Step_Discovery_and_Localization_in_Instructional_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18952-18961.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从人类演示的视频中学习程序性任务，特别是在视频中的指导步骤。<br>
                    动机：传统的视频指导步骤定位方法需要人工标注，不适用于大型数据集。<br>
                    方法：提出StepFormer模型，这是一种自我监督的模型，可以在视频中自动发现和定位指导步骤。StepFormer是一个注意力机制的转换器解码器，通过学习查询来关注视频，并生成一个捕获视频中关键步骤的序列。<br>
                    效果：在三个具有挑战性的基准测试中，StepFormer模型在步骤检测和定位方面优于所有先前的无监督和弱监督方法。此外，该模型还表现出解决零样本多步定位的能力，并在该任务上超越了所有相关基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Instructional videos are an important resource to learn procedural tasks from human demonstrations. However, the instruction steps in such videos are typically short and sparse, with most of the video being irrelevant to the procedure. This motivates the need to temporally localize the instruction steps in such videos, i.e. the task called key-step localization. Traditional methods for key-step localization require video-level human annotations and thus do not scale to large datasets. In this work, we tackle the problem with no human supervision and introduce StepFormer, a self-supervised model that discovers and localizes instruction steps in a video. StepFormer is a transformer decoder that attends to the video with learnable queries, and produces a sequence of slots capturing the key-steps in the video. We train our system on a large dataset of instructional videos, using their automatically-generated subtitles as the only source of supervision. In particular, we supervise our system with a sequence of text narrations using an order-aware loss function that filters out irrelevant phrases. We show that our model outperforms all previous unsupervised and weakly-supervised approaches on step detection and localization by a large margin on three challenging benchmarks. Moreover, our model demonstrates an emergent property to solve zero-shot multi-step localization and outperforms all relevant baselines at this task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">37.Learning Procedure-Aware Video Representation From Instructional Videos and Their Narrations</span><br>
                <span class="as">Zhong, YiwuandYu, LichengandBai, YangandLi, ShangwenandYan, XuetingandLi, Yin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Learning_Procedure-Aware_Video_Representation_From_Instructional_Videos_and_Their_Narrations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14825-14835.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过大规模网络教学视频和其旁白，学习编码动作步骤及其时间顺序的视频表示，而无需使用人工注释。<br>
                    动机：互联网上丰富的教学视频和旁白为理解程序活动提供了激动人心的途径。<br>
                    方法：基于大规模的网络教学视频和旁白数据集，提出一种联合学习视频表示的方法，该方法同时编码单个步骤概念和一个深度概率模型，以捕捉时间依赖性和步骤顺序的巨大个体差异。<br>
                    效果：实验证明，学习时间顺序不仅可以增强程序推理的新能力，还可以加强单个步骤的识别。该方法在步骤分类（在COIN / EPIC-Kitchens上+2.8% / +3.3%）和步骤预测（在COIN上+7.4%）方面显著提高了最先进的结果。此外，该方法在零样本推理、预测不完整程序的多样化和合理步骤方面也取得了有希望的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The abundance of instructional videos and their narrations over the Internet offers an exciting avenue for understanding procedural activities. In this work, we propose to learn video representation that encodes both action steps and their temporal ordering, based on a large-scale dataset of web instructional videos and their narrations, without using human annotations. Our method jointly learns a video representation to encode individual step concepts, and a deep probabilistic model to capture both temporal dependencies and immense individual variations in the step ordering. We empirically demonstrate that learning temporal ordering not only enables new capabilities for procedure reasoning, but also reinforces the recognition of individual steps. Our model significantly advances the state-of-the-art results on step classification (+2.8% / +3.3% on COIN / EPIC-Kitchens) and step forecasting (+7.4% on COIN). Moreover, our model attains promising results in zero-shot inference for step classification and forecasting, as well as in predicting diverse and plausible steps for incomplete procedures. Our code is available at https://github.com/facebookresearch/ProcedureVRL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">38.NeuralPCI: Spatio-Temporal Neural Field for 3D Point Cloud Multi-Frame Non-Linear Interpolation</span><br>
                <span class="as">Zheng, ZehanandWu, DanniandLu, RuisiandLu, FanandChen, GuangandJiang, Changjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_NeuralPCI_Spatio-Temporal_Neural_Field_for_3D_Point_Cloud_Multi-Frame_Non-Linear_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/909-918.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的点云插值问题，特别是针对非线性大运动场景的插值。<br>
                    动机：尽管视频插值取得了显著进展，但点云插值仍鲜有探索。同时，真实世界中存在大量非线性大运动，使得点云插值任务更具挑战性。<br>
                    方法：我们提出了NeuralPCI，一种用于3D点云插值的端到端4D时空神经场。该模型隐式地整合了多帧信息，以处理室内外场景中的非线性大运动。<br>
                    效果：我们在DHB（动态人体）和NL-Drive数据集上测试了NeuralPCI，结果显示其在这两个数据集上都达到了最先进的性能。此外，该方法还可以自然地扩展到点云外推、变形和自动标记等其他领域。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, there has been a significant increase in focus on the interpolation task of computer vision. Despite the tremendous advancement of video interpolation, point cloud interpolation remains insufficiently explored. Meanwhile, the existence of numerous nonlinear large motions in real-world scenarios makes the point cloud interpolation task more challenging. In light of these issues, we present NeuralPCI: an end-to-end 4D spatio-temporal Neural field for 3D Point Cloud Interpolation, which implicitly integrates multi-frame information to handle nonlinear large motions for both indoor and outdoor scenarios. Furthermore, we construct a new multi-frame point cloud interpolation dataset called NL-Drive for large nonlinear motions in autonomous driving scenes to better demonstrate the superiority of our method. Ultimately, NeuralPCI achieves state-of-the-art performance on both DHB (Dynamic Human Bodies) and NL-Drive datasets. Beyond the interpolation task, our method can be naturally extended to point cloud extrapolation, morphing, and auto-labeling, which indicates substantial potential in other domains. Codes are available at https://github.com/ispc-lab/NeuralPCI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">39.A Generalized Framework for Video Instance Segmentation</span><br>
                <span class="as">Heo, MiranandHwang, SukjunandHyun, JeongseokandKim, HanjungandOh, SeoungWugandLee, Joon-YoungandKim, SeonJoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Heo_A_Generalized_Framework_for_Video_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14623-14632.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频实例分割（VIS）中，如何处理复杂、遮挡的长视频序列是一个新挑战。<br>
                    动机：现有方法在处理这一挑战时存在局限，主要问题在于训练与推理之间的差异。<br>
                    方法：提出一种通用的视频实例分割框架GenVIS，通过学习策略和新颖的目标标签分配进行顺序学习，并引入一个有效获取先前状态信息的内存。<br>
                    效果：在YouTube-VIS 2019/2021/2022和Occluded VIS (OVIS)等流行VIS基准测试中，该方法取得了最先进的结果，特别是在长VIS基准测试（OVIS）上，使用ResNet-50主干网络提高了5.6 AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The handling of long videos with complex and occluded sequences has recently emerged as a new challenge in the video instance segmentation (VIS) community. However, existing methods have limitations in addressing this challenge. We argue that the biggest bottleneck in current approaches is the discrepancy between training and inference. To effectively bridge this gap, we propose a Generalized framework for VIS, namely GenVIS, that achieves state-of-the-art performance on challenging benchmarks without designing complicated architectures or requiring extra post-processing. The key contribution of GenVIS is the learning strategy, which includes a query-based training pipeline for sequential learning with a novel target label assignment. Additionally, we introduce a memory that effectively acquires information from previous states. Thanks to the new perspective, which focuses on building relationships between separate frames or clips, GenVIS can be flexibly executed in both online and semi-online manner. We evaluate our approach on popular VIS benchmarks, achieving state-of-the-art results on YouTube-VIS 2019/2021/2022 and Occluded VIS (OVIS). Notably, we greatly outperform the state-of-the-art on the long VIS benchmark (OVIS), improving 5.6 AP with ResNet-50 backbone. Code is available at https://github.com/miranheo/GenVIS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">40.Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning</span><br>
                <span class="as">Tan, ChengandGao, ZhangyangandWu, LirongandXu, YongjieandXia, JunandLi, SiyuanandLi, StanZ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18770-18782.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在调查现有的时空预测学习方法，并提出一个通用的时空预测学习框架。<br>
                    动机：主流方法使用循环单元来捕获长期时间依赖性，但由于其无法比拟的架构，计算效率低下。<br>
                    方法：我们提出了一种时空注意力单元（TAU），将时间注意力分解为帧内静态注意力和帧间动态注意力，以并行化时间模块。同时，引入了一种新的微分散度正则化方法，以考虑帧间变化。<br>
                    效果：实验结果表明，该方法使模型在各种时空预测基准测试中实现了竞争性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spatiotemporal predictive learning aims to generate future frames by learning from historical frames. In this paper, we investigate existing methods and present a general framework of spatiotemporal predictive learning, in which the spatial encoder and decoder capture intra-frame features and the middle temporal module catches inter-frame correlations. While the mainstream methods employ recurrent units to capture long-term temporal dependencies, they suffer from low computational efficiency due to their unparallelizable architectures. To parallelize the temporal module, we propose the Temporal Attention Unit (TAU), which decomposes temporal attention into intra-frame statical attention and inter-frame dynamical attention. Moreover, while the mean squared error loss focuses on intra-frame errors, we introduce a novel differential divergence regularization to take inter-frame variations into account. Extensive experiments demonstrate that the proposed method enables the derived model to achieve competitive performance on various spatiotemporal prediction benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">41.Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals</span><br>
                <span class="as">Shibata, YutoandKawashima, YutakaandIsogawa, MarikoandIrie, GoandKimura, AkisatoandAoki, Yoshimitsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13323-13332.png><br>
            
            <span class="tt"><span class="t0">研究问题：仅通过声音信号，我们能推断出多少关于人类行为的信息？<br>
                    动机：现有的方法由于使用包含人类语音或特定动作的声音的信号，因此存在隐私问题。我们探索了如何通过一对麦克风和扬声器的主动声感测来估计3D人体姿势，利用低级别的声学信号提供足够的线索。<br>
                    方法：我们引入了一个框架，将多通道音频特征编码为3D人体姿势。为了捕捉微妙的声音变化以揭示详细的体位信息，我们从声学信号中显式提取相位特征以及典型的频谱特征，并将它们输入到我们的人体姿态估计网络中。<br>
                    效果：实验表明，仅使用低维的声学信息，我们的方法就优于基线方法。本项目使用的数据集和代码将公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given only acoustic signals without any high-level information, such as voices or sounds of scenes/actions, how much can we infer about the behavior of humans? Unlike existing methods, which suffer from privacy issues because they use signals that include human speech or the sounds of specific actions, we explore how low-level acoustic signals can provide enough clues to estimate 3D human poses by active acoustic sensing with a single pair of microphones and loudspeakers (see Fig. 1). This is a challenging task since sound is much more diffractive than other signals and therefore covers up the shape of objects in a scene. Accordingly, we introduce a framework that encodes multichannel audio features into 3D human poses. Aiming to capture subtle sound changes to reveal detailed pose information, we explicitly extract phase features from the acoustic signals together with typical spectrum features and feed them into our human pose estimation network. Also, we show that reflected or diffracted sounds are easily influenced by subjects' physique differences e.g., height and muscularity, which deteriorates prediction accuracy. We reduce these gaps by using a subject discriminator to improve accuracy. Our experiments suggest that with the use of only low-dimensional acoustic information, our method outperforms baseline methods. The datasets and codes used in this project will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">42.SViTT: Temporal Learning of Sparse Video-Text Transformers</span><br>
                <span class="as">Li, YiandMin, KyleandTripathi, SubarnaandVasconcelos, Nuno</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SViTT_Temporal_Learning_of_Sparse_Video-Text_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18919-18929.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频-文本转换器是否学习了跨帧的时间关系？<br>
                    动机：尽管视频-文本模型具有巨大的容量和丰富的多模态训练数据，但最近的研究表明，这些模型往往偏向于基于帧的空间表示，而时间推理仍未得到解决。<br>
                    方法：我们提出了一种稀疏的视频-文本架构SViTT，该架构通过限制自我注意中令牌之间的查询-键通信和丢弃无信息的视觉令牌，以明显低于简单转换器的计算成本进行多帧推理。<br>
                    效果：在多个视频-文本检索和问答基准测试中，SViTT优于密集变换器基线，并且计算成本仅为其一小部分。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Do video-text transformers learn to model temporal relationships across frames? Despite their immense capacity and the abundance of multimodal training data, recent work has revealed the strong tendency of video-text models towards frame-based spatial representations, while temporal reasoning remains largely unsolved. In this work, we identify several key challenges in temporal learning of video-text transformers: the spatiotemporal trade-off from limited network size; the curse of dimensionality for multi-frame modeling; and the diminishing returns of semantic information by extending clip length. Guided by these findings, we propose SViTT, a sparse video-text architecture that performs multi-frame reasoning with significantly lower cost than naive transformers with dense attention. Analogous to graph-based networks, SViTT employs two forms of sparsity: edge sparsity that limits the query-key communications between tokens in self-attention, and node sparsity that discards uninformative visual tokens. Trained with a curriculum which increases model sparsity with the clip length, SViTT outperforms dense transformer baselines on multiple video-text retrieval and question answering benchmarks, with a fraction of computational cost. Project page: http://svcl.ucsd.edu/projects/svitt.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">43.Large-Capacity and Flexible Video Steganography via Invertible Neural Network</span><br>
                <span class="as">Mou, ChongandXu, YouminandSong, JiechongandZhao, ChenandGhanem, BernardandZhang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mou_Large-Capacity_and_Flexible_Video_Steganography_via_Invertible_Neural_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22606-22615.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频隐写术中容量小、方法固定的问题。<br>
                    动机：目前的大部分视频隐写术方法容量有限，方法固定，无法满足多样化的需求。<br>
                    方法：提出了一种大容量、灵活的视频隐写术网络（LF-VSN）。通过一个可逆的神经网络实现多部视频的隐藏和恢复，提高了容量；并通过密钥控制方案和可扩展的多视频隐藏策略，增强了灵活性。<br>
                    效果：实验证明，LF-VSN在视频隐写术性能上有显著提升，具有高安全性、大容量和灵活性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video steganography is the art of unobtrusively concealing secret data in a cover video and then recovering the secret data through a decoding protocol at the receiver end. Although several attempts have been made, most of them are limited to low-capacity and fixed steganography. To rectify these weaknesses, we propose a Large-capacity and Flexible Video Steganography Network (LF-VSN) in this paper. For large-capacity, we present a reversible pipeline to perform multiple videos hiding and recovering through a single invertible neural network (INN). Our method can hide/recover 7 secret videos in/from 1 cover video with promising performance. For flexibility, we propose a key-controllable scheme, enabling different receivers to recover particular secret videos from the same cover video through specific keys. Moreover, we further improve the flexibility by proposing a scalable strategy in multiple videos hiding, which can hide variable numbers of secret videos in a cover video with a single model and a single training session. Extensive experiments demonstrate that with the significant improvement of the video steganography performance, our proposed LF-VSN has high security, large hiding capacity, and flexibility. The source code is available at https://github.com/MC-E/LF-VSN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">44.EVAL: Explainable Video Anomaly Localization</span><br>
                <span class="as">Singh, AshishandJones, MichaelJ.andLearned-Miller, ErikG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_EVAL_Explainable_Video_Anomaly_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18717-18726.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种新的单场景视频异常定位框架，使系统做出的决策具有人类可理解的原因。<br>
                    动机：现有的视频异常检测方法缺乏对异常原因的解释性，我们希望通过深度学习模型提供人类可理解的异常原因。<br>
                    方法：首先使用深度网络学习物体及其运动的基本表示，然后利用这些表示构建特定场景的高级位置相关模型，用于检测新视频中的异常。<br>
                    效果：在标准的视频异常检测数据集上进行实验，结果显著优于先前最先进的方法。所有代码和额外数据集将公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We develop a novel framework for single-scene video anomaly localization that allows for human-understandable reasons for the decisions the system makes. We first learn general representations of objects and their motions (using deep networks) and then use these representations to build a high-level, location-dependent model of any particular scene. This model can be used to detect anomalies in new videos of the same scene. Importantly, our approach is explainable -- our high-level appearance and motion features can provide human-understandable reasons for why any part of a video is classified as normal or anomalous. We conduct experiments on standard video anomaly detection datasets (Street Scene, CUHK Avenue, ShanghaiTech and UCSD Ped1, Ped2) and show significant improvements over the previous state-of-the-art. All of our code and extra datasets will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">45.SeqTrack: Sequence to Sequence Learning for Visual Object Tracking</span><br>
                <span class="as">Chen, XinandPeng, HouwenandWang, DongandLu, HuchuanandHu, Han</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14572-14581.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的视觉跟踪序列学习框架SeqTrack。<br>
                    动机：将视觉跟踪视为一个序列生成问题，通过自动预测物体边界框，避免了复杂的头部网络设计。<br>
                    方法：SeqTrack仅采用简单的编码器-解码器转换器架构，其中编码器使用双向转换器提取视觉特征，解码器则使用因果转换器自动生成边界框序列。<br>
                    效果：这种序列学习范式不仅简化了跟踪框架，还在基准测试中实现了竞争性能，例如在LaSOT上获得72.5%的AUC，创造了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a new sequence-to-sequence learning framework for visual tracking, dubbed SeqTrack. It casts visual tracking as a sequence generation problem, which predicts object bounding boxes in an autoregressive fashion. This is different from prior Siamese trackers and transformer trackers, which rely on designing complicated head networks, such as classification and regression heads. SeqTrack only adopts a simple encoder-decoder transformer architecture. The encoder extracts visual features with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregressively with a causal transformer. The loss function is a plain cross-entropy. Such a sequence learning paradigm not only simplifies tracking framework, but also achieves competitive performance on benchmarks. For instance, SeqTrack gets 72.5% AUC on LaSOT, establishing a new state-of-the-art performance. Code and models are available at https://github.com/microsoft/VideoX.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">46.Aligning Step-by-Step Instructional Diagrams to Video Demonstrations</span><br>
                <span class="as">Zhang, JiahaoandCherian, AnoopandLiu, YanbinandBen-Shabat, YizhakandRodriguez, CristianandGould, Stephen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Aligning_Step-by-Step_Instructional_Diagrams_to_Video_Demonstrations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2483-2492.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现多模态对齐，即通过一种模态的查询来检索另一种模态的实例？<br>
                    动机：本研究旨在解决一种新颖的多模态对齐问题，即装配图（常见于宜家装配手册）中的指令步骤与来自真实世界视频片段的视频之间的对齐。<br>
                    方法：引入一种新的监督对比学习方法，通过一组新的损失函数指导学习将视频与装配图中的细微细节进行对齐。<br>
                    效果：在IAW（宜家装配在野外）数据集上进行的大量实验表明，该方法在两个任务上都优于其他替代方案，这两个任务是：在视频片段和插图之间进行最近邻检索，以及为每个视频对指令步骤和片段进行对齐。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal alignment facilitates the retrieval of instances from one modality when queried using another. In this paper, we consider a novel setting where such an alignment is between (i) instruction steps that are depicted as assembly diagrams (commonly seen in Ikea assembly manuals) and (ii) video segments from in-the-wild videos; these videos comprising an enactment of the assembly actions in the real world. To learn this alignment, we introduce a novel supervised contrastive learning method that learns to align videos with the subtle details in the assembly diagrams, guided by a set of novel losses. To study this problem and demonstrate the effectiveness of our method, we introduce a novel dataset: IAW---for Ikea assembly in the wild---consisting of 183 hours of videos from diverse furniture assembly collections and nearly 8,300 illustrations from their associated instruction manuals and annotated for their ground truth alignments. We define two tasks on this dataset: First, nearest neighbor retrieval between video segments and illustrations, and, second, alignment of instruction steps and the segments for each video. Extensive experiments on IAW demonstrate superior performances of our approach against alternatives.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">47.Collecting Cross-Modal Presence-Absence Evidence for Weakly-Supervised Audio-Visual Event Perception</span><br>
                <span class="as">Gao, JunyuandChen, MengyuanandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Collecting_Cross-Modal_Presence-Absence_Evidence_for_Weakly-Supervised_Audio-Visual_Event_Perception_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18827-18836.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督视听事件感知（WS-AVEP）任务，即在只有视频级别事件标签的情况下，对视听事件进行时间定位和分类。<br>
                    动机：尽管现有方法取得了一些进展，但大多数方法要么忽略了视听轨道的不同步特性，要么忽视了互补模态的显式增强。<br>
                    方法：本文提出了一个统一的框架来收集跨模态存在-不存在证据（CMPAE）。具体来说，通过利用单模态和跨模态表示，设计了一个基于主观逻辑理论的存在-不存在证据收集器（PAEC）。为了学习可靠范围内的证据，提出了一种联合模态互学习（JML）过程，该过程自适应地动态校准各种可听、可见和可听可见事件的 evidence。<br>
                    效果：大量实验表明，该方法超越了现有技术（例如，在事件级别的视觉和音频指标上分别实现了3.6%和6.1%的绝对增益）。代码可在github.com/MengyuanChen21/CVPR2023-CMPAE获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With only video-level event labels, this paper targets at the task of weakly-supervised audio-visual event perception (WS-AVEP), which aims to temporally localize and categorize events belonging to each modality. Despite the recent progress, most existing approaches either ignore the unsynchronized property of audio-visual tracks or discount the complementary modality for explicit enhancement. We argue that, for an event residing in one modality, the modality itself should provide ample presence evidence of this event, while the other complementary modality is encouraged to afford the absence evidence as a reference signal. To this end, we propose to collect Cross-Modal Presence-Absence Evidence (CMPAE) in a unified framework. Specifically, by leveraging uni-modal and cross-modal representations, a presence-absence evidence collector (PAEC) is designed under Subjective Logic theory. To learn the evidence in a reliable range, we propose a joint-modal mutual learning (JML) process, which calibrates the evidence of diverse audible, visible, and audi-visible events adaptively and dynamically. Extensive experiments show that our method surpasses state-of-the-arts (e.g., absolute gains of 3.6% and 6.1% in terms of event-level visual and audio metrics). Code is available in github.com/MengyuanChen21/CVPR2023-CMPAE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">48.Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards</span><br>
                <span class="as">Sun, MingyangandZhao, MengchenandHou, YaqingandLi, MingleiandXu, HuangandXu, SongcenandHao, Jianye</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2331-2340.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动为虚拟角色合成共现手势的需求日益增长，但输入语音和目标手势之间的复杂关系使其成为一项挑战。<br>
                    动机：大多数现有工作都专注于预测最适合数据的下一个手势，但这些方法短视且缺乏对未来手势进行规划的能力。<br>
                    方法：本文提出了一种名为RACER的新型强化学习（RL）框架，用于生成最大化整体满意度的手势序列。RACER使用矢量量化变分自编码器来学习手势的紧凑表示，并使用基于GPT的策略架构来自动生成连贯的手势序列。<br>
                    效果：实验结果表明，我们的方法在客观度量和主观人类判断方面均显著优于现有基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>There is a growing demand of automatically synthesizing co-speech gestures for virtual characters. However, it remains a challenge due to the complex relationship between input speeches and target gestures. Most existing works focus on predicting the next gesture that fits the data best, however, such methods are myopic and lack the ability to plan for future gestures. In this paper, we propose a novel reinforcement learning (RL) framework called RACER to generate sequences of gestures that maximize the overall satisfactory. RACER employs a vector quantized variational autoencoder to learn compact representations of gestures and a GPT-based policy architecture to generate coherent sequence of gestures autoregressively. In particular, we propose a contrastive pre-training approach to calculate the rewards, which integrates contextual information into action evaluation and successfully captures the complex relationships between multi-modal speech-gesture data. Experimental results show that our method significantly outperforms existing baselines in terms of both objective metrics and subjective human judgements. Demos can be found at https://github.com/RLracer/RACER.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">49.Reconstructing Signing Avatars From Video Using Linguistic Priors</span><br>
                <span class="as">Forte, Maria-PaolaandKulits, PeterandHuang, Chun-HaoP.andChoutas, VasileiosandTzionas, DimitriosandKuchenbecker, KatherineJ.andBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Forte_Reconstructing_Signing_Avatars_From_Video_Using_Linguistic_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12791-12801.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从手语视频中自动提取精细的手势、面部表情和身体动作，以创建具有表现力的3D化身。<br>
                    动机：现有的手语学习工具主要是孤立的手语视频字典，而使用3D化身可以改善学习效果，并实现AR/VR应用，提高技术访问和在线媒体的使用。<br>
                    方法：提出一种新的语言先验方法，该方法对手语普遍适用，并为3D手势提供约束，有助于解决孤立手语中的模糊性。这种方法被称为SGNify，可以从自然环境中的单目手语视频中全自动捕获精细的手势、面部表情和身体运动。<br>
                    效果：通过使用商业运动捕捉系统计算与单目视频同步的3D化身，对SGNify进行了定量评估。实验结果表明，SGNify在手语视频上的3D身体姿势和形状估计方面优于最先进的方法。感知研究表明，SGNify的3D重建比先前的方法更易于理解和自然，并与源视频相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sign language (SL) is the primary method of communication for the 70 million Deaf people around the world. Video dictionaries of isolated signs are a core SL learning tool. Replacing these with 3D avatars can aid learning and enable AR/VR applications, improving access to technology and online media. However, little work has attempted to estimate expressive 3D avatars from SL video; occlusion, noise, and motion blur make this task difficult. We address this by introducing novel linguistic priors that are universally applicable to SL and provide constraints on 3D hand pose that help resolve ambiguities within isolated signs. Our method, SGNify, captures fine-grained hand pose, facial expression, and body movement fully automatically from in-the-wild monocular SL videos. We evaluate SGNify quantitatively by using a commercial motion-capture system to compute 3D avatars synchronized with monocular video. SGNify outperforms state-of-the-art 3D body-pose- and shape-estimation methods on SL videos. A perceptual study shows that SGNify's 3D reconstructions are significantly more comprehensible and natural than those of previous methods and are on par with the source videos. Code and data are available at sgnify.is.tue.mpg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">50.TempSAL - Uncovering Temporal Information for Deep Saliency Prediction</span><br>
                <span class="as">Aydemir, BaharandHoffstetter, LudoandZhang, TongandSalzmann, MathieuandS\&quot;usstrunk, Sabine</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Aydemir_TempSAL_-_Uncovering_Temporal_Information_for_Deep_Saliency_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6461-6470.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的显著性预测模型，该模型通过利用人类的时间注意力模式来学习在连续的时间间隔中输出显著性图。<br>
                    动机：现有的显著性预测算法通常依赖于额外的信息，如场景上下文、语义关系、注视方向和对象差异性，但并未考虑到图像观察期间注视转移的时间性质。<br>
                    方法：我们的方法通过结合学习到的时间映射局部调整显著性预测。<br>
                    效果：实验结果表明，我们的方法在SALICON基准测试和CodeCharts1k数据集上优于包括多持续时间显著性模型在内的最先进的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep saliency prediction algorithms complement the object recognition features, they typically rely on additional information such as scene context, semantic relationships, gaze direction, and object dissimilarity. However, none of these models consider the temporal nature of gaze shifts during image observation. We introduce a novel saliency prediction model that learns to output saliency maps in sequential time intervals by exploiting human temporal attention patterns. Our approach locally modulates the saliency predictions by combining the learned temporal maps. Our experiments show that our method outperforms the state-of-the-art models, including a multi-duration saliency model, on the SALICON benchmark and CodeCharts1k dataset. Our code is publicly available on GitHub.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">51.A Unified Pyramid Recurrent Network for Video Frame Interpolation</span><br>
                <span class="as">Jin, XinandWu, LonghaiandChen, JieandChen, YouxinandKoo, JayoonandHahm, Cheul-hee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_A_Unified_Pyramid_Recurrent_Network_for_Video_Frame_Interpolation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1578-1587.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种统一的金字塔循环网络（UPR-Net）用于帧插值，以解决光流估计和中间帧合成的问题。<br>
                    动机：现有的帧插值方法在处理大运动情况时的稳定性有待提高。<br>
                    方法：通过构建一个灵活的金字塔框架，利用轻量级的循环模块进行双向光流估计和中间帧合成。在每个金字塔级别上，利用估计的双向光流生成用于帧合成的前向扭曲表示；在整个金字塔级别上，实现对光流和中间帧的迭代细化。<br>
                    效果：实验结果表明，我们的迭代合成策略可以显著提高大运动情况下帧插值的稳定性。尽管非常轻量级（1.7M参数），但我们的UPR-Net基本版本在各种基准测试中表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Flow-guided synthesis provides a common framework for frame interpolation, where optical flow is estimated to guide the synthesis of intermediate frames between consecutive inputs. In this paper, we present UPR-Net, a novel Unified Pyramid Recurrent Network for frame interpolation. Cast in a flexible pyramid framework, UPR-Net exploits lightweight recurrent modules for both bi-directional flow estimation and intermediate frame synthesis. At each pyramid level, it leverages estimated bi-directional flow to generate forward-warped representations for frame synthesis; across pyramid levels, it enables iterative refinement for both optical flow and intermediate frame. In particular, we show that our iterative synthesis strategy can significantly improve the robustness of frame interpolation on large motion cases. Despite being extremely lightweight (1.7M parameters), our base version of UPR-Net achieves excellent performance on a large range of benchmarks. Code and trained models of our UPR-Net series are available at: https://github.com/srcn-ivl/UPR-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">52.PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation</span><br>
                <span class="as">Zhao, QitaoandZheng, CeandLiu, MengyuanandWang, PichaoandChen, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_PoseFormerV2_Exploring_Frequency_Domain_for_Efficient_and_Robust_3D_Human_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8877-8886.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于变换器的方法在人体姿态估计任务中受到输入关节序列长度和二维关节检测质量的限制。<br>
                    动机：为了解决这些问题，本文提出了PoseFormerV2，该方法利用了频率域中的长骨骼序列的紧凑表示形式，以有效地扩大感受野并提高对噪声的鲁棒性。<br>
                    方法：PoseFormerV2通过在时间和频率域中有效地融合特征，实现了比其前身更好的速度-准确性权衡。<br>
                    效果：在两个基准数据集（即Human3.6M和MPI-INF-3DHP）上的大量实验表明，该方法显著优于原始的PoseFormer和其他基于变换器的变体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, transformer-based methods have gained significant success in sequential 2D-to-3D lifting human pose estimation. As a pioneering work, PoseFormer captures spatial relations of human joints in each video frame and human dynamics across frames with cascaded transformer layers and has achieved impressive performance. However, in real scenarios, the performance of PoseFormer and its follow-ups is limited by two factors: (a) The length of the input joint sequence; (b) The quality of 2D joint detection. Existing methods typically apply self-attention to all frames of the input sequence, causing a huge computational burden when the frame number is increased to obtain advanced estimation accuracy, and they are not robust to noise naturally brought by the limited capability of 2D joint detectors. In this paper, we propose PoseFormerV2, which exploits a compact representation of lengthy skeleton sequences in the frequency domain to efficiently scale up the receptive field and boost robustness to noisy 2D joint detection. With minimum modifications to PoseFormer, the proposed method effectively fuses features both in the time domain and frequency domain, enjoying a better speed-accuracy trade-off than its precursor. Extensive experiments on two benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that the proposed approach significantly outperforms the original PoseFormer and other transformer-based variants. Code is released at https://github.com/QitaoZhao/PoseFormerV2.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">53.Neural Koopman Pooling: Control-Inspired Temporal Dynamics Encoding for Skeleton-Based Action Recognition</span><br>
                <span class="as">Wang, XinghanandXu, XinandMu, Yadong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Koopman_Pooling_Control-Inspired_Temporal_Dynamics_Encoding_for_Skeleton-Based_Action_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10597-10607.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于骨架的动作识别方法在捕捉高阶动态信息方面存在不足。<br>
                    动机：为了解决这一问题，我们提出了一种基于Koopman理论的参数化高阶池化技术，称为Koopman池化。<br>
                    方法：我们训练了一个CNN或GCN作为主干网络来提取空间-时间特征，并使用Koopman池化模块进行聚合。我们还提出了一种特征值归一化方法，以鼓励学习到的动态保持稳定且不衰减。此外，我们还展示了当与动态模式分解结合时，我们的Koopman池化框架可以很容易地扩展到单次动作识别。<br>
                    效果：我们在三个基准数据集上进行了评估，即NTU RGB+D 60、120和NW-UCLA。实验结果表明，Koopman池化在全数据集和单次设置下都显著提高了性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Skeleton-based human action recognition is becoming increasingly important in a variety of fields. Most existing works train a CNN or GCN based backbone to extract spatial-temporal features, and use temporal average/max pooling to aggregate the information. However, these pooling methods fail to capture high-order dynamics information. To address the problem, we propose a plug-and-play module called Koopman pooling, which is a parameterized high-order pooling technique based on Koopman theory. The Koopman operator linearizes a non-linear dynamics system, thus providing a way to represent the complex system through the dynamics matrix, which can be used for classification. We also propose an eigenvalue normalization method to encourage the learned dynamics to be non-decaying and stable. Besides, we also show that our Koopman pooling framework can be easily extended to one-shot action recognition when combined with Dynamic Mode Decomposition. The proposed method is evaluated on three benchmark datasets, namely NTU RGB+D 60, 120 and NW-UCLA. Our experiments clearly demonstrate that Koopman pooling significantly improves the performance under both full-dataset and one-shot settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">54.Few-Shot Referring Relationships in Videos</span><br>
                <span class="as">Kumar, YogeshandMishra, Anand</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Few-Shot_Referring_Relationships_in_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2289-2298.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过给定的查询视觉关系，在测试视频中定位出通过谓词连接的主体和对象。<br>
                    动机：虽然现代视觉语言理解能力可以解决这个问题，但为每个主体、对象和谓词的组合进行注释既繁琐又昂贵，甚至可能无法实现。因此，需要一种模型能够仅使用共享相同谓词的支持集视频，学习空间和时间上定位通过未见过的谓词连接的主体和对象。<br>
                    方法：将此问题定义为最小化目标函数，该函数定义在一个T部分随机场中。随机场的顶点对应于主体和对象的候选边界框，T表示测试视频中的帧数。目标函数由帧级别和视觉关系相似性势组成。为了学习这些势能，我们使用一个关系网络，该网络以查询条件转换关系嵌入作为输入，并使用支持集视频进行元训练。此外，通过在随机场上进行基于信念传播的消息传递来最小化目标函数，以获取主体和对象的时空定位或轨迹。<br>
                    效果：我们在两个公共基准测试（即ImageNet-VidVRD和VidOR）上进行了大量实验，并将所提出的方法与具有竞争力的基线进行比较，以评估其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Interpreting visual relationships is a core aspect of comprehensive video understanding. Given a query visual relationship as <subject, predicate, object> and a test video, our objective is to localize the subject and object that are connected via the predicate. Given modern visio-lingual understanding capabilities, solving this problem is achievable, provided that there are large-scale annotated training examples available. However, annotating for every combination of subject, object, and predicate is cumbersome, expensive, and possibly infeasible. Therefore, there is a need for models that can learn to spatially and temporally localize subjects and objects that are connected via an unseen predicate using only a few support set videos sharing the common predicate. We address this challenging problem, referred to as few-shot referring relationships in videos for the first time. To this end, we pose the problem as a minimization of an objective function defined over a T-partite random field. Here, the vertices of the random field correspond to candidate bounding boxes for the subject and object, and T represents the number of frames in the test video. This objective function is composed of frame level and visual relationship similarity potentials. To learn these potentials, we use a relation network that takes query-conditioned translational relationship embedding as inputs and is meta-trained using support set videos in an episodic manner. Further, the objective function is minimized using a belief propagation-based message passing on the random field to obtain the spatiotemporal localization or subject and object trajectories. We perform extensive experiments using two public benchmarks, namely ImageNet-VidVRD and VidOR, and compare the proposed approach with competitive baselines to assess its efficacy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">55.Frame-Event Alignment and Fusion Network for High Frame Rate Tracking</span><br>
                <span class="as">Zhang, JiqingandWang, YuanchenandLiu, WenxiandLi, MengandBai, JinpengandYin, BaocaiandYang, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frame-Event_Alignment_and_Fusion_Network_for_High_Frame_Rate_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9781-9790.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何结合常规帧和事件，实现高帧率的目标跟踪。<br>
                    动机：现有的基于RGB的跟踪器主要针对30帧每秒的低帧率基准，限制了其在现实世界中的功能性，特别是在快速运动中。而事件相机由于其高时间分辨率，为高帧率跟踪提供了巨大的潜力。然而，事件相机无法像传统相机那样提供精细的纹理信息。这种独特的互补性促使我们将常规帧和事件结合起来，以应对各种具有挑战性的情况。<br>
                    方法：我们提出了一个端到端的网络，包括多模态对齐和融合模块，以有效地从两种模态中提取有意义的信息。对齐模块负责在事件提供的移动线索的指导下，进行帧和事件模态之间的跨模态和跨帧速率对齐。融合模块则负责通过两种模态的相互补充，强调有价值的特征并抑制噪声信息。<br>
                    效果：大量的实验表明，我们的方法在高帧率跟踪方面显著优于最先进的跟踪器。使用FE240hz数据集，我们的方法实现了高达240Hz的高帧率跟踪。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. In this paper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-modality and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">56.Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields</span><br>
                <span class="as">Kim, TaewooandChae, YujeongandJang, Hyun-KurlandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18032-18042.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频帧插值（VFI）旨在生成连续输入帧之间的中间视频帧，但研究问题：视频帧插值（VFI）旨在生成连续输入帧之间的中间视频帧，但现有方法在估计双向帧间运动场时只考虑了事件或近似值，无法考虑到真实世界中的复杂运动。<br>
                    动机：由于事件相机是一种生物启发的传感器，仅以微秒级的时序分辨率编码亮度变化，因此一些工作利用事件相机来提高VFI的性能。<br>
                    方法：我们提出了一种新颖的事件基础VFI框架，具有跨模态不对称双向运动场估计。具体来说，我们的EIF-BiOFNet直接估计帧间运动场，无需任何近似方法，充分利用了事件和图像的每个有价值的特征。此外，我们还开发了一个交互式注意力基础的帧合成网络，有效地利用了基于扭曲和基于合成的特征。<br>
                    效果：我们构建了一个大规模的事件基础VFI数据集ERF-X170FPS，具有高帧率、极端运动和动态纹理，克服了以前事件基础VFI数据集的限制。广泛的实验结果表明，我们的方法在各种数据集上比最先进的VFI方法表现出显著的性能改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video Frame Interpolation (VFI) aims to generate intermediate video frames between consecutive input frames. Since the event cameras are bio-inspired sensors that only encode brightness changes with a micro-second temporal resolution, several works utilized the event camera to enhance the performance of VFI. However, existing methods estimate bidirectional inter-frame motion fields with only events or approximations, which can not consider the complex motion in real-world scenarios. In this paper, we propose a novel event-based VFI framework with cross-modal asymmetric bidirectional motion field estimation. In detail, our EIF-BiOFNet utilizes each valuable characteristic of the events and images for direct estimation of inter-frame motion fields without any approximation methods.Moreover, we develop an interactive attention-based frame synthesis network to efficiently leverage the complementary warping-based and synthesis-based features. Finally, we build a large-scale event-based VFI dataset, ERF-X170FPS, with a high frame rate, extreme motion, and dynamic textures to overcome the limitations of previous event-based VFI datasets. Extensive experimental results validate that our method shows significant performance improvement over the state-of-the-art VFI methods on various datasets.Our project pages are available at: https://github.com/intelpro/CBMNet</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">57.MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos</span><br>
                <span class="as">Zhang, ZichengandWu, WeiandSun, WeiandTu, DanyangandLu, WeiandMin, XiongkuoandChen, YingandZhai, Guangtao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MD-VQA_Multi-Dimensional_Quality_Assessment_for_UGC_Live_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1746-1755.png><br>
            
            <span class="tt"><span class="t0">研究问题：UGC直播视频在采集过程中常受各种失真影响，视觉质量多样，且在分发过程中还会被媒体服务器提供商压缩和转码。<br>
                    动机：由于UGC直播视频的盛行，需要有效的视频质量评估工具来监控和优化直播流视频的质量。<br>
                    方法：构建了一个包含418个源UGC视频和3762个不同比特率压缩视频的UGC直播VQA数据库，并基于此数据库开发了一个多维度VQA（MD-VQA）评估器，从语义、失真和运动三个方面测量UGC直播视频的视觉质量。<br>
                    效果：实验结果表明，MD-VQA在UGC直播VQA数据库和现有的压缩UGC VQA数据库上均取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>User-generated content (UGC) live videos are often bothered by various distortions during capture procedures and thus exhibit diverse visual qualities. Such source videos are further compressed and transcoded by media server providers before being distributed to end-users. Because of the flourishing of UGC live videos, effective video quality assessment (VQA) tools are needed to monitor and perceptually optimize live streaming videos in the distributing process. Unfortunately, existing compressed UGC VQA databases are either small in scale or employ high-quality UGC videos as source videos, so VQA models developed on these databases have limited abilities to evaluate UGC live videos. In this paper, we address UGC Live VQA problems by constructing a first-of-a-kind subjective UGC Live VQA database and developing an effective evaluation tool. Concretely, 418 source UGC videos are collected in real live streaming scenarios and 3,762 compressed ones at different bit rates are generated for the subsequent subjective VQA experiments. Based on the built database, we develop a Multi-Dimensional VQA (MD-VQA) evaluator to measure the visual quality of UGC live videos from semantic, distortion, and motion aspects respectively. Extensive experimental results show that MD-VQA achieves state-of-the-art performance on both our UGC Live VQA database and existing compressed UGC VQA databases.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">58.Natural Language-Assisted Sign Language Recognition</span><br>
                <span class="as">Zuo, RonglaiandWei, FangyunandMak, Brian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zuo_Natural_Language-Assisted_Sign_Language_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14890-14900.png><br>
            
            <span class="tt"><span class="t0">研究问题：手语识别中存在大量视觉上无法区分的手势（VISigns），限制了视觉神经网络的识别能力。<br>
                    动机：为了解决这个问题，提出了自然语言辅助手语识别（NLA-SLR）框架，利用语义信息来提高识别效果。<br>
                    方法：首先，对于具有相似语义意义的VISigns，提出语言感知标签平滑技术，通过生成每个训练手势的软标签来缓解训练难度。其次，对于具有不同语义意义的VISigns，提出模态混合技术，将视觉和标签特征混合在一起，以进一步最大化不同手势之间的可分性。此外，还引入了一种新的骨干网络——视频关键点网络，该网络不仅能够对RGB视频和人体关键点进行建模，还能从不同时间感受野的手语视频中获取知识。<br>
                    效果：实验结果表明，该方法在三个广泛采用的基准测试（MSASL、WLASL和NMFs-CSL）上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sign languages are visual languages which convey information by signers' handshape, facial expression, body movement, and so forth. Due to the inherent restriction of combinations of these visual ingredients, there exist a significant number of visually indistinguishable signs (VISigns) in sign languages, which limits the recognition capacity of vision neural networks. To mitigate the problem, we propose the Natural Language-Assisted Sign Language Recognition (NLA-SLR) framework, which exploits semantic information contained in glosses (sign labels). First, for VISigns with similar semantic meanings, we propose language-aware label smoothing by generating soft labels for each training sign whose smoothing weights are computed from the normalized semantic similarities among the glosses to ease training. Second, for VISigns with distinct semantic meanings, we present an inter-modality mixup technique which blends vision and gloss features to further maximize the separability of different signs under the supervision of blended labels. Besides, we also introduce a novel backbone, video-keypoint network, which not only models both RGB videos and human body keypoints but also derives knowledge from sign videos of different temporal receptive fields. Empirically, our method achieves state-of-the-art performance on three widely-adopted benchmarks: MSASL, WLASL, and NMFs-CSL. Codes are available at https://github.com/FangyunWei/SLRT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">59.MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation</span><br>
                <span class="as">Ruan, LudanandMa, YiyangandYang, HuanandHe, HuiguoandLiu, BeiandFu, JianlongandYuan, NicholasJingandJin, QinandGuo, Baining</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10219-10228.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出首个同时生成音频和视频的框架，以实现高质量的真实视频。<br>
                    动机：现有的模型无法同时提供吸引人的视听体验，我们的目标是通过联合训练音频和视频来生成高质量的真实视频。<br>
                    方法：我们提出了一种新的多模态扩散模型（MM-Diffusion），它包含两个耦合的去噪自动编码器。与现有的单模态扩散模型不同，MM-Diffusion由一个顺序的多模态U-Net组成，用于设计联合去噪过程。两个子网络分别学习从高斯噪声中逐渐生成对齐的音频-视频对。为了确保跨模态的语义一致性，我们提出了一种基于随机移位的注意力块，该模块跨越两个子网络，实现了有效的跨模态对齐，从而增强了音频和视频之间的相互保真度。<br>
                    效果：大量的实验表明，我们的模型在无条件音频-视频生成和零样本条件任务（如视频到音频）上表现出优越的结果。特别是在Landscape和AIST++舞蹈数据集上，我们实现了最好的FVD和FAD。在10k次图灵测试中，我们的模型得到了明显的青睐。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose the first joint audio-video generation framework that brings engaging watching and listening experiences simultaneously, towards high-quality realistic videos. To generate joint audio-video pairs, we propose a novel Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled denoising autoencoders. In contrast to existing single-modal diffusion models, MM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising process by design. Two subnets for audio and video learn to gradually generate aligned audio-video pairs from Gaussian noises. To ensure semantic consistency across modalities, we propose a novel random-shift based attention block bridging over the two subnets, which enables efficient cross-modal alignment, and thus reinforces the audio-video fidelity for each other. Extensive experiments show superior results in unconditional audio-video generation, and zero-shot conditional tasks (e.g., video-to-audio). In particular, we achieve the best FVD and FAD on Landscape and AIST++ dancing datasets. Turing tests of 10k votes further demonstrate dominant preferences for our model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">60.MAGVIT: Masked Generative Video Transformer</span><br>
                <span class="as">Yu, LijunandCheng, YongandSohn, KihyukandLezama, Jos\&#x27;eandZhang, HanandChang, HuiwenandHauptmann, AlexanderG.andYang, Ming-HsuanandHao, YuanandEssa, IrfanandJiang, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10459-10469.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用一个单一的模型解决各种视频合成任务。<br>
                    动机：现有的方法在处理视频合成任务时，通常需要多个模型，效率低下。<br>
                    方法：我们引入了Masked Generative VIdeo Transformer（MAGVIT）和3D标记器，将视频量化为时空视觉标记，并提出了一种用于掩蔽视频标记建模的嵌入方法，以便于多任务学习。<br>
                    效果：实验结果表明，MAGVIT在各项指标上都优于现有方法，并在三个视频生成基准测试中建立了最好的FVD，包括具有挑战性的Kinetics-600。此外，与扩散模型相比，MAGVIT的推理速度快两个数量级，与自回归模型相比快60倍。单个MAGVIT模型支持十种不同的生成任务，并能适应不同视觉领域的视频。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60x against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">61.End-to-End Video Matting With Trimap Propagation</span><br>
                <span class="as">Huang, Wei-LunandLee, Ming-Sui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_End-to-End_Video_Matting_With_Trimap_Propagation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14337-14347.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频遮罩技术主要关注时间连贯性，并已通过神经网络取得了显著改进。然而，遮罩通常依赖用户标注的trimaps来估计alpha值，这是一个劳动密集型的问题。<br>
                    动机：尽管最近的一些研究利用视频对象分割方法传播给定的trimaps，但其结果并不一致。因此，我们提出了一种更强大、更快的端到端视频遮罩模型，名为FTP-VM（快速Trimap传播-视频遮罩）。<br>
                    方法：FTP-VM将trimap传播和视频遮罩结合在一个模型中，其中额外的内存匹配主干被替换为提出的轻量级trimap融合模块。我们还采用了来自汽车分割的分割一致性损失，以适应trimap分割，并利用RNN（循环神经网络）提高时间连贯性。<br>
                    效果：实验结果表明，FTP-VM在合成和真实视频中仅使用少量给定的trimaps就能表现出竞争力。其效率比最先进的方法高出八倍，这证实了其在实时场景中的鲁棒性和适用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The research of video matting mainly focuses on temporal coherence and has gained significant improvement via neural networks. However, matting usually relies on user-annotated trimaps to estimate alpha values, which is a labor-intensive issue. Although recent studies exploit video object segmentation methods to propagate the given trimaps, they suffer inconsistent results. Here we present a more robust and faster end-to-end video matting model equipped with trimap propagation called FTP-VM (Fast Trimap Propagation - Video Matting). The FTP-VM combines trimap propagation and video matting in one model, where the additional backbone in memory matching is replaced with the proposed lightweight trimap fusion module. The segmentation consistency loss is adopted from automotive segmentation to fit trimap segmentation with the collaboration of RNN (Recurrent Neural Network) to improve the temporal coherence. The experimental results demonstrate that the FTP-VM performs competitively both in composited and real videos only with few given trimaps. The efficiency is eight times higher than the state-of-the-art methods, which confirms its robustness and applicability in real-time scenarios. The code is available at https://github.com/csvt32745/FTP-VM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">62.DropMAE: Masked Autoencoders With Spatial-Attention Dropout for Tracking Tasks</span><br>
                <span class="as">Wu, QiangqiangandYang, TianyuandLiu, ZiquanandWu, BaoyuanandShan, YingandChan, AntoniB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_DropMAE_Masked_Autoencoders_With_Spatial-Attention_Dropout_for_Tracking_Tasks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14561-14571.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究视频中基于匹配的下游任务，包括视觉目标跟踪（VOT）和视频对象分割（VOS）。<br>
                    动机：现有的掩码自动编码器（MAE）在视频帧重建时过于依赖空间线索而忽视时间关系，导致其在VOT和VOS等匹配任务上的表现不佳。<br>
                    方法：提出DropMAE，通过在帧重建过程中自适应地执行空间注意力丢弃，以促进视频中的时间对应关系学习。<br>
                    效果：实验证明，DropMAE是一种强大且高效的时间匹配学习器，其预训练速度比基于ImageNet的MAE快2倍，并在匹配任务上的微调结果优于后者。同时，发现预训练视频中的动作多样性比场景多样性更能提高VOT和VOS的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study masked autoencoder (MAE) pretraining on videos for matching-based downstream tasks, including visual object tracking (VOT) and video object segmentation (VOS). A simple extension of MAE is to randomly mask out frame patches in videos and reconstruct the frame pixels. However, we find that this simple baseline heavily relies on spatial cues while ignoring temporal relations for frame reconstruction, thus leading to sub-optimal temporal matching representations for VOT and VOS. To alleviate this problem, we propose DropMAE, which adaptively performs spatial-attention dropout in the frame reconstruction to facilitate temporal correspondence learning in videos. We show that our DropMAE is a strong and efficient temporal matching learner, which achieves better finetuning results on matching-based tasks than the ImageNetbased MAE with 2x faster pre-training speed. Moreover, we also find that motion diversity in pre-training videos is more important than scene diversity for improving the performance on VOT and VOS. Our pre-trained DropMAE model can be directly loaded in existing ViT-based trackers for fine-tuning without further modifications. Notably, DropMAE sets new state-of-the-art performance on 8 out of 9 highly competitive video tracking and segmentation datasets. Our code and pre-trained models are available at https://github.com/jimmy-dq/DropMAE.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">63.Are Binary Annotations Sufficient? Video Moment Retrieval via Hierarchical Uncertainty-Based Active Learning</span><br>
                <span class="as">Ji, WeiandLiang, RenjieandZheng, ZhedongandZhang, WenqiaoandZhang, ShengyuandLi, JunchengandLi, MengzeandChua, Tat-seng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Are_Binary_Annotations_Sufficient_Video_Moment_Retrieval_via_Hierarchical_Uncertainty-Based_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23013-23022.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频时刻检索的研究大多集中在提高准确性、效率和鲁棒性，这些都需要大量高质量的标注。<br>
                    动机：精确的帧级标注既耗时又昂贵，而对标注过程的关注却很少。<br>
                    方法：我们探索了一种新的交互方式来刺激视频时刻检索任务中的人类参与注释过程。关键挑战是选择“模糊”的帧和视频进行二元标注以便于网络训练。具体来说，我们提出了一种新的基于不确定性的层次建模方法，明确考虑了在整个与查询描述对应的视频序列中对每个帧的不确定性进行建模，并选择不确定性最高的帧进行标注。<br>
                    效果：在获得专家提供的少量标签后，我们发现这足以在这种恶劣环境中学习出具有竞争力的视频时刻检索模型。此外，我们将视频中帧的不确定性分数作为一个整体来处理，并估计每个视频的难度，这可以进一步减轻视频选择的负担。总的来说，我们的主动学习策略不仅在帧级别上起作用，而且在序列级别上也起作用。在两个公共数据集上的实验验证了我们提出的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent research on video moment retrieval has mostly focused on enhancing the performance of accuracy, efficiency, and robustness, all of which largely rely on the abundance of high-quality annotations. While the precise frame-level annotations are time-consuming and cost-expensive, few attentions have been paid to the labeling process. In this work, we explore a new interactive manner to stimulate the process of human-in-the-loop annotation in video moment retrieval task. The key challenge is to select "ambiguous" frames and videos for binary annotations to facilitate the network training. To be specific, we propose a new hierarchical uncertainty-based modeling that explicitly considers modeling the uncertainty of each frame within the entire video sequence corresponding to the query description, and selecting the frame with the highest uncertainty. Only selected frame will be annotated by the human experts, which can largely reduce the workload. After obtaining a small number of labels provided by the expert, we show that it is sufficient to learn a competitive video moment retrieval model in such a harsh environment. Moreover, we treat the uncertainty score of frames in a video as a whole, and estimate the difficulty of each video, which can further relieve the burden of video selection. In general, our active learning strategy for video moment retrieval works not only at the frame level but also at the sequence level. Experiments on two public datasets validate the effectiveness of our proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">64.Multi-Label Compound Expression Recognition: C-EXPR Database \&amp; Network</span><br>
                <span class="as">Kollias, Dimitrios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5589-5598.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决复合表情识别（CER）的问题，因为现有的面部表情分析主要集中在基本七种表情上，而复合表情更能准确反映我们日常生活中的情感复杂性和微妙性。<br>
                    动机：由于只有少数几个小型、实验室控制、不平衡和静态的数据库存在，对复合表情识别的研究非常有限。<br>
                    方法：本文提出了一个在野外环境下的音频/视频数据库C-EXPR-DB，包含400个视频，20万个帧，13种复合表情的标注，以及情感描述符、动作单元、语音、面部地标和属性的标注。同时，提出了C-EXPR-NET，一种用于复合表情识别和动作单元检测的多任务学习方法。<br>
                    效果：通过广泛的实验研究，验证了C-EXPR-NET的出色性能，并证明了其在新的情境下能有效地进行零样本泛化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Research in automatic analysis of facial expressions mainly focuses on recognising the seven basic ones. However, compound expressions are more diverse and represent the complexity and subtlety of our daily affective displays more accurately. Limited research has been conducted for compound expression recognition (CER), because only a few databases exist, which are small, lab controlled, imbalanced and static. In this paper we present an in-the-wild A/V database, C-EXPR-DB, consisting of 400 videos of 200K frames, annotated in terms of 13 compound expressions, valence-arousal emotion descriptors, action units, speech, facial landmarks and attributes. We also propose C-EXPR-NET, a multi-task learning (MTL) method for CER and AU detection (AU-D); the latter task is introduced to enhance CER performance. For AU-D we incorporate AU semantic description along with visual information. For CER we use a multi-label formulation and the KL-divergence loss. We also propose a distribution matching loss for coupling CER and AU-D tasks to boost their performance and alleviate negative transfer (i.e., when MT model's performance is worse than that of at least one single-task model). An extensive experimental study has been conducted illustrating the excellent performance of C-EXPR-NET, validating the theoretical claims. Finally, C-EXPR-NET is shown to effectively generalize its knowledge in new emotion recognition contexts, in a zero-shot manner.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">65.Physics-Driven Diffusion Models for Impact Sound Synthesis From Videos</span><br>
                <span class="as">Su, KunandQian, KaizhiandShlizerman, EliandTorralba, AntonioandGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Physics-Driven_Diffusion_Models_for_Impact_Sound_Synthesis_From_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9749-9759.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地合成物体交互声音，以实现真实和虚拟世界的沉浸式感知体验。<br>
                    动机：传统的冲击声音合成方法需要详细的物体几何形状和碰撞位置信息，这在现实世界中很少见，也无法用于合成常见视频的冲击声音。现有的基于深度学习的视频驱动方法由于缺乏物理知识，只能捕捉到视觉内容和冲击声音之间的弱对应关系。<br>
                    方法：我们提出了一种基于物理的扩散模型，可以合成无声视频片段的高保真冲击声音。除了视频内容外，我们还使用额外的物理先验来指导冲击声音合成过程。这些物理先验包括直接从嘈杂的真实世界冲击声音示例中估计出的物理参数，以及通过神经网络解释声音环境的学习的剩余参数。<br>
                    效果：实验结果表明，我们的模型在生成现实冲击声音方面优于几个现有的系统。更重要的是，基于物理的表示是完全可解释和透明的，从而使我们能够灵活地进行声音编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modeling sounds emitted from physical object interactions is critical for immersive perceptual experiences in real and virtual worlds. Traditional methods of impact sound synthesis use physics simulation to obtain a set of physics parameters that could represent and synthesize the sound. However, they require fine details of both the object geometries and impact locations, which are rarely available in the real world and can not be applied to synthesize impact sounds from common videos. On the other hand, existing video-driven deep learning-based approaches could only capture the weak correspondence between visual content and impact sounds since they lack of physics knowledge. In this work, we propose a physics-driven diffusion model that can synthesize high-fidelity impact sound for a silent video clip. In addition to the video content, we propose to use additional physics priors to guide the impact sound synthesis procedure. The physics priors include both physics parameters that are directly estimated from noisy real-world impact sound examples without sophisticated setup and learned residual parameters that interpret the sound environment via neural networks. We further implement a novel diffusion model with specific training and inference strategies to combine physics priors and visual information for impact sound synthesis. Experimental results show that our model outperforms several existing systems in generating realistic impact sounds. More importantly, the physics-based representations are fully interpretable and transparent, thus enabling us to perform sound editing flexibly. We encourage the readers to visit our project page to watch demo videos with audio turned on to experience the results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">66.Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding</span><br>
                <span class="as">Lin, ZihangandTan, ChaoleiandHu, Jian-FangandJin, ZhiandYe, TiancaiandZheng, Wei-Shi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23100-23109.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决空间-时间视频定位（STVG）问题，即根据给定的语言查询在空间和时间上定位目标对象。<br>
                    动机：该任务具有挑战性，模型需要理解语言描述中的动态视觉线索（如运动）和静态视觉线索（如物体外观），这需要有效地联合建模空间-时间视觉-语言依赖关系。<br>
                    方法：我们提出了一个新的框架，其中开发了一个静态视觉-语言流和一个动态视觉-语言流来共同推理目标对象。静态流在单帧中进行跨模态理解，并根据像物体外观这样的帧内视觉线索学习关注目标对象的 spatially。动态流在多个连续帧中建模视觉-语言依赖关系，以捕捉像运动这样的动态线索。我们还设计了两个流之间的新的跨流协同模块，使静态流和动态流能够相互传递有用和互补的信息，实现协同推理。<br>
                    效果：实验结果表明两个流的协作效果显著，我们的整个框架在HCSTVG和VidSTG数据集上都取得了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spatio-Temporal Video Grounding (STVG) aims to localize the target object spatially and temporally according to the given language query. It is a challenging task in which the model should well understand dynamic visual cues (e.g., motions) and static visual cues (e.g., object appearances) in the language description, which requires effective joint modeling of spatio-temporal visual-linguistic dependencies. In this work, we propose a novel framework in which a static vision-language stream and a dynamic vision-language stream are developed to collaboratively reason the target tube. The static stream performs cross-modal understanding in a single frame and learns to attend to the target object spatially according to intra-frame visual cues like object appearances. The dynamic stream models visual-linguistic dependencies across multiple consecutive frames to capture dynamic cues like motions. We further design a novel cross-stream collaborative block between the two streams, which enables the static and dynamic streams to transfer useful and complementary information from each other to achieve collaborative reasoning. Experimental results show the effectiveness of the collaboration of the two streams and our overall framework achieves new state-of-the-art performance on both HCSTVG and VidSTG datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">67.Micron-BERT: BERT-Based Facial Micro-Expression Recognition</span><br>
                <span class="as">Nguyen, Xuan-BacandDuong, ChiNhanandLi, XinandGauch, SusanandSeo, Han-SeokandLuu, Khoa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_Micron-BERT_BERT-Based_Facial_Micro-Expression_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1482-1492.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地识别短暂且微小的面部表情，即微表情。<br>
                    动机：尽管预训练的深度双向转换器（BERT）在计算机视觉的自我监督学习任务上取得了显著的进步，但标准的BERT在视觉问题上的设计仅能从完整的图像或视频中学习，无法准确检测到面部微表情的细节。<br>
                    方法：提出了Micron-BERT（u-BERT），一种新颖的面部微表情识别方法。该方法基于两个关键思想自动捕获这些运动，一是采用对角微注意力（DMA）来检测两帧之间的微小差异；二是引入新的关注区域（PoI）模块来定位和突出微表情的关注区域，同时减少噪声背景和干扰。<br>
                    效果：通过将这些组件集成到一个端到端的深层网络中，所提出的u-BERT在各种微表情任务上都大大超过了以往的工作。u-BERT可以在大规模的未标记数据集上进行训练，并在新的未见过的面部微表情数据集上实现高精度。实验证明，u-BERT在四个微表情基准测试中，包括SAMM、CASME II、SMIC和CASME3，始终优于最先进的性能，且优势明显。代码将在https://github.com/uark-cviu/Micron-BERT上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Micro-expression recognition is one of the most challenging topics in affective computing. It aims to recognize tiny facial movements difficult for humans to perceive in a brief period, i.e., 0.25 to 0.5 seconds. Recent advances in pre-training deep Bidirectional Transformers (BERT) have significantly improved self-supervised learning tasks in computer vision. However, the standard BERT in vision problems is designed to learn only from full images or videos, and the architecture cannot accurately detect details of facial micro-expressions. This paper presents Micron-BERT (u-BERT), a novel approach to facial micro-expression recognition. The proposed method can automatically capture these movements in an unsupervised manner based on two key ideas. First, we employ Diagonal Micro-Attention (DMA) to detect tiny differences between two frames. Second, we introduce a new Patch of Interest (PoI) module to localize and highlight micro-expression interest regions and simultaneously reduce noisy backgrounds and distractions. By incorporating these components into an end-to-end deep network, the proposed u-BERT significantly outperforms all previous work in various micro-expression tasks. u-BERT can be trained on a large-scale unlabeled dataset, i.e., up to 8 million images, and achieves high accuracy on new unseen facial micro-expression datasets. Empirical experiments show u-BERT consistently outperforms state-of-the-art performance on four micro-expression benchmarks, including SAMM, CASME II, SMIC, and CASME3, by significant margins. Code will be available at https://github.com/uark-cviu/Micron-BERT</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">68.SVFormer: Semi-Supervised Video Transformer for Action Recognition</span><br>
                <span class="as">Xing, ZhenandDai, QiandHu, HanandChen, JingjingandWu, ZuxuanandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18816-18826.png><br>
            
            <span class="tt"><span class="t0">研究问题：半监督动作识别是一个重要但困难的任务，因为视频标注的成本高。<br>
                    动机：现有的方法主要使用卷积神经网络，而目前革命性的视频变换器模型尚未得到充分探索。<br>
                    方法：本文提出了一种名为SVFormer的变换器模型，该模型采用了稳定的伪标签框架（即EMA-Teacher）来处理未标记的视频样本。同时，还引入了两种新的数据增强策略：Tube TokenMix和时间扭曲增强。<br>
                    效果：在Kinetics-400、UCF-101和HMDB-51三个数据集上的大量实验验证了SVFormer的优势。特别是在Kinetics-400上，SVFormer在1%的标注率下，仅用较少的训练轮次就比最先进的方法提高了31.5%的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised action recognition is a challenging but critical task due to the high cost of video annotations. Existing approaches mainly use convolutional neural networks, yet current revolutionary vision transformer models have been less explored. In this paper, we investigate the use of transformer models under the SSL setting for action recognition. To this end, we introduce SVFormer, which adopts a steady pseudo-labeling framework (ie, EMA-Teacher) to cope with unlabeled video samples. While a wide range of data augmentations have been shown effective for semi-supervised image classification, they generally produce limited results for video recognition. We therefore introduce a novel augmentation strategy, Tube TokenMix, tailored for video data where video clips are mixed via a mask with consistent masked tokens over the temporal axis. In addition, we propose a temporal warping augmentation to cover the complex temporal variation in videos, which stretches selected frames to various temporal durations in the clip. Extensive experiments on three datasets Kinetics-400, UCF-101, and HMDB-51 verify the advantage of SVFormer. In particular, SVFormer outperforms the state-of-the-art by 31.5% with fewer training epochs under the 1% labeling rate of Kinetics-400. Our method can hopefully serve as a strong benchmark and encourage future search on semi-supervised action recognition with Transformer networks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">69.NaQ: Leveraging Narrations As Queries To Supervise Episodic Memory</span><br>
                <span class="as">Ramakrishnan, SanthoshKumarandAl-Halah, ZiadandGrauman, Kristen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6694-6703.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用自然语言查询（NLQ）在长者心视频中进行搜索，以增强现实和机器人技术。<br>
                    动机：由于学习问题的结构化特性（自由形式的文本查询输入，局部化的视频时间窗口输出）及其大海捞针的特性，使得其具有挑战性和昂贵的监督成本。<br>
                    方法：我们引入了叙述作为查询（NaQ）的数据增强策略，将标准的录像-文本叙述转化为视频查询定位模型的训练数据。<br>
                    效果：在Ego4D基准测试中验证了我们的想法，发现在实践中它具有巨大的影响力。NaQ大大提高了多个顶级模型的性能（甚至将其准确性提高了一倍），并在Ego4D NLQ挑战中取得了迄今为止最好的结果，全面超越了CVPR和ECCV 2022竞赛的所有挑战获胜者，并登上了当前公开排行榜的首位。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Searching long egocentric videos with natural language queries (NLQ) has compelling applications in augmented reality and robotics, where a fluid index into everything that a person (agent) has seen before could augment human memory and surface relevant information on demand. However, the structured nature of the learning problem (free-form text query inputs, localized video temporal window outputs) and its needle-in-a-haystack nature makes it both technically challenging and expensive to supervise. We introduce Narrations-as-Queries (NaQ), a data augmentation strategy that transforms standard video-text narrations into training data for a video query localization model. Validating our idea on the Ego4D benchmark, we find it has tremendous impact in practice. NaQ improves multiple top models by substantial margins (even doubling their accuracy), and yields the very best results to date on the Ego4D NLQ challenge, soundly outperforming all challenge winners in the CVPR and ECCV 2022 competitions and topping the current public leaderboard. Beyond achieving the state-of-the-art for NLQ, we also demonstrate unique properties of our approach such as the ability to perform zero-shot and few-shot NLQ, and improved performance on queries about long-tail object categories. Code and models: http://vision.cs.utexas.edu/projects/naq.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">70.Unsupervised Space-Time Network for Temporally-Consistent Segmentation of Multiple Motions</span><br>
                <span class="as">Meunier, EtienneandBouthemy, Patrick</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meunier_Unsupervised_Space-Time_Network_for_Temporally-Consistent_Segmentation_of_Multiple_Motions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22139-22148.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的主要任务之一，即运动分割，并提出一种新颖的无监督时空框架。<br>
                    动机：运动分割是计算机视觉中的重要任务，但其关键特性——时间一致性往往被忽视。<br>
                    方法：本文提出了一种新颖的、完全无监督的时空框架，用于从光流进行运动分割。具体来说，我们定义了一个3D网络来进行多次运动分割，该网络以连续的光流子体积为输入，并相应地生成一个连贯的分割图子体积。<br>
                    效果：在几个VOS基准测试上进行的实验表明，该方法在没有使用外观和任何真实数据训练的情况下，取得了令人信服的定量结果。通过可视化结果，我们还突出了我们的光流分割方法带来的短期和长期时间一致性的独特贡献。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Motion segmentation is one of the main tasks in computer vision and is relevant for many applications. The optical flow (OF) is the input generally used to segment every frame of a video sequence into regions of coherent motion. Temporal consistency is a key feature of motion segmentation, but it is often neglected. In this paper, we propose an original unsupervised spatio-temporal framework for motion segmentation from optical flow that fully investigates the temporal dimension of the problem. More specifically, we have defined a 3D network for multiple motion segmentation that takes as input a sub-volume of successive optical flows and delivers accordingly a sub-volume of coherent segmentation maps. Our network is trained in a fully unsupervised way, and the loss function combines a flow reconstruction term involving spatio-temporal parametric motion models, and a regularization term enforcing temporal consistency on the masks. We have specified an easy temporal linkage of the predicted segments. Besides, we have proposed a flexible and efficient way of coding U-nets. We report experiments on several VOS benchmarks with convincing quantitative results, while not using appearance and not training with any ground-truth data. We also highlight through visual results the distinctive contribution of the short- and long-term temporal consistency brought by our OF segmentation method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">71.iQuery: Instruments As Queries for Audio-Visual Sound Separation</span><br>
                <span class="as">Chen, JiabenandZhang, RenruiandLian, DongzeandYang, JiaqiandZeng, ZiyaoandShi, Jianbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14675-14686.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的视听分离方法存在一个共同的架构设计问题，即音频研究问题：目前的视听分离方法存在一个共同的架构设计问题，即音频编码器-解码器网络与视觉编码特征在编码器瓶颈处融合，这混淆了多模态特征编码和稳健声音解码的学习。<br>
                    动机：为了推广到新的乐器，必须对所有乐器微调整个视觉和音频网络。<br>
                    方法：我们重新定义了视听分离任务，并提出了“Instruments as Queries”(iQuery)方法，该方法具有灵活的查询扩展机制。我们利用“视觉命名”的查询来启动音频查询的学习，并使用跨模态注意力来消除估计波形中的潜在声音源干扰。为了推广到新的乐器或事件类别，我们从文本提示设计中获得灵感，插入额外的查询作为音频提示，同时冻结注意力机制。<br>
                    效果：实验结果表明，我们的iQuery方法提高了视听音源分离的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current audio-visual separation methods share a standard architecture design where an audio encoder-decoder network is fused with visual encoding features at the encoder bottleneck. This design confounds the learning of multi-modal feature encoding with robust sound decoding for audio separation. To generalize to a new instrument, one must fine-tune the entire visual and audio network for all musical instruments. We re-formulate the visual-sound separation task and propose Instruments as Queries (iQuery) with a flexible query expansion mechanism. Our approach ensures cross-modal consistency and cross-instrument disentanglement. We utilize "visually named" queries to initiate the learning of audio queries and use cross-modal attention to remove potential sound source interference at the estimated waveforms. To generalize to a new instrument or event class, drawing inspiration from the text-prompt design, we insert additional queries as audio prompts while freezing the attention mechanism. Experimental results on three benchmarks demonstrate that our iQuery improves audio-visual sound source separation performance. Code is available at https://github.com/JiabenChen/iQuery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">72.Look Around for Anomalies: Weakly-Supervised Anomaly Detection via Context-Motion Relational Learning</span><br>
                <span class="as">Cho, MyeongAhandKim, MinjungandHwang, SangwonandPark, ChaewonandLee, KyungjaeandLee, Sangyoun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Look_Around_for_Anomalies_Weakly-Supervised_Anomaly_Detection_via_Context-Motion_Relational_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12137-12146.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用弱监督的视频标注训练数据进行视频异常检测。<br>
                    动机：在现实世界中，正常和异常的界限模糊且因情况而异，难以通过单一的主干分支探索类别代表性特征。<br>
                    方法：提出类激活特征学习（CLAV）和上下文-运动关联模块（CoMo），提取依赖于类别权重的特征并扩大类别特征之间的相对差距，同时模型化周围环境和运动之间的关系，而不仅仅使用时间依赖性或运动信息。<br>
                    效果：该方法在四个基准测试中表现出最先进的性能，包括大规模的真实世界数据集，并通过定性结果和泛化能力分析展示了关系信息的重要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-supervised Video Anomaly Detection is the task of detecting frame-level anomalies using video-level labeled training data. It is difficult to explore class representative features using minimal supervision of weak labels with a single backbone branch. Furthermore, in real-world scenarios, the boundary between normal and abnormal is ambiguous and varies depending on the situation. For example, even for the same motion of running person, the abnormality varies depending on whether the surroundings are a playground or a roadway. Therefore, our aim is to extract discriminative features by widening the relative gap between classes' features from a single branch. In the proposed Class-Activate Feature Learning (CLAV), the features are extracted as per the weights that are implicitly activated depending on the class, and the gap is then enlarged through relative distance learning. Furthermore, as the relationship between context and motion is important in order to identify the anomalies in complex and diverse scenes, we propose a Context--Motion Interrelation Module (CoMo), which models the relationship between the appearance of the surroundings and motion, rather than utilizing only temporal dependencies or motion information. The proposed method shows SOTA performance on four benchmarks including large-scale real-world datasets, and we demonstrate the importance of relational information by analyzing the qualitative results and generalization ability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">73.Continuous Intermediate Token Learning With Implicit Motion Manifold for Keyframe Based Motion Interpolation</span><br>
                <span class="as">Mo, ClintonA.andHu, KunandLong, ChengjiangandWang, Zhiyong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mo_Continuous_Intermediate_Token_Learning_With_Implicit_Motion_Manifold_for_Keyframe_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13894-13903.png><br>
            
            <span class="tt"><span class="t0">研究问题：从稀疏关键帧中推导复杂的3D运动是一个特别具有挑战性的问题，因为需要保持连续性和骨骼的精确性。<br>
                    动机：现有的方法通常使用基础插值方法对关键帧进行插值以生成中间帧，这在训练过程中会导致一个微不足道的局部最小值。<br>
                    方法：本文提出了一种新的框架，通过关键帧约束来形成潜在的运动流形，考虑了中间表示的连续性。该框架包括两个阶段来确定潜在的运动子空间，即关键帧编码阶段和中间标记生成阶段，以及随后的运动合成阶段，从流形中推断和合成运动数据。<br>
                    效果：通过在LaFAN1和CMU Mocap数据集上进行的大量实验，所提出的方法在插值精度和与真实运动的高度视觉相似性方面都表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deriving sophisticated 3D motions from sparse keyframes is a particularly challenging problem, due to continuity and exceptionally skeletal precision. The action features are often derivable accurately from the full series of keyframes, and thus, leveraging the global context with transformers has been a promising data-driven embedding approach. However, existing methods are often with inputs of interpolated intermediate frame for continuity using basic interpolation methods with keyframes, which result in a trivial local minimum during training. In this paper, we propose a novel framework to formulate latent motion manifolds with keyframe-based constraints, from which the continuous nature of intermediate token representations is considered. Particularly, our proposed framework consists of two stages for identifying a latent motion subspace, i.e., a keyframe encoding stage and an intermediate token generation stage, and a subsequent motion synthesis stage to extrapolate and compose motion data from manifolds. Through our extensive experiments conducted on both the LaFAN1 and CMU Mocap datasets, our proposed method demonstrates both superior interpolation accuracy and high visual similarity to ground truth motions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">74.HierVL: Learning Hierarchical Video-Language Embeddings</span><br>
                <span class="as">Ashutosh, KumarandGirdhar, RohitandTorresani, LorenzoandGrauman, Kristen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23066-23078.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有视频-语言嵌入方法只能捕捉到几秒钟长的视频片段与其附带文本之间的短期关联性的问题。<br>
                    动机：现有的视频-语言嵌入方法只能捕捉到几秒钟长的视频片段与其附带文本之间的短期关联性，而无法同时考虑到长期和短期的关联性。<br>
                    方法：本文提出了一种新的分层视频-语言嵌入方法HierVL，该方法同时考虑了长期和短期的关联性。通过使用带有时间戳的动作描述文本以及整个视频的高级活动总结文本作为训练数据，引入了一个分层对比训练目标，鼓励在剪辑级别和视频级别上实现文本-视觉对齐。<br>
                    效果：实验结果表明，HierVL的剪辑表示优于其单层对应物，同时其在需要长期视频建模的任务上也取得了最佳结果。HierVL成功转移到多个具有挑战性的下游任务中，并在零样本和微调设置下都表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video-language embeddings are a promising avenue for injecting semantics into visual representations, but existing methods capture only short-term associations between seconds-long video clips and their accompanying text. We propose HierVL, a novel hierarchical video-language embedding that simultaneously accounts for both long-term and short-term associations. As training data, we take videos accompanied by timestamped text descriptions of human actions, together with a high-level text summary of the activity throughout the long video (as are available in Ego4D). We introduce a hierarchical contrastive training objective that encourages text-visual alignment at both the clip level and video level. While the clip-level constraints use the step-by-step descriptions to capture what is happening in that instant, the video-level constraints use the summary text to capture why it is happening, i.e., the broader context for the activity and the intent of the actor. Our hierarchical scheme yields a clip representation that outperforms its single-level counterpart, as well as a long-term video representation that achieves SotA results on tasks requiring long-term video modeling. HierVL successfully transfers to multiple challenging downstream tasks (in EPIC-KITCHENS-100, Charades-Ego, HowTo100M) in both zero-shot and fine-tuned settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">75.Watch or Listen: Robust Audio-Visual Speech Recognition With Visual Corruption Modeling and Reliability Scoring</span><br>
                <span class="as">Hong, JoannaandKim, MinsuandChoi, JeongsooandRo, YongMan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_Watch_or_Listen_Robust_Audio-Visual_Speech_Recognition_With_Visual_Corruption_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18783-18794.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了在音频和视觉输入都被破坏的情况下的视听语音识别（AVSR），这是以前的研究方向没有很好解决的问题。<br>
                    动机：以往的研究主要关注如何用干净的视觉输入来补充被破坏的音频输入，但在实际生活中，干净的视觉输入并不总是可用的，甚至可能被遮挡的嘴唇区域或噪音所破坏。<br>
                    方法：我们首先分析出以前的AVSR模型对多模态输入流的破坏并不像单模态模型那样鲁棒。然后，我们设计了多模态输入破坏模型来开发鲁棒的AVSR模型。最后，我们提出了一种新的AVSR框架，即音频-视觉可靠性评分模块（AV-RelScore），它可以确定哪个输入模态流是可靠的，也可以在预测中利用更可靠的流。<br>
                    效果：通过在流行的基准数据库LRS2和LRS3上进行全面的实验，我们评估了所提出方法的有效性。我们还发现，AV-RelScore获得的可靠性分数很好地反映了破坏的程度，并使提出的模型专注于可靠的多模态表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal input corruption situation where audio inputs and visual inputs are both corrupted, which is not well addressed in previous research directions. Previous studies have focused on how to complement the corrupted audio inputs with the clean visual inputs with the assumption of the availability of clean visual inputs. However, in real life, the clean visual inputs are not always accessible and can even be corrupted by occluded lip region or with noises. Thus, we firstly analyze that the previous AVSR models are not indeed robust to the corruption of multimodal input streams, the audio and the visual inputs, compared to uni-modal models. Then, we design multimodal input corruption modeling to develop robust AVSR models. Lastly, we propose a novel AVSR framework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that is robust to the corrupted multimodal inputs. The AV-RelScore can determine which input modal stream is reliable or not for the prediction and also can exploit the more reliable streams in prediction. The effectiveness of the proposed method is evaluated with comprehensive experiments on popular benchmark databases, LRS2 and LRS3. We also show that the reliability scores obtained by AV-RelScore well reflect the degree of corruption and make the proposed model focus on the reliable multimodal representations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">76.Real-Time Multi-Person Eyeblink Detection in the Wild for Untrimmed Video</span><br>
                <span class="as">Zeng, WenzhengandXiao, YangandWei, SichengandGan, JinfangandZhang, XintaoandCao, ZhiguoandFang, ZhiwenandZhou, JoeyTianyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Real-Time_Multi-Person_Eyeblink_Detection_in_the_Wild_for_Untrimmed_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13854-13863.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前，实时的眼动检测主要关注于修剪视频中的单人情况，但在未修剪的视频中多人场景的眼动检测在实际应用中也非常重要，但尚未得到充分关注。<br>
                    动机：为了解决这一问题，我们首次对这一研究领域进行了深入研究，并在数据集、理论和实践方面做出了重要贡献。<br>
                    方法：我们提出了一种实时的多人眼动检测方法，该方法采用端到端的学习方式，一阶段同时处理面部检测、面部追踪和人体实例级的眼动检测子任务。<br>
                    效果：我们在MPEblink数据集上进行的实验验证了实时多人眼动检测在未修剪视频中的主要挑战。我们的方法比现有的方法有显著的性能提升，并且具有很高的推理速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-time eyeblink detection in the wild can widely serve for fatigue detection, face anti-spoofing, emotion analysis, etc. The existing research efforts generally focus on single-person cases towards trimmed video. However, multi-person scenario within untrimmed videos is also important for practical applications, which has not been well concerned yet. To address this, we shed light on this research field for the first time with essential contributions on dataset, theory, and practices. In particular, a large-scale dataset termed MPEblink that involves 686 untrimmed videos with 8748 eyeblink events is proposed under multi-person conditions. The samples are captured from unconstrained films to reveal "in the wild" characteristics. Meanwhile, a real-time multi-person eyeblink detection method is also proposed. Being different from the existing counterparts, our proposition runs in a one-stage spatio-temporal way with an end-to-end learning capacity. Specifically, it simultaneously addresses the sub-tasks of face detection, face tracking, and human instance-level eyeblink detection. This paradigm holds 2 main advantages: (1) eyeblink features can be facilitated via the face's global context (e.g., head pose and illumination condition) with joint optimization and interaction, and (2) addressing these sub-tasks in parallel instead of sequential manner can save time remarkably to meet the real-time running requirement. Experiments on MPEblink verify the essential challenges of real-time multi-person eyeblink detection in the wild for untrimmed video. Our method also outperforms existing approaches by large margins and with a high inference speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">77.PDPP:Projected Diffusion for Procedure Planning in Instructional Videos</span><br>
                <span class="as">Wang, HanlinandWu, YiluandGuo, ShengandWang, Limin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PDPPProjected_Diffusion_for_Procedure_Planning_in_Instructional_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14836-14845.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了在无结构的真实生活视频中，根据当前的视觉观察进行目标导向的计划的问题。<br>
                    动机：以往的工作将此问题视为序列规划问题，并利用繁重的中间视觉观察或自然语言指令作为监督，导致复杂的学习方案和昂贵的注释成本。<br>
                    方法：我们将此问题视为分布拟合问题，用扩散模型（PDPP）来模拟整个中间动作序列分布，从而将规划问题转化为从这个分布中进行采样的过程。同时，我们移除了昂贵的中间监督，仅使用来自教学视频的任务标签作为监督。<br>
                    效果：我们的模型是一个基于U-Net的扩散模型，可以直接从给定的开始和结束观察中采样动作序列。实验表明，即使在没有任务监督的情况下，我们的PDPP模型也能在多个指标上达到最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study the problem of procedure planning in instructional videos, which aims to make goal-directed plans given the current visual observations in unstructured real-life videos. Previous works cast this problem as a sequence planning problem and leverage either heavy intermediate visual observations or natural language instructions as supervision, resulting in complex learning schemes and expensive annotation costs. In contrast, we treat this problem as a distribution fitting problem. In this sense, we model the whole intermediate action sequence distribution with a diffusion model (PDPP), and thus transform the planning problem to a sampling process from this distribution. In addition, we remove the expensive intermediate supervision, and simply use task labels from instructional videos as supervision instead. Our model is a U-Net based diffusion model, which directly samples action sequences from the learned distribution with the given start and end observations. Furthermore, we apply an efficient projection method to provide accurate conditional guides for our model during the learning and sampling process. Experiments on three datasets with different scales show that our PDPP model can achieve the state-of-the-art performance on multiple metrics, even without the task supervision. Code and trained models are available at https://github.com/MCG-NJU/PDPP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">78.ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding</span><br>
                <span class="as">Wang, LanandMittal, GauravandSajeev, SandraandYu, YeandHall, MatthewandBoddeti, VishnuNareshandChen, Mei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6575-6585.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频时序定位（VTG）任务中，由于缺乏大规模标注的未修剪视频数据集进行预训练，导致预训练特征缺乏时间边界概念，影响视频-文本对齐的问题。<br>
                    动机：目前所有的视频时序定位方法都依赖于修剪视频上预训练的视频主干特征，这主要是由于缺乏大规模的良好标注的未修剪视频数据集进行预训练。因此，预训练的特征缺乏时间边界的概念，导致视频-文本对齐在正确和错误的位置上的区别不明显。<br>
                    方法：本文提出了ProTeGe，这是第一种基于未修剪预训练的视频时序定位方法，用于弥合修剪预训练主干和下游视频时序定位任务之间的差距。ProTeGe重新配置了HowTo100M数据集，将其转换为一个视频时序定位数据集，并引入了一个新颖的视频-文本相似性基础模块和一个预训练目标，使预训练能够抵抗HowTo100M中的噪声。<br>
                    效果：通过在多个下游任务的各种监督变体上的大量实验，验证了ProTeGe预训练的特征可以显著优于修剪预训练主干的特征在视频时序定位上的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video temporal grounding (VTG) is the task of localizing a given natural language text query in an arbitrarily long untrimmed video. While the task involves untrimmed videos, all existing VTG methods leverage features from video backbones pretrained on trimmed videos. This is largely due to the lack of large-scale well-annotated VTG dataset to perform pretraining. As a result, the pretrained features lack a notion of temporal boundaries leading to the video-text alignment being less distinguishable between correct and incorrect locations. We present ProTeGe as the first method to perform VTG-based untrimmed pretraining to bridge the gap between trimmed pretrained backbones and downstream VTG tasks. ProTeGe reconfigures the HowTo100M dataset, with noisily correlated video-text pairs, into a VTG dataset and introduces a novel Video-Text Similarity-based Grounding Module and a pretraining objective to make pretraining robust to noise in HowTo100M. Extensive experiments on multiple datasets across downstream tasks with all variations of supervision validate that pretrained features from ProTeGe can significantly outperform features from trimmed pretrained backbones on VTG.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">79.MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object Tracking</span><br>
                <span class="as">Qin, ZhengandZhou, SanpingandWang, LeandDuan, JinghaiandHua, GangandTang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_MotionTrack_Learning_Robust_Short-Term_and_Long-Term_Motions_for_Multi-Object_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17939-17948.png><br>
            
            <span class="tt"><span class="t0">研究问题：多目标跟踪（MOT）的主要挑战在于为每个目标保持连续的轨迹。<br>
                    动机：现有的方法通常学习可靠的运动模式以匹配相邻帧之间的同一目标，并利用鉴别性外观特征在长时间丢失后重新识别目标。然而，密集的人群和极端的遮挡可能会轻易影响运动预测的可靠性和外观的辨别能力。<br>
                    方法：本文提出了一种简单而有效的多目标跟踪器，即MotionTrack，它在一个统一的框架中学习鲁棒的短期和长期运动，以关联从短到长范围的轨迹。对于密集的人群，设计了一个新颖的交互模块，从短期轨迹中学习交互感知的运动，可以估计每个目标的复杂移动。对于极端遮挡，构建了一个新颖的重找模块，从目标的历史轨迹中学习可靠的长期运动，可以将中断的轨迹与其相应的检测连接起来。我们的交互模块和重找模块嵌入在知名的通过检测进行跟踪的方法中，可以协同工作以保持优越的性能。<br>
                    效果：在MOT17和MOT20数据集上的大量实验结果表明，我们的方法在具有挑战性的场景中具有优越性，并在各种MOT指标上实现了最先进的性能。我们将公开代码和训练好的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The main challenge of Multi-Object Tracking (MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics. We will make the code and trained models publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">80.TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning With Structure-Trajectory Prompted Reconstruction for Person Re-Identification</span><br>
                <span class="as">Rao, HaocongandMiao, Chunyan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rao_TranSG_Transformer-Based_Skeleton_Graph_Prototype_Contrastive_Learning_With_Structure-Trajectory_Prompted_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22118-22128.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过3D骨架数据进行人员重识别，并提出了一种新的方法。<br>
                    动机：现有的方法通常设计骨架描述符或执行骨架序列表示学习，但它们不能同时建模不同的身体组件关系，也很少从身体关节的细粒度表示中探索有用的语义。<br>
                    方法：本文提出了一种基于Transformer的骨架图原型对比学习方法（TranSG），通过结构-轨迹提示重建来充分捕捉骨架图中的骨骼关系和有价值的空间-时间语义，用于人员重识别。<br>
                    效果：实验结果表明，TranSG显著优于现有的最先进方法，并在不同的图形建模、RGB估计骨架和无监督场景下显示出其通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Person re-identification (re-ID) via 3D skeleton data is an emerging topic with prominent advantages. Existing methods usually design skeleton descriptors with raw body joints or perform skeleton sequence representation learning. However, they typically cannot concurrently model different body-component relations, and rarely explore useful semantics from fine-grained representations of body joints. In this paper, we propose a generic Transformer-based Skeleton Graph prototype contrastive learning (TranSG) approach with structure-trajectory prompted reconstruction to fully capture skeletal relations and valuable spatial-temporal semantics from skeleton graphs for person re-ID. Specifically, we first devise the Skeleton Graph Transformer (SGT) to simultaneously learn body and motion relations within skeleton graphs, so as to aggregate key correlative node features into graph representations. Then, we propose the Graph Prototype Contrastive learning (GPC) to mine the most typical graph features (graph prototypes) of each identity, and contrast the inherent similarity between graph representations and different prototypes from both skeleton and sequence levels to learn discriminative graph representations. Last, a graph Structure-Trajectory Prompted Reconstruction (STPR) mechanism is proposed to exploit the spatial and temporal contexts of graph nodes to prompt skeleton graph reconstruction, which facilitates capturing more valuable patterns and graph semantics for person re-ID. Empirical evaluations demonstrate that TranSG significantly outperforms existing state-of-the-art methods. We further show its generality under different graph modeling, RGB-estimated skeletons, and unsupervised scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">81.You Can Ground Earlier Than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos</span><br>
                <span class="as">Fang, XiangandLiu, DaizongandZhou, PanandNan, Guoshun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_You_Can_Ground_Earlier_Than_See_An_Effective_and_Efficient_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2448-2460.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有视频时空句子定位方法在处理压缩视频和查询模型时的问题，如表示能力不足和训练测试计算复杂度高。<br>
                    动机：现有的视频时空句子定位方法仅关注从连续解码帧中提取的高级视觉特征，无法处理用于查询建模的压缩视频，导致表示能力不足和训练测试计算复杂度高。<br>
                    方法：本文提出了一种新的压缩域时空句子定位（TSG）设置，直接使用压缩视频而非完全解码的帧作为视觉输入。为了处理原始视频比特流输入，提出了一种新颖的三分支压缩域时空融合（TCSF）框架，提取并聚合了三种低级别的视觉特征（I帧、运动矢量和残差特征）进行有效的时空定位。<br>
                    效果：实验结果表明，TCSF在三个具有挑战性的数据集上的表现优于其他最先进的方法，同时计算复杂度更低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given an untrimmed video, temporal sentence grounding (TSG) aims to locate a target moment semantically according to a sentence query. Although previous respectable works have made decent success, they only focus on high-level visual features extracted from the consecutive decoded frames and fail to handle the compressed videos for query modelling, suffering from insufficient representation capability and significant computational complexity during training and testing. In this paper, we pose a new setting, compressed-domain TSG, which directly utilizes compressed videos rather than fully-decompressed frames as the visual input. To handle the raw video bit-stream input, we propose a novel Three-branch Compressed-domain Spatial-temporal Fusion (TCSF) framework, which extracts and aggregates three kinds of low-level visual features (I-frame, motion vector and residual features) for effective and efficient grounding. Particularly, instead of encoding the whole decoded frames like previous works, we capture the appearance representation by only learning the I-frame feature to reduce delay or latency. Besides, we explore the motion information not only by learning the motion vector feature, but also by exploring the relations of neighboring frames via the residual feature. In this way, a three-branch spatial-temporal attention layer with an adaptive motion-appearance fusion module is further designed to extract and aggregate both appearance and motion information for the final grounding. Experiments on three challenging datasets shows that our TCSF achieves better performance than other state-of-the-art methods with lower complexity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">82.Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</span><br>
                <span class="as">Zhu, LingtingandLiu, XianandLiu, XuanyuandQian, RuiandLiu, ZiweiandYu, Lequan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10544-10553.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地捕捉跨模态的音频到手势关联，并保持时间连贯性，以生成高保真度的音频驱动的共语音手势。<br>
                    动机：现有的方法主要依赖于生成对抗网络（GANs），但这种方法通常存在著名的模式崩溃和训练不稳定的问题，使得学习准确的音频-手势联合分布变得困难。<br>
                    方法：提出了一种新的基于扩散的框架，名为Diffusion Co-Speech Gesture (DiffGesture)，通过在骨架序列片段和音频上建立扩散条件生成过程，来有效地捕捉音频到手势的关联，并保持时间连贯性。<br>
                    效果：实验结果表明，DiffGesture实现了最先进的性能，能够生成具有更好模式覆盖和更强音频相关性的连贯手势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Animating virtual avatars to make co-speech gestures facilitates various applications in human-machine interaction. The existing methods mainly rely on generative adversarial networks (GANs), which typically suffer from notorious mode collapse and unstable training, thus making it difficult to learn accurate audio-gesture joint distributions. In this work, we propose a novel diffusion-based framework, named Diffusion Co-Speech Gesture (DiffGesture), to effectively capture the cross-modal audio-to-gesture associations and preserve temporal coherence for high-fidelity audio-driven co-speech gesture generation. Specifically, we first establish the diffusion-conditional generation process on clips of skeleton sequences and audio to enable the whole framework. Then, a novel Diffusion Audio-Gesture Transformer is devised to better attend to the information from multiple modalities and model the long-term temporal dependency. Moreover, to eliminate temporal inconsistency, we propose an effective Diffusion Gesture Stabilizer with an annealed noise sampling strategy. Benefiting from the architectural advantages of diffusion models, we further incorporate implicit classifier-free guidance to trade off between diversity and gesture quality. Extensive experiments demonstrate that DiffGesture achieves state-of-the-art performance, which renders coherent gestures with better mode coverage and stronger audio correlations. Code is available at https://github.com/Advocate99/DiffGesture.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">83.HaLP: Hallucinating Latent Positives for Skeleton-Based Self-Supervised Learning of Actions</span><br>
                <span class="as">Shah, AnshulandRoy, AniketandShah, KetulandMishra, ShlokandJacobs, DavidandCherian, AnoopandChellappa, Rama</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shah_HaLP_Hallucinating_Latent_Positives_for_Skeleton-Based_Self-Supervised_Learning_of_Actions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18846-18856.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在无标签的情况下训练用于动作识别的骨架序列编码器。<br>
                    动机：虽然对比学习在姿态序列上的应用已经取得了一些成果，但学习到的表示质量往往与用于构造正例的数据增强紧密相关，而对姿态序列进行数据增强是一项困难的任务。<br>
                    方法：提出了一种新的对比学习方法来训练无标签的骨架动作识别模型。主要贡献是一个新的模块HaLP，通过探索姿态的潜在空间生成新的正例。<br>
                    效果：实验表明，使用这些生成的正例在标准的对比学习框架中可以显著提高在NTU-60、NTU-120和PKU-II等基准测试中的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Supervised learning of skeleton sequence encoders for action recognition has received significant attention in recent times. However, learning such encoders without labels continues to be a challenging problem. While prior works have shown promising results by applying contrastive learning to pose sequences, the quality of the learned representations is often observed to be closely tied to data augmentations that are used to craft the positives. However, augmenting pose sequences is a difficult task as the geometric constraints among the skeleton joints need to be enforced to make the augmentations realistic for that action. In this work, we propose a new contrastive learning approach to train models for skeleton-based action recognition without labels. Our key contribution is a simple module, HaLP - to Hallucinate Latent Positives for contrastive learning. Specifically, HaLP explores the latent space of poses in suitable directions to generate new positives. To this end, we present a novel optimization formulation to solve for the synthetic positives with an explicit control on their hardness. We propose approximations to the objective, making them solvable in closed form with minimal overhead. We show via experiments that using these generated positives within a standard contrastive learning framework leads to consistent improvements across benchmarks such as NTU-60, NTU-120, and PKU-II on tasks like linear evaluation, transfer learning, and kNN evaluation. Our code can be found at https://github.com/anshulbshah/HaLP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">84.Context-Aware Relative Object Queries To Unify Video Instance and Panoptic Segmentation</span><br>
                <span class="as">Choudhuri, AnwesaandChowdhary, GirishandSchwing, AlexanderG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choudhuri_Context-Aware_Relative_Object_Queries_To_Unify_Video_Instance_and_Panoptic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6377-6386.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地处理视频帧并跨帧无缝传播对象查询，以及如何产生在时间和表达上一致的对象查询。<br>
                    动机：现有的对象查询方法无法很好地处理视频分割等时间任务，如跟踪、遮挡和物体再出现等问题。<br>
                    方法：提出“上下文相关的相对对象查询”，这是一种连续逐帧传播的对象查询方法，可以无缝跟踪物体，处理遮挡和物体再出现的问题，无需后处理。<br>
                    效果：实验结果表明，上下文相关的相对对象查询能更好地捕捉运动物体的位置变化，并在视频实例分割、多目标跟踪和分割、视频全景分割等任务上达到或超过最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object queries have emerged as a powerful abstraction to generically represent object proposals. However, their use for temporal tasks like video segmentation poses two questions: 1) How to process frames sequentially and propagate object queries seamlessly across frames. Using independent object queries per frame doesn't permit tracking, and requires post-processing. 2) How to produce temporally consistent, yet expressive object queries that model both appearance and position changes. Using the entire video at once doesn't capture position changes and doesn't scale to long videos. As one answer to both questions we propose 'context-aware relative object queries', which are continuously propagated frame-by-frame. They seamlessly track objects and deal with occlusion and re-appearance of objects, without post-processing. Further, we find context-aware relative object queries better capture position changes of objects in motion. We evaluate the proposed approach across three challenging tasks: video instance segmentation, multi-object tracking and segmentation, and video panoptic segmentation. Using the same approach and architecture, we match or surpass state-of-the art results on the diverse and challenging OVIS, Youtube-VIS, Cityscapes-VPS, MOTS 2020 and KITTI-MOTS data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">85.Learning To Dub Movies via Hierarchical Prosody Models</span><br>
                <span class="as">Cong, GaoxiangandLi, LiangandQi, YuankaiandZha, Zheng-JunandWu, QiandWang, WenyuandJiang, BinandYang, Ming-HsuanandHuang, Qingming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cong_Learning_To_Dub_Movies_via_Hierarchical_Prosody_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14687-14697.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone, V2C) task aims to generate speeches that match the speaker's emotion presented in the video using the desired speaker voice as reference. V2C is more challenging than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the varying emotions and speaking speed presented in the video. Unlike previous works, we propose a novel movie dubbing architecture to tackle these problems via hierarchical prosody modeling, which bridges the visual information to corresponding speech prosody from three aspects: lip, face, and scene. Specifically, we align lip movement to the speech duration, and convey facial expression to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by the psychology findings. Moreover, we design an emotion booster to capture the atmosphere from global video scenes. All these embeddings are used together to generate mel-spectrogram, which is then converted into speech waves by an existing vocoder. Extensive experimental results on the V2C and Chem benchmark datasets demonstrate the favourable performance of the proposed method. The code and trained models will be made available at https://github.com/GalaxyCong/HPMDubbing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">86.Progressive Spatio-Temporal Alignment for Efficient Event-Based Motion Estimation</span><br>
                <span class="as">Huang, XueyanandZhang, YueyiandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Progressive_Spatio-Temporal_Alignment_for_Efficient_Event-Based_Motion_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1537-1546.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种高效的基于事件的动作估计框架，用于各种动作模型。<br>
                    动机：与以往工作不同，我们设计了一种渐进的事件到映射对齐方案，并利用时空相关性进行对齐。<br>
                    方法：我们逐步将一个事件批次中采样的事件对齐到时间-表面图，并通过最小化一种新的时间-表面损失来获取更新的动作模型。此外，我们还应用了动态的批量大小策略，以自适应地调整批量大小，使批次中的所有事件都与当前的动作模型保持一致。<br>
                    效果：我们的框架有三个优点：a) 渐进的方案迭代地细化动作参数，实现了精确的动作估计；b) 在一个迭代中，只有一小部分事件参与优化，大大减少了总运行时间；c) 动态的批量大小策略确保恒定速度假设始终成立。我们在具有三个动作模型（旋转、单应和6自由度）的挑战性高速场景上进行了全面实验。实验结果表明，我们的框架在估计精度和效率上都达到了最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose an efficient event-based motion estimation framework for various motion models. Different from previous works, we design a progressive event-to-map alignment scheme and utilize the spatio-temporal correlations to align events. In detail, we progressively align sampled events in an event batch to the time-surface map and obtain the updated motion model by minimizing a novel time-surface loss. In addition, a dynamic batch size strategy is applied to adaptively adjust the batch size so that all events in the batch are consistent with the current motion model. Our framework has three advantages: a) the progressive scheme refines motion parameters iteratively, achieving accurate motion estimation; b) within one iteration, only a small portion of events are involved in optimization, which greatly reduces the total runtime; c) the dynamic batch size strategy ensures that the constant velocity assumption always holds. We conduct comprehensive experiments to evaluate our framework on challenging high-speed scenes with three motion models: rotational, homography, and 6-DOF models. Experimental results demonstrate that our framework achieves state-of-the-art estimation accuracy and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">87.VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud</span><br>
                <span class="as">Wang, ZiqinandCheng, BowenandZhao, LichenandXu, DongandTang, YangandSheng, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VL-SAT_Visual-Linguistic_Semantics_Assisted_Training_for_3D_Semantic_Scene_Graph_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21560-21569.png><br>
            
            <span class="tt"><span class="t0">研究问题：在点云中进行3D语义场景图预测的任务具有挑战性，因为与2D图像相比，3D点云只捕捉到有限的语义几何结构，且长尾关系分布阻碍了无偏预测的学习。<br>
                    动机：由于2D图像提供丰富的语义信息，而场景图本质上是使用语言处理的，因此本研究提出视觉-语言语义辅助训练（VL-SAT）方案，以显著增强3DSSG预测模型对长尾和模糊语义关系的辨别能力。<br>
                    方法：我们训练了一个强大的多模态神谕模型来协助3D模型。这个神谕模型基于视觉、语言和3D几何的语义学习可靠的结构表示，其优势可以在训练阶段异构地传递给3D模型。通过有效地利用视觉-语言语义进行训练，我们的VL-SAT可以显著提升仅使用3D输入进行推理的常见3DSSG预测模型，特别是在处理长尾关系三元组时。<br>
                    效果：我们在3DSSG数据集上进行了全面评估和消融研究，验证了所提出方案的有效性。代码可在https://github.com/wz7in/CVPR2023-VLSAT获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of 3D semantic scene graph (3DSSG) prediction in the point cloud is challenging since (1) the 3D point cloud only captures geometric structures with limited semantics compared to 2D images, and (2) long-tailed relation distribution inherently hinders the learning of unbiased prediction. Since 2D images provide rich semantics and scene graphs are in nature coped with languages, in this study, we propose Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme that can significantly empower 3DSSG prediction models with discrimination about long-tailed and ambiguous semantic relations. The key idea is to train a powerful multi-modal oracle model to assist the 3D model. This oracle learns reliable structural representations based on semantics from vision, language, and 3D geometry, and its benefits can be heterogeneously passed to the 3D model during the training stage. By effectively utilizing visual-linguistic semantics in training, our VL-SAT can significantly boost common 3DSSG prediction models, such as SGFN and SGGpoint, only with 3D inputs in the inference stage, especially when dealing with tail relation triplets. Comprehensive evaluations and ablation studies on the 3DSSG dataset have validated the effectiveness of the proposed scheme. Code is available at https://github.com/wz7in/CVPR2023-VLSAT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">88.Learning Emotion Representations From Verbal and Nonverbal Communication</span><br>
                <span class="as">Zhang, SitaoandPan, YimuandWang, JamesZ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18993-19004.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决人工智能中情感理解这一重要但极具挑战性的问题，特别是缺乏大量标注数据集的问题。<br>
                    动机：现有的情感理解方法主要依赖于数值标签或描述，而人类的情感信息是通过语言和非语言交流自然包含的。因此，从交流中提取情感表示更符合人类的学习过程。<br>
                    方法：本文提出了EmotionCLIP，这是第一种仅使用未分类数据从言语和非言语交流中提取视觉情感表示的预训练范式。通过主题感知的上下文编码引导EmotionCLIP关注非语言情感线索，使用情感引导的对比学习方法获取语言情感线索。<br>
                    效果：大量的实验验证了EmotionCLIP的有效性和可转移性。在仅仅使用线性探测评估协议的情况下，EmotionCLIP在各种基准测试中优于最先进的有监督视觉情感识别方法，并与许多多模态方法相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Emotion understanding is an essential but highly challenging component of artificial general intelligence. The absence of extensive annotated datasets has significantly impeded advancements in this field. We present EmotionCLIP, the first pre-training paradigm to extract visual emotion representations from verbal and nonverbal communication using only uncurated data. Compared to numerical labels or descriptions used in previous methods, communication naturally contains emotion information. Furthermore, acquiring emotion representations from communication is more congruent with the human learning process. We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emotion cues using sentiment-guided contrastive learning. Extensive experiments validate the effectiveness and transferability of EmotionCLIP. Using merely linear-probe evaluation protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and rivals many multimodal approaches across various benchmarks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related domains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">89.Blur Interpolation Transformer for Real-World Motion From Blur</span><br>
                <span class="as">Zhong, ZhihangandCao, MingdengandJi, XiangandZheng, YinqiangandSato, Imari</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhong_Blur_Interpolation_Transformer_for_Real-World_Motion_From_Blur_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5713-5723.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从模糊中恢复运动这一挑战性问题，也被称为联合去模糊和插值或模糊时间超分辨率。<br>
                    动机：当前的方法在视觉质量上仍有改进空间，尤其是在合成数据集上，并且对真实世界数据的泛化能力较差。<br>
                    方法：我们提出了一种模糊插值变换器（BiT），以有效揭示编码在模糊中的潜在时间相关性。基于多尺度残差Swin变换器块，我们引入了双端时间监督和时间对称集成策略，以生成用于时变运动渲染的有效特征。此外，我们还设计了一个混合相机系统，收集了第一个一对多的模糊-清晰视频对的真实世界数据集。<br>
                    效果：实验结果表明，BiT在公开的Adobe240数据集上比最先进的方法有显著的改进。此外，提出的真实世界数据集有效地帮助模型泛化到真实的模糊场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies the challenging problem of recovering motion from blur, also known as joint deblurring and interpolation or blur temporal super-resolution. The challenges are twofold: 1) the current methods still leave considerable room for improvement in terms of visual quality even on the synthetic dataset, and 2) poor generalization to real-world data. To this end, we propose a blur interpolation transformer (BiT) to effectively unravel the underlying temporal correlation encoded in blur. Based on multi-scale residual Swin transformer blocks, we introduce dual-end temporal supervision and temporally symmetric ensembling strategies to generate effective features for time-varying motion rendering. In addition, we design a hybrid camera system to collect the first real-world dataset of one-to-many blur-sharp video pairs. Experimental results show that BiT has a significant gain over the state-of-the-art methods on the public dataset Adobe240. Besides, the proposed real-world dataset effectively helps the model generalize well to real blurry scenarios. Code and data are available at https://github.com/zzh-tech/BiT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">90.Procedure-Aware Pretraining for Instructional Video Understanding</span><br>
                <span class="as">Zhou, HongluandMart{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Procedure-Aware_Pretraining_for_Instructional_Video_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10727-10738.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从无标签的视频中提取出程序性知识，如任务的身份（例如，“制作拿铁”），步骤（例如，“倒牛奶”）或在执行过程中可能的下一步。<br>
                    动机：由于可用注释数量有限，因此从无标签视频中提取程序性知识是理解教学视频中过程的一个关键挑战。<br>
                    方法：通过将来自基于文本的程序性知识数据库和未标记的教学视频语料库的信息相结合来构建一个过程知识图（PKG），然后使用它生成伪标签来训练一种编码过程知识的易于理解的视频表示形式，以便推广到多个过程理解任务。<br>
                    效果：Paprika模型在COIN和CrossTask上的任务识别、步骤识别和步骤预测等过程理解任务上的表现优于现有技术，准确率提高了11.23%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Our goal is to learn a video representation that is useful for downstream procedure understanding tasks in instructional videos. Due to the small amount of available annotations, a key challenge in procedure understanding is to be able to extract from unlabeled videos the procedural knowledge such as the identity of the task (e.g., 'make latte'), its steps (e.g., 'pour milk'), or the potential next steps given partial progress in its execution. Our main insight is that instructional videos depict sequences of steps that repeat between instances of the same or different tasks, and that this structure can be well represented by a Procedural Knowledge Graph (PKG), where nodes are discrete steps and edges connect steps that occur sequentially in the instructional activities. This graph can then be used to generate pseudo labels to train a video representation that encodes the procedural knowledge in a more accessible form to generalize to multiple procedure understanding tasks. We build a PKG by combining information from a text-based procedural knowledge database and an unlabeled instructional video corpus and then use it to generate training pseudo labels with four novel pre-training objectives. We call this PKG-based pre-training procedure and the resulting model Paprika, Procedure-Aware PRe-training for Instructional Knowledge Acquisition. We evaluate Paprika on COIN and CrossTask for procedure understanding tasks such as task recognition, step recognition, and step forecasting. Paprika yields a video representation that improves over the state of the art: up to 11.23% gains in accuracy in 12 evaluation settings. Implementation is available at https://github.com/salesforce/paprika.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">91.Therbligs in Action: Video Understanding Through Motion Primitives</span><br>
                <span class="as">Dessalene, EadomandMaynord, MichaelandFerm\&quot;uller, CorneliaandAloimonos, Yiannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dessalene_Therbligs_in_Action_Video_Understanding_Through_Motion_Primitives_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10618-10626.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文介绍了一种基于规则、组合和分层的动作建模方法，使用Therbligs作为基本单位。<br>
                    动机：现有的动作表示方法在表达一致性和联系中心性方面存在不足，因此提出了使用Therbligs进行动作表示的方法。<br>
                    方法：通过引入可微分的规则推理方法对逻辑一致性进行正则化，构建了一个以Therbligs为中心的模型。<br>
                    效果：在两个流行的视频数据集EPIC Kitchens 100和50-Salads上进行了实验，观察到平均相对改进分别为10.5%/7.53%/6.5%和8.9%/6.63%/4.8%。同时，该方法可以与其他现有架构的表示互补，不会取代它们。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper we introduce a rule-based, compositional, and hierarchical modeling of action using Therbligs as our atoms. Introducing these atoms provides us with a consistent, expressive, contact-centered representation of action. Over the atoms we introduce a differentiable method of rule-based reasoning to regularize for logical consistency. Our approach is complementary to other approaches in that the Therblig-based representations produced by our architecture augment rather than replace existing architectures' representations. We release the first Therblig-centered annotations over two popular video datasets - EPIC Kitchens 100 and 50-Salads. We also broadly demonstrate benefits to adopting Therblig representations through evaluation on the following tasks: action segmentation, action anticipation, and action recognition - observing an average 10.5%/7.53%/6.5% relative improvement, respectively, over EPIC Kitchens and an average 8.9%/6.63%/4.8% relative improvement, respectively, over 50 Salads. Code and data will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">92.Tell Me What Happened: Unifying Text-Guided Video Completion via Multimodal Masked Video Generation</span><br>
                <span class="as">Fu, Tsu-JuiandYu, LichengandZhang, NingandFu, Cheng-YangandSu, Jong-ChyiandWang, WilliamYangandBell, Sean</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Tell_Me_What_Happened_Unifying_Text-Guided_Video_Completion_via_Multimodal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10681-10692.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用自然语言指导完成视频生成任务，包括视频预测、倒放和填充。<br>
                    动机：目前的模型在处理视频生成任务时，往往忽视了时间连贯性的问题，而基于少量帧的提示可能有多种结果，因此需要一种能遵循自然语言进行视频完成的系统来提高可控性。<br>
                    方法：提出一种新的任务——文本引导的视频完成（TVC），并设计了多模态掩蔽视频生成（MMVG）模型来解决这一问题。在训练过程中，MMVG将视频帧离散化为视觉标记并进行大部分掩蔽以完成任意时间点的视频生成；在推理阶段，通过应用相应的掩蔽条件，一个MMVG模型就可以解决TVC的所有三种情况。<br>
                    效果：在多种视频场景下进行评估，包括自我中心、动画和游戏等，实验结果表明MMVG能有效利用文本指导完成高质量的视频生成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating a video given the first several static frames is challenging as it anticipates reasonable future frames with temporal coherence. Besides video prediction, the ability to rewind from the last frame or infilling between the head and tail is also crucial, but they have rarely been explored for video completion. Since there could be different outcomes from the hints of just a few frames, a system that can follow natural language to perform video completion may significantly improve controllability. Inspired by this, we introduce a novel task, text-guided video completion (TVC), which requests the model to generate a video from partial frames guided by an instruction. We then propose Multimodal Masked Video Generation (MMVG) to address this TVC task. During training, MMVG discretizes the video frames into visual tokens and masks most of them to perform video completion from any time point. At inference time, a single MMVG model can address all 3 cases of TVC, including video prediction, rewind, and infilling, by applying corresponding masking conditions. We evaluate MMVG in various video scenarios, including egocentric, animation, and gaming. Extensive experimental results indicate that MMVG is effective in generating high-quality visual appearances with text guidance for TVC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">93.Graph Representation for Order-Aware Visual Transformation</span><br>
                <span class="as">Qiu, YueandSun, YanjunandMatsuzawa, FumiyaandIwata, KenjiandKataoka, Hirokatsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Graph_Representation_for_Order-Aware_Visual_Transformation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22793-22802.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何发现图像对之间的变化及其时间顺序，这是人类认知的基本方面。<br>
                    动机：尽管现有的AI系统已经能够识别和描述图像对之间的变化，但它们主要考虑的是同步发生的变化，忽视了这些变化中可能存在的顺序。<br>
                    方法：我们首先提出了一个视觉变换图结构来传达有顺序的变化，然后在我们的新生成的数据集上对以前的方法进行基准测试，并找出了现有方法在识别变化顺序方面的问题。最后，我们通过引入一个新的模型，该模型明确地关联不同的变化，然后在图形表示中识别变化及其顺序，从而显著提高了有顺序的变化识别能力。<br>
                    效果：实验结果表明，我们的新模型在识别图像对之间有顺序的变化方面取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a new visual reasoning formulation that aims at discovering changes between image pairs and their temporal orders. Recognizing scene dynamics and their chronological orders is a fundamental aspect of human cognition. The aforementioned abilities make it possible to follow step-by-step instructions, reason about and analyze events, recognize abnormal dynamics, and restore scenes to their previous states. However, it remains unclear how well current AI systems perform in these capabilities. Although a series of studies have focused on identifying and describing changes from image pairs, they mainly consider those changes that occur synchronously, thus neglecting potential orders within those changes. To address the above issue, we first propose a visual transformation graph structure for conveying order-aware changes. Then, we benchmarked previous methods on our newly generated dataset and identified the issues of existing methods for change order recognition. Finally, we show a significant improvement in order-aware change recognition by introducing a new model that explicitly associates different changes and then identifies changes and their orders in a graph representation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">94.Exploring Discontinuity for Video Frame Interpolation</span><br>
                <span class="as">Lee, SangjinandLee, HyeongminandShin, ChajinandSon, HanbinandLee, Sangyoun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Exploring_Discontinuity_for_Video_Frame_Interpolation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9791-9800.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频帧插值（VFI）任务中，现有深度学习模型对非连续运动物体（如标志、用户界面和字幕）的处理能力不足的问题。<br>
                    动机：大部分现有的VFI研究主要关注于如何进行适当的帧扭曲操作和优化扭曲后的帧，但这些方法在处理包含非连续运动物体的实际视频时效果不佳。<br>
                    方法：本文提出了三种技术来增强现有的深度学习基础的VFI架构对这些元素的鲁棒性。首先，提出了一种新的数据增强策略——图形文本混合（FTM），使模型在训练阶段学习非连续运动，而无需额外的数据集。其次，提出了一种简单但有效的模块，该模块预测一个名为“不连续性图”（D-map）的映射，该映射可以密集地区分连续和非连续运动区域。最后，提出了针对非连续运动区域的监督损失函数，这些损失函数可以与FTM和D-map一起使用。<br>
                    效果：通过应用到各种最新的VFI网络，该方法不仅在GDM数据集上显著提高了插值质量，而且在只包含连续运动的现有基准测试集（如Vimeo90K、UCF101和DAVIS）上也取得了显著的改善。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video frame interpolation (VFI) is the task that synthesizes the intermediate frame given two consecutive frames. Most of the previous studies have focused on appropriate frame warping operations and refinement modules for the warped frames. These studies have been conducted on natural videos containing only continuous motions. However, many practical videos contain various unnatural objects with discontinuous motions such as logos, user interfaces and subtitles. We propose three techniques that can make the existing deep learning-based VFI architectures robust to these elements. First is a novel data augmentation strategy called figure-text mixing (FTM) which can make the models learn discontinuous motions during training stage without any extra dataset. Second, we propose a simple but effective module that predicts a map called discontinuity map (D-map), which densely distinguishes between areas of continuous and discontinuous motions. Lastly, we propose loss functions to give supervisions of the discontinuous motion areas which can be applied along with FTM and D-map. We additionally collect a special test benchmark called Graphical Discontinuous Motion (GDM) dataset consisting of some mobile games and chatting videos. Applied to the various state-of-the-art VFI networks, our method significantly improves the interpolation qualities on the videos from not only GDM dataset, but also the existing benchmarks containing only continuous motions such as Vimeo90K, UCF101, and DAVIS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">95.DynamicStereo: Consistent Dynamic Depth From Stereo Videos</span><br>
                <span class="as">Karaev, NikitaandRocco, IgnacioandGraham, BenjaminandNeverova, NataliaandVedaldi, AndreaandRupprecht, Christian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karaev_DynamicStereo_Consistent_Dynamic_Depth_From_Stereo_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13229-13239.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从立体相机观察到的动态场景中重建深度信息，并解决现有方法在时间一致性上的不足。<br>
                    动机：为了提高沉浸式AR或VR场景中的用户体验，需要预测出具有时间一致性的深度信息。<br>
                    方法：提出DynamicStereo，一种基于变压器的新型架构，通过学习邻近帧的信息来提高预测的时间一致性，并通过分割注意力层有效处理立体视频。<br>
                    效果：创建了Dynamic Replica数据集，用于训练和评估更接近真实应用的动态立体方法。使用此数据集进行训练后，所提出的DynamicStereo以及先前的方法的预测质量都有所提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the problem of reconstructing a dynamic scene observed from a stereo camera. Most existing methods for depth from stereo treat different stereo frames independently, leading to temporally inconsistent depth predictions. Temporal consistency is especially important for immersive AR or VR scenarios, where flickering greatly diminishes the user experience. We propose DynamicStereo, a novel transformer-based architecture to estimate disparity for stereo videos. The network learns to pool information from neighboring frames to improve the temporal consistency of its predictions. Our architecture is designed to process stereo videos efficiently through divided attention layers. We also introduce Dynamic Replica, a new benchmark dataset containing synthetic videos of people and animals in scanned environments, which provides complementary training and evaluation data for dynamic stereo closer to real applications than existing datasets. Training with this dataset further improves the quality of predictions of our proposed DynamicStereo as well as prior methods. Finally, it acts as a benchmark for consistent stereo methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">96.Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition</span><br>
                <span class="as">Wang, HanyangandLi, BoandWu, ShuangandShen, SiyuanandLiu, FengandDing, ShouhongandZhou, Aimin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Learning_Paradigm_for_Dynamic_Facial_Expression_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17958-17968.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地识别视频中的动态面部表情。<br>
                    动机：以前的研究将非目标帧视为噪声，但作者认为应将其视为弱监督问题。同时，作者发现在动态面部表情识别中存在短期和长期时间关系的不平衡。<br>
                    方法：提出了多实例学习（MIL）的多3D动态面部表情学习（M3DFEL）框架，用于处理不精确的标签。该框架生成3D实例来模拟强烈的短期时间关系，并使用3DCNN进行特征提取。然后利用动态长期实例聚合模块（DLIAM）学习长期时间关系并动态聚合实例。<br>
                    效果：在DFER和FERV39K数据集上的实验表明，M3DFEL优于现有的最先进的方法，具有基本的R3D18主干网络。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dynamic Facial Expression Recognition (DFER) is a rapidly developing field that focuses on recognizing facial expressions in video format. Previous research has considered non-target frames as noisy frames, but we propose that it should be treated as a weakly supervised problem. We also identify the imbalance of short- and long-term temporal relationships in DFER. Therefore, we introduce the Multi-3D Dynamic Facial Expression Learning (M3DFEL) framework, which utilizes Multi-Instance Learning (MIL) to handle inexact labels. M3DFEL generates 3D-instances to model the strong short-term temporal relationship and utilizes 3DCNNs for feature extraction. The Dynamic Long-term Instance Aggregation Module (DLIAM) is then utilized to learn the long-term temporal relationships and dynamically aggregate the instances. Our experiments on DFEW and FERV39K datasets show that M3DFEL outperforms existing state-of-the-art approaches with a vanilla R3D18 backbone. The source code is available at https://github.com/faceeyes/M3DFEL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">97.Ham2Pose: Animating Sign Language Notation Into Pose Sequences</span><br>
                <span class="as">Arkushin, RotemShalevandMoryossef, AmitandFried, Ohad</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Arkushin_Ham2Pose_Animating_Sign_Language_Notation_Into_Pose_Sequences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21046-21056.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将口语转化为手语，以实现听力正常和听力受损群体之间的开放交流。<br>
                    动机：为了解决这一问题，我们提出了一种将HamNoSys文本动画化的方法，这是一种通用的手势语言符号表示法。<br>
                    方法：我们的方法使用transformer编码器逐步生成姿势预测，同时考虑文本和姿势的空间和时间信息。我们使用弱监督进行训练，并展示了该方法可以从部分和不准确的数据中学习。<br>
                    效果：我们提供了一种新的距离测量方法，用于衡量缺失关键点的姿势序列之间的距离。我们在大型手语数据集AUTSL上验证了其正确性，并证明它比现有测量方法更准确地衡量姿势序列之间的距离。我们的代码已公开发布，供未来研究使用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correctness using AUTSL, a large-scale Sign language dataset, show that it measures the distance between pose sequences more accurately than existing measurements, and use it to assess the quality of our generated pose sequences. Code for the data pre-processing, the model, and the distance measurement is publicly released for future research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">98.&#x27;&#x27;Seeing&#x27;&#x27; Electric Network Frequency From Events</span><br>
                <span class="as">Xu, LexuanandHua, GuangandZhang, HaijianandYu, LeiandQiao, Ning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Seeing_Electric_Network_Frequency_From_Events_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18022-18031.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从常规视频中估计电网频率（ENF）？<br>
                    动机：现有的基于视频的ENF（V-ENF）估计方法受到摄像质量、非理想采样、运动和极端照明条件的影响。<br>
                    方法：本文提出了一种无需上述限制的新方法，即事件相机，这是一种神经形态传感器，可以编码光强变化并以极高的时间分辨率和动态范围异步发出事件。我们首先验证了事件中捕获的ENF的物理机制，然后通过模式过滤和谐波增强提出了一种简单而稳健的事件基础ENF（E-ENF）估计方法。<br>
                    效果：我们在各种场景中创建了一个事件视频ENF数据集（EV-ENFD），并在其上进行了大量实验。实验结果表明，我们提出的方法能够提取更准确的ENF轨迹，大大超过传统的V-ENF，特别是在有物体运动和极端照明条件的挑战性环境中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most of the artificial lights fluctuate in response to the grid's alternating current and exhibit subtle variations in terms of both intensity and spectrum, providing the potential to estimate the Electric Network Frequency (ENF) from conventional frame-based videos. Nevertheless, the performance of Video-based ENF (V-ENF) estimation largely relies on the imaging quality and thus may suffer from significant interference caused by non-ideal sampling, motion, and extreme lighting conditions. In this paper, we show that the ENF can be extracted without the above limitations from a new modality provided by the so-called event camera, a neuromorphic sensor that encodes the light intensity variations and asynchronously emits events with extremely high temporal resolution and high dynamic range. Specifically, we first formulate and validate the physical mechanism for the ENF captured in events, and then propose a simple yet robust Event-based ENF (E-ENF) estimation method through mode filtering and harmonic enhancement. Furthermore, we build an Event-Video ENF Dataset (EV-ENFD) that records both events and videos in diverse scenes. Extensive experiments on EV-ENFD demonstrate that our proposed E-ENF method can extract more accurate ENF traces, outperforming the conventional V-ENF by a large margin, especially in challenging environments with object motions and extreme lighting conditions. The code and dataset are available at https://github.com/xlx-creater/E-ENF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">99.Sound to Visual Scene Generation by Audio-to-Visual Latent Alignment</span><br>
                <span class="as">Sung-Bin, KimandSenocak, ArdaandHa, HyunwooandOwens, AndrewandOh, Tae-Hyun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sung-Bin_Sound_to_Visual_Scene_Generation_by_Audio-to-Visual_Latent_Alignment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6430-6440.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过声音生成场景图像？<br>
                    动机：解决视觉和听觉之间存在的巨大信息鸿沟。<br>
                    方法：设计一种模型，通过学习将音频与视觉模式关联起来，以弥补信息差距。主要思想是通过学习将音频对齐到视觉潜在空间来丰富音频特征。<br>
                    效果：在VEGAS和VGGSound数据集上取得了比先前方法更好的结果，并且可以通过对输入波形或潜在空间进行简单操作来控制模型的预测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How does audio describe the world around us? In this paper, we propose a method for generating an image of a scene from sound. Our method addresses the challenges of dealing with the large gaps that often exist between sight and sound. We design a model that works by scheduling the learning procedure of each model component to associate audio-visual modalities despite their information gaps. The key idea is to enrich the audio features with visual information by learning to align audio to visual latent space. We translate the input audio to visual features, then use a pre-trained generator to produce an image. To further improve the quality of our generated images, we use sound source localization to select the audio-visual pairs that have strong cross-modal correlations. We obtain substantially better results on the VEGAS and VGGSound datasets than prior approaches. We also show that we can control our model's predictions by applying simple manipulations to the input waveform, or to the latent space.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">100.Text With Knowledge Graph Augmented Transformer for Video Captioning</span><br>
                <span class="as">Gu, XinandChen, GuangandWang, YufeiandZhang, LiboandLuo, TiejianandWen, Longyin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Text_With_Knowledge_Graph_Augmented_Transformer_for_Video_Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18941-18951.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频字幕生成旨在使用自然语言描述视频内容，但面临长尾和开放词集问题。<br>
                    动机：为了解决这些问题，本文提出了一种结合知识图谱的文本与视频字幕生成模型（TextKG）。<br>
                    方法：TextKG是一个双流变压器模型，由外部流和内部流组成。外部流吸收外部知识，如预构建的知识图谱，以缓解开放词集的挑战；内部流则利用原始视频的多模态信息，如视频帧的外观、语音转录和视频字幕，以处理长尾问题。两个流之间还使用了交叉注意力机制来共享信息。<br>
                    效果：在四个具有挑战性的视频字幕生成数据集上进行的大量实验表明，该方法优于最先进的方法。具体来说，TextKG方法在YouCookII数据集上的表现比最佳已发布结果提高了18.7%的绝对CIDEr分数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video captioning aims to describe the content of videos using natural language. Although significant progress has been made, there is still much room to improve the performance for real-world applications, mainly due to the long-tail and open set issues of words. In this paper, we propose a text with knowledge graph augmented transformer (TextKG) for video captioning. Notably, TextKG is a two-stream transformer, formed by the external stream and internal stream. The external stream is designed to absorb external knowledge, which models the interactions between the external knowledge, e.g., pre-built knowledge graph, and the built-in information of videos, e.g., the salient object regions, speech transcripts, and video captions, to mitigate the open set of words challenge. Meanwhile, the internal stream is designed to exploit the multi-modality information in original videos (e.g., the appearance of video frames, speech transcripts, and video captions) to deal with the long-tail issue. In addition, the cross attention mechanism is also used in both streams to share information. In this way, the two streams can help each other for more accurate results. Extensive experiments conducted on four challenging video captioning datasets, i.e., YouCookII, ActivityNet Captions, MSR-VTT, and MSVD, demonstrate that the proposed method performs favorably against the state-of-the-art methods. Specifically, the proposed TextKG method outperforms the best published results by improving 18.7% absolute CIDEr scores on the YouCookII dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">101.IS-GGT: Iterative Scene Graph Generation With Generative Transformers</span><br>
                <span class="as">Kundu, SanjoyandAakur, SathyanarayananN.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kundu_IS-GGT_Iterative_Scene_Graph_Generation_With_Generative_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6292-6301.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从图像中生成场景图，同时减少计算开销？<br>
                    动机：现有的场景图生成方法需要对所有可能的对象间边进行标注，这增加了计算负担。<br>
                    方法：提出一种基于生成性变压器的场景图生成方法，首先从检测到的对象及其视觉特征中采样可能的场景图结构，然后对采样的边进行谓词分类以生成最终的场景图。<br>
                    效果：在Visual Genome数据集上的大量实验表明，该方法在场景图生成（SGG）方面效率高，平均mR@100达到20.7%，优于最先进的SGG方法，与无偏SGG方法具有竞争力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scene graphs provide a rich, structured representation of a scene by encoding the entities (objects) and their spatial relationships in a graphical format. This representation has proven useful in several tasks, such as question answering, captioning, and even object detection, to name a few. Current approaches take a generation-by-classification approach where the scene graph is generated through labeling of all possible edges between objects in a scene, which adds computational overhead to the approach. This work introduces a generative transformer-based approach to generating scene graphs beyond link prediction. Using two transformer-based components, we first sample a possible scene graph structure from detected objects and their visual features. We then perform predicate classification on the sampled edges to generate the final scene graph. This approach allows us to efficiently generate scene graphs from images with minimal inference overhead. Extensive experiments on the Visual Genome dataset demonstrate the efficiency of the proposed approach. Without bells and whistles, we obtain, on average, 20.7% mean recall (mR@100) across different settings for scene graph generation (SGG), outperforming state-of-the-art SGG approaches while offering competitive performance to unbiased SGG approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">102.SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition</span><br>
                <span class="as">Fan, XinqiandChen, XueliandJiang, MingjieandShahid, AliRazaandYan, Hong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_SelfME_Self-Supervised_Motion_Learning_for_Micro-Expression_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13834-13843.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习进行面部微表情识别，并克服传统方法需要预处理的问题。<br>
                    动机：面部微表情能反映人的真实情绪，对于测谎、犯罪分析等领域有重要价值。现有的深度学习方法需要借助传统的光流法提取面部运动作为输入，存在局限性。<br>
                    方法：提出一种基于自我监督学习的面部微表情识别框架（SelfME），首次使用自动自学的运动技术进行面部微表情识别。同时，为解决学习过程中可能忽视面部左右对称动作的问题，开发了一种对称对比视觉变换器（SCViT）来约束面部左右部分相似动作特征的学习。<br>
                    效果：在两个基准数据集上进行的实验表明，该方法取得了最先进的性能，消融研究证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Facial micro-expressions (MEs) refer to brief spontaneous facial movements that can reveal a person's genuine emotion. They are valuable in lie detection, criminal analysis, and other areas. While deep learning-based ME recognition (MER) methods achieved impressive success, these methods typically require pre-processing using conventional optical flow-based methods to extract facial motions as inputs. To overcome this limitation, we proposed a novel MER framework using self-supervised learning to extract facial motion for ME (SelfME). To the best of our knowledge, this is the first work using an automatically self-learned motion technique for MER. However, the self-supervised motion learning method might suffer from ignoring symmetrical facial actions on the left and right sides of faces when extracting fine features. To address this issue, we developed a symmetric contrastive vision transformer (SCViT) to constrain the learning of similar facial action features for the left and right parts of faces. Experiments were conducted on two benchmark datasets showing that our method achieved state-of-the-art performance, and ablation studies demonstrated the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">103.NewsNet: A Novel Dataset for Hierarchical Temporal Segmentation</span><br>
                <span class="as">Wu, HaoqianandChen, KeyuandLiu, HaozheandZhuge, MingchenandLi, BingandQiao, RuizhiandShu, XiujunandGan, BeiandXu, LiangshengandRen, BoandXu, MengmengandZhang, WentianandRamachandra, RaghavendraandLin, Chia-WenandGhanem, Bernard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10669-10680.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有视频分析方法在处理复杂结构视频时，无法全面理解较大时间跨度的问题。<br>
                    动机：现有的视频分析方法虽然可以对视频进行细致的切割，但在处理复杂和结构化的视频时，缺乏对更大时间跨度的全面理解。<br>
                    方法：本文提出了两种抽象级别的时间视频分割，并研究了它们与现有细粒度级别的层次关系。同时收集了NewsNet，这是一个包含1000个视频、900多小时的新闻视频数据集，用于进行分层的时间视频分割。<br>
                    效果：通过在NewsNet上的研究，可以增进对复杂结构化视频的理解，同时对短视频创作、个性化广告、数字教学等领域有所裨益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal video segmentation is the get-to-go automatic video analysis, which decomposes a long-form video into smaller components for the following-up understanding tasks. Recent works have studied several levels of granularity to segment a video, such as shot, event, and scene. Those segmentations can help compare the semantics in the corresponding scales, but lack a wider view of larger temporal spans, especially when the video is complex and structured. Therefore, we present two abstractive levels of temporal segmentations and study their hierarchy to the existing fine-grained levels. Accordingly, we collect NewsNet, the largest news video dataset consisting of 1,000 videos in over 900 hours, associated with several tasks for hierarchical temporal video segmentation. Each news video is a collection of stories on different topics, represented as aligned audio, visual, and textual data, along with extensive frame-wise annotations in four granularities. We assert that the study on NewsNet can advance the understanding of complex structured video and benefit more areas such as short-video creation, personalized advertisement, digital instruction, and education. Our dataset and code is publicly available at: https://github.com/NewsNet-Benchmark/NewsNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">104.LSTFE-Net:Long Short-Term Feature Enhancement Network for Video Small Object Detection</span><br>
                <span class="as">Xiao, JinshengandWu, YuanxuandChen, YunhuaandWang, ShuruiandWang, ZhongyuanandMa, Jiayi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_LSTFE-NetLong_Short-Term_Feature_Enhancement_Network_for_Video_Small_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14613-14622.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频小物体检测由于缺乏对象信息而具有挑战性。<br>
                    动机：现有的方法主要通过增加更多的时间信息来获取更强大的高级特征，但往往无法明确指出小物体的最关键信息，导致特征不足或不适当。<br>
                    方法：提出了一种长短期特征增强网络（LSTFE-Net）用于视频小物体检测。首先开发了一个即插即用的时空特征对齐模块，以在短期帧和当前帧之间创建时间对应关系。然后，提出了一个帧选择模块，以选择能够提供最多额外上下文信息的长期帧。最后，提出了一个长短期特征聚合模块，以融合长短期特征。<br>
                    效果：与其他最先进的方法相比，我们的LSTFE-Net在FL-Drones数据集上实现了4.4%的AP绝对提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video small object detection is a difficult task due to the lack of object information. Recent methods focus on adding more temporal information to obtain more potent high-level features, which often fail to specify the most vital information for small objects, resulting in insufficient or inappropriate features. Since information from frames at different positions contributes differently to small objects, it is not ideal to assume that using one universal method will extract proper features. We find that context information from the long-term frame and temporal information from the short-term frame are two useful cues for video small object detection. To fully utilize these two cues, we propose a long short-term feature enhancement network (LSTFE-Net) for video small object detection. First, we develop a plug-and-play spatio-temporal feature alignment module to create temporal correspondences between the short-term and current frames. Then, we propose a frame selection module to select the long-term frame that can provide the most additional context information. Finally, we propose a long short-term feature aggregation module to fuse long short-term features. Compared to other state-of-the-art methods, our LSTFE-Net achieves 4.4% absolute boosts in AP on the FL-Drones dataset. More details can be found at https://github.com/xiaojs18/LSTFE-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">105.Joint Video Multi-Frame Interpolation and Deblurring Under Unknown Exposure Time</span><br>
                <span class="as">Shang, WeiandRen, DongweiandYang, YiandZhang, HongzhiandMa, KedeandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Joint_Video_Multi-Frame_Interpolation_and_Deblurring_Under_Unknown_Exposure_Time_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13935-13944.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善由动态场景复杂性、镜头和传感器缺陷以及不理想的曝光设置等因素导致的低帧率和运动模糊问题。<br>
                    动机：现有的视频插值和去模糊方法都假设曝光时间是已知且固定的，这在现实中并不成立。因此，本文旨在解决未知曝光时间下的视频多帧插值和去模糊这一更具挑战性的任务。<br>
                    方法：首先，采用一种变种的有监督对比学习方法从输入的模糊帧中构建一个与曝光相关的表示。然后，训练两个U-Nets分别进行内部运动和外部运动分析，通过增益调整适应学习到的曝光表示。最后，通过逐步曝光自适应卷积和运动细化，基于曝光和运动表示构建视频重建网络。<br>
                    效果：在模拟和真实世界的数据集上的大量实验表明，优化后的方法在联合视频x8插值和去模糊任务上取得了显著的性能提升。此外，在看似不可能的x16插值任务上，该方法比现有方法在PSNR上提高了1.5 dB以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Natural videos captured by consumer cameras often suffer from low framerate and motion blur due to the combination of dynamic scene complexity, lens and sensor imperfection, and less than ideal exposure setting. As a result, computational methods that jointly perform video frame interpolation and deblurring begin to emerge with the unrealistic assumption that the exposure time is known and fixed. In this work, we aim ambitiously for a more realistic and challenging task - joint video multi-frame interpolation and deblurring under unknown exposure time. Toward this goal, we first adopt a variant of supervised contrastive learning to construct an exposure-aware representation from input blurred frames. We then train two U-Nets for intra-motion and inter-motion analysis, respectively, adapting to the learned exposure representation via gain tuning. We finally build our video reconstruction network upon the exposure and motion representation by progressive exposure-adaptive convolution and motion refinement. Extensive experiments on both simulated and real-world datasets show that our optimized method achieves notable performance gains over the state-of-the-art on the joint video x8 interpolation and deblurring task. Moreover, on the seemingly implausible x16 interpolation task, our method outperforms existing methods by more than 1.5 dB in terms of PSNR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">106.MMG-Ego4D: Multimodal Generalization in Egocentric Action Recognition</span><br>
                <span class="as">Gong, XinyuandMohan, SreyasandDhingra, NainaandBazin, Jean-CharlesandLi, YileiandWang, ZhangyangandRanjan, Rakesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_MMG-Ego4D_Multimodal_Generalization_in_Egocentric_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6481-6491.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了一种新的自中心动作识别问题，称为“多模态泛化”（MMG），主要探讨在数据的某些模态有限或甚至完全缺失的情况下，系统如何进行泛化。<br>
                    动机：为了支持现实世界应用中的安全性和效率考虑，我们设计了两种新的情境来深入研究MMG，包括训练时存在但推理时缺失的模态的缺失模态泛化，以及推理时和训练时模态不相交的跨模态零射一泛化。<br>
                    方法：我们构建了一个新的数据集MMG-Ego4D，包含视频、音频和惯性运动传感器（IMU）模态的数据点。通过引入新的融合模块，包括模态丢弃训练、对比性对齐训练和一种新的跨模态原型损失，以提高少样本性能。<br>
                    效果：我们在MMG-Ego4D上评估了各种模型，并提出了具有改善泛化能力的新方法。实验结果表明，这些新方法在各种知识驱动任务上取得了显著改进，并在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study a novel problem in egocentric action recognition, which we term as "Multimodal Generalization" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D dataset, but processed and thoroughly re-annotated by human experts to facilitate research in the MMG problem. We evaluate a diverse array of models on MMG-Ego4D and propose new methods with improved generalization ability. In particular, we introduce a new fusion module with modality dropout training, contrastive-based alignment training, and a novel cross-modal prototypical loss for better few-shot performance. We hope this study will serve as a benchmark and guide future research in multimodal generalization problems. The benchmark and code are available at https://github.com/facebookresearch/MMG_Ego4D</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">107.Panoptic Video Scene Graph Generation</span><br>
                <span class="as">Yang, JingkangandPeng, WenxuanandLi, XiangtaiandGuo, ZujinandChen, LiangyuandLi, BoandMa, ZhengandZhou, KaiyangandZhang, WayneandLoy, ChenChangeandLiu, Ziwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Panoptic_Video_Scene_Graph_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18675-18685.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决全面真实世界视觉感知系统中的一个新问题，即全景场景图生成（PVSG）。<br>
                    动机：现有的视频场景图生成（VidSGG）问题主要关注人类和物体之间的时间交互，但使用边界框检测非刚性物体和背景时常常会导致VidSGG系统遗漏对全面视频理解至关重要的关键细节。相比之下，PVSG需要通过更精确的像素级分割掩码将场景图中的节点与场景理解整体化联系起来。<br>
                    方法：为了推进这一新领域的研究，我们贡献了一个高质量的PVSG数据集，其中包括400个视频（289个第三人称+111个自我中心视频），总共有15万个帧，标注了全景分割掩码以及精细的时间场景图。我们还提供了各种基线方法和有用的设计实践供未来工作参考。<br>
                    效果：实验结果表明，我们在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Towards building comprehensive real-world visual perception systems, we propose and study a new problem called panoptic scene graph generation (PVSG). PVSG is related to the existing video scene graph generation (VidSGG) problem, which focuses on temporal interactions between humans and objects localized with bounding boxes in videos. However, the limitation of bounding boxes in detecting non-rigid objects and backgrounds often causes VidSGG systems to miss key details that are crucial for comprehensive video understanding. In contrast, PVSG requires nodes in scene graphs to be grounded by more precise, pixel-level segmentation masks, which facilitate holistic scene understanding. To advance research in this new area, we contribute a high-quality PVSG dataset, which consists of 400 videos (289 third-person + 111 egocentric videos) with totally 150K frames labeled with panoptic segmentation masks as well as fine, temporal scene graphs. We also provide a variety of baseline methods and share useful design practices for future work.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">108.Class Prototypes Based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos</span><br>
                <span class="as">Gupta, RohitandRoy, AnirbanandChristensen, ClaireandKim, SujeongandGerard, SarahandCincebeaux, MadelineandDivakaran, AjayandGrindal, ToddandShah, Mubarak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Class_Prototypes_Based_Contrastive_Learning_for_Classifying_Multi-Label_and_Fine-Grained_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19923-19933.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为幼儿过滤出合适的在线教育内容。<br>
                    动机：随着儿童对在线媒体消费的增长，需要数据驱动的工具帮助教育工作者为年幼的学习者筛选适当的教育内容。<br>
                    方法：提出一种在在线视频中检测教育内容的方法，主要关注两大常用的教育内容类别：读写和数学。针对每一类，根据共同核心标准选择显著的代码（子类别）。将此问题视为细粒度的多标签分类问题，因为视频可能包含多种类型的教育内容，且内容类别在视觉上可能相似。提出一种基于类原型的监督对比学习方法，可以处理与多个标签相关的细粒度样本。学习每个类别的类原型，并使用损失函数最小化类原型与该类别样本之间的距离，同时最大化类原型与其他类别样本之间的距离。考虑到视觉和听觉提示之间的对齐对于有效理解至关重要，因此考虑使用多模态转换器网络在学习视频嵌入的同时捕获视频中的视觉和听觉提示之间的交互。<br>
                    效果：通过一个由教育研究人员从YouTube标注的教育视频组成的数据集APPROVE进行评估，该数据集包括193小时的专家标注视频，共19个类别。所提出的方法在APPROVE和其他基准测试（如Youtube-8M，COIN）上优于强大的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent growth in the consumption of online media by children during early childhood necessitates data-driven tools enabling educators to filter out appropriate educational content for young learners. This paper presents an approach for detecting educational content in online videos. We focus on two widely used educational content classes: literacy and math. For each class, we choose prominent codes (sub-classes) based on the Common Core Standards. For example, literacy codes include 'letter names', 'letter sounds', and math codes include 'counting', 'sorting'. We pose this as a fine-grained multilabel classification problem as videos can contain multiple types of educational content and the content classes can get visually similar (e.g., 'letter names' vs 'letter sounds'). We propose a novel class prototypes based supervised contrastive learning approach that can handle fine-grained samples associated with multiple labels. We learn a class prototype for each class and a loss function is employed to minimize the distances between a class prototype and the samples from the class. Similarly, distances between a class prototype and the samples from other classes are maximized. As the alignment between visual and audio cues are crucial for effective comprehension, we consider a multimodal transformer network to capture the interaction between visual and audio cues in videos while learning the embedding for videos. For evaluation, we present a dataset, APPROVE, employing educational videos from YouTube labeled with fine-grained education classes by education researchers. APPROVE consists of 193 hours of expert-annotated videos with 19 classes. The proposed approach outperforms strong baselines on APPROVE and other benchmarks such as Youtube-8M, and COIN. The dataset is available at https://nusci.csl.sri.com/project/APPROVE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">109.Decoupled Multimodal Distilling for Emotion Recognition</span><br>
                <span class="as">Li, YongandWang, YuanzhiandCui, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6631-6640.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多模态情感识别（MER）中存在的模态异质性和不同模态贡献差异大的问题。<br>
                    动机：尽管现有的MER方法表现优秀，但模态异质性和模态贡献差异仍然是一个挑战。<br>
                    方法：本文提出了一种解耦的多模态蒸馏（DMD）方法，通过灵活和自适应的跨模态知识蒸馏来增强每个模态的判别特征。具体来说，将每个模态的表示解耦为两个部分，即模态无关/独占空间，并使用图蒸馏单元（GD-Unit）进行知识蒸馏。<br>
                    效果：实验结果表明，DMD在各种任务上都优于最先进的MER方法，且其图边显示出有意义的分布模式。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human multimodal emotion recognition (MER) aims to perceive human emotions via language, visual and acoustic modalities. Despite the impressive performance of previous MER approaches, the inherent multimodal heterogeneities still haunt and the contribution of different modalities varies significantly. In this work, we mitigate this issue by proposing a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each modality. Specially, the representation of each modality is decoupled into two parts, i.e., modality-irrelevant/-exclusive spaces, in a self-regression manner. DMD utilizes a graph distillation unit (GD-Unit) for each decoupled part so that each GD can be performed in a more specialized and effective manner. A GD-Unit consists of a dynamic graph where each vertice represents a modality and each edge indicates a dynamic knowledge distillation. Such GD paradigm provides a flexible knowledge transfer manner where the distillation weights can be automatically learned, thus enabling diverse crossmodal knowledge transfer patterns. Experimental results show DMD consistently obtains superior performance than state-of-the-art MER methods. Visualization results show the graph edges in DMD exhibit meaningful distributional patterns w.r.t. the modality-irrelevant/-exclusive feature spaces. Codes are released at https://github.com/mdswyz/DMD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">110.Multivariate, Multi-Frequency and Multimodal: Rethinking Graph Neural Networks for Emotion Recognition in Conversation</span><br>
                <span class="as">Chen, FeiyuandShao, JieandZhu, ShuyuanandShen, HengTao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Multivariate_Multi-Frequency_and_Multimodal_Rethinking_Graph_Neural_Networks_for_Emotion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10761-10770.png><br>
            
            <span class="tt"><span class="t0">研究问题：在对话中的情感识别（ERC）任务中，跨模态和上下文维度的高阶复杂关系是一个关键挑战。<br>
                    动机：现有的方法往往以松散的方式编码多模态和上下文关系，这可能损害关系建模。图神经网络（GNN）在捕捉数据关系方面表现出优势，为ERC提供了新的解决方案。<br>
                    方法：我们提出了一种基于GNN的模型，该模型探索了多元关系，并通过评估多频信号来捕捉情感差异和共性的变化重要性。<br>
                    效果：实验结果表明，我们提出的方法在两个流行的多模态ERC数据集上优于先前最先进的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Complex relationships of high arity across modality and context dimensions is a critical challenge in the Emotion Recognition in Conversation (ERC) task. Yet, previous works tend to encode multimodal and contextual relationships in a loosely-coupled manner, which may harm relationship modelling. Recently, Graph Neural Networks (GNN) which show advantages in capturing data relations, offer a new solution for ERC. However, existing GNN-based ERC models fail to address some general limits of GNNs, including assuming pairwise formulation and erasing high-frequency signals, which may be trivial for many applications but crucial for the ERC task. In this paper, we propose a GNN-based model that explores multivariate relationships and captures the varying importance of emotion discrepancy and commonality by valuing multi-frequency signals. We empower GNNs to better capture the inherent relationships among utterances and deliver more sufficient multimodal and contextual modelling. Experimental results show that our proposed method outperforms previous state-of-the-art works on two popular multimodal ERC datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">111.DeFeeNet: Consecutive 3D Human Motion Prediction With Deviation Feedback</span><br>
                <span class="as">Sun, XiaoningandSun, HuaijiangandLi, BinandWei, DongandLi, WeiqingandLu, Jianfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_DeFeeNet_Consecutive_3D_Human_Motion_Prediction_With_Deviation_Feedback_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5527-5536.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决当前预测人类运动的技术无法满足实际需求的问题，如人机协作。<br>
                    动机：现有的预测模型将预测人类运动简化为基于历史观察的短期未来序列预测过程，忽视了在实际应用中，运动预测是一个连续的过程，每轮预测的结果都会在下一轮中得到验证和反馈。<br>
                    方法：本文提出了DeFeeNet，这是一个可以添加到现有一次性预测模型中的简单有效的网络，用于实现连续运动预测任务中的偏差感知和反馈。在每轮预测中，前一轮产生的偏差首先由DeFeeNet编码，然后被整合到现有的预测器中，实现了偏差感知的预测方式。<br>
                    效果：实验结果表明，无论基础模型如何，本文提出的网络都能提高连续人类运动预测的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Let us rethink the real-world scenarios that require human motion prediction techniques, such as human-robot collaboration. Current works simplify the task of predicting human motions into a one-off process of forecasting a short future sequence (usually no longer than 1 second) based on a historical observed one. However, such simplification may fail to meet practical needs due to the neglect of the fact that motion prediction in real applications is not an isolated "observe then predict" unit, but a consecutive process composed of many rounds of such unit, semi-overlapped along the entire sequence. As time goes on, the predicted part of previous round has its corresponding ground truth observable in the new round, but their deviation in-between is neither exploited nor able to be captured by existing isolated learning fashion. In this paper, we propose DeFeeNet, a simple yet effective network that can be added on existing one-off prediction models to realize deviation perception and feedback when applied to consecutive motion prediction task. At each prediction round, the deviation generated by previous unit is first encoded by our DeFeeNet, and then incorporated into the existing predictor to enable a deviation-aware prediction manner, which, for the first time, allows for information transmit across adjacent prediction units. We design two versions of DeFeeNet as MLP-based and GRU-based, respectively. On Human3.6M and more complicated BABEL, experimental results indicate that our proposed network improves consecutive human motion prediction performance regardless of the basic model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">112.Align and Attend: Multimodal Summarization With Dual Contrastive Losses</span><br>
                <span class="as">He, BoandWang, JunandQiu, JielinandBui, TrungandShrivastava, AbhinavandWang, Zhaowen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14867-14878.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态摘要的目标是从不同的模态中提取最重要的信息来形成摘要，但现有的方法未能利用不同模态之间的时间对应关系和内在关联。<br>
                    动机：为了解决这个问题，我们提出了Align and Attend Multimodal Summarization（A2Summ）模型，这是一个统一的多模态基于变压器的模型，可以有效地对齐和关注多模态输入。<br>
                    方法：我们引入了两种新的对比损失函数，以模拟样本内和样本间的关联性。<br>
                    效果：在两个标准的视频摘要数据集（TVSum和SumMe）以及两个多模态摘要数据集（Daily Mail和CNN）上的大量实验表明，A2Summ在所有数据集上都取得了最先进的性能。我们还收集了一个大规模的多模态摘要数据集BLiSS，其中包含直播视频和带注释摘要的转录文本。我们的代码和数据集可在https://boheumd.github.io/A2Summ/上公开获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of multimodal summarization is to extract the most important information from different modalities to form summaries. Unlike unimodal summarization, the multimodal summarization task explicitly leverages cross-modal information to help generate more reliable and high-quality summaries. However, existing methods fail to leverage the temporal correspondence between different modalities and ignore the intrinsic correlation between different samples. To address this issue, we introduce Align and Attend Multimodal Summarization (A2Summ), a unified multimodal transformer-based model which can effectively align and attend the multimodal input. In addition, we propose two novel contrastive losses to model both inter-sample and intra-sample correlations. Extensive experiments on two standard video summarization datasets (TVSum and SumMe) and two multimodal summarization datasets (Daily Mail and CNN) demonstrate the superiority of A2Summ, achieving state-of-the-art performances on all datasets. Moreover, we collected a large-scale multimodal summarization dataset BLiSS, which contains livestream videos and transcribed texts with annotated summaries. Our code and dataset are publicly available at https://boheumd.github.io/A2Summ/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">113.Movies2Scenes: Using Movie Metadata To Learn Scene Representation</span><br>
                <span class="as">Chen, ShixingandLiu, Chun-HaoandHao, XiangandNie, XiaohanandArap, MaximandHamid, Raffay</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Movies2Scenes_Using_Movie_Metadata_To_Learn_Scene_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6535-6544.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地理解电影中的场景，用于视频审查、搜索和推荐等应用。<br>
                    动机：手动标注场景费时费力，而电影级别的元数据（如类型、剧情简介等）作为电影制作过程的一部分，通常更容易获取。<br>
                    方法：提出一种新的对比学习方法，利用电影元数据学习通用场景表示。具体来说，使用电影元数据定义一种电影相似性度量，并在对比学习中使用它来限制对正场景对的搜索，只考虑彼此相似的电影。<br>
                    效果：在多个基准数据集上评估的多种任务中，我们学习的场景表示始终优于现有的最先进方法。特别是在LVU数据集上，我们的表示在七个分类任务上平均提高了7.9%，在两个回归任务上提高了9.7%。此外，使用新收集的电影数据集，我们在一系列视频审查任务上展示了我们的场景表示的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding scenes in movies is crucial for a variety of applications such as video moderation, search, and recommendation. However, labeling individual scenes is a time-consuming process. In contrast, movie level metadata (e.g., genre, synopsis, etc.) regularly gets produced as part of the film production process, and is therefore significantly more commonly available. In this work, we propose a novel contrastive learning approach that uses movie metadata to learn a general-purpose scene representation. Specifically, we use movie metadata to define a measure of movie similarity, and use it during contrastive learning to limit our search for positive scene-pairs to only the movies that are considered similar to each other. Our learned scene representation consistently outperforms existing state-of-the-art methods on a diverse set of tasks evaluated using multiple benchmark datasets. Notably, our learned representation offers an average improvement of 7.9% on the seven classification tasks and 9.7% improvement on the two regression tasks in LVU dataset. Furthermore, using a newly collected movie dataset, we present comparative results of our scene representation on a set of video moderation tasks to demonstrate its generalizability on previously less explored tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">114.Neural Video Compression With Diverse Contexts</span><br>
                <span class="as">Li, JiahaoandLi, BinandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neural_Video_Compression_With_Diverse_Contexts_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22616-22626.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视频编码的效率和压缩比。<br>
                    动机：传统的视频编码方法需要大量的计算时间来寻找相关上下文，而新兴的神经网络视频编码器（NVC）的上下文有限，导致压缩比较低。<br>
                    方法：提出在时间和空间维度上增加上下文多样性的方法。通过学习跨帧的分层质量模式来丰富长期的高质量时间上下文，并引入基于光流的分组偏移多样性以更好地挖掘上下文信息。同时，采用四叉树分区并行编码潜在表示以增加空间上下文多样性。<br>
                    效果：实验表明，该方法比之前的SOTA NVC节省了23.5%的比特率，并在RGB和YUV420颜色空间中超过了未成熟的下一代传统编码器/ECM，在PSNR方面表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For any video codecs, the coding efficiency highly relies on whether the current signal to be encoded can find the relevant contexts from the previous reconstructed signals. Traditional codec has verified more contexts bring substantial coding gain, but in a time-consuming manner. However, for the emerging neural video codec (NVC), its contexts are still limited, leading to low compression ratio. To boost NVC, this paper proposes increasing the context diversity in both temporal and spatial dimensions. First, we guide the model to learn hierarchical quality patterns across frames, which enriches long-term and yet high-quality temporal contexts. Furthermore, to tap the potential of optical flow-based coding framework, we introduce a group-based offset diversity where the cross-group interaction is proposed for better context mining. In addition, this paper also adopts a quadtree-based partition to increase spatial context diversity when encoding the latent representation in parallel. Experiments show that our codec obtains 23.5% bitrate saving over previous SOTA NVC. Better yet, our codec has surpassed the under-developing next generation traditional codec/ECM in both RGB and YUV420 colorspaces, in terms of PSNR. The codes are at https://github.com/microsoft/DCVC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">115.Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning</span><br>
                <span class="as">Cao, ChengzhiandFu, XueyangandLiu, HongjianandHuang, YukunandWang, KunyuandLuo, JieboandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17990-17999.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频行人重识别（Re-ID）是计算机视觉领域的重要课题，由于其在视频监控应用中的广泛应用。<br>
                    动机：现有的方法主要利用帧序列中的空间和时间相关性来获取区别性的人的特征。然而，由于在帧中包含的模糊等不可避免的退化，会导致纹理噪声和时间干扰的歧义，从而丢失了身份区分的线索。<br>
                    方法：受生物启发的新型传感器事件相机的出现为Re-ID任务带来了新的活力。该相机可以异步记录强度变化，具有微秒级的分辨率和低延迟，即使在上述退化环境中也能准确捕捉行人的运动。因此，我们提出了一种稀疏-密集互补学习框架，通过充分利用密集帧和稀疏事件的互补信息来有效提取身份特征。<br>
                    效果：实验结果表明，通过将事件和脉冲神经网络（SNN）应用于Re-ID，我们的方法显著优于竞争方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video-based person re-identification (Re-ID) is a prominent computer vision topic due to its wide range of video surveillance applications. Most existing methods utilize spatial and temporal correlations in frame sequences to obtain discriminative person features. However, inevitable degradations, e.g., motion blur contained in frames often cause ambiguity texture noise and temporal disturbance, leading to the loss of identity-discriminating cues. Recently, a new bio-inspired sensor called event camera, which can asynchronously record intensity changes, brings new vitality to the Re-ID task. With the microsecond resolution and low latency, event cameras can accurately capture the movements of pedestrians even in the aforementioned degraded environments. Inspired by the properties of event cameras, in this work, we propose a Sparse-Dense Complementary Learning Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events. Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step-by-step, while for event streams, we design a bio-inspired spiking neural backbone, which encodes event signals into sparse feature maps in a spiking form, to present the dynamic motion cues of pedestrians. Finally, a cross feature alignment module is constructed to complementarily fuse motion information from events and appearance cues from frames to enhance identity representation learning. Experiments on several benchmarks show that by employing events and SNN into Re-ID, our method significantly outperforms competitive methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">116.Unsupervised Contour Tracking of Live Cells by Mechanical and Cycle Consistency Losses</span><br>
                <span class="as">Jang, JunbongandLee, KwonmooandKim, Tae-Kyun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jang_Unsupervised_Contour_Tracking_of_Live_Cells_by_Mechanical_and_Cycle_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/227-236.png><br>
            
            <span class="tt"><span class="t0">研究问题：分析细胞形态的动态变化对于理解活细胞的各种功能和特性至关重要。<br>
                    动机：由于细胞的流动性，以及局部轮廓特征的扩张和收缩等复杂运动，使得细胞轮廓上的局部形状和纹理不易观察，因此需要跟踪活细胞视频的每一帧中高度可变形的细胞轮廓上的所有点。<br>
                    方法：我们提出了第一种基于深度学习的细胞（或更一般的粘弹性材料）轮廓跟踪方法，通过融合两个轮廓之间的密集表示和交叉注意力来实现点对应。<br>
                    效果：在两个相差显微镜拍摄的活细胞数据集上进行的定量评估表明，我们的轮廓跟踪器在数量上优于比较的方法，并产生更有利的定性结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Analyzing the dynamic changes of cellular morphology is important for understanding the various functions and characteristics of live cells, including stem cells and metastatic cancer cells. To this end, we need to track all points on the highly deformable cellular contour in every frame of live cell video. Local shapes and textures on the contour are not evident, and their motions are complex, often with expansion and contraction of local contour features. The prior arts for optical flow or deep point set tracking are unsuited due to the fluidity of cells, and previous deep contour tracking does not consider point correspondence. We propose the first deep learning-based tracking of cellular (or more generally viscoelastic materials) contours with point correspondence by fusing dense representation between two contours with cross attention. Since it is impractical to manually label dense tracking points on the contour, unsupervised learning comprised of the mechanical and cyclical consistency losses is proposed to train our contour tracker. The mechanical loss forcing the points to move perpendicular to the contour effectively helps out. For quantitative evaluation, we labeled sparse tracking points along the contour of live cells from two live cell datasets taken with phase contrast and confocal fluorescence microscopes. Our contour tracker quantitatively outperforms compared methods and produces qualitatively more favorable results. Our code and data are publicly available at https://github.com/JunbongJang/contour-tracking/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">117.VideoTrack: Learning To Track Objects via Video Transformer</span><br>
                <span class="as">Xie, FeiandChu, LeiandLi, JiahaoandLu, YanandMa, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22826-22835.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的Siamese跟踪方法在效率和工业部署上存在限制，因为它们严重依赖于研究问题：现有的Siamese跟踪方法在效率和工业部署上存在限制，因为它们严重依赖于两个单帧之间的成对匹配，并需要复杂的机制来利用连续视频帧之间的时间信息。<br>
                    动机：为了解决这些问题，我们转向了序列级别的目标匹配，通过前馈视频模型将时间上下文编码到空间特征中。<br>
                    方法：我们修改了标准的 video transformer 架构，使其能够直接从帧级别补丁序列进行时空特征学习，以适应跟踪任务。我们还通过顺序多分支三元组块混合视频剪辑中的时空信息，形成了一个视频transformer主干。<br>
                    效果：我们的实验研究表明，我们的方法（命名为VideoTrack）在实时运行的同时实现了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing Siamese tracking methods, which are built on pair-wise matching between two single frames, heavily rely on additional sophisticated mechanism to exploit temporal information among successive video frames, hindering them from high efficiency and industrial deployments. In this work, we resort to sequence-level target matching that can encode temporal contexts into the spatial features through a neat feedforward video model. Specifically, we adapt the standard video transformer architecture to visual tracking by enabling spatiotemporal feature learning directly from frame-level patch sequences. To better adapt to the tracking task, we carefully blend the spatiotemporal information in the video clips through sequential multi-branch triplet blocks, which formulates a video transformer backbone. Our experimental study compares different model variants, such as tokenization strategies, hierarchical structures, and video attention schemes. Then, we propose a disentangled dual-template mechanism that decouples static and dynamic appearance changes over time, and reduces the temporal redundancy in video frames. Extensive experiments show that our method, named as VideoTrack, achieves state-of-the-art results while running in real-time.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">118.Modeling Video As Stochastic Processes for Fine-Grained Video Representation Learning</span><br>
                <span class="as">Zhang, HengandLiu, DaqingandZheng, QiandSu, Bing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Modeling_Video_As_Stochastic_Processes_for_Fine-Grained_Video_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2225-2234.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的细粒度视频表示学习方法在学习帧特征时，忽视了视频的内在动态过程。<br>
                    动机：提出一种新的基于过程的对比学习框架，通过区分视频过程并捕捉过程中的动态变化来学习视频表示。<br>
                    方法：将视频建模为随机过程，通过过程对比损失强制目标序列的嵌入在潜在空间中逼近布朗桥，以实现对视频过程的区分和动态捕捉。<br>
                    效果：在四个数据集上的实验结果表明，该方法在各种视频理解任务上表现优秀，包括阶段进展、阶段分类和帧检索。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A meaningful video is semantically coherent and changes smoothly. However, most existing fine-grained video representation learning methods learn frame-wise features by aligning frames across videos or exploring relevance between multiple views, neglecting the inherent dynamic process of each video. In this paper, we propose to learn video representations by modeling Video as Stochastic Processes (VSP) via a novel process-based contrastive learning framework, which aims to discriminate between video processes and simultaneously capture the temporal dynamics in the processes. Specifically, we enforce the embeddings of the frame sequence of interest to approximate a goal-oriented stochastic process, i.e., Brownian bridge, in the latent space via a process-based contrastive loss. To construct the Brownian bridge, we adapt specialized sampling strategies under different annotations for both self-supervised and weakly-supervised learning. Experimental results on four datasets show that VSP stands as a state-of-the-art method for various video understanding tasks, including phase progression, phase classification and frame retrieval. Code is available at 'https://github.com/hengRUC/VSP'.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">119.CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning</span><br>
                <span class="as">Cheng, YitingandWei, FangyunandBao, JianminandChen, DongandZhang, Wenqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_CiCo_Domain-Aware_Sign_Language_Retrieval_via_Cross-Lingual_Contrastive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19016-19026.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决手语理解中的一项新任务——手语检索，包括文本到手语视频（T2V）和手语视频到文本（V2T）的检索。<br>
                    动机：由于手语既是自然语言，又包含丰富的语义信息，因此传统的视频-文本检索方法无法满足需求。同时，手语数据集的规模远小于语音识别数据集，这增加了数据稀缺性的问题。<br>
                    方法：本文将手语检索定义为跨语言检索问题和视频-文本检索任务，并采用跨语言对比学习的方法在联合嵌入空间中对手语和自然语言的文本以及手语视频进行对比，以确定精细的跨语言（即手语到单词）映射。<br>
                    效果：通过在大规模手语视频上预训练领域不可知的手语编码器并通过伪标签将其引入目标领域，本文提出的框架在各种数据集上均大幅超越了先前的方法，例如在How2Sign数据集上T2V和V2T R@1指标分别提高了+22.4和+28.0，在PHOENIX-2014T数据集上T2V和V2T R@1指标分别提高了+13.7和+17.1。代码和模型可在https://github.com/FangyunWei/SLRT获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work focuses on sign language retrieval--a recently proposed task for sign language understanding. Sign language retrieval consists of two sub-tasks: text-to-sign-video (T2V) retrieval and sign-video-to-text (V2T) retrieval. Different from traditional video-text retrieval, sign language videos, not only contain visual signals but also carry abundant semantic meanings by themselves due to the fact that sign languages are also natural languages. Considering this character, we formulate sign language retrieval as a cross-lingual retrieval problem as well as a video-text retrieval task. Concretely, we take into account the linguistic properties of both sign languages and natural languages, and simultaneously identify the fine-grained cross-lingual (i.e., sign-to-word) mappings while contrasting the texts and the sign videos in a joint embedding space. This process is termed as cross-lingual contrastive learning. Another challenge is raised by the data scarcity issue--sign language datasets are orders of magnitude smaller in scale than that of speech recognition. We alleviate this issue by adopting a domain-agnostic sign encoder pre-trained on large-scale sign videos into the target domain via pseudo-labeling. Our framework, termed as domain-aware sign language retrieval via Cross-lingual Contrastive learning or CiCo for short, outperforms the pioneering method by large margins on various datasets, e.g., +22.4 T2V and +28.0 V2T R@1 improvements on How2Sign dataset, and +13.7 T2V and +17.1 V2T R@1 improvements on PHOENIX-2014T dataset. Code and models are available at: https://github.com/FangyunWei/SLRT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">120.Relational Space-Time Query in Long-Form Videos</span><br>
                <span class="as">Yang, XitongandChu, Fu-JenandFeiszli, MattandGoyal, RaghavandTorresani, LorenzoandTran, Du</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6398-6408.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视频基准测试独立地研究活动、对象及其交互的问题，并且是在短的、经过策划的剪辑上进行。然而，真实世界的应用，如AR助手，需要将这些问题捆绑在一起进行模型开发和评估。<br>
                    动机：为了解决这一问题，本文提出了一个联合框架用于长视频理解。<br>
                    方法：首先，提出了一个集成框架Relational Space-Time Query（ReST），通过模板化的时空查询来评估视频理解模型。其次，引入了两个新的基准测试ReST-ADL和ReST-Ego4D，这两个基准测试通过ReST框架生成的丰富的查询注释来增强现有的自我中心视频数据集。<br>
                    效果：实验结果表明，ReST框架和基准测试有助于在长视频中进行综合的多步推理，并相信这将促进下一代视频理解模型的发展。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Egocentric videos are often available in the form of uninterrupted, uncurated long videos capturing the camera wearers' daily life activities.Understanding these videos requires models to be able to reason about activities, objects, and their interactions. However, current video benchmarks study these problems independently and under short, curated clips. In contrast, real-world applications, e.g., AR assistants, require bundling these problems for both model development and evaluation. In this paper, we propose to study these problems in a joint framework for long video understanding. Our contributions are three-fold. First, we propose an integrated framework, namely Relational Space-Time Query (ReST), for evaluating video understanding models via templated spatiotemporal queries. Second, we introduce two new benchmarks, ReST-ADL and ReST-Ego4D, which augment the existing egocentric video datasets with abundant query annotations generated by the ReST framework. Finally, we present a set of baselines and in-depth analysis on the two benchmarks and provide insights about the query tasks. We view our integrated framework and benchmarks as a step towards comprehensive, multi-step reasoning in long videos, and believe it will facilitate the development of next generations of video understanding models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">121.Video Dehazing via a Multi-Range Temporal Alignment Network With Physical Prior</span><br>
                <span class="as">Xu, JiaqiandHu, XiaoweiandZhu, LeiandDou, QiandDai, JifengandQiao, YuandHeng, Pheng-Ann</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Video_Dehazing_via_a_Multi-Range_Temporal_Alignment_Network_With_Physical_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18053-18062.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的视频去雾框架，以恢复高可见性和对比度的无雾帧。<br>
                    动机：现有的视频去雾方法往往忽视了物理雾的先验知识和时间信息的聚合，导致去雾效果不佳。<br>
                    方法：我们设计了一个基于记忆的物理先验指导模块，将与先验相关的特征编码到长程记忆中。同时，我们构建了一个多范围场景辐射恢复模块，以捕捉多个时空范围内的空间-时间依赖性，从而有效地从相邻帧中聚合时间信息。<br>
                    效果：我们在各种真实世界场景中创建了第一个大型户外视频去雾基准数据集。实验结果表明，我们的方法在合成和真实条件下都表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video dehazing aims to recover haze-free frames with high visibility and contrast. This paper presents a novel framework to effectively explore the physical haze priors and aggregate temporal information. Specifically, we design a memory-based physical prior guidance module to encode the prior-related features into long-range memory. Besides, we formulate a multi-range scene radiance recovery module to capture space-time dependencies in multiple space-time ranges, which helps to effectively aggregate temporal information from adjacent frames. Moreover, we construct the first large-scale outdoor video dehazing benchmark dataset, which contains videos in various real-world scenarios. Experimental results on both synthetic and real conditions show the superiority of our proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">122.BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation</span><br>
                <span class="as">Park, JunheumandKim, JintaeandKim, Chang-Su</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_BiFormer_Learning_Bilateral_Motion_Estimation_via_Bilateral_Transformer_for_4K_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1568-1577.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种基于双向变换器的4K视频帧插值器。<br>
                    动机：改进现有的视频帧插值方法，提高插值效果。<br>
                    方法：通过全局运动估计、局部运动精细化和帧合成三个步骤进行插值。其中，全局运动估计采用双向变换器预测对称的双侧运动场；局部运动精细化利用块状双向成本体积高效地优化全局运动场；最后，使用优化后的运动场对输入帧进行变形并融合以生成中间帧。<br>
                    效果：实验证明，提出的双向变换器算法在4K数据集上表现出优秀的插值性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A novel 4K video frame interpolator based on bilateral transformer (BiFormer) is proposed in this paper, which performs three steps: global motion estimation, local motion refinement, and frame synthesis. First, in global motion estimation, we predict symmetric bilateral motion fields at a coarse scale. To this end, we propose BiFormer, the first transformer-based bilateral motion estimator. Second, we refine the global motion fields efficiently using blockwise bilateral cost volumes (BBCVs). Third, we warp the input frames using the refined motion fields and blend them to synthesize an intermediate frame. Extensive experiments demonstrate that the proposed BiFormer algorithm achieves excellent interpolation performance on 4K datasets. The source codes are available at https://github.com/JunHeum/BiFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">123.Learning From Unique Perspectives: User-Aware Saliency Modeling</span><br>
                <span class="as">Chen, ShiandValliappan, NachiappanandShen, ShaoleiandYe, XinyuandKohlhoff, KaiandHe, Junfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_From_Unique_Perspectives_User-Aware_Saliency_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2701-2710.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用视觉偏好进行用户关注模型的研究。<br>
                    动机：目前的显著性模型通常只适用于一般人群，忽视了用户行为之间的差异。通过理解用户的视觉偏好，可以更好地理解不同用户的精细注意力模式，并有助于开发定制的应用。<br>
                    方法：提出了一个新的模型，能够灵活捕捉各种用户组合的注意力模式，同时自适应预测个性化关注、用户群体关注和一般显著性。并通过逐步理解视觉注意力的原则学习方法，增强模型对不同用户注意力构成的知识。<br>
                    效果：在包括自然图像和网页等多种刺激下进行的实验结果表明，该方法能有效地捕捉不同用户的视觉行为差异和视觉刺激的一般显著性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Everyone is unique. Given the same visual stimuli, people's attention is driven by both salient visual cues and their own inherent preferences. Knowledge of visual preferences not only facilitates understanding of fine-grained attention patterns of diverse users, but also has the potential of benefiting the development of customized applications. Nevertheless, existing saliency models typically limit their scope to attention as it applies to the general population and ignore the variability between users' behaviors. In this paper, we identify the critical roles of visual preferences in attention modeling, and for the first time study the problem of user-aware saliency modeling. Our work aims to advance attention research from three distinct perspectives: (1) We present a new model with the flexibility to capture attention patterns of various combinations of users, so that we can adaptively predict personalized attention, user group attention, and general saliency at the same time with one single model; (2) To augment models with knowledge about the composition of attention from different users, we further propose a principled learning method to understand visual attention in a progressive manner; and (3) We carry out extensive analyses on publicly available saliency datasets to shed light on the roles of visual preferences. Experimental results on diverse stimuli, including naturalistic images and web pages, demonstrate the advantages of our method in capturing the distinct visual behaviors of different users and the general saliency of visual stimuli.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">124.MoStGAN-V: Video Generation With Temporal Motion Styles</span><br>
                <span class="as">Shen, XiaoqianandLi, XiangandElhoseiny, Mohamed</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_MoStGAN-V_Video_Generation_With_Temporal_Motion_Styles_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5652-5661.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频生成任务由于时空复杂性和需要合成多样化运动且具有时间连贯性的挑战，仍然十分困难。<br>
                    动机：现有的视频生成方法在生成任意长度的视频时，或者采用自回归方式，或者将时间视为连续信号，但都难以合成详细且多样化的运动，并会在几个时间步后产生重复的场景。<br>
                    方法：本文提出引入额外的时间依赖运动风格来模拟各种运动模式，同时提出一种名为MoStAtt的动态风格注意力调制机制，以增强每个特定尺度（即层）的帧的生动动态。<br>
                    效果：实验结果表明，该方法在四个无条件256^2视频合成基准测试中取得了最先进的性能，并且仅使用每段3帧进行训练就能产生更好的动态运动质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video generation remains a challenging task due to spatiotemporal complexity and the requirement of synthesizing diverse motions with temporal consistency. Previous works attempt to generate videos in arbitrary lengths either in an autoregressive manner or regarding time as a continuous signal. However, they struggle to synthesize detailed and diverse motions with temporal coherence and tend to generate repetitive scenes after a few time steps. In this work, we argue that a single time-agnostic latent vector of style-based generator is insufficient to model various and temporally-consistent motions. Hence, we introduce additional time-dependent motion styles to model diverse motion patterns. In addition, a Motion Style Attention modulation mechanism, dubbed as MoStAtt, is proposed to augment frames with vivid dynamics for each specific scale (i.e., layer), which assigns attention score for each motion style w.r.t deconvolution filter weights in the target synthesis layer and softly attends different motion styles for weight modulation. Experimental results show our model achieves state-of-the-art performance on four unconditional 256^2 video synthesis benchmarks trained with only 3 frames per clip and produces better qualitative results with respect to dynamic motions. Code and videos have been made available at https://github.com/xiaoqian-shen/MoStGAN-V.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">125.ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data</span><br>
                <span class="as">Zhao, HaojieandChen, JunsongandWang, LijunandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_ARKitTrack_A_New_Diverse_Dataset_for_Tracking_Using_Mobile_RGB-D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5126-5135.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决传统RGB-only视觉跟踪中缺乏数据集的问题，提出一种新的RGB-D跟踪数据集ARKitTrack。<br>
                    动机：目前RGB-D跟踪的数据集较少，而RGB-D数据能够提供更丰富的信息，有助于提高跟踪的准确性和鲁棒性。<br>
                    方法：利用消费者级LiDAR扫描仪捕获静态和动态场景的RGB-D序列，构建了一个新的RGB-D跟踪数据集ARKitTrack，包含300个RGB-D序列、455个目标和229.7K个视频帧。同时，还提供了边界框注释、帧级别属性以及123.9K个像素级别的目标掩码。<br>
                    效果：通过在ARKitTrack数据集上进行实验，验证了该数据集对RGB-D跟踪的促进作用，并提出了一个新的基线方法，该方法结合了RGB特征和鸟瞰图表示，能够更好地探索跨模态3D几何信息。实验结果表明，该方法在跟踪准确性和鲁棒性方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compared with traditional RGB-only visual tracking, few datasets have been constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new RGB-D tracking dataset for both static and dynamic scenes captured by consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total. Along with the bounding box annotations and frame-level attributes, we also annotate this dataset with 123.9K pixel-level target masks. Besides, the camera intrinsic and camera pose of each frame are provided for future developments. To demonstrate the potential usefulness of this dataset, we further present a unified baseline for both box-level and pixel-level tracking, which integrates RGB features with bird's-eye-view representations to better explore cross-modality 3D geometry. In-depth empirical analysis has verified that the ARKitTrack dataset can significantly facilitate RGB-D tracking and that the proposed baseline method compares favorably against the state of the arts. The source code and dataset will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">126.Learning Action Changes by Measuring Verb-Adverb Textual Relationships</span><br>
                <span class="as">Moltisanti, DavideandKeller, FrankandBilen, HakanandSevilla-Lara, Laura</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Moltisanti_Learning_Action_Changes_by_Measuring_Verb-Adverb_Textual_Relationships_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23110-23118.png><br>
            
            <span class="tt"><span class="t0">研究问题：理解视频中动作的执行方式，即给定一个视频，预测修饰动作的副词（如“精细地切”）。<br>
                    动机：现有的数据集对于学习副词识别存在噪声干扰或包含的动作不受副词影响，导致评估不可靠。<br>
                    方法：将此问题转化为回归任务，通过测量动词和副词之间的文本关系来生成代表要学习的动作变化的回归目标。并在一系列数据集上进行测试，实现在副词预测和反义词分类上的最先进结果。<br>
                    效果：收集并创建了一个新的高质量数据集：Adverbs in Recipes (AIR)。结果显示，模型从更干净的AIR视频中学习效果更好，同时，对AIR上的副词预测具有挑战性，表明还有很大的改进空间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of this work is to understand the way actions are performed in videos. That is, given a video, we aim to predict an adverb indicating a modification applied to the action (e.g. cut "finely"). We cast this problem as a regression task. We measure textual relationships between verbs and adverbs to generate a regression target representing the action change we aim to learn. We test our approach on a range of datasets and achieve state-of-the-art results on both adverb prediction and antonym classification. Furthermore, we outperform previous work when we lift two commonly assumed conditions: the availability of action labels during testing and the pairing of adverbs as antonyms. Existing datasets for adverb recognition are either noisy, which makes learning difficult, or contain actions whose appearance is not influenced by adverbs, which makes evaluation less reliable. To address this, we collect a new high quality dataset: Adverbs in Recipes (AIR). We focus on instructional recipes videos, curating a set of actions that exhibit meaningful visual changes when performed differently. Videos in AIR are more tightly trimmed and were manually reviewed by multiple annotators to ensure high labelling quality. Results show that models learn better from AIR given its cleaner videos. At the same time, adverb prediction on AIR is challenging, demonstrating that there is considerable room for improvement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">127.Feature Aggregated Queries for Transformer-Based Video Object Detectors</span><br>
                <span class="as">Cui, Yiming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Feature_Aggregated_Queries_for_Transformer-Based_Video_Object_Detectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6365-6376.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频目标检测需要解决在图像领域很少发生的功能退化情况。<br>
                    动机：现有的基于变压器的视频目标检测器仍然遵循与经典目标检测器相同的流程，如通过聚合增强对象特征表示。<br>
                    方法：我们提出了一种改进的查询聚合模块，根据相邻帧的特征对查询进行加权平均，并将其扩展到一个更实用的版本，根据输入帧的特征生成和聚合查询。<br>
                    效果：在具有挑战性的ImageNet VID基准测试中，当与我们提出的模块集成时，当前最先进的基于变压器的目标检测器在mAP和AP50上可以提高超过2.4%和4.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video object detection needs to solve feature degradation situations that rarely happen in the image domain. One solution is to use the temporal information and fuse the features from the neighboring frames. With Transformer-based object detectors getting a better performance on the image domain tasks, recent works began to extend those methods to video object detection. However, those existing Transformer-based video object detectors still follow the same pipeline as those used for classical object detectors, like enhancing the object feature representations by aggregation. In this work, we take a different perspective on video object detection. In detail, we improve the qualities of queries for the Transformer-based models by aggregation. To achieve this goal, we first propose a vanilla query aggregation module that weighted averages the queries according to the features of the neighboring frames. Then, we extend the vanilla module to a more practical version, which generates and aggregates queries according to the features of the input frames. Extensive experimental results validate the effectiveness of our proposed methods: On the challenging ImageNet VID benchmark, when integrated with our proposed modules, the current state-of-the-art Transformer-based object detectors can be improved by more than 2.4% on mAP and 4.2% on AP50.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">128.Decomposed Cross-Modal Distillation for RGB-Based Temporal Action Detection</span><br>
                <span class="as">Lee, PilhyeonandKim, TaeohandShim, MinhoandWee, DongyoonandByun, Hyeran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Decomposed_Cross-Modal_Distillation_for_RGB-Based_Temporal_Action_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2373-2383.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决时间动作检测中的计算成本高和推理速度慢的问题。<br>
                    动机：现有的双流模型由于依赖计算量大的光流，导致推理速度慢。<br>
                    方法：提出了一种分解的跨模态蒸馏框架，通过转移运动模态的知识来构建强大的基于RGB的检测器。具体地，我们提出分别学习RGB和运动表示，然后将它们组合进行动作定位。<br>
                    效果：广泛的实验证明，该方法在提高基于RGB的动作检测器方面非常有效。特别是，我们的框架与不同的模型组合无关，带来了一致的收益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal action detection aims to predict the time intervals and the classes of action instances in the video. Despite the promising performance, existing two-stream models exhibit slow inference speed due to their reliance on computationally expensive optical flow. In this paper, we introduce a decomposed cross-modal distillation framework to build a strong RGB-based detector by transferring knowledge of the motion modality. Specifically, instead of direct distillation, we propose to separately learn RGB and motion representations, which are in turn combined to perform action localization. The dual-branch design and the asymmetric training objectives enable effective motion knowledge transfer while preserving RGB information intact. In addition, we introduce a local attentive fusion to better exploit the multimodal complementarity. It is designed to preserve the local discriminability of the features that is important for action localization. Extensive experiments on the benchmarks verify the effectiveness of the proposed method in enhancing RGB-based action detectors. Notably, our framework is agnostic to backbones and detection heads, bringing consistent gains across different model combinations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">129.Event-Based Frame Interpolation With Ad-Hoc Deblurring</span><br>
                <span class="as">Sun, LeiandSakaridis, ChristosandLiang, JingyunandSun, PengandCao, JiezhangandZhang, KaiandJiang, QiandWang, KaiweiandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Event-Based_Frame_Interpolation_With_Ad-Hoc_Deblurring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18043-18052.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频帧插值的性能与处理输入场景中运动的能力密切相关。<br>
                    动机：尽管先前的研究认识到异步事件信息对这项任务的效用，但他们忽略了运动可能导致输入视频模糊的事实，这取决于要插值的帧的曝光时间和运动速度，并假设输入视频是锐利的，限制自己进行帧插值，或者认为它是模糊的，在插值之前在其管道中包含一个明确的单独去模糊阶段。<br>
                    方法：我们提出了一种通用的事件基帧插值方法，该方法临时执行去模糊操作，因此适用于锐利和模糊的输入视频。我们的模型由一个双向循环网络组成，该网络自然地结合了插值的时间维度，并根据其时间接近度自适应地融合了来自输入帧和事件的信息。此外，我们还引入了一个具有事件和彩色视频的新型真实世界高分辨率数据集，为所审查的任务提供了具有挑战性的评估设置。<br>
                    效果：在标准的GoPro基准测试和我们的数据集上进行的大量实验表明，我们的网络在帧插值、单图像去模糊和插值与去模糊的联合任务上始终优于先前最先进的方法。我们的代码和数据集将在https://github.com/AHupuJR/REFID上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The performance of video frame interpolation is inherently correlated with the ability to handle motion in the input scene. Even though previous works recognize the utility of asynchronous event information for this task, they ignore the fact that motion may or may not result in blur in the input video to be interpolated, depending on the length of the exposure time of the frames and the speed of the motion, and assume either that the input video is sharp, restricting themselves to frame interpolation, or that it is blurry, including an explicit, separate deblurring stage before interpolation in their pipeline. We instead propose a general method for event-based frame interpolation that performs deblurring ad-hoc and thus works both on sharp and blurry input videos. Our model consists in a bidirectional recurrent network that naturally incorporates the temporal dimension of interpolation and fuses information from the input frames and the events adaptively based on their temporal proximity. In addition, we introduce a novel real-world high-resolution dataset with events and color videos which provides a challenging evaluation setting for the examined task. Extensive experiments on the standard GoPro benchmark and on our dataset show that our network consistently outperforms previous state-of-the-art methods on frame interpolation, single image deblurring and the joint task of interpolation and deblurring. Our code and dataset will be available at https://github.com/AHupuJR/REFID.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">130.Egocentric Video Task Translation</span><br>
                <span class="as">Xue, ZihuiandSong, YaleandGrauman, KristenandTorresani, Lorenzo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Egocentric_Video_Task_Translation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2310-2320.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频理解任务中，不同任务之间缺乏统一处理的问题。<br>
                    动机：现有的视频理解任务通常被孤立处理，而可穿戴摄像头提供了一种连续的、由人的目标驱动的、围绕人与世界互动的沉浸式自我中心视角，这需要一种更加统一的处理方法。<br>
                    方法：本文提出了EgoTask Translation（EgoT2）模型，该模型通过学习将优化在单独任务上的模型输出进行转换，以提高同时对所有任务的性能。与传统的转移学习或多任务学习不同，EgoT2的“翻转设计”包括所有任务共享的任务翻译器和特定于任务的骨干网络，以捕捉甚至异构任务之间的协同作用并减轻任务竞争。<br>
                    效果：通过对Ego4D的一系列视频任务进行演示，本文展示了EgoT2模型优于现有转移范式的优势，并在Ego4D 2022年的四个基准挑战中取得了顶级排名的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Different video understanding tasks are typically treated in isolation, and even with distinct types of curated data (e.g., classifying sports in one dataset, tracking animals in another). However, in wearable cameras, the immersive egocentric perspective of a person engaging with the world around them presents an interconnected web of video understanding tasks---hand-object manipulations, navigation in the space, or human-human interactions---that unfold continuously, driven by the person's goals. We argue that this calls for a much more unified approach. We propose EgoTask Translation (EgoT2), which takes a collection of models optimized on separate tasks and learns to translate their outputs for improved performance on any or all of them at once. Unlike traditional transfer or multi-task learning, EgoT2's "flipped design" entails separate task-specific backbones and a task translator shared across all tasks, which captures synergies between even heterogeneous tasks and mitigates task competition. Demonstrating our model on a wide array of video tasks from Ego4D, we show its advantages over existing transfer paradigms and achieve top-ranked results on four of the Ego4D 2022 benchmark challenges.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">131.AdamsFormer for Spatial Action Localization in the Future</span><br>
                <span class="as">Chi, Hyung-gunandLee, KwonjoonandAgarwal, NakulandXu, YiandRamani, KarthikandChoi, Chiho</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_AdamsFormer_for_Spatial_Action_Localization_in_the_Future_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17885-17895.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确预测未来动作的位置，特别是在未来帧中。<br>
                    动机：在人机协作等应用中，预测未来动作位置至关重要，但现有方法在此领域仍有改进空间。<br>
                    方法：提出了一种名为“空间动作在未来的定位”（SALF）的新任务，并使用NeuralODE的概念来解决，通过神经网络解决常微分方程来模拟序列数据的潜动态。<br>
                    效果：提出的AdamsFormer模型在UCF101-24和JHMDB-21数据集上的表现优于现有的长范围时间建模方法，显著提高了帧-mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting future action locations is vital for applications like human-robot collaboration. While some computer vision tasks have made progress in predicting human actions, accurately localizing these actions in future frames remains an area with room for improvement. We introduce a new task called spatial action localization in the future (SALF), which aims to predict action locations in both observed and future frames. SALF is challenging because it requires understanding the underlying physics of video observations to predict future action locations accurately. To address SALF, we use the concept of NeuralODE, which models the latent dynamics of sequential data by solving ordinary differential equations (ODE) with neural networks. We propose a novel architecture, AdamsFormer, which extends observed frame features to future time horizons by modeling continuous temporal dynamics through ODE solving. Specifically, we employ the Adams method, a multi-step approach that efficiently uses information from previous steps without discarding it. Our extensive experiments on UCF101-24 and JHMDB-21 datasets demonstrate that our proposed model outperforms existing long-range temporal modeling methods by a significant margin in terms of frame-mAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">132.Learning Discriminative Representations for Skeleton Based Action Recognition</span><br>
                <span class="as">Zhou, HuanyuandLiu, QingjieandWang, Yunhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Learning_Discriminative_Representations_for_Skeleton_Based_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10608-10617.png><br>
            
            <span class="tt"><span class="t0">研究问题：人体动作识别旨在从视频片段中分类人类动作类别。<br>
                    动机：虽然基于图卷积网络（GCN）的模型在处理骨架数据时比其他模态如RGB帧更有效和鲁棒，但在使用骨架数据时会丢失一些重要线索，如相关物品，导致难以区分的模糊动作容易被误分类。<br>
                    方法：提出一种辅助特征提炼头（FR Head），包括空间-时间解耦和对比特征提炼，以获取骨架的判别性表示。在特征空间中动态发现和校准模糊样本。此外，FR Head可以施加在GCN的不同阶段，建立多级提炼进行更强的监督。<br>
                    效果：在NTU RGB+D、NTU RGB+D 120和NW-UCLA数据集上进行了大量实验。所提出的模型取得了与最先进的方法相当的结果，并能有助于区分那些模糊样本。代码可在https://github.com/zhysora/FR-Head获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human action recognition aims at classifying the category of human action from a segment of a video. Recently, people have dived into designing GCN-based models to extract features from skeletons for performing this task, because skeleton representations are much more efficient and robust than other modalities such as RGB frames. However, when employing the skeleton data, some important clues like related items are also discarded. It results in some ambiguous actions that are hard to be distinguished and tend to be misclassified. To alleviate this problem, we propose an auxiliary feature refinement head (FR Head), which consists of spatial-temporal decoupling and contrastive feature refinement, to obtain discriminative representations of skeletons. Ambiguous samples are dynamically discovered and calibrated in the feature space. Furthermore, FR Head could be imposed on different stages of GCNs to build a multi-level refinement for stronger supervision. Extensive experiments are conducted on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets. Our proposed models obtain competitive results from state-of-the-art methods and can help to discriminate those ambiguous samples. Codes are available at https://github.com/zhysora/FR-Head.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">133.Token Turing Machines</span><br>
                <span class="as">Ryoo, MichaelS.andGopalakrishnan, KeerthanaandKahatapitiya, KumaraandXiao, TedandRao, KanishkaandStone, AustinandLu, YaoandIbarz, JulianandArnab, Anurag</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ryoo_Token_Turing_Machines_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19070-19081.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出Token Turing Machines (TTM)模型，一种具有记忆功能的序列自回归Transformer模型，用于真实世界的序列视觉理解。<br>
                    动机：受到神经图灵机的启发，设计了一种外部记忆模块，该模块由一组总结先前历史的令牌组成，以解决处理长序列时计算成本高的问题。<br>
                    方法：使用Transformer作为处理单元/控制器在每一步高效地寻址、读取和写入内存。模型的内存模块确保新观察只会与内存内容（而非整个历史）一起处理，从而有效地处理具有有界计算成本的长期序列。<br>
                    效果：实验表明，TTM在两个真实世界的序列视觉理解任务上优于其他替代方案，如专为长序列设计的其他Transformer模型和循环神经网络，包括在线视频活动检测和基于视觉的机器人动作策略学习。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Token Turing Machines (TTM), a sequential, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which summarise the previous history (i.e., frames). This memory is efficiently addressed, read and written using a Transformer as the processing unit/controller at each step. The model's memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efficiently process long sequences with a bounded computational cost at each step. We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning. Code is publicly available at: https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">134.Learning Event Guided High Dynamic Range Video Reconstruction</span><br>
                <span class="as">Yang, YixinandHan, JinandLiang, JinxiuandSato, ImariandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Learning_Event_Guided_High_Dynamic_Range_Video_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13924-13934.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用事件相机和传统相机捕捉的视觉信号进行HDR视频重建。<br>
                    动机：传统的基于帧的HDR视频重建在处理动态场景时存在曝光比例平衡和鬼影等问题，而事件相机可以提供更高的动态范围和时间分辨率，为LDR视频的HDR成像提供了有效的指导。<br>
                    方法：提出了一种多模态学习框架，通过多模态表示对齐策略来学习共享的潜在空间，并设计了一个融合模块来补充两种不同类型的信号在不同区域的动态范围。同时，利用了时间相关性来抑制重建的HDR视频中的闪烁效应。<br>
                    效果：所提出的HDRev-Net在合成数据和真实世界数据上都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Limited by the trade-off between frame rate and exposure time when capturing moving scenes with conventional cameras, frame based HDR video reconstruction suffers from scene-dependent exposure ratio balancing and ghosting artifacts. Event cameras provide an alternative visual representation with a much higher dynamic range and temporal resolution free from the above issues, which could be an effective guidance for HDR imaging from LDR videos. In this paper, we propose a multimodal learning framework for event guided HDR video reconstruction. In order to better leverage the knowledge of the same scene from the two modalities of visual signals, a multimodal representation alignment strategy to learn a shared latent space and a fusion module tailored to complementing two types of signals for different dynamic ranges in different regions are proposed. Temporal correlations are utilized recurrently to suppress the flickering effects in the reconstructed HDR video. The proposed HDRev-Net demonstrates state-of-the-art performance quantitatively and qualitatively for both synthetic and real-world data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">135.CASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective</span><br>
                <span class="as">Xiong, JunwenandWang, GanglaiandZhang, PengandHuang, WeiandZha, YufeiandZhai, Guangtao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_CASP-Net_Rethinking_Video_Saliency_Prediction_From_an_Audio-Visual_Consistency_Perceptual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6441-6450.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用音频流实现视频显著性预测，模仿人类大脑的选择性注意力机制。<br>
                    动机：大多数视频显著性预测方法只关注视觉和听觉模态之间的语义关联，忽视了由于视听内在时间不一致性带来的负面影响。<br>
                    方法：提出一种具有一致性感知的视听显著性预测网络（CASP-Net），全面考虑视听语义交互和一致感知。设计了优雅关联视频帧和相应声源的双流编码器，以及新颖的一致性感知预测编码，以迭代提高音频和视觉表示的一致性。引入了一个显著性解码器，进一步聚合多尺度的视听信息，生成最终的显著性图。<br>
                    效果：在六个具有挑战性的视听眼动追踪数据集上，所提出的CASP-Net优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Incorporating the audio stream enables Video Saliency Prediction (VSP) to imitate the selective attention mechanism of human brain. By focusing on the benefits of joint auditory and visual information, most VSP methods are capable of exploiting semantic correlation between vision and audio modalities but ignoring the negative effects due to the temporal inconsistency of audio-visual intrinsics. Inspired by the biological inconsistency-correction within multi-sensory information, in this study, a consistency-aware audio-visual saliency prediction network (CASP-Net) is proposed, which takes a comprehensive consideration of the audio-visual semantic interaction and consistent perception. In addition a two-stream encoder for elegant association between video frames and corresponding sound source, a novel consistency-aware predictive coding is also designed to improve the consistency within audio and visual representations iteratively. To further aggregate the multi-scale audio-visual information, a saliency decoder is introduced for the final saliency map generation. Substantial experiments demonstrate that the proposed CASP-Net outperforms the other state-of-the-art methods on six challenging audio-visual eye-tracking datasets. For a demo of our system please see https://woshihaozhu.github.io/CASP-Net/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">136.Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos</span><br>
                <span class="as">Wen, YilinandPan, HaoandYang, LeiandPan, JiaandKomura, TakuandWang, Wenping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21243-21253.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决从自我中心RGB视频中理解动态手势和动作的挑战，由于自我遮挡和模糊性。<br>
                    动机：由于自我遮挡和模糊性，这是一项基本但具有挑战性的任务。<br>
                    方法：开发了一个基于变压器的框架来利用时间信息进行稳健估计。构建了一个具有两个级联变压器编码器的网络层次结构，其中第一个编码器利用短期时间线索进行手部姿态估计，后者在较长的时间跨度内聚合每帧的姿态和物体信息以识别动作。<br>
                    效果：该方法在两个第一人称手势基准测试（FPHA和H2O）上取得了有竞争力的结果，广泛的消融研究验证了我们的设计选择。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding dynamic hand motions and actions from egocentric RGB videos is a fundamental yet challenging task due to self-occlusion and ambiguity. To address occlusion and ambiguity, we develop a transformer-based framework to exploit temporal information for robust estimation. Noticing the different temporal granularity of and the semantic correlation between hand pose estimation and action recognition, we build a network hierarchy with two cascaded transformer encoders, where the first one exploits the short-term temporal cue for hand pose estimation, and the latter aggregates per-frame pose and object information over a longer time span to recognize the action. Our approach achieves competitive results on two first-person hand action benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design choices.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">137.Simultaneously Short- and Long-Term Temporal Modeling for Semi-Supervised Video Semantic Segmentation</span><br>
                <span class="as">Lao, JiangweiandHong, WeixiangandGuo, XinandZhang, YingyingandWang, JianandChen, JingdongandChu, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lao_Simultaneously_Short-_and_Long-Term_Temporal_Modeling_for_Semi-Supervised_Video_Semantic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14763-14772.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用未标注的帧降低视频语义分割任务的成本。<br>
                    动机：现有的方法主要通过分配伪标签或进行特征增强来利用未标注的帧，但这些方法往往只关注短期对应关系，忽视了长期时间相关性。<br>
                    方法：本文提出了一种新的特征增强网络，该网络可以同时模拟短期和长期的时序关联。与仅利用短期对应关系的方法相比，从远距离帧获取的长期时间相关性可以有效地扩大时间感知范围，提供更丰富的上下文先验信息。更重要的是，同时建模相邻和远距离帧可以减轻过拟合的风险，从而为训练集中的远距离未标注帧和测试集中的未见过的视频产生高质量的特征表示。<br>
                    效果：在每段视频只有一个标注帧的设置下，该方法在具有挑战性的VSPW数据集上的性能比最先进的方法高出2%-3% mIoU。此外，当与基于伪标签的方法（如MeanTeacher）结合使用时，我们的最终模型仅比手动标注所有帧的上限性能低0.13% mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In order to tackle video semantic segmentation task at a lower cost, e.g., only one frame annotated per video, lots of efforts have been devoted to investigate the utilization of those unlabeled frames by either assigning pseudo labels or performing feature enhancement. In this work, we propose a novel feature enhancement network to simultaneously model short- and long-term temporal correlation. Compared with existing work that only leverage short-term correspondence, the long-term temporal correlation obtained from distant frames can effectively expand the temporal perception field and provide richer contextual prior. More importantly, modeling adjacent and distant frames together can alleviate the risk of over-fitting, hence produce high-quality feature representation for the distant unlabeled frames in training set and unseen videos in testing set. To this end, we term our method SSLTM, short for Simultaneously Short- and Long-Term Temporal Modeling. In the setting of only one frame annotated per video, SSLTM significantly outperforms the state-of-the-art methods by 2%   3% mIoU on the challenging VSPW dataset. Furthermore, when working with a pseudo label based method such as MeanTeacher, our final model only exhibits 0.13% mIoU less than the ceiling performance (i.e., all frames are manually annotated).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">138.Conditional Generation of Audio From Video via Foley Analogies</span><br>
                <span class="as">Du, YuexiandChen, ZiyangandSalamon, JustinandRussell, BryanandOwens, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Conditional_Generation_of_Audio_From_Video_via_Foley_Analogies_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2426-2436.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为与实际声音不同的视频创建匹配的音效？<br>
                    动机：为了解决视频音效设计与真实场景声音不符的问题，提出条件性Foley问题。<br>
                    方法：通过训练模型预测输入视频片段的音效，使用来自同一源视频的另一时间点的有条件音视片段作为样本；并提出一个模型，根据用户提供的示例为无声输入视频生成音效。<br>
                    效果：通过人类研究和自动化评估指标，证明该模型能成功从视频中生成音效，并根据提供的示例调整输出。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The sound effects that designers add to videos are designed to convey a particular artistic effect and, thus, may be quite different from a scene's true sound. Inspired by the challenges of creating a soundtrack for a video that differs from its true sound, but that nonetheless matches the actions occurring on screen, we propose the problem of conditional Foley. We present the following contributions to address this problem. First, we propose a pretext task for training our model to predict sound for an input video clip using a conditional audio-visual clip sampled from another time within the same source video. Second, we propose a model for generating a soundtrack for a silent input video, given a user-supplied example that specifies what the video should "sound like". We show through human studies and automated evaluation metrics that our model successfully generates sound from video, while varying its output according to the content of a supplied example.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">139.Diverse 3D Hand Gesture Prediction From Body Dynamics by Bilateral Hand Disentanglement</span><br>
                <span class="as">Qi, XingqunandLiu, ChenandSun, MuyiandLi, LinchengandFan, ChangjieandYu, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Diverse_3D_Hand_Gesture_Prediction_From_Body_Dynamics_by_Bilateral_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4616-4626.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从上身动态中预测自然且多样的3D手势，这是虚拟角色创建中的一个实践但具有挑战性的任务。<br>
                    动机：先前的工作通常忽视了两只手之间的非对称运动，并以整体的方式生成两只手，导致不自然的结果。<br>
                    方法：本文提出了一种新的基于双侧手解耦的两阶段3D手部生成方法，以从身体动态中实现自然且多样的3D手部预测。在第一阶段，我们通过两个手部解耦分支生成自然的手部姿势。考虑到两只手的非对称姿势和运动，我们引入了空间残差记忆（SRM）模块，通过残差学习来模型化身体与每只手之间的空间交互。为了全面提升两只手相对于身体动态的协调性，我们接着提出了时间-运动记忆（TMM）模块。TMM可以有效地模型化身体动态与两只手的运动之间的时间关联性。第二阶段建立在以下洞察之上：给定连续的身体姿态，3D手部预测应该是非决定性的。因此，我们进一步根据第一阶段的初始输出来多样化我们的3D手部预测。具体来说，我们提出了原型记忆采样策略（PSS），通过基于梯度的马尔科夫链蒙特卡罗（MCMC）采样来生成非决定性的手势。<br>
                    效果：大量实验证明，我们的方法在B2H数据集和我们新收集的TED Hands数据集上都优于最先进的模型。数据集和代码可在以下链接获取：https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at: https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">140.MOSO: Decomposing MOtion, Scene and Object for Video Prediction</span><br>
                <span class="as">Sun, MingzhenandWang, WeiningandZhu, XinxinandLiu, Jing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MOSO_Decomposing_MOtion_Scene_and_Object_for_Video_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18727-18737.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从视频中分离出运动、场景和对象，并利用这些组件进行视频预测。<br>
                    动机：视频中的运动、场景和对象是其三个主要视觉组成部分，理解并有效利用这些信息对于视频预测等任务至关重要。<br>
                    方法：提出了一个两阶段的运动、场景和对象分解框架（MOSO），包括MOSO-VQVAE和MOSO-Transformer。首先，MOSO-VQVAE将前一视频片段分解为运动、场景和对象组件，并将它们表示为离散的标记组。然后，MOSO-Transformer根据先前的标记预测后续视频片段的对象和场景标记，并在标记级别向生成的对象和场景标记添加动态运动。<br>
                    效果：实验结果表明，该方法在五个具有挑战性的视频预测和无条件视频生成基准测试中实现了新的最先进的性能：BAIR，RoboNet，KTH，KITTI和UCF101。此外，MOSO可以通过组合来自不同视频的对象和场景来生成逼真的视频。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Motion, scene and object are three primary visual components of a video. In particular, objects represent the foreground, scenes represent the background, and motion traces their dynamics. Based on this insight, we propose a two-stage MOtion, Scene and Object decomposition framework (MOSO) for video prediction, consisting of MOSO-VQVAE and MOSO-Transformer. In the first stage, MOSO-VQVAE decomposes a previous video clip into the motion, scene and object components, and represents them as distinct groups of discrete tokens. Then, in the second stage, MOSO-Transformer predicts the object and scene tokens of the subsequent video clip based on the previous tokens and adds dynamic motion at the token level to the generated object and scene tokens. Our framework can be easily extended to unconditional video generation and video frame interpolation tasks. Experimental results demonstrate that our method achieves new state-of-the-art performance on five challenging benchmarks for video prediction and unconditional video generation: BAIR, RoboNet, KTH, KITTI and UCF101. In addition, MOSO can produce realistic videos by combining objects and scenes from different videos.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">141.Unifying Short and Long-Term Tracking With Graph Hierarchies</span><br>
                <span class="as">Cetintas, OrcunandBras\&#x27;o, GuillemandLeal-Taix\&#x27;e, Laura</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22877-22887.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在长视频中追踪多个对象，包括未被遮挡的对象和被遮挡后重新出现的对象。<br>
                    动机：目前处理这两种任务的方法通常是分离的，针对特定场景进行设计，而表现最好的方法往往是各种技术的结合，导致需要大量的工程工作，缺乏通用性。<br>
                    方法：我们提出了SUSHI，一种统一且可扩展的多目标跟踪器。我们将长视频分割成一系列子视频进行处理，实现了高度的可扩展性。利用图神经网络处理所有级别的子视频，使我们的模型在整个时间尺度上保持一致，具有很高的通用性。<br>
                    效果：在四个不同的数据集上，我们的方法都取得了显著优于现有技术的效果。我们的代码和模型可以在bit.ly/sushi-mot获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Tracking objects over long videos effectively means solving a spectrum of problems, from short-term association for un-occluded objects to long-term association for objects that are occluded and then reappear in the scene. Methods tackling these two tasks are often disjoint and crafted for specific scenarios, and top-performing approaches are often a mix of techniques, which yields engineering-heavy solutions that lack generality. In this work, we question the need for hybrid approaches and introduce SUSHI, a unified and scalable multi-object tracker. Our approach processes long clips by splitting them into a hierarchy of subclips, which enables high scalability. We leverage graph neural networks to process all levels of the hierarchy, which makes our model unified across temporal scales and highly general. As a result, we obtain significant improvements over state-of-the-art on four diverse datasets. Our code and models are available at bit.ly/sushi-mot.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">142.Recurrence Without Recurrence: Stable Video Landmark Detection With Deep Equilibrium Models</span><br>
                <span class="as">Micaelli, PaulandVahdat, ArashandYin, HongxuandKautz, JanandMolchanov, Pavlo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Micaelli_Recurrence_Without_Recurrence_Stable_Video_Landmark_Detection_With_Deep_Equilibrium_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22814-22825.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改进地标检测模型的预测精度和稳定性？<br>
                    动机：目前的地标检测模型在处理视频数据时，由于训练数据的缺乏，往往会出现“闪烁”现象，影响预测的稳定性。<br>
                    方法：提出了一种称为Recurrence without Recurrence（RwR）的新范式，通过将DEQs重新表述为约束优化问题，模拟推理时的递归，即使在训练时没有时间序列数据也能减少地标闪烁。<br>
                    效果：实验结果表明，使用RwR的LDEQ在WFLW-V数据集上取得了显著的改进，NME和NMF分别提高了10%和13%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cascaded computation, whereby predictions are recurrently refined over several stages, has been a persistent theme throughout the development of landmark detection models. In this work, we show that the recently proposed Deep Equilibrium Model (DEQ) can be naturally adapted to this form of computation. Our Landmark DEQ (LDEQ) achieves state-of-the-art performance on the challenging WFLW facial landmark dataset, reaching 3.92 NME with fewer parameters and a training memory cost of O(1) in the number of recurrent modules. Furthermore, we show that DEQs are particularly suited for landmark detection in videos. In this setting, it is typical to train on still images due to the lack of labelled videos. This can lead to a "flickering" effect at inference time on video, whereby a model can rapidly oscillate between different plausible solutions across consecutive frames. By rephrasing DEQs as a constrained optimization, we emulate recurrence at inference time, despite not having access to temporal data at training time. This Recurrence without Recurrence (RwR) paradigm helps in reducing landmark flicker, which we demonstrate by introducing a new metric, normalized mean flicker (NMF), and contributing a new facial landmark video dataset (WFLW-V) targeting landmark uncertainty. On the WFLW-V hard subset made up of 500 videos, our LDEQ with RwR improves the NME and NMF by 10 and 13% respectively, compared to the strongest previously published model using a hand-tuned conventional filter.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">143.Egocentric Audio-Visual Object Localization</span><br>
                <span class="as">Huang, ChaoandTian, YapengandKumar, AnuragandXu, Chenliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Egocentric_Audio-Visual_Object_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22910-22921.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合处理多模态输入，实现以第一人称视角进行音频-视觉对象定位。<br>
                    动机：人类通过视听融合自然感知周围环境，机器也需要学习从自我中心的角度处理多模态输入来接近人类的智能。<br>
                    方法：提出了一个几何感知的时间聚合模块来显式处理自我运动问题，并利用估计的时空几何变换更新视觉表示来减轻自我运动的影响。同时，提出级联特征增强模块来提高跨模态定位的鲁棒性，通过解耦视觉指示的音频表示来实现。在训练过程中，利用自然可用的音视频时间同步作为"免费"的自我监督，避免昂贵的标签化。<br>
                    效果：实验表明该方法在自我中心的视屏中实现了最先进的定位性能，并能推广到多样化的音视频场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans naturally perceive surrounding scenes by unifying sound and sight in a first-person view. Likewise, machines are advanced to approach human intelligence by learning with multisensory inputs from an egocentric perspective. In this paper, we explore the challenging egocentric audio-visual object localization task and observe that 1) egomotion commonly exists in first-person recordings, even within a short duration; 2) The out-of-view sound components can be created while wearers shift their attention. To address the first problem, we propose a geometry-aware temporal aggregation module to handle the egomotion explicitly. The effect of egomotion is mitigated by estimating the temporal geometry transformation and exploiting it to update visual representations. Moreover, we propose a cascaded feature enhancement module to tackle the second issue. It improves cross-modal localization robustness by disentangling visually-indicated audio representation. During training, we take advantage of the naturally available audio-visual temporal synchronization as the "free" self-supervision to avoid costly labeling. We also annotate and create the Epic Sounding Object dataset for evaluation purposes. Extensive experiments show that our method achieves state-of-the-art localization performance in egocentric videos and can be generalized to diverse audio-visual scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">144.Unbiased Scene Graph Generation in Videos</span><br>
                <span class="as">Nag, SayakandMin, KyleandTripathi, SubarnaandRoy-Chowdhury, AmitK.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nag_Unbiased_Scene_Graph_Generation_in_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22803-22813.png><br>
            
            <span class="tt"><span class="t0">研究问题：动态场景图生成（SGG）任务复杂且具有挑战性，包括场景的固有动态性、模型预测的时间波动以及视觉关系的长尾分布。<br>
                    动机：现有的动态SGG方法主要通过复杂的架构捕捉时空上下文，但并未解决上述挑战，特别是关系长尾分布的问题，这常常导致生成有偏的场景图。<br>
                    方法：我们提出了一个新的框架TEMPURA，它采用基于变压器的序列建模实现对象级别的时间一致性，通过记忆引导训练学习合成无偏的关系表示，并使用高斯混合模型（GMM）衰减视觉关系的预测不确定性。<br>
                    效果：大量实验证明，该方法比现有方法性能提高了10%，在生成更无偏的场景图方面表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of dynamic scene graph generation (SGG) from videos is complicated and challenging due to the inherent dynamics of a scene, temporal fluctuation of model predictions, and the long-tailed distribution of the visual relationships in addition to the already existing challenges in image-based SGG. Existing methods for dynamic SGG have primarily focused on capturing spatio-temporal context using complex architectures without addressing the challenges mentioned above, especially the long-tailed distribution of relationships. This often leads to the generation of biased scene graphs. To address these challenges, we introduce a new framework called TEMPURA: TEmporal consistency and Memory Prototype guided UnceRtainty Attenuation for unbiased dynamic SGG. TEMPURA employs object-level temporal consistencies via transformer-based sequence modeling, learns to synthesize unbiased relationship representations using memory-guided training, and attenuates the predictive uncertainty of visual relations using a Gaussian Mixture Model (GMM). Extensive experiments demonstrate that our method achieves significant (up to 10% in some cases) performance gain over existing methods highlight- ing its superiority in generating more unbiased scene graphs. Code: https://github.com/sayaknag/unbiasedSGG.git</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">145.MIST: Multi-Modal Iterative Spatial-Temporal Transformer for Long-Form Video Question Answering</span><br>
                <span class="as">Gao, DifeiandZhou, LuoweiandJi, LeiandZhu, LinchaoandYang, YiandShou, MikeZheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_MIST_Multi-Modal_Iterative_Spatial-Temporal_Transformer_for_Long-Form_Video_Question_Answering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14773-14783.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何构建能够从具有复杂事件的长视频中寻找答案的视频问答系统。<br>
                    动机：现有的多模态视频问答模型在处理图像或短视频时表现良好，但在处理长视频时面临新的挑战。<br>
                    方法：提出了一种新的名为“多模态迭代空间-时间变压器”（MIST）的模型，通过将传统的密集空间-时间自注意力分解为级联的片段和区域选择模块，以更好地适应预训练模型进行长视频问答。<br>
                    效果：在四个视频问答数据集上进行的实验结果表明，MIST实现了最先进的性能，并在计算效率和可解释性方面表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To build Video Question Answering (VideoQA) systems capable of assisting humans in daily activities, seeking answers from long-form videos with diverse and complex events is a must. Existing multi-modal VQA models achieve promising performance on images or short video clips, especially with the recent success of large-scale multi-modal pre-training. However, when extending these methods to long-form videos, new challenges arise. On the one hand, using a dense video sampling strategy is computationally prohibitive. On the other hand, methods relying on sparse sampling struggle in scenarios where multi-event and multi-granularity visual reasoning are required. In this work, we introduce a new model named Multi-modal Iterative Spatial-temporal Transformer (MIST) to better adapt pre-trained models for long-form VideoQA. Specifically, MIST decomposes traditional dense spatial-temporal self-attention into cascaded segment and region selection modules that adaptively select frames and image regions that are closely relevant to the question itself. Visual concepts at different granularities are then processed efficiently through an attention module. In addition, MIST iteratively conducts selection and attention over multiple layers to support reasoning over multiple events. The experimental results on four VideoQA datasets, including AGQA, NExT-QA, STAR, and Env-QA, show that MIST achieves state-of-the-art performance and is superior at computation efficiency and interpretability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">146.Two-Stage Co-Segmentation Network Based on Discriminative Representation for Recovering Human Mesh From Videos</span><br>
                <span class="as">Zhang, BoyangandMa, KehuaandWu, SupingandYuan, Zhixiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Two-Stage_Co-Segmentation_Network_Based_on_Discriminative_Representation_for_Recovering_Human_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5662-5670.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从视频中恢复3D人体网格，特别是在极端光照和混乱背景的情况下。<br>
                    动机：现有的方法主要关注视频的时间连续性，忽视了复杂场景中的空间表示，导致在极端光照和混乱背景下无法恢复合理且平滑的人体网格序列。<br>
                    方法：提出了一种基于判别性表示的两阶段协同分割网络，用于从视频中恢复人体网格。第一阶段对视频空间域进行分割，突出空间精细信息，并通过双激励机制和频域增强模块学习并增强帧内判别性表示，同时抑制无关信息（如背景）。第二阶段通过动态整合策略对视频时间域进行分割，建立帧间判别性表示。<br>
                    效果：通过精心设计的地标锚区域损失来约束人体运动区域的变化，有效地生成合理的人体判别动作。在大量公开数据集上的实验结果表明，该方法优于大多数最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recovering 3D human mesh from videos has recently made significant progress. However, most of the existing methods focus on the temporal consistency of videos, while ignoring the spatial representation in complex scenes, thus failing to recover a reasonable and smooth human mesh sequence under extreme illumination and chaotic backgrounds.To alleviate this problem, we propose a two-stage co-segmentation network based on discriminative representation for recovering human body meshes from videos. Specifically, the first stage of the network segments the video spatial domain to spotlight spatially fine-grained information, and then learns and enhances the intra-frame discriminative representation through a dual-excitation mechanism and a frequency domain enhancement module, while suppressing irrelevant information (e.g., background). The second stage focuses on temporal context by segmenting the video temporal domain, and models inter-frame discriminative representation via a dynamic integration strategy.Further, to efficiently generate reasonable human discriminative actions, we carefully elaborate a landmark anchor area loss to constrain the variation of the human motion area. Extensive experimental results on large publicly available datasets indicate the superiority in comparison with most state-of-the-art. Code will be made public.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">147.Actionlet-Dependent Contrastive Learning for Unsupervised Skeleton-Based Action Recognition</span><br>
                <span class="as">Lin, LilangandZhang, JiahangandLiu, Jiaying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Actionlet-Dependent_Contrastive_Learning_for_Unsupervised_Skeleton-Based_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2363-2372.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自监督预训练方法在骨架动作识别中取得了成功，但这些方法对运动和静态部分同等对待，缺乏针对不同部分的自适应设计，这对动作识别的准确性产生了负面影响。<br>
                    动机：为了实现对运动和静态部分的自适应动作建模，我们提出了一种基于动作片段对比学习的Actionlet-Dependent Contrastive Learning方法（ActCLR）。<br>
                    方法：我们将动作片段定义为人体骨架的判别子集，有效地分解了运动区域以更好地进行动作建模。具体来说，通过与没有运动的静态锚点进行对比，我们在无监督的方式下提取出骨架数据的运动区域，作为动作片段。然后，围绕动作片段构建了一个运动自适应的数据转换方法。不同的数据转换被应用于动作片段和非动作片段区域，引入更多的多样性，同时保持它们自身的特点。同时，我们提出了一种语义感知的特征池化方法，以区分的方式在运动和静态区域之间建立特征表示。<br>
                    效果：我们在NTU RGB+D和PKUMMD上进行了广泛的实验，结果表明我们提出的方法在动作识别方面取得了显著的性能提升。更多的可视化和定量实验证明了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The self-supervised pretraining paradigm has achieved great success in skeleton-based action recognition. However, these methods treat the motion and static parts equally, and lack an adaptive design for different parts, which has a negative impact on the accuracy of action recognition. To realize the adaptive action modeling of both parts, we propose an Actionlet-Dependent Contrastive Learning method (ActCLR). The actionlet, defined as the discriminative subset of the human skeleton, effectively decomposes motion regions for better action modeling. In detail, by contrasting with the static anchor without motion, we extract the motion region of the skeleton data, which serves as the actionlet, in an unsupervised manner. Then, centering on actionlet, a motion-adaptive data transformation method is built. Different data transformations are applied to actionlet and non-actionlet regions to introduce more diversity while maintaining their own characteristics. Meanwhile, we propose a semantic-aware feature pooling method to build feature representations among motion and static regions in a distinguished manner. Extensive experiments on NTU RGB+D and PKUMMD show that the proposed method achieves remarkable action recognition performance. More visualization and quantitative experiments demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">148.ReVISE: Self-Supervised Speech Resynthesis With Visual Input for Universal and Generalized Speech Regeneration</span><br>
                <span class="as">Hsu, Wei-NingandRemez, TalandShi, BowenandDonley, JacobandAdi, Yossi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_ReVISE_Self-Supervised_Speech_Resynthesis_With_Visual_Input_for_Universal_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18795-18805.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过视觉输入改善语音质量，将分离、修复和视频转语音等不同听觉失真类型统一起来，并专注于改进语音的某些方面。<br>
                    动机：现有的研究通常分别研究每种听觉失真类型，并提出定制的算法。本文提出将这些主题统一起来，研究通用的语音再生，重点不在于重建精确的参考清洁信号，而在于提高语音的某些方面，如可理解性、质量和视频同步。<br>
                    方法：本文将问题表述为视听语音再生，包括两个步骤：伪视听语音识别（P-AVSR）和伪文本转语音合成（P-TTS）。P-AVSR和P-TTS通过来自自监督语音模型的离散单元连接。此外，还利用自监督视听语音模型初始化P-AVSR。提出的模型称为ReVISE。<br>
                    效果：ReVISE是第一个高质量的野外视频转语音合成模型，在LRS3的所有视听再生任务上都取得了优异的性能。为了证明其在实际世界中的适用性，还在EasyCom上进行了评估，这是一个在具有挑战性的声学条件下收集的仅有1.6小时训练数据的视听基准。同样，ReVISE大大抑制了噪声并提高了质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prior works on improving speech quality with visual input typically study each type of auditory distortion separately (e.g., separation, inpainting, video-to-speech) and present tailored algorithms. This paper proposes to unify these subjects and study Generalized Speech Regeneration, where the goal is not to reconstruct the exact reference clean signal, but to focus on improving certain aspects of speech while not necessarily preserving the rest such as voice. In particular, this paper concerns intelligibility, quality, and video synchronization. We cast the problem as audio-visual speech resynthesis, which is composed of two steps: pseudo audio-visual speech recognition (P-AVSR) and pseudo text-to-speech synthesis (P-TTS). P-AVSR and P-TTS are connected by discrete units derived from a self-supervised speech model. Moreover, we utilize self-supervised audio-visual speech model to initialize P-AVSR. The proposed model is coined ReVISE. ReVISE is the first high-quality model for in-the-wild video-to-speech synthesis and achieves superior performance on all LRS3 audio-visual regeneration tasks with a single model. To demonstrates its applicability in the real world, ReVISE is also evaluated on EasyCom, an audio-visual benchmark collected under challenging acoustic conditions with only 1.6 hours of training data. Similarly, ReVISE greatly suppresses noise and improves quality. Project page: https://wnhsu.github.io/ReVISE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">149.Two-Stream Networks for Weakly-Supervised Temporal Action Localization With Semantic-Aware Mechanisms</span><br>
                <span class="as">Wang, YuandLi, YadongandWang, Hongbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Two-Stream_Networks_for_Weakly-Supervised_Temporal_Action_Localization_With_Semantic-Aware_Mechanisms_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18878-18887.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督的时间动作定位旨在在只有视频级别的标注下，检测未修剪视频中的动作边界。<br>
                    动机：大多数现有的方案只关注对视频级别分类最敏感的时间段，忽视了帧之间的语义一致性。<br>
                    方法：设计一个可学习的字典，其中条目是相应动作类别的类质心。被识别为同一动作类别的片段表示被引导接近相同的类质心，这指导网络感知帧的语义并避免不合理的定位。此外，提出了一个双流框架，集成了注意力机制和多实例学习策略，以分别提取细粒度的线索和显著的特征。<br>
                    效果：通过在公开可用的THUMOS-14和ActivityNet-1.3数据集上进行验证，大量的实验和分析表明，我们的模型比现有方法取得了显著的进步。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-supervised temporal action localization aims to detect action boundaries in untrimmed videos with only video-level annotations. Most existing schemes detect temporal regions that are most responsive to video-level classification, but they overlook the semantic consistency between frames. In this paper, we hypothesize that snippets with similar representations should be considered as the same action class despite the absence of supervision signals on each snippet. To this end, we devise a learnable dictionary where entries are the class centroids of the corresponding action categories. The representations of snippets identified as the same action category are induced to be close to the same class centroid, which guides the network to perceive the semantics of frames and avoid unreasonable localization. Besides, we propose a two-stream framework that integrates the attention mechanism and the multiple-instance learning strategy to extract fine-grained clues and salient features respectively. Their complementarity enables the model to refine temporal boundaries. Finally, the developed model is validated on the publicly available THUMOS-14 and ActivityNet-1.3 datasets, where substantial experiments and analyses demonstrate that our model achieves remarkable advances over existing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">150.Egocentric Auditory Attention Localization in Conversations</span><br>
                <span class="as">Ryan, FionaandJiang, HaoandShukla, AbhinavandRehg, JamesM.andIthapu, VamsiKrishna</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ryan_Egocentric_Auditory_Attention_Localization_in_Conversations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14663-14674.png><br>
            
            <span class="tt"><span class="t0">研究问题：在嘈杂的交谈环境中，如何识别人们正在关注哪个说话者。<br>
                    动机：发展理解社会行为和增强人类听力的技术。<br>
                    方法：提出一种端到端的深度学习方法，利用自我中心视频和多通道音频预测摄像头佩戴者的听觉注意力热图。<br>
                    效果：该方法利用空间-时间音视特征和场景的整体推理进行预测，并在具有挑战性的多说话者对话数据集上超越了一组基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In a noisy conversation environment such as a dinner party, people often exhibit selective auditory attention, or the ability to focus on a particular speaker while tuning out others. Recognizing who somebody is listening to in a conversation is essential for developing technologies that can understand social behavior and devices that can augment human hearing by amplifying particular sound sources. The computer vision and audio research communities have made great strides towards recognizing sound sources and speakers in scenes. In this work, we take a step further by focusing on the problem of localizing auditory attention targets in egocentric video, or detecting who in a camera wearer's field of view they are listening to. To tackle the new and challenging Selective Auditory Attention Localization problem, we propose an end-to-end deep learning approach that uses egocentric video and multichannel audio to predict the heatmap of the camera wearer's auditory attention. Our approach leverages spatiotemporal audiovisual features and holistic reasoning about the scene to make predictions, and outperforms a set of baselines on a challenging multi-speaker conversation dataset. Project page: https://fkryan.github.io/saal</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">151.A New Comprehensive Benchmark for Semi-Supervised Video Anomaly Detection and Anticipation</span><br>
                <span class="as">Cao, CongqiandLu, YueandWang, PengandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_A_New_Comprehensive_Benchmark_for_Semi-Supervised_Video_Anomaly_Detection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20392-20401.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频异常检测（VAD）中的一种重要异常类型——场景依赖性异常，以及预防异常事件发生的更重要的任务——异常预期。<br>
                    动机：目前，对于场景依赖性异常和异常预期的研究还很少，且缺乏大规模的半监督VAD数据集。<br>
                    方法：本文提出了一个新的综合数据集NWPU Campus，包含43个场景、28种异常事件类别和16小时的视频，是当前最大的半监督VAD数据集。同时，本文还提出了一种新的模型，能够同时检测和预期异常事件。<br>
                    效果：与近年来7种优秀的VAD算法相比，该方法在处理场景依赖性异常检测和异常预期方面都表现出色，并在ShanghaiTech、CUHK Avenue、IITB Corridor和新的NWPU Campus数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised video anomaly detection (VAD) is a critical task in the intelligent surveillance system. However, an essential type of anomaly in VAD named scene-dependent anomaly has not received the attention of researchers. Moreover, there is no research investigating anomaly anticipation, a more significant task for preventing the occurrence of anomalous events. To this end, we propose a new comprehensive dataset, NWPU Campus, containing 43 scenes, 28 classes of abnormal events, and 16 hours of videos. At present, it is the largest semi-supervised VAD dataset with the largest number of scenes and classes of anomalies, the longest duration, and the only one considering the scene-dependent anomaly. Meanwhile, it is also the first dataset proposed for video anomaly anticipation. We further propose a novel model capable of detecting and anticipating anomalous events simultaneously. Compared with 7 outstanding VAD algorithms in recent years, our method can cope with scene-dependent anomaly detection and anomaly anticipation both well, achieving state-of-the-art performance on ShanghaiTech, CUHK Avenue, IITB Corridor and the newly proposed NWPU Campus datasets consistently. Our dataset and code is available at: https://campusvad.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">152.3Mformer: Multi-Order Multi-Mode Transformer for Skeletal Action Recognition</span><br>
                <span class="as">Wang, LeiandKoniusz, Piotr</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_3Mformer_Multi-Order_Multi-Mode_Transformer_for_Skeletal_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5620-5631.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频异常检测（VAD）中的一种重要异常类型——场景依赖性异常，以及预防异常事件发生的更重要的任务——异常预期。<br>
                    动机：目前，对于场景依赖性异常和异常预期的研究还很少，且缺乏大规模的半监督VAD数据集。<br>
                    方法：本文提出了一个新的综合数据集NWPU Campus，包含43个场景、28种异常事件类别和16小时的视频，是当前最大的半监督VAD数据集。同时，本文还提出了一种新的模型，能够同时检测和预期异常事件。<br>
                    效果：与近年来7种优秀的VAD算法相比，该方法在处理场景依赖性异常检测和异常预期方面都表现出色，并在ShanghaiTech、CUHK Avenue、IITB Corridor和新的NWPU Campus数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many skeletal action recognition models use GCNs to represent the human body by 3D body joints connected body parts. GCNs aggregate one- or few-hop graph neighbourhoods, and ignore the dependency between not linked body joints. We propose to form hypergraph to model hyper-edges between graph nodes (e.g., third- and fourth-order hyper-edges capture three and four nodes) which help capture higher-order motion patterns of groups of body joints. We split action sequences into temporal blocks, Higher-order Transformer (HoT) produces embeddings of each temporal block based on (i) the body joints, (ii) pairwise links of body joints and (iii) higher-order hyper-edges of skeleton body joints. We combine such HoT embeddings of hyper-edges of orders 1, ..., r by a novel Multi-order Multi-mode Transformer (3Mformer) with two modules whose order can be exchanged to achieve coupled-mode attention on coupled-mode tokens based on 'channel-temporal block', 'order-channel-body joint', 'channel-hyper-edge (any order)' and 'channel-only' pairs. The first module, called Multi-order Pooling (MP), additionally learns weighted aggregation along the hyper-edge mode, whereas the second module, Temporal block Pooling (TP), aggregates along the temporal block mode. Our end-to-end trainable network yields state-of-the-art results compared to GCN-, transformer- and hypergraph-based counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">153.STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition</span><br>
                <span class="as">Zhu, XiaoyuandHuang, Po-YaoandLiang, JunweianddeMelo, CelsoM.andHauptmann, AlexanderG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_STMT_A_Spatial-Temporal_Mesh_Transformer_for_MoCap-Based_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1526-1536.png><br>
            
            <span class="tt"><span class="t0">研究问题：使用运动捕捉序列进行人体动作识别。<br>
                    动机：现有技术需要多个手动步骤来提取标准骨架表示作为模型输入，我们提出了一种新的空间-时间网格变换器（STMT）直接对网格序列进行建模。<br>
                    方法：该模型使用具有帧内偏移注意力和帧间自注意力的分层变换器。注意力机制允许模型在任意两个顶点补丁之间自由关注，以学习空间-时间域中的非局部关系。<br>
                    效果：通过使用蒙版顶点建模和未来帧预测作为两个自我监督任务，充分激活了我们分层变换器中的双向和自回归注意力。所提出的方法在常见的运动捕捉基准测试中，与基于骨架和点云的模型相比，取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standardized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn non-local relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">154.Prototype-Based Embedding Network for Scene Graph Generation</span><br>
                <span class="as">Zheng, ChaofanandLyu, XinyuandGao, LianliandDai, BoandSong, Jingkuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Prototype-Based_Embedding_Network_for_Scene_Graph_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22783-22792.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前的场景图生成（SGG）方法在预测实体对之间的关系时，由于视觉研究问题：当前的场景图生成（SGG）方法在预测实体对之间的关系时，由于视觉外观的多样性和类别间的相似性，导致模型难以获取可靠的特征。<br>
                    动机：为了解决上述问题，本文提出利用谓词类别的内在语义作为类别原型，以缓解视觉外观的多样性和类别间的相似性带来的挑战。<br>
                    方法：本文提出了基于原型的嵌入网络（PE-Net），该网络通过原型对齐的紧凑和独特的表示来建模实体/谓词，并在一个共同的嵌入空间中建立实体对和谓词之间的匹配关系，用于关系识别。同时，引入了原型引导学习（PL）来帮助PE-Net高效地学习这种实体-谓词匹配，并设计了原型正则化（PR）来解决由谓词的语义重叠引起的模糊实体-谓词匹配问题。<br>
                    效果：实验结果表明，该方法在场景图生成方面具有优越的关系识别能力，在Visual Genome和Open Images数据集上均取得了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current Scene Graph Generation (SGG) methods explore contextual information to predict relationships among entity pairs. However, due to the diverse visual appearance of numerous possible subject-object combinations, there is a large intra-class variation within each predicate category, e.g., "man-eating-pizza, giraffe-eating-leaf", and the severe inter-class similarity between different classes, e.g., "man-holding-plate, man-eating-pizza", in model's latent space. The above challenges prevent current SGG methods from acquiring robust features for reliable relation prediction. In this paper, we claim that predicate's categoryinherent semantics can serve as class-wise prototypes in the semantic space for relieving the above challenges caused by the diverse visual appearances. To the end, we propose the Prototype-based Embedding Network (PE-Net), which models entities/predicates with prototype-aligned compact and distinctive representations and establishes matching between entity pairs and predicates in a common embedding space for relation recognition. Moreover, Prototypeguided Learning (PL) is introduced to help PE-Net efficiently learn such entity-predicate matching, and Prototype Regularization (PR) is devised to relieve the ambiguous entity-predicate matching caused by the predicate's semantic overlap. Extensive experiments demonstrate that our method gains superior relation recognition capability on SGG, achieving new state-of-the-art performances on both Visual Genome and Open Images datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">155.Music-Driven Group Choreography</span><br>
                <span class="as">Le, NhatandPham, ThangandDo, TuongandTjiputra, ErmanandTran, QuangD.andNguyen, Anh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Le_Music-Driven_Group_Choreography_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8673-8682.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从音乐中生成舞蹈动作，特别是群体舞蹈。<br>
                    动机：虽然现有的方法可以生成单个舞者的舞蹈动作，但群体舞蹈的生成仍是一个未解决的问题。<br>
                    方法：提出了AIOZ-GDANCE，一个新的大规模数据集和相应的半自动标注方法，用于音乐驱动的群体舞蹈生成。同时，还提出了一种新的方法，该方法接受音乐序列和一组舞者的位置作为输入，以有效地生成多个群体一致的舞蹈编排。<br>
                    效果：实验结果表明，直接将单人舞蹈生成技术应用于群体舞蹈可能会产生不令人满意的结果，如舞者之间的动作不一致和碰撞。而新的方法能够有效生成群体一致的舞蹈编排，为未来的群体舞蹈生成研究提供了便利。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Music-driven choreography is a challenging problem with a wide variety of industrial applications. Recently, many methods have been proposed to synthesize dance motions from music for a single dancer. However, generating dance motion for a group remains an open problem. In this paper, we present AIOZ-GDANCE, a new largescale dataset for music-driven group dance generation. Unlike existing datasets that only support single dance, our new dataset contains group dance videos, hence supporting the study of group choreography. We propose a semiautonomous labeling method with humans in the loop to obtain the 3D ground truth for our dataset. The proposed dataset consists of 16.7 hours of paired music and 3D motion from in-the-wild videos, covering 7 dance styles and 16 music genres. We show that naively applying single dance generation technique to creating group dance motion may lead to unsatisfactory results, such as inconsistent movements and collisions between dancers. Based on our new dataset, we propose a new method that takes an input music sequence and a set of 3D positions of dancers to efficiently produce multiple group-coherent choreographies. We propose new evaluation metrics for measuring group dance quality and perform intensive experiments to demonstrate the effectiveness of our method. Our project facilitates future research on group dance generation and is available at https://aioz-ai.github.io/AIOZ-GDANCE/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">156.Efficient Movie Scene Detection Using State-Space Transformers</span><br>
                <span class="as">Islam, MdMohaiminulandHasan, MahmudulandAthrey, KishanShamsundarandBraskich, TonyandBertasius, Gedas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18749-18758.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确检测电影场景，理解电影故事线。<br>
                    动机：现有的视频识别模型主要针对短程视频分析，对于长段电影视频的精确场景检测存在挑战。<br>
                    方法：提出一种State-Space Transformer模型，利用新型S4A构建模块有效捕捉长篇电影视频中的依赖关系，进行准确的电影场景检测。<br>
                    效果：提出的TranS4mer模型在MovieNet、BBC和OVSD三个电影场景检测数据集上的表现优于所有先前的方法，同时比标准的Transformer模型快2倍，需要的GPU内存少3倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability to distinguish between different movie scenes is critical for understanding the storyline of a movie. However, accurately detecting movie scenes is often challenging as it requires the ability to reason over very long movie segments. This is in contrast to most existing video recognition models, which are typically designed for short-range video analysis. This work proposes a State-Space Transformer model that can efficiently capture dependencies in long movie videos for accurate movie scene detection. Our model, dubbed TranS4mer, is built using a novel S4A building block, which combines the strengths of structured state-space sequence (S4) and self-attention (A) layers. Given a sequence of frames divided into movie shots (uninterrupted periods where the camera position does not change), the S4A block first applies self-attention to capture short-range intra-shot dependencies. Afterward, the state-space operation in the S4A block is used to aggregate long-range inter-shot cues. The final TranS4mer model, which can be trained end-to-end, is obtained by stacking the S4A blocks one after the other multiple times. Our proposed TranS4mer outperforms all prior methods in three movie scene detection datasets, including MovieNet, BBC, and OVSD, while also being 2x faster and requiring 3x less GPU memory than standard Transformer models. We will release our code and models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">157.Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert</span><br>
                <span class="as">Wang, JiadongandQian, XinyuanandZhang, MaluandTan, RobbyT.andLi, Haizhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14653-14662.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决说话人脸部生成（Talking face generation）中，嘴唇动作内容研究问题：本文旨在解决说话人脸部生成（Talking face generation）中，嘴唇动作内容（即所说话的可视性）的问题。<br>
                    动机：尽管现有的说话人脸部生成技术在唇音同步和视觉质量上取得了很大进步，但它们很少关注嘴唇动作的内容，即所说话的可视性，这是一个重要的生成质量方面。<br>
                    方法：我们提出使用唇读专家来提高生成的嘴唇区域的可理解性，通过惩罚错误的生成结果。为了弥补数据的稀缺性，我们在视听自监督的方式下训练唇读专家。我们还提出了一种新的对比学习方法来增强唇音同步，并使用一个转换器来编码音频与视频同步，同时考虑音频的全局时间依赖性。<br>
                    效果：实验表明，我们的提案在阅读可理解性上优于其他最先进的方法，如Wav2Lip，在LRS2数据集上的词错误率超过38%，在LRW数据集上的准确率为27.8%。我们在唇音同步上也达到了最先进的性能，并在视觉质量上取得了相当的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Talking face generation, also known as speech-to-lip generation, reconstructs facial motions concerning lips given coherent speech input. The previous studies revealed the importance of lip-speech synchronization and visual quality. Despite much progress, they hardly focus on the content of lip movements i.e., the visual intelligibility of the spoken words, which is an important aspect of generation quality. To address the problem, we propose using a lip-reading expert to improve the intelligibility of the generated lip regions by penalizing the incorrect generation results. Moreover, to compensate for data scarcity, we train the lip-reading expert in an audio-visual self-supervised manner. With a lip-reading expert, we propose a novel contrastive learning to enhance lip-speech synchronization, and a transformer to encode audio synchronically with video, while considering global temporal dependency of audio. For evaluation, we propose a new strategy with two different lip-reading experts to measure intelligibility of the generated videos. Rigorous experiments show that our proposal is superior to other State-of-the-art (SOTA) methods, such as Wav2Lip, in reading intelligibility i.e., over 38% Word Error Rate (WER) on LRS2 dataset and 27.8% accuracy on LRW dataset. We also achieve the SOTA performance in lip-speech synchronization and comparable performances in visual quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">158.Range-Nullspace Video Frame Interpolation With Focalized Motion Estimation</span><br>
                <span class="as">Yu, ZhiyangandZhang, YuandZou, DongqingandChen, XijunandRen, JimmyS.andRen, Shunqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Range-Nullspace_Video_Frame_Interpolation_With_Focalized_Motion_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22159-22168.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过预训练语言模型充分利用结构化知识，以提升语言理解能力。<br>
                    动机：现有的预训练语言模型往往忽视了知识图谱中的有信息量的实体，而这些实体可以增强语言表示，提升模型性能。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，该模型利用大规模文本语料库和知识图谱进行联合训练，能够同时捕捉词汇、句法和知识信息。<br>
                    效果：实验结果显示，ERNIE在各种知识驱动任务上表现优秀，且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continuous-time video frame interpolation is a fundamental technique in computer vision for its flexibility in synthesizing motion trajectories and novel video frames at arbitrary intermediate time steps. Yet, how to infer accurate intermediate motion and synthesize high-quality video frames are two critical challenges. In this paper, we present a novel VFI framework with improved treatment for these challenges. To address the former, we propose focalized trajectory fitting, which performs confidence-aware motion trajectory estimation by learning to pay focus to reliable optical flow candidates while suppressing the outliers. The second is range-nullspace synthesis, a novel frame renderer cast as solving an ill-posed problem addressed by learning decoupled components in orthogonal subspaces. The proposed framework sets new records on 7 of 10 public VFI benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">159.TransFlow: Transformer As Flow Learner</span><br>
                <span class="as">Lu, YawenandWang, QifanandMa, SiqiandGeng, TongandChen, YingjieVictorandChen, HuaijinandLiu, Dongfang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_TransFlow_Transformer_As_Flow_Learner_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18063-18073.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种纯转换器架构的光学流估计方法，以解决计算机视觉中的重要任务。<br>
                    动机：目前的基于CNN的方法在处理诸如遮挡和运动模糊等复杂情况时存在信息丢失的问题，而利用空间自注意力和跨注意力机制可以更准确地捕获全局依赖关系，从而改进光学流估计。<br>
                    方法：提出了TransFlow模型，该模型通过在相邻帧之间使用空间自注意力和跨注意力机制来有效地捕捉全局依赖关系，并通过长范围的时间关联恢复动态场景中的受损信息。<br>
                    效果：实验结果表明，TransFlow在Sintel、KITTI-15等数据集上取得了最先进的结果，并在视频对象检测、插值和稳定化等下游任务上也表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optical flow is an indispensable building block for various important computer vision tasks, including motion estimation, object tracking, and disparity measurement. In this work, we propose TransFlow, a pure transformer architecture for optical flow estimation. Compared to dominant CNN-based methods, TransFlow demonstrates three advantages. First, it provides more accurate correlation and trustworthy matching in flow estimation by utilizing spatial self-attention and cross-attention mechanisms between adjacent frames to effectively capture global dependencies; Second, it recovers more compromised information (e.g., occlusion and motion blur) in flow estimation through long-range temporal association in dynamic scenes; Third, it enables a concise self-learning paradigm and effectively eliminate the complex and laborious multi-stage pre-training procedures. We achieve the state-of-the-art results on the Sintel, KITTI-15, as well as several downstream tasks, including video object detection, interpolation and stabilization. For its efficacy, we hope TransFlow could serve as a flexible baseline for optical flow estimation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">160.Simple Cues Lead to a Strong Multi-Object Tracker</span><br>
                <span class="as">Seidenschwarz, JennyandBras\&#x27;o, GuillemandSerrano, V{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13813-13823.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文探讨了在多目标跟踪中，是否简单的检测跟踪方法也能实现与端到端模型相同的性能。<br>
                    动机：尽管基于注意力的方法在多目标跟踪中取得了显著的效果，但作者想知道传统的检测跟踪方法是否也能达到同样的效果。<br>
                    方法：作者提出了两个关键要素，使标准的再识别网络能在外观跟踪上表现出色。这两个要素是外观特征和简单的运动模型的组合。<br>
                    效果：通过广泛的分析失败案例，作者发现这种组合能产生强大的跟踪结果。该跟踪器在四个公共数据集上进行了泛化，包括MOT17、MOT20、BDD100k和DanceTrack，实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For a long time, the most common paradigm in MultiObject Tracking was tracking-by-detection (TbD), where objects are first detected and then associated over video frames. For association, most models resourced to motion and appearance cues, e.g., re-identification networks. Recent approaches based on attention propose to learn the cues in a data-driven manner, showing impressive results. In this paper, we ask ourselves whether simple good old TbD methods are also capable of achieving the performance of end-to-end models. To this end, we propose two key ingredients that allow a standard re-identification network to excel at appearance-based tracking. We extensively analyse its failure cases, and show that a combination of our appearance features with a simple motion model leads to strong tracking results. Our tracker generalizes to four public datasets, namely MOT17, MOT20, BDD100k, and DanceTrack, achieving state-ofthe-art performance. https://github.com/dvl-tum/GHOST</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">161.TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</span><br>
                <span class="as">Dave, IshanRajendrakumarandRizve, MamshadNayeemandChen, ChenandShah, Mubarak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dave_TimeBalance_Temporally-Invariant_and_Temporally-Distinctive_Video_Representations_for_Semi-Supervised_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2341-2352.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频理解任务需要同时考虑空间和时间维度，现有的半监督学习方法主要依赖于硬输入的启发式偏差，如使用两种模态（RGB和光流）或不同播放速度的两股流。<br>
                    动机：由于视频标注成本高且维度大，半监督学习在视频领域可能比图像更有利。此外，任何视频理解任务都需要对空间和时间维度进行推理。<br>
                    方法：我们提出了一种学生-教师的半监督学习框架TimeBalance，利用了时间不变和时间独特的自我监督视频表示。这两种表示形式根据动作的性质互补。我们根据一种新的基于时间相似性的重加权方案，根据未标记视频的性质动态地结合这两个教师的知识。<br>
                    效果：在三个动作识别基准测试集UCF101、HMDB51和Kinetics400上，我们的方法实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-Supervised Learning can be more beneficial for the video domain compared to images because of its higher annotation cost and dimensionality. Besides, any video understanding task requires reasoning over both spatial and temporal dimensions. In order to learn both the static and motion related features for the semi-supervised action recognition task, existing methods rely on hard input inductive biases like using two-modalities (RGB and Optical-flow) or two-stream of different playback rates. Instead of utilizing unlabeled videos through diverse input streams, we rely on self-supervised video representations, particularly, we utilize temporally-invariant and temporally-distinctive representations. We observe that these representations complement each other depending on the nature of the action. Based on this observation, we propose a student-teacher semi-supervised learning framework, TimeBalance, where we distill the knowledge from a temporally-invariant and a temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme. Our method achieves state-of-the-art performance on three action recognition benchmarks: UCF101, HMDB51, and Kinetics400. Code: https://github.com/DAVEISHAN/TimeBalance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">162.Unified Keypoint-Based Action Recognition Framework via Structured Keypoint Pooling</span><br>
                <span class="as">Hachiuma, RyoandSato, FumiakiandSekii, Taiki</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hachiuma_Unified_Keypoint-Based_Action_Recognition_Framework_via_Structured_Keypoint_Pooling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22962-22971.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文解决了传统基于骨架的动作识别的三个限制，包括骨架检测和跟踪错误、目标动作种类有限以及个人和帧级的动作识别。<br>
                    动机：引入了点云深度学习范式到动作识别中，并提出了一种新的深度神经网络架构——结构化关键点池化，以解决上述问题。<br>
                    方法：提出了一种统一框架和新颖的深度神经网络架构，该架构通过稀疏聚合关键点特征，实现了对输入错误的鲁棒性，并能有效地处理由人类骨架和非人类物体轮廓组成的时间序列关键点作为输入3D点云。<br>
                    效果：实验结果表明，该方法在各种限制下均表现出色，优于最先进的基于骨架的动作识别和时空动作定位方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper simultaneously addresses three limitations associated with conventional skeleton-based action recognition; skeleton detection and tracking errors, poor variety of the targeted actions, as well as person-wise and frame-wise action recognition. A point cloud deep-learning paradigm is introduced to the action recognition, and a unified framework along with a novel deep neural network architecture called Structured Keypoint Pooling is proposed. The proposed method sparsely aggregates keypoint features in a cascaded manner based on prior knowledge of the data structure (which is inherent in skeletons), such as the instances and frames to which each keypoint belongs, and achieves robustness against input errors. Its less constrained and tracking-free architecture enables time-series keypoints consisting of human skeletons and nonhuman object contours to be efficiently treated as an input 3D point cloud and extends the variety of the targeted action. Furthermore, we propose a Pooling-Switching Trick inspired by Structured Keypoint Pooling. This trick switches the pooling kernels between the training and inference phases to detect person-wise and frame-wise actions in a weakly supervised manner using only video-level action labels. This trick enables our training scheme to naturally introduce novel data augmentation, which mixes multiple point clouds extracted from different videos. In the experiments, we comprehensively verify the effectiveness of the proposed method against the limitations, and the method outperforms state-of-the-art skeleton-based action recognition and spatio-temporal action localization methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">163.A Large-Scale Robustness Analysis of Video Action Recognition Models</span><br>
                <span class="as">Schiappa, MadelineChantryandBiyani, NamanandKamtam, PrudviandVyas, ShrutiandPalangi, HamidandVineet, VibhavandRawat, YogeshS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Schiappa_A_Large-Scale_Robustness_Analysis_of_Video_Action_Recognition_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14698-14708.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在对现有的视频动作识别模型进行大规模鲁棒性分析，主要关注真实世界分布偏移扰动下的鲁棒性，而非对抗性扰动。<br>
                    动机：近年来，视频动作识别领域取得了很大进展，出现了基于卷积神经网络（CNN）和基于变换器的先进方法。然而，这些模型在面对真实世界的分布偏移时是否仍然具有鲁棒性尚不清楚。<br>
                    方法：本文提出了四个不同的基准数据集（HMDB51-P、UCF101-P、Kinetics400-P和SSv2-P），用于分析现有模型的鲁棒性。研究了六种最先进的动作识别模型在90种不同扰动下的表现。<br>
                    效果：研究发现，1) 基于变换器的方法相对于基于CNN的方法更具鲁棒性；2) 预训练对基于变换器的方法的鲁棒性提升作用大于基于CNN的方法；3) 所有研究模型在所有数据集上对时间扰动都具有鲁棒性，但SSv2除外，这表明时间信息在动作识别中的重要性因数据集和活动而异。此外，研究还探讨了数据增强在模型鲁棒性中的作用，并提出了包含真实世界分布偏移的UCF101-DS数据集，以进一步验证部分发现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We have seen great progress in video action recognition in recent years. There are several models based on convolutional neural network (CNN) and some recent transformer based approaches which provide top performance on existing benchmarks. In this work, we perform a large-scale robustness analysis of these existing models for video action recognition. We focus on robustness against real-world distribution shift perturbations instead of adversarial perturbations. We propose four different benchmark datasets, HMDB51-P, UCF101-P, Kinetics400-P, and SSv2-P to perform this analysis. We study robustness of six state-of-the-art action recognition models against 90 different perturbations. The study reveals some interesting findings, 1) Transformer based models are consistently more robust compared to CNN based models, 2) Pre-training improves robustness for Transformer based models more than CNN based models, and 3) All of the studied models are robust to temporal perturbations for all datasets but SSv2; suggesting the importance of temporal information for action recognition varies based on the dataset and activities. Next, we study the role of augmentations in model robustness and present a real-world dataset, UCF101-DS, which contains realistic distribution shifts, to further validate some of these findings. We believe this study will serve as a benchmark for future research in robust video action recognition.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">164.Blind Video Deflickering by Neural Filtering With a Flawed Atlas</span><br>
                <span class="as">Lei, ChenyangandRen, XuanchiandZhang, ZhaoxiangandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_Blind_Video_Deflickering_by_Neural_Filtering_With_a_Flawed_Atlas_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10439-10448.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地去除视频中的闪烁瑕疵？<br>
                    动机：许多视频都存在闪烁的瑕疵，而现有的去闪烁方法通常需要特定的指导，如闪烁频率、手动标注或额外的一致视频。<br>
                    方法：本文提出了一种通用的去闪烁框架，只需要输入一个有闪烁的视频，无需其他指导。该框架的核心是利用神经图谱和神经过滤策略。<br>
                    效果：通过在真实世界的闪烁视频上进行大量实验，验证了该方法的有效性，其性能甚至超过了使用额外指导的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many videos contain flickering artifacts; common causes of flicker include video processing algorithms, video generation algorithms, and capturing videos under specific situations. Prior work usually requires specific guidance such as the flickering frequency, manual annotations, or extra consistent videos to remove the flicker. In this work, we propose a general flicker removal framework that only receives a single flickering video as input without additional guidance. Since it is blind to a specific flickering type or guidance, we name this "blind deflickering." The core of our approach is utilizing the neural atlas in cooperation with a neural filtering strategy. The neural atlas is a unified representation for all frames in a video that provides temporal consistency guidance but is flawed in many cases. To this end, a neural network is trained to mimic a filter to learn the consistent features (e.g., color, brightness) and avoid introducing the artifacts in the atlas. To validate our method, we construct a dataset that contains diverse real-world flickering videos. Extensive experiments show that our method achieves satisfying deflickering performance and even outperforms baselines that use extra guidance on a public benchmark. The source code is publicly available at https://chenyanglei.github.io/deflicker.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">165.Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting</span><br>
                <span class="as">Peng, XiaogangandMao, SiyuanandWu, Zizhao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Trajectory-Aware_Body_Interaction_Transformer_for_Multi-Person_Pose_Forecasting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17121-17130.png><br>
            
            <span class="tt"><span class="t0">研究问题：多人姿态预测在复杂人群中的精细人体交互建模中仍然是一个挑战。<br>
                    动机：现有的方法通常将整个姿态序列表示为一个时间序列，忽视了基于骨骼身体部位的人与人之间的互动影响。<br>
                    方法：我们提出了一种新的轨迹感知的身体互动转换器（TBIFormer）进行多人姿态预测，通过有效地模拟身体部位之间的互动。具体来说，我们构建了一个临时身体部分模块，将所有的姿态序列转换为多个人身体部分序列，以保留基于身体语义的空间和时间信息。然后，我们设计了一个社会身体互动自我注意力（SBI-MSA）模块，利用转换后的序列学习身体部位的动态交互。此外，与先前的欧几里得距离为基础的空间编码不同，我们提出了一种新颖而有效的轨迹感知的相对位置编码，为SBI-MSA提供区分性的空间信息和额外的交互线索。<br>
                    效果：我们在CMU-Mocap、MuPoTS-3D以及合成数据集（6-10人）上进行了实证评估，无论是短期还是长期，都大大超过了最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-person pose forecasting remains a challenging problem, especially in modeling fine-grained human body interaction in complex crowd scenarios. Existing methods typically represent the whole pose sequence as a temporal series, yet overlook interactive influences among people based on skeletal body parts. In this paper, we propose a novel Trajectory-Aware Body Interaction Transformer (TBIFormer) for multi-person pose forecasting via effectively modeling body part interactions. Specifically, we construct a Temporal Body Partition Module that transforms all the pose sequences into a Multi-Person Body-Part sequence to retain spatial and temporal information based on body semantics. Then, we devise a Social Body Interaction Self-Attention (SBI-MSA) module, utilizing the transformed sequence to learn body part dynamics for inter- and intra-individual interactions. Furthermore, different from prior Euclidean distance-based spatial encodings, we present a novel and efficient Trajectory-Aware Relative Position Encoding for SBI-MSA to offer discriminative spatial information and additional interactive clues. On both short- and long-term horizons, we empirically evaluate our framework on CMU-Mocap, MuPoTS-3D as well as synthesized datasets (6   10 persons), and demonstrate that our method greatly outperforms the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">166.Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution</span><br>
                <span class="as">Lu, YunfanandWang, ZipengandLiu, MinjieandWang, HongjianandWang, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Learning_Spatial-Temporal_Implicit_Neural_Representations_for_Event-Guided_Video_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1557-1567.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用事件的时间高分辨率特性，实现任意尺度的视频超分辨率（VSR）任务。<br>
                    动机：由于事件相机的异步感知强度变化和高动态范围、低延迟的特性，启发了研究者使用事件来引导具有挑战性的VSR任务。<br>
                    方法：提出了一个新颖的框架，将事件的空间-时间插值引入到VSR的统一框架中。主要思路是从查询到的空间-时间坐标和RGB帧和事件的特征中学习隐式神经表示。<br>
                    效果：通过在真实世界的数据集中进行大量实验，证明该方法显著超越了现有技术，并实现了任意尺度的VSR。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video super-resolution (VSR) task. In this paper, we make the first at tempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks more explicit motion information from the events near the queried timestamp and generates the 2D features. Lastly, the Spatial-Temporal Implicit Representation (STIR) module recovers the SR frame in arbitrary resolutions from the outputs of these two modules. In addition, we collect a real-world dataset with spatially aligned events and RGB frames. Extensive experiments show that our method significantly surpass the prior-arts and achieves VSR with random scales, e.g., 6.5. Code and dataset are available at https://.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">167.Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network</span><br>
                <span class="as">Zhang, ZhichengandWang, LijuanandYang, Jufeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Video_Emotion_Detection_and_Prediction_via_Cross-Modal_Temporal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18888-18897.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地预测用户生成视频（UGVs）的情绪？<br>
                    动机：现有方法主要关注少数关键视觉帧，可能限制了其编码描绘预期情绪的上下文的能力。<br>
                    方法：提出一种跨模态的时间擦除网络，弱监督方式下不仅定位关键帧，也获取上下文和音频相关信息。首先利用不同片段之间的内部和外部关系准确选择关键帧，然后迭代擦除关键帧以鼓励模型专注于包含互补信息的上下文。<br>
                    效果：在三个具有挑战性的视频情感基准上进行的大量实验表明，该方法优于最先进的方法。代码已在https://github.com/nku-zhichengzhang/WECL上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatically predicting the emotions of user-generated videos (UGVs) receives increasing interest recently. However, existing methods mainly focus on a few key visual frames, which may limit their capacity to encode the context that depicts the intended emotions. To tackle that, in this paper, we propose a cross-modal temporal erasing network that locates not only keyframes but also context and audio-related information in a weakly-supervised manner. In specific, we first leverage the intra- and inter-modal relationship among different segments to accurately select keyframes. Then, we iteratively erase keyframes to encourage the model to concentrate on the contexts that include complementary information. Extensive experiments on three challenging video emotion benchmarks demonstrate that our method performs favorably against state-of-the-art approaches. The code is released on https://github.com/nku-zhichengzhang/WECL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">168.Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline</span><br>
                <span class="as">Geng, TiantianandWang, TengandDuan, JinmingandCong, RunminandZheng, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Dense-Localizing_Audio-Visual_Events_in_Untrimmed_Videos_A_Large-Scale_Benchmark_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22942-22951.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的音视频事件定位方法只处理手动剪辑的视频，每个视频中只有一个实例，这不符合现实生活中自然视频中存在多个不同类别的音视频事件的情况。<br>
                    动机：为了更好地适应现实生活应用，本文关注密集音视频事件定位任务，旨在联合定位和识别未剪辑视频中发生的所有音视频事件。<br>
                    方法：引入第一个未剪辑音视频（UnAV-100）数据集，包含1万个未剪辑的视频和3万个以上的音视频事件。然后，使用新的学习框架来解决这个问题，该框架能够充分整合音频和视觉模态，一次性定位各种长度的音视频事件并捕捉它们之间的依赖关系。<br>
                    效果：大量实验表明了该方法的有效性以及多尺度跨模态感知和依赖建模对于此任务的重要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing audio-visual event localization (AVE) handles manually trimmed videos with only a single instance in each of them. However, this setting is unrealistic as natural videos often contain numerous audio-visual events with different categories. To better adapt to real-life applications, in this paper we focus on the task of dense-localizing audio-visual events, which aims to jointly localize and recognize all audio-visual events occurring in an untrimmed video. The problem is challenging as it requires fine-grained audio-visual scene and context understanding. To tackle this problem, we introduce the first Untrimmed Audio-Visual (UnAV-100) dataset, which contains 10K untrimmed videos with over 30K audio-visual events. Each video has 2.8 audio-visual events on average, and the events are usually related to each other and might co-occur as in real-life scenes. Next, we formulate the task using a new learning-based framework, which is capable of fully integrating audio and visual modalities to localize audio-visual events with various lengths and capture dependencies between them in a single pass. Extensive experiments demonstrate the effectiveness of our method as well as the significance of multi-scale cross-modal perception and dependency modeling for this task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">169.Streaming Video Model</span><br>
                <span class="as">Zhao, YuchengandLuo, ChongandTang, ChuanxinandChen, DongdongandCodella, NoelandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Streaming_Video_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14602-14612.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频理解任务的传统模型架构问题，即序列和帧基的视频任务通常由两种不同的架构分别处理。<br>
                    动机：现有的视频理解任务模型将序列和帧基的任务分开处理，导致效率低下。因此，作者提出将这两种任务统一到一个流媒体视频架构中。<br>
                    方法：作者提出了一种名为Streaming Vision Transformer（S-ViT）的新型流媒体视频架构。该架构首先使用记忆增强的时态感知空间编码器生成帧级特征，以满足帧基视频任务的需求。然后，将帧特征输入到与任务相关的时序解码器中，以获取用于序列基任务的时空特征。<br>
                    效果：实验结果表明，S-ViT在序列基的动作识别任务上达到了最先进的准确率，并在帧基的多目标跟踪任务上比传统架构具有竞争优势。作者认为，流媒体视频模型的概念和S-ViT的实现是朝着视频理解的统一深度学习架构迈出的坚实一步。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at https://github.com/yuzhms/Streaming-Video-Model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">170.Masked Motion Encoding for Self-Supervised Video Representation Learning</span><br>
                <span class="as">Sun, XinyuandChen, PeihaoandChen, LiangweiandLi, ChanghaoandLi, ThomasH.andTan, MingkuiandGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Masked_Motion_Encoding_for_Self-Supervised_Video_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2235-2245.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从无标签视频中学习判别性视频表示是视频分析中具有挑战性但至关重要的问题。<br>
                    动机：现有的方法通过预测被遮蔽区域中的外观内容来学习表示模型，但这可能不足以建模时间线索，因为外观内容可以从单帧轻松重建。<br>
                    方法：提出Masked Motion Encoding（MME）新预训练范式，通过重建外观和运动信息来探索时间线索。具体来说，我们关注两个关键挑战以改善表示性能：1）如何在多个帧上良好地表示可能的长期运动；2）如何从稀疏采样的视频中获得细粒度的时间线索。<br>
                    效果：在MME预训练范式下，模型能够预测长期和细粒度的运动细节。代码可在https://github.com/XinyuSun/MME获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How to learn discriminative video representation from unlabeled videos is challenging but crucial for video analysis. The latest attempts seek to learn a representation model by predicting the appearance contents in the masked regions. However, simply masking and recovering appearance contents may not be sufficient to model temporal clues as the appearance contents can be easily reconstructed from a single frame. To overcome this limitation, we present Masked Motion Encoding (MME), a new pre-training paradigm that reconstructs both appearance and motion information to explore temporal clues. In MME, we focus on addressing two critical challenges to improve the representation performance: 1) how to well represent the possible long-term motion across multiple frames; and 2) how to obtain fine-grained temporal clues from sparsely sampled videos. Motivated by the fact that human is able to recognize an action by tracking objects' position changes and shape changes, we propose to reconstruct a motion trajectory that represents these two kinds of change in the masked regions. Besides, given the sparse video input, we enforce the model to reconstruct dense motion trajectories in both spatial and temporal dimensions. Pre-trained with our MME paradigm, the model is able to anticipate long-term and fine-grained motion details. Code is available at https://github.com/XinyuSun/MME.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">171.Text-Visual Prompting for Efficient 2D Temporal Video Grounding</span><br>
                <span class="as">Zhang, YimengandChen, XinandJia, JinghanandLiu, SijiaandDing, Ke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Text-Visual_Prompting_for_Efficient_2D_Temporal_Video_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14794-14804.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了时间视频定位（TVG）问题，即在一段未剪辑的长视频中预测文本句子描述的时刻的开始/结束时间点。<br>
                    动机：尽管近年来受益于精细的3D视觉特征，TVG技术取得了显著的进步，但3D卷积神经网络的高复杂性使得提取密集的3D视觉特征耗时且需要大量的内存和计算资源。<br>
                    方法：为了实现高效的TVG，我们提出了一种新的文本-视觉提示（TVP）框架，该框架将优化的扰动模式（我们称之为“提示”）纳入TVG模型的视觉输入和文本特征中。与3D CNNs不同，我们证明TVP允许我们在2D TVG模型中有效地共同训练视觉编码器和语言编码器，并仅使用低复杂度的稀疏2D视觉特征来提高跨模态特征融合的性能。此外，我们还提出了一种有效的学习TVG的时序距离IoU（TDIoU）损失函数。<br>
                    效果：在Charades-STA和ActivityNet Captions两个基准数据集上的实验表明，所提出的TVP显著提高了2D TVG的性能（例如，在Charades-STA上提高了9.79%，在ActivityNet Captions上提高了30.77%），并且与使用3D视觉特征的TVG相比，实现了5倍的推理加速。代码可在Open.</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA and ActivityNet Captions datasets, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D visual features. Codes are available at Open.Intel.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">172.Fast Contextual Scene Graph Generation With Unbiased Context Augmentation</span><br>
                <span class="as">Jin, TianleiandGuo, FangtaiandMeng, QiweiandZhu, ShiqiangandXi, XiangmingandWang, WenandMu, ZonghaoandSong, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Fast_Contextual_Scene_Graph_Generation_With_Unbiased_Context_Augmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6302-6311.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决场景图生成方法中的长尾偏差和推理速度慢的问题。<br>
                    动机：人类可以通过上下文描述来分析物体之间的关系，这种抽象的认知过程可能受到经验的指导。<br>
                    方法：提出了一种不使用视觉信息的上下文场景图生成（C-SGG）方法，并引入了上下文增强方法。通过在原始数据集上使用基于位置和大小的对象轻微扰动的上下文增强方法，产生多样化的上下文描述，用于无偏的C-SGG训练以减轻长尾偏差。此外，还引入了一种上下文引导的视觉场景图生成（CV-SGG）方法，利用C-SGG的经验来指导视觉关注可能的谓词。<br>
                    效果：实验结果表明，C-SGG缓解了长尾偏差，省略了视觉特征提取的巨大计算，实现了实时的场景图生成。CV-SGG在常见谓词和尾部谓词之间取得了很好的平衡。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scene graph generation (SGG) methods have historically suffered from long-tail bias and slow inference speed. In this paper, we notice that humans can analyze relationships between objects relying solely on context descriptions,and this abstract cognitive process may be guided by experience. For example, given descriptions of cup and table with their spatial locations, humans can speculate possible relationships < cup, on, table > or < table, near, cup >. Even without visual appearance information, some impossible predicates like flying in and looking at can be empirically excluded. Accordingly, we propose a contextual scene graph generation (C-SGG) method without using visual information and introduce a context augmentation method. We propose that slight perturbations in the position and size of objects do not essentially affect the relationship between objects. Therefore, at the context level, we can produce diverse context descriptions by using a context augmentation method based on the original dataset. These diverse context descriptions can be used for unbiased training of C-SGG to alleviate long-tail bias. In addition, we also introduce a context guided visual scene graph generation (CV-SGG) method, which leverages the C-SGG experience to guide vision to focus on possible predicates. Through extensive experiments on the publicly available dataset, C-SGG alleviates long-tail bias and omits the huge computation of visual feature extraction to realize real-time SGG. CV-SGG achieves a great trade-off between common predicates and tail predicates.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">173.Event-Based Blurry Frame Interpolation Under Blind Exposure</span><br>
                <span class="as">Weng, WenmingandZhang, YueyiandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_Event-Based_Blurry_Frame_Interpolation_Under_Blind_Exposure_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1588-1598.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从低帧率模糊视频中恢复高帧率清晰视频。<br>
                    动机：现有的模糊帧插值方法需要预定义和已知的曝光时间，当应用于野外捕获的视频时，性能会严重下降。<br>
                    方法：利用事件相机进行盲目曝光下的模糊帧插值。首先，通过事件流引导的曝光估计策略来估计丢失的曝光先验，使盲目曝光问题得到良好解决。其次，通过迭代残差学习，用一个时间-曝光控制策略来模拟相互约束。<br>
                    效果：在盲目曝光下，该方法在合成和自行收集的真实世界数据集上的性能均优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Restoring sharp high frame-rate videos from low frame-rate blurry videos is a challenging problem. Existing blurry frame interpolation methods assume a predefined and known exposure time, which suffer from severe performance drop when applied to videos captured in the wild. In this paper, we study the problem of blurry frame interpolation under blind exposure with the assistance of an event camera. The high temporal resolution of the event camera is beneficial to obtain the exposure prior that is lost during the imaging process. Besides, sharp frames can be restored using event streams and blurry frames relying on the mutual constraint among them. Therefore, we first propose an exposure estimation strategy guided by event streams to estimate the lost exposure prior, transforming the blind exposure problem well-posed. Second, we propose to model the mutual constraint with a temporal-exposure control strategy through iterative residual learning. Our blurry frame interpolation method achieves a distinct performance boost over existing methods on both synthetic and self-collected real-world datasets under blind exposure.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">174.Modular Memorability: Tiered Representations for Video Memorability Prediction</span><br>
                <span class="as">Dumont, Th\&#x27;eoandHevia, JuanSegundoandFosco, CamiloL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dumont_Modular_Memorability_Tiered_Representations_for_Video_Memorability_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10751-10760.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何最好地估计视觉内容的记忆力，目前是记忆力社区的争论焦点。<br>
                    动机：不同的图像和视频的关键属性如何影响它们的记忆巩固。<br>
                    方法：分析几种特征的影响，开发一个模型来模拟提出的“记忆路径”最重要的部分。构建M3-S模型，这是一个新颖的记忆网络，以模块化的方式处理输入的视频。<br>
                    效果：我们的模块学习的不同表示是非平凡的，并且彼此之间有显著的差异。此外，我们发现某些表示在记忆预测任务上比其他表示表现得更好。我们的方法超越了在两个最大的视频记忆数据集上的最先进的技术，并为该领域开辟了新的应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The question of how to best estimate the memorability of visual content is currently a source of debate in the memorability community. In this paper, we propose to explore how different key properties of images and videos affect their consolidation into memory. We analyze the impact of several features and develop a model that emulates the most important parts of a proposed "pathway to memory": a simple but effective way of representing the different hurdles that new visual content needs to surpass to stay in memory. This framework leads to the construction of our M3-S model, a novel memorability network that processes input videos in a modular fashion. Each module of the network emulates one of the four key steps of the pathway to memory: raw encoding, scene understanding, event understanding and memory consolidation. We find that the different representations learned by our modules are non-trivial and substantially different from each other. Additionally, we observe that certain representations tend to perform better at the task of memorability prediction than others, and we introduce an in-depth ablation study to support our results. Our proposed approach surpasses the state of the art on the two largest video memorability datasets and opens the door to new applications in the field.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">175.Re2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal Action Localization</span><br>
                <span class="as">Zhao, ChenandLiu, ShumingandMangalam, KarttikeyaandGhanem, Bernard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10637-10647.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行长期形式推理，预测各种持续时间和复杂内容的动作？<br>
                    动机：由于GPU内存有限，对长视频进行端到端的TAL训练是一个重大挑战。大多数方法只能在没有优化特征的情况下训练预提取的特征，从而限制了定位性能。<br>
                    方法：我们提出了一种新的端到端方法Re2TAL，通过重新连接预训练的视频主干网络来实现可逆的TAL。Re2TAL构建了一个具有可逆模块的主干网络，输入可以从输出中恢复，从而在训练过程中清除内存中的大量中间激活。<br>
                    效果：Re2TAL仅使用RGB模态，在ActivityNet-v1.3上达到了37.01%的平均mAP，创造了新的最先进的记录，并在THUMOS-14上以tIoU=0.5达到64.9%的mAP，超过了所有其他仅使用RGB的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal action localization (TAL) requires long-form reasoning to predict actions of various durations and complex content. Given limited GPU memory, training TAL end to end (i.e., from videos to predictions) on long videos is a significant challenge. Most methods can only train on pre-extracted features without optimizing them for the localization problem, consequently limiting localization performance. In this work, to extend the potential in TAL networks, we propose a novel end-to-end method Re2TAL, which rewires pretrained video backbones for reversible TAL. Re2TAL builds a backbone with reversible modules, where the input can be recovered from the output such that the bulky intermediate activations can be cleared from memory during training. Instead of designing one single type of reversible module, we propose a network rewiring mechanism, to transform any module with a residual connection to a reversible module without changing any parameters. This provides two benefits: (1) a large variety of reversible networks are easily obtained from existing and even future model designs, and (2) the reversible models require much less training effort as they reuse the pre-trained parameters of their original non-reversible versions. Re2TAL, only using the RGB modality, reaches 37.01% average mAP on ActivityNet-v1.3, a new state-of-the-art record, and mAP 64.9% at tIoU=0.5 on THUMOS-14, outperforming all other RGB-only methods. Code is available at https://github.com/coolbay/Re2TAL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">176.Data-Driven Feature Tracking for Event Cameras</span><br>
                <span class="as">Messikommer, NicoandFang, CarterandGehrig, MathiasandScaramuzza, Davide</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5642-5651.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高事件相机在低延迟和低带宽特征跟踪任务中的性能。<br>
                    动机：现有的事件相机特征跟踪方法需要大量参数调整，对噪声敏感，且无法适应不同的场景。<br>
                    方法：提出一种数据驱动的事件相机特征跟踪器，利用低延迟事件追踪灰度帧中检测到的特征。通过一种新型的帧注意模块实现稳健性能，该模块在特征轨迹之间共享信息。<br>
                    效果：通过直接从合成数据到真实数据的零样本转移，所提出的方法在相对特征年龄方面比现有方法提高了120%，同时实现了最低的延迟。通过一种新的自我监督策略将跟踪器适应于真实数据，这种性能差距进一步增加到130%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Because of their high temporal resolution, increased resilience to motion blur, and very sparse output, event cameras have been shown to be ideal for low-latency and low-bandwidth feature tracking, even in challenging scenarios. Existing feature tracking methods for event cameras are either handcrafted or derived from first principles but require extensive parameter tuning, are sensitive to noise, and do not generalize to different scenarios due to unmodeled effects. To tackle these deficiencies, we introduce the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in a grayscale frame. We achieve robust performance via a novel frame attention module, which shares information across feature tracks. By directly transferring zero-shot from synthetic to real data, our data-driven tracker outperforms existing approaches in relative feature age by up to 120% while also achieving the lowest latency. This performance gap is further increased to 130% by adapting our tracker to real data with a novel self-supervision strategy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">177.Autoregressive Visual Tracking</span><br>
                <span class="as">Wei, XingandBai, YifanandZheng, YongchaoandShi, DahuandGong, Yihong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9697-9706.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种自动回归的视觉目标跟踪框架。<br>
                    动机：现有的基于模板匹配的目标跟踪器仅考虑每帧的定位精度，而忽视了目标在连续帧之间的运动轨迹。<br>
                    方法：ARTrack采用时间自回归的方法来模拟目标运动轨迹的序列演化，从而在连续帧中持续追踪目标。<br>
                    效果：ARTrack简单直接，无需定制的定位头和后处理，尽管其简洁，但在主流基准数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ARTrack, an autoregressive framework for visual object tracking. ARTrack tackles tracking as a coordinate sequence interpretation task that estimates object trajectories progressively, where the current estimate is induced by previous states and in turn affects subsequences. This time-autoregressive approach models the sequential evolution of trajectories to keep tracing the object across frames, making it superior to existing template matching based trackers that only consider the per-frame localization accuracy. ARTrack is simple and direct, eliminating customized localization heads and post-processings. Despite its simplicity, ARTrack achieves state-of-the-art performance on prevailing benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">178.ASPnet: Action Segmentation With Shared-Private Representation of Multiple Data Sources</span><br>
                <span class="as">vanAmsterdam, BeatriceandKadkhodamohammadi, AbdolrahimandLuengo, ImanolandStoyanov, Danail</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/van_Amsterdam_ASPnet_Action_Segmentation_With_Shared-Private_Representation_of_Multiple_Data_Sources_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2384-2393.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地融合互补信息，提高动作分割模型的鲁棒性和准确性。<br>
                    动机：大多数最先进的动作分割方法基于单一输入模态或多数据源的简单融合，而有效的信息融合可以强化分割模型，使其对传感器噪声更具鲁棒性，并在较小的训练数据集上更准确。<br>
                    方法：我们提出了一种多模态分割模型，将隐藏特征解耦为模态共享组件和私有组件，然后使用注意力瓶颈来捕获数据中的长范围时间依赖性，同时保留连续处理层的解耦性。<br>
                    效果：在50salads、Breakfast和RARP45数据集上的评估表明，我们的多模态方法在多视图和多模态数据源上都优于不同的数据融合基线，与最先进的方法相比，获得了竞争或更好的结果。我们的模型对添加的传感器噪声更具鲁棒性，即使训练数据较少，也可以实现与强大的视频基线相媲美的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most state-of-the-art methods for action segmentation are based on single input modalities or naive fusion of multiple data sources. However, effective fusion of complementary information can potentially strengthen segmentation models and make them more robust to sensor noise and more accurate with smaller training datasets. In order to improve multimodal representation learning for action segmentation, we propose to disentangle hidden features of a multi-stream segmentation model into modality-shared components, containing common information across data sources, and private components; we then use an attention bottleneck to capture long-range temporal dependencies in the data while preserving disentanglement in consecutive processing layers. Evaluation on 50salads, Breakfast and RARP45 datasets shows that our multimodal approach outperforms different data fusion baselines on both multiview and multimodal data sources, obtaining competitive or better results compared with the state-of-the-art. Our model is also more robust to additive sensor noise and can achieve performance on par with strong video baselines even with less training data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">179.Skinned Motion Retargeting With Residual Perception of Motion Semantics \&amp; Geometry</span><br>
                <span class="as">Zhang, JiaxuandWeng, JunwuandKang, DiandZhao, FangandHuang, ShaoliandZhe, XuefeiandBao, LinchaoandShan, YingandWang, JueandTu, Zhigang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Skinned_Motion_Retargeting_With_Residual_Perception_of_Motion_Semantics__CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13864-13872.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行运动重定向，同时考虑到源和目标在骨架和形状几何级别上的差异。<br>
                    动机：现有的运动重定向方法无法很好地处理源和目标在骨架和形状几何级别上的差异，导致重定向效果不理想。<br>
                    方法：提出一种新的残差RETargeting网络（R2ET）结构，通过两个神经修改模块逐步调整源动作以适应目标骨架和形状。具体来说，引入了一个骨架感知模块来保留源动作的语义，设计了一个形状感知模块来感知目标角色的几何形状，以减少穿透和接触缺失。通过探索的距离损失模型显式地模拟运动语义和几何形状，这两个模块可以在单次推理中学习源动作的剩余运动修改，生成合理的重定向运动，无需后处理。为了平衡这两种修改，进一步提出了一个平衡门来进行线性插值。<br>
                    效果：在公共数据集Mixamo上的大量实验表明，我们的R2ET实现了最先进的性能，并在保持运动语义的同时有效地减少了穿透和接触缺失。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A good motion retargeting cannot be reached without reasonable consideration of source-target differences on both the skeleton and shape geometry levels. In this work, we propose a novel Residual RETargeting network (R2ET) structure, which relies on two neural modification modules, to adjust the source motions to fit the target skeletons and shapes progressively. In particular, a skeleton-aware module is introduced to preserve the source motion semantics. A shape-aware module is designed to perceive the geometries of target characters to reduce interpenetration and contact-missing. Driven by our explored distance-based losses that explicitly model the motion semantics and geometry, these two modules can learn residual motion modifications on the source motion to generate plausible retargeted motion in a single inference without post-processing. To balance these two modifications, we further present a balancing gate to conduct linear interpolation between them. Extensive experiments on the public dataset Mixamo demonstrate that our R2ET achieves the state-of-the-art performance, and provides a good balance between the preservation of motion semantics as well as the attenuation of interpenetration and contact-missing. Code is available at https://github.com/Kebii/R2ET.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">180.Learning Situation Hyper-Graphs for Video Question Answering</span><br>
                <span class="as">Urooj, AishaandKuehne, HildeandWu, BoandChheu, KimandBousselham, WalidandGan, ChuangandLobo, NielsandShah, Mubarak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Urooj_Learning_Situation_Hyper-Graphs_for_Video_Question_Answering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14879-14889.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频问答需要捕捉演员、物体及其关系的出现和演变。<br>
                    动机：现有的视频问答模型无法充分捕获这些关系，因此提出一种基于情况超图的视频问答架构。<br>
                    方法：通过训练一个情况超图解码器，从输入视频片段中隐式识别动作和人/物关系的图形表示，并使用预测的情况超图与问题嵌入之间的交叉注意力来预测正确答案。<br>
                    效果：在两个具有挑战性的基准测试AGQA和STAR上进行广泛评估，结果显示学习底层情况超图有助于显著提高系统在视频问答任务上的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Answering questions about complex situations in videos requires not only capturing of the presence of actors, objects, and their relations, but also the evolution of these relationships over time. A situation hyper-graph is a representation that describes situations as scene sub-graphs for video frames and hyper-edges for connected sub-graphs, and has been proposed to capture all such information in a compact structured form. In this work, we propose an architecture for Video Question Answering (VQA) that enables answering questions related to video content by predicting situation hyper-graphs, coined Situation Hyper-Graph based Video Question Answering (SHG-VQA). To this end, we train a situation hyper-graph decoder to implicitly identify graph representations with actions and object/human-object relationships from the input video clip and to use cross-attention between the predicted situation hyper-graphs and the question embedding to predict the correct answer. The proposed method is trained in an end-to-end manner and optimized by a cross-entropy based VQA loss function and a Hungarian matching loss for the situation graph prediction. The effectiveness of the proposed architecture is extensively evaluated on two challenging benchmarks: AGQA and STAR. Our results show that learning the underlying situation hyper-graphs helps the system to significantly improve its performance for novel challenges of video question answering task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">181.Leveraging Temporal Context in Low Representational Power Regimes</span><br>
                <span class="as">Fosco, CamiloL.andJin, SouYoungandJosephs, EmilieandOliva, Aude</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10693-10703.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高低参数模型（如边缘设备上的模型）的性能？<br>
                    动机：计算机视觉模型虽然能很好地识别和利用世界中的规律，但从头开始学习这些规律计算成本高，对低参数模型来说是个挑战。<br>
                    方法：通过引入事件转移矩阵（ETM），从未剪辑的视频数据集中的动作标签中捕获给定动作的时间上下文（即该动作被集合中的其他动作前后跟随的可能性），并在训练过程中包含来自ETM的信息来提高动作识别和预测性能。<br>
                    效果：实验结果表明，包括来自ETM的信息可以显著提高各种自我中心视频数据集上的动作识别和预测性能，且这种明确表示时间上下文的好处在小型模型上最为明显。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Computer vision models are excellent at identifying and exploiting regularities in the world. However, it is computationally costly to learn these regularities from scratch. This presents a challenge for low-parameter models, like those running on edge devices (e.g. smartphones). Can the performance of models with low representational power be improved by supplementing training with additional information about these statistical regularities? We explore this in the domains of action recognition and action anticipation, leveraging the fact that actions are typically embedded in stereotypical sequences. We introduce the Event Transition Matrix (ETM), computed from action labels in an untrimmed video dataset, which captures the temporal context of a given action, operationalized as the likelihood that it was preceded or followed by each other action in the set. We show that including information from the ETM during training improves action recognition and anticipation performance on various egocentric video datasets. Through ablation and control studies, we show that the coherent sequence of information captured by our ETM is key to this effect, and we find that the benefit of this explicit representation of temporal context is most pronounced for smaller models. Code, matrices and models are available in our project page: https://camilofosco.com/etm_website.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">182.Audio-Visual Grouping Network for Sound Localization From Mixtures</span><br>
                <span class="as">Mo, ShentongandTian, Yapeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mo_Audio-Visual_Grouping_Network_for_Sound_Localization_From_Mixtures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10565-10574.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决音频源定位问题，即预测视频中声音来源的位置。<br>
                    动机：现有的单源方法主要使用音视关联作为线索来定位每个帧中的声音对象，但对于混合的多源情况处理能力有限。<br>
                    方法：本文提出了一种新的音视分组网络（AVGN），该网络可以直接从输入的音频混合物和帧中学习每个源的类别感知语义特征，以同时定位多个源。<br>
                    效果：实验结果表明，与现有的多源方法相比，新的AVGN框架可以定位灵活数量的源，并分离个体声音源的类别感知音视表示。在MUSIC、VGGSound-Instruments和VGG-Sound Sources基准测试中，AVGN达到了最先进的声音对象定位性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sound source localization is a typical and challenging task that predicts the location of sound sources in a video. Previous single-source methods mainly used the audio-visual association as clues to localize sounding objects in each frame. Due to the mixed property of multiple sound sources in the original space, there exist rare multi-source approaches to localizing multiple sources simultaneously, except for one recent work using a contrastive random walk in the graph with images and separated sound as nodes. Despite their promising performance, they can only handle a fixed number of sources, and they cannot learn compact class-aware representations for individual sources. To alleviate this shortcoming, in this paper, we propose a novel audio-visual grouping network, namely AVGN, that can directly learn category-wise semantic features for each source from the input audio mixture and frame to localize multiple sources simultaneously. Specifically, our AVGN leverages learnable audio-visual class tokens to aggregate class-aware source features. Then, the aggregated semantic features for each source can be used as guidance to localize the corresponding visual regions. Compared to existing multi-source methods, our new framework can localize a flexible number of sources and disentangle category-aware audio-visual representations for individual sound sources. We conduct extensive experiments on MUSIC, VGGSound-Instruments, and VGG-Sound Sources benchmarks. The results demonstrate that the proposed AVGN can achieve state-of-the-art sounding object localization performance on both single-source and multi-source scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">183.Tracking Multiple Deformable Objects in Egocentric Videos</span><br>
                <span class="as">Huang, MingzhenandLi, XiaoxingandHu, JunandPeng, HonghongandLyu, Siwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tracking_Multiple_Deformable_Objects_in_Egocentric_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1461-1471.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多目标跟踪方法在处理高度变形的目标时存在困难，特别是在自我中心的视频中。<br>
                    动机：为了解决这些问题，我们提出了一种新的多目标跟踪方法DETracker，该方法可以有效地检测和跟踪自我中心视频中的变形目标。<br>
                    方法：DETracker使用了三个新的模块：运动解耦网络（MDN）、补丁关联网络（PAN）和补丁记忆网络（PMN），以明确解决由严重的自我运动和快速变形的目标对象引起的困难。<br>
                    效果：DETracker是端到端可训练的，并且实现了接近实时的速度。我们在智能眼镜上收集了一个大规模的变形多目标跟踪数据集DogThruGlasses，包含150个视频和73K个标注帧。实验结果表明，DETracker在DogThruGlasses数据集和YouTube-Hand数据集上都优于现有的最先进方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing multiple object tracking (MOT) methods that solely rely on appearance features struggle in tracking highly deformable objects. Other MOT methods that use motion clues to associate identities across frames have difficulty handling egocentric videos effectively or efficiently. In this work, we propose DETracker, a new MOT method that jointly detects and tracks deformable objects in egocentric videos. DETracker uses three novel modules, namely the motion disentanglement network (MDN), the patch association network (PAN) and the patch memory network (PMN), to explicitly tackle the difficulties caused by severe ego motion and fast morphing target objects. DETracker is end-to-end trainable and achieves near real-time speed. We also present DogThruGlasses, a large-scale deformable multi-object tracking dataset, with 150 videos and 73K annotated frames, collected by smart glasses. DETracker outperforms existing state-of-the-art method on the DogThruGlasses dataset and YouTube-Hand dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">184.Open Set Action Recognition via Multi-Label Evidential Learning</span><br>
                <span class="as">Zhao, ChenandDu, DaweiandHoogs, AnthonyandFunk, Christopher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Open_Set_Action_Recognition_via_Multi-Label_Evidential_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22982-22991.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的开放集动作识别方法主要关注假设视频片段只显示单一动作的新颖性检测，这在现实世界中是不现实的。<br>
                    动机：我们提出了一种新的开放集动作识别和新颖性检测方法，通过MUlti-Label Evidential learning（MULE），超越了以前的新动作检测方法，解决了同一场景中的单个或多个演员以及任何演员同时进行多个动作的更一般性问题。<br>
                    方法：我们的方法使用Beta Evidential神经网络，基于演员-上下文-对象关系表示，用Beta密度估计多动作不确定性。我们还添加了一个证据去偏约束到优化目标函数中，以减少视频表示的静态偏差，这种偏差可能会错误地将预测与静态线索相关联。此外，我们开发了一种原始-对偶平均方案更新学习算法来优化提出的问题，并提供了相应的理论分析。我们还制定了基于不确定性和信念的新颖性估计机制来检测新的动作。<br>
                    效果：我们在两个真实世界的视频数据集上进行了广泛的实验，结果显示，我们的方法在单/多演员、单/多动作设置下都取得了良好的性能。我们的代码和模型可以在https://github.com/charliezhaoyinpeng/mule上找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods for open set action recognition focus on novelty detection that assumes video clips show a single action, which is unrealistic in the real world. We propose a new method for open set action recognition and novelty detection via MUlti-Label Evidential learning (MULE), that goes beyond previous novel action detection methods by addressing the more general problems of single or multiple actors in the same scene, with simultaneous action(s) by any actor. Our Beta Evidential Neural Network estimates multi-action uncertainty with Beta densities based on actor-context-object relation representations. An evidence debiasing constraint is added to the objective func- tion for optimization to reduce the static bias of video representations, which can incorrectly correlate predictions and static cues. We develop a primal-dual average scheme update-based learning algorithm to optimize the proposed problem and provide corresponding theoretical analysis. Besides, uncertainty and belief-based novelty estimation mechanisms are formulated to detect novel actions. Extensive experiments on two real-world video datasets show that our proposed approach achieves promising performance in single/multi-actor, single/multi-action settings. Our code and models are released at https://github.com/charliezhaoyinpeng/mule.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">185.Rethinking Optical Flow From Geometric Matching Consistent Perspective</span><br>
                <span class="as">Dong, QiaoleandCao, ChenjieandFu, Yanwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Rethinking_Optical_Flow_From_Geometric_Matching_Consistent_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1337-1347.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高光流估计的准确性和鲁棒性。<br>
                    动机：目前的深度学习模型在训练时通常从零开始，限制了其对图像特征的鲁棒几何匹配能力。<br>
                    方法：提出一种新颖的光流估计方法MatchFlow，利用几何图像匹配（GIM）作为预训练任务，通过大规模真实世界数据学习物体和场景的基本特征相关性。<br>
                    效果：实验表明，该方法具有很好的跨数据集泛化能力，并在Sintel和KITTI数据集上取得了显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optical flow estimation is a challenging problem remaining unsolved. Recent deep learning based optical flow models have achieved considerable success. However, these models often train networks from the scratch on standard optical flow data, which restricts their ability to robustly and geometrically match image features. In this paper, we propose a rethinking to previous optical flow estimation. We particularly leverage Geometric Image Matching (GIM) as a pre-training task for the optical flow estimation (MatchFlow) with better feature representations, as GIM shares some common challenges as optical flow estimation, and with massive labeled real-world data. Thus, matching static scenes helps to learn more fundamental feature correlations of objects and scenes with consistent displacements. Specifically, the proposed MatchFlow model employs a QuadTree attention-based network pre-trained on MegaDepth to extract coarse features for further flow regression. Extensive experiments show that our model has great cross-dataset generalization. Our method achieves 11.5% and 10.1% error reduction from GMA on Sintel clean pass and KITTI test set. At the time of anonymous submission, our MatchFlow(G) enjoys state-of-theart performance on Sintel clean and final pass compared to published approaches with comparable computation and memory footprint. Codes and models will be released in https://github.com/DQiaole/MatchFlow.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">186.Learning Optical Expansion From Scale Matching</span><br>
                <span class="as">Ling, HanandSun, YinghuiandSun, QuansenandRen, Zhenwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_Learning_Optical_Expansion_From_Scale_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5445-5454.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决光学扩展（OE）的问题，这是在单目3D视觉任务中描述两帧之间的对象尺度变化。<br>
                    动机：现有的方法主要从光流结果来估计光学扩展，但这种两阶段架构使得其结果受限于光流的准确性，且不够稳健。<br>
                    方法：我们提出了通过将光学扩展整合到二维光流中来实现三维光流的概念，这由一个即插即用的模块TPCV实现。TPCV在正确的位置和尺度上匹配特征，从而实现了光流和光学扩展任务的同步优化。<br>
                    效果：实验结果表明，基于RAFT光流基准线的TPCV可以显著提高基线光流性能。此外，我们将光流和光学扩展的结果应用于各种动态3D视觉任务，包括深度运动、碰撞时间和场景流，通常比先前的最佳方法有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper address the problem of optical expansion (OE). OE describes the object scale change between two frames, widely used in monocular 3D vision tasks. Previous methods estimate optical expansion mainly from optical flow results, but this two-stage architecture makes their results limited by the accuracy of optical flow and less robust. To solve these problems, we propose the concept of 3D optical flow by integrating optical expansion into the 2D optical flow, which is implemented by a plug-and-play module, namely TPCV. TPCV implements matching features at the correct location and scale, thus allowing the simultaneous optimization of optical flow and optical expansion tasks. Experimentally, we apply TPCV to the RAFT optical flow baseline. Experimental results show that the baseline optical flow performance is substantially improved. Moreover, we apply the optical flow and optical expansion results to various dynamic 3D vision tasks, including motion-in-depth, time-to-collision, and scene flow, often achieving significant improvement over the prior SOTA. Code will be available at https://github.com/HanLingsgjk/TPCV.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">187.Context De-Confounded Emotion Recognition</span><br>
                <span class="as">Yang, DingkangandChen, ZhaoyuandWang, YuzhengandWang, ShunliandLi, MingchengandLiu, SiaoandZhao, XiaoandHuang, ShuaiandDong, ZhiyanandZhai, PengandZhang, Lihua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Context_De-Confounded_Emotion_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19005-19015.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有情感识别任务中存在的数据集上下文偏差问题，该偏差会导致不同情境下的情感状态分布严重不平衡，从而影响模型的性能。<br>
                    动机：当前的情感识别方法主要关注从主体和上下文中提取有意义的表示，但忽视了数据集中的上下文偏差问题。这种偏差会误导模型学习基于传统似然估计的虚假相关性，从而限制模型的性能。<br>
                    方法：本文从因果关系的角度出发，通过定制的因果图来描述CAER任务中变量之间的因果关系。然后，提出了一种基于后门调整的情境因果干预模块（CCIM），以消除混淆因子并利用真实的因果关系进行模型训练。<br>
                    效果：在三个基准数据集上的大量实验表明，我们的CCIM及其因果关系见解的有效性显著提高了各种最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Context-Aware Emotion Recognition (CAER) is a crucial and challenging task that aims to perceive the emotional states of the target person with contextual information. Recent approaches invariably focus on designing sophisticated architectures or mechanisms to extract seemingly meaningful representations from subjects and contexts. However, a long-overlooked issue is that a context bias in existing datasets leads to a significantly unbalanced distribution of emotional states among different context scenarios. Concretely, the harmful bias is a confounder that misleads existing models to learn spurious correlations based on conventional likelihood estimation, significantly limiting the models' performance. To tackle the issue, this paper provides a causality-based perspective to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a tailored causal graph. Then, we propose a Contextual Causal Intervention Module (CCIM) based on the backdoor adjustment to de-confound the confounder and exploit the true causal effect for model training. CCIM is plug-in and model-agnostic, which improves diverse state-of-the-art approaches by considerable margins. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our CCIM and the significance of causal insight.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">188.Breaking the &#x27;&#x27;Object&#x27;&#x27; in Video Object Segmentation</span><br>
                <span class="as">Tokmakov, PavelandLi, JieandGaidon, Adrien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tokmakov_Breaking_the_Object_in_Video_Object_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22836-22845.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视频对象分割（VOS）基准主要缺乏对物体变换的考虑。<br>
                    动机：物体在变化过程中，其颜色、形状和纹理都会发生显著改变，但现有的VOS方法主要依赖静态的外观线索，对于这种复杂的场景处理能力较弱。<br>
                    方法：收集了一个新的视频对象分割数据集——Video Object Segmentation under Transformations (VOST)，包含700多个高分辨率的视频片段，这些视频都经过了详细的标注。通过一系列细致的评估和实验，提出了一些改进现有VOS方法的方案。<br>
                    效果：实验结果表明，现有的VOS方法在处理物体变换时表现不佳，而新提出的改进方案可以更好地捕捉空间-时间信息，从而提高了VOS的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The appearance of an object can be fleeting when it transforms. As eggs are broken or paper is torn, their color, shape, and texture can change dramatically, preserving virtually nothing of the original except for the identity itself. Yet, this important phenomenon is largely absent from existing video object segmentation (VOS) benchmarks. In this work, we close the gap by collecting a new dataset for Video Object Segmentation under Transformations (VOST). It consists of more than 700 high-resolution videos, captured in diverse environments, which are 20 seconds long on average and densely labeled with instance masks. A careful, multi-step approach is adopted to ensure that these videos focus on complex object transformations, capturing their full temporal extent. We then extensively evaluate state-of-the-art VOS methods and make a number of important discoveries. In particular, we show that existing methods struggle when applied to this novel task and that their main limitation lies in over-reliance on static, appearance cues. This motivates us to propose a few modifications for the top-performing baseline that improve its performance by better capturing spatio-temporal information. But more broadly, the hope is to stimulate discussion on learning more robust video object representations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">189.CIGAR: Cross-Modality Graph Reasoning for Domain Adaptive Object Detection</span><br>
                <span class="as">Liu, YaboandWang, JinghuaandHuang, ChaoandWang, YaoweiandXu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_CIGAR_Cross-Modality_Graph_Reasoning_for_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23776-23786.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无监督领域自适应目标检测（UDA-OD）中的问题，即如何从标记的源领域泛化知识到未标记的目标领域。<br>
                    动机：尽管现有的基于图的方法在UDA-OD上表现良好，但它们不能学习出合适的图节点集，并且这些方法仅基于视觉特征构建图，没有考虑语义原型所携带的语言知识。<br>
                    方法：为了克服这些问题，我们提出了一种跨模态图推理适应（CIGAR）方法，利用视觉和语言知识。具体来说，我们的方法在语言模态图和视觉模态图之间进行跨模态图推理，以增强它们的表示。我们还提出了一个判别性特征选择器，以找到最具判别性的特征，并将其作为视觉图的节点，以提高效率和效果。此外，我们还使用语言图匹配损失来规范语言图的更新，并在训练过程中保持其语义表示。<br>
                    效果：全面的实验验证了我们提出的CIGAR的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised domain adaptive object detection (UDA-OD) aims to learn a detector by generalizing knowledge from a labeled source domain to an unlabeled target domain. Though the existing graph-based methods for UDA-OD perform well in some cases, they cannot learn a proper node set for the graph. In addition, these methods build the graph solely based on the visual features and do not consider the linguistic knowledge carried by the semantic prototypes, e.g., dataset labels. To overcome these problems, we propose a cross-modality graph reasoning adaptation (CIGAR) method to take advantage of both visual and linguistic knowledge. Specifically, our method performs cross-modality graph reasoning between the linguistic modality graph and visual modality graphs to enhance their representations. We also propose a discriminative feature selector to find the most discriminative features and take them as the nodes of the visual graph for both efficiency and effectiveness. In addition, we employ the linguistic graph matching loss to regulate the update of linguistic graphs and maintain their semantic representation during the training process. Comprehensive experiments validate the effectiveness of our proposed CIGAR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">190.TriDet: Temporal Action Detection With Relative Boundary Modeling</span><br>
                <span class="as">Shi, DingfengandZhong, YujieandCao, QiongandMa, LinandLi, JiaandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18857-18866.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频中动作边界模糊导致的边界预测不准确的问题。<br>
                    动机：现有的方法在处理视频中的动作边界时，由于边界的模糊性，常常导致预测的边界不准确。<br>
                    方法：提出了一种名为TriDet的单阶段框架进行时间动作检测。其中，设计了一种新的Trident-head来通过估计边界周围的相对概率分布来建模动作边界；并在TriDet的特征金字塔中，提出了一种可扩展粒度感知（SGP）层，用于聚合不同时间粒度的信息，这比最近的基于变压器的特征金字塔更有效。<br>
                    效果：得益于Trident-head和基于SGP的特征金字塔，TriDet在THUMOS14、HACS和EPIC-KITCHEN 100三个具有挑战性的基准测试上取得了最先进的性能，并且计算成本更低。例如，TriDet在THUMOS14上的平均mAP达到了69.3%，比之前的最佳结果提高了2.5%，但其延迟仅为74.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a one-stage framework TriDet for temporal action detection. Existing methods often suffer from imprecise boundary predictions due to the ambiguous action boundaries in videos. To alleviate this problem, we propose a novel Trident-head to model the action boundary via an estimated relative probability distribution around the boundary. In the feature pyramid of TriDet, we propose a Scalable-Granularity Perception (SGP) layer to aggregate information across different temporal granularities, which is much more efficient than the recent transformer-based feature pyramid. Benefiting from the Trident-head and the SGP-based feature pyramid, TriDet achieves state-of-the-art performance on three challenging benchmarks: THUMOS14, HACS and EPIC-KITCHEN 100, with lower computational costs, compared to previous methods. For example, TriDet hits an average mAP of 69.3% on THUMOS14, outperforming the previous best by 2.5%, but with only 74.6% of its latency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">191.Joint Appearance and Motion Learning for Efficient Rolling Shutter Correction</span><br>
                <span class="as">Fan, BinandMao, YuxinandDai, YuchaoandWan, ZhexiongandLiu, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_Joint_Appearance_and_Motion_Learning_for_Efficient_Rolling_Shutter_Correction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5671-5681.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高滚动快门修正（RSC）的效率和性能。<br>
                    动机：现有的RSC方法通常采用两阶段网络结构，忽视了内在信息交互，且推理速度慢。<br>
                    方法：提出一种名为JAMNet的单阶段编码器-解码器网络进行高效RSC。首先从连续的RS输入中提取金字塔特征，然后在联合学习解码器中同时优化两种互补信息（即全局快门外观和无畸变运动场），实现相互促进。<br>
                    效果：在各种基准测试中，该方法比最先进的方法有显著改进，特别是在真实世界的RSC上，PSNR提高了4.7 dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rolling shutter correction (RSC) is becoming increasingly popular for RS cameras that are widely used in commercial and industrial applications. Despite the promising performance, existing RSC methods typically employ a two-stage network structure that ignores intrinsic information interactions and hinders fast inference. In this paper, we propose a single-stage encoder-decoder-based network, named JAMNet, for efficient RSC. It first extracts pyramid features from consecutive RS inputs, and then simultaneously refines the two complementary information (i.e., global shutter appearance and undistortion motion field) to achieve mutual promotion in a joint learning decoder. To inject sufficient motion cues for guiding joint learning, we introduce a transformer-based motion embedding module and propose to pass hidden states across pyramid levels. Moreover, we present a new data augmentation strategy "vertical flip + inverse order" to release the potential of the RSC datasets. Experiments on various benchmarks show that our approach surpasses the state-of-the-art methods by a large margin, especially with a 4.7 dB PSNR leap on real-world RSC. Code is available at https://github.com/GitCVfb/JAMNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">192.Selective Structured State-Spaces for Long-Form Video Understanding</span><br>
                <span class="as">Wang, JueandZhu, WentaoandWang, PichaoandYu, XiangandLiu, LindaandOmar, MohamedandHamid, Raffay</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Selective_Structured_State-Spaces_for_Long-Form_Video_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6387-6397.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对长视频中的复杂时空依赖关系进行建模？<br>
                    动机：尽管最近的结构化状态空间序列（S4）模型在处理此问题上具有线性复杂度，但其对所有图像标记的平等处理可能会影响其效率和准确性。<br>
                    方法：我们提出了一种新的选择性S4（即S5）模型，该模型使用轻量级掩码生成器来自适应地选择信息丰富的图像标记，从而更有效地对视频中的长期时空依赖关系进行建模。<br>
                    效果：通过在三个具有挑战性的长视频理解数据集（LVU、COIN和Breakfast）上进行的广泛比较，我们的模型比之前最先进的S4模型提高了9.6%的准确性，同时减少了23%的内存占用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Effective modeling of complex spatiotemporal dependencies in long-form videos remains an open problem. The recently proposed Structured State-Space Sequence (S4) model with its linear complexity offers a promising direction in this space. However, we demonstrate that treating all image-tokens equally as done by S4 model can adversely affect its efficiency and accuracy. To address this limitation, we present a novel Selective S4 (i.e., S5) model that employs a lightweight mask generator to adaptively select informative image tokens resulting in more efficient and accurate modeling of long-term spatiotemporal dependencies in videos. Unlike previous mask-based token reduction methods used in transformers, our S5 model avoids the dense self-attention calculation by making use of the guidance of the momentum-updated S4 model. This enables our model to efficiently discard less informative tokens and adapt to various long-form video understanding tasks more effectively. However, as is the case for most token reduction methods, the informative image tokens could be dropped incorrectly. To improve the robustness and the temporal horizon of our model, we propose a novel long-short masked contrastive learning (LSMCL) approach that enables our model to predict longer temporal context using shorter input videos. We present extensive comparative results using three challenging long-form video understanding datasets (LVU, COIN and Breakfast), demonstrating that our approach consistently outperforms the previous state-of-the-art S4 model by up to 9.6% accuracy while reducing its memory footprint by 23%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">193.Motion Information Propagation for Neural Video Compression</span><br>
                <span class="as">Qi, LinfengandLi, JiahaoandLi, BinandLi, HouqiangandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Motion_Information_Propagation_for_Neural_Video_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6111-6120.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有神经网络视频编码器中信息流单向性的问题，即只有运动编码为帧编码提供运动向量。<br>
                    动机：作者认为通过信息交互，可以实现运动编码和帧编码之间的协同作用。<br>
                    方法：通过引入运动信息传播，实现了运动编码和帧编码之间的双向信息交互。在生成帧编码的临时上下文时，来自运动解码器的高维运动特征作为运动指导以减少对齐误差。同时，除了协助当前时间步长的帧编码外，生成的上下文特征也将作为后续运动潜在编码的运动条件进行传播。通过这种交互循环，建立了运动编码的特征传播，增强了利用长期时间相关性的能力。此外，还提出了混合上下文生成来利用多尺度上下文特征并提供更好的运动条件。<br>
                    效果：实验表明，该方法比之前最先进的神经网络视频编码器能够实现12.9%的比特率节省。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In most existing neural video codecs, the information flow therein is uni-directional, where only motion coding provides motion vectors for frame coding. In this paper, we argue that, through information interactions, the synergy between motion coding and frame coding can be achieved. We effectively introduce bi-directional information interactions between motion coding and frame coding via our Motion Information Propagation. When generating the temporal contexts for frame coding, the high-dimension motion feature from the motion decoder serves as motion guidance to mitigate the alignment errors. Meanwhile, besides assisting frame coding at the current time step, the feature from context generation will be propagated as motion condition when coding the subsequent motion latent. Through the cycle of such interactions, feature propagation on motion coding is built, strengthening the capacity of exploiting long-range temporal correlation. In addition, we propose hybrid context generation to exploit the multi-scale context features and provide better motion condition. Experiments show that our method can achieve 12.9% bit rate saving over the previous SOTA neural video codec.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>