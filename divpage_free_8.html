<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic8</h2>
      <b>Topic words : &ensp;</b>image, &ensp;diffusion, &ensp;generation, &ensp;images, &ensp;face, &ensp;quality, &ensp;high, &ensp;latent</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1755.DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-Aware Scene Synthesis</span><br>
                <span class="as">Xu, YinghaoandChai, MengleiandShi, ZifanandPeng, SidaandSkorokhodov, IvanandSiarohin, AliaksandrandYang, CeyuanandShen, YujunandLee, Hsin-YingandZhou, BoleiandTulyakov, Sergey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4402-4412.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D感知图像合成方法主要关注生成单个标准对象，对于合成包含多种对象的复杂场景的能力有限。<br>
                    动机：为了解决这个问题，本文提出了DisCoScene，一种用于高质量和可控场景合成的3D感知生成模型。<br>
                    方法：DisCoScene的关键成分是一种非常抽象的对象级表示（即没有语义标注的3D边界框），作为场景布局先验。这种表示简单易得，通用性强，能够描述各种场景内容，同时具有区分对象和背景的信息量。此外，它还可以作为直观的用户控制手段进行场景编辑。基于这种先验，所提出的模型通过仅学习2D图像并利用全局-局部判别来将整个场景在空间上解耦为以对象为中心的生成辐射场。<br>
                    效果：DisCoScene模型在许多场景数据集上表现出了最先进的性能，包括具有挑战性的Waymo户外数据集。我们的代码将公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing 3D-aware image synthesis approaches mainly focus on generating a single canonical object and show limited capacity in composing a complex scene containing a variety of objects. This work presents DisCoScene: a 3D-aware generative model for high-quality and controllable scene synthesis. The key ingredient of our method is a very abstract object-level representation (i.e., 3D bounding boxes without semantic annotation) as the scene layout prior, which is simple to obtain, general to describe various scene contents, and yet informative to disentangle objects and background. Moreover, it serves as an intuitive user control for scene editing. Based on such a prior, the proposed model spatially disentangles the whole scene into object-centric generative radiance fields by learning on only 2D images with the global-local discrimination. Our model obtains the generation fidelity and editing flexibility of individual objects while being able to efficiently compose objects and the background into a complete scene. We demonstrate state-of-the-art performance on many scene datasets, including the challenging Waymo outdoor dataset. Our code will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1756.An Image Quality Assessment Dataset for Portraits</span><br>
                <span class="as">Chahine, NicolasandCalarasanu, StefaniaandGarcia-Civiero, DavideandCayla, Th\&#x27;eoandFerradans, SiraandPonce, Jean</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chahine_An_Image_Quality_Assessment_Dataset_for_Portraits_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9968-9978.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高智能手机照片的质量，特别是在人像摄影方面。<br>
                    动机：随着对更优质智能手机照片的需求不断增长，制造商在开发智能手机相机时使用感知质量标准。然而，这种成本高昂的过程可以通过基于学习的自动图像质量评估方法部分替代。<br>
                    方法：本文介绍了PIQ23，这是一个包含5116张图片的特定于人像的IQA数据集，涵盖了50个预定义的场景，由100部智能手机拍摄，覆盖了各种品牌、型号和使用情况。数据集包括各种性别和种族的个人，他们明确知情并同意他们的肖像用于公共研究。数据集通过成对比较（PWC）进行注释，收集了超过30位图像质量专家对三个图像属性的注释：面部细节保留、面部目标曝光和整体图像质量。<br>
                    效果：通过对这些注释进行深入的统计分析，我们可以评估PIQ23的一致性。最后，我们通过与现有基线的广泛比较表明，语义信息（图像上下文）可以用于改善IQA预测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Year after year, the demand for ever-better smartphone photos continues to grow, in particular in the domain of portrait photography. Manufacturers thus use perceptual quality criteria throughout the development of smartphone cameras. This costly procedure can be partially replaced by automated learning-based methods for image quality assessment (IQA). Due to its subjective nature, it is necessary to estimate and guarantee the consistency of the IQA process, a characteristic lacking in the mean opinion scores (MOS) widely used for crowdsourcing IQA. In addition, existing blind IQA (BIQA) datasets pay little attention to the difficulty of cross-content assessment, which may degrade the quality of annotations. This paper introduces PIQ23, a portrait-specific IQA dataset of 5116 images of 50 predefined scenarios acquired by 100 smartphones, covering a high variety of brands, models, and use cases. The dataset includes individuals of various genders and ethnicities who have given explicit and informed consent for their photographs to be used in public research. It is annotated by pairwise comparisons (PWC) collected from over 30 image quality experts for three image attributes: face detail preservation, face target exposure, and overall image quality. An in-depth statistical analysis of these annotations allows us to evaluate their consistency over PIQ23. Finally, we show through an extensive comparison with existing baselines that semantic information (image context) can be used to improve IQA predictions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1757.Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation</span><br>
                <span class="as">Wei, XiwenandXu, ZhenandLiu, ChengandWu, SiandYu, ZhiwenandWong, HauSan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Text-Guided_Unsupervised_Latent_Transformation_for_Multi-Attribute_Image_Manipulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19285-19294.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像编辑方法主要关注于有监督学习的语义潜在空间遍历方向，每个操作步骤通常针对单个属性确定。<br>
                    动机：为了解决这个限制，我们提出了一种基于文本引导的无监督StyleGAN潜在变换（TUSLT）模型，该模型自适应地推断出在StyleGAN的潜在空间中的单个转换步骤，以同时操纵给定输入图像的多个属性。<br>
                    方法：我们采用了两阶段架构的潜伏映射网络来将转换过程分解为两个可管理的步骤。首先，网络学习适应输入图像的一组多样化的语义方向，然后非线性融合与目标属性相关的那些方向，以推断出残差向量。<br>
                    效果：通过利用CLIP的跨模态文本-图像表示，我们可以基于预设属性文本描述和训练图像之间的语义相似性进行伪标注，并进一步与潜在映射网络联合训练一个辅助属性分类器，提供语义指导。实验结果表明，所采用的策略有助于提高TUSLT的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Great progress has been made in StyleGAN-based image editing. To associate with preset attributes, most existing approaches focus on supervised learning for semantically meaningful latent space traversal directions, and each manipulation step is typically determined for an individual attribute. To address this limitation, we propose a Text-guided Unsupervised StyleGAN Latent Transformation (TUSLT) model, which adaptively infers a single transformation step in the latent space of StyleGAN to simultaneously manipulate multiple attributes on a given input image. Specifically, we adopt a two-stage architecture for a latent mapping network to break down the transformation process into two manageable steps. Our network first learns a diverse set of semantic directions tailored to an input image, and later nonlinearly fuses the ones associated with the target attributes to infer a residual vector. The resulting tightly interlinked two-stage architecture delivers the flexibility to handle diverse attribute combinations. By leveraging the cross-modal text-image representation of CLIP, we can perform pseudo annotations based on the semantic similarity between preset attribute text descriptions and training images, and further jointly train an auxiliary attribute classifier with the latent mapping network to provide semantic guidance. We perform extensive experiments to demonstrate that the adopted strategies contribute to the superior performance of TUSLT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1758.SIEDOB: Semantic Image Editing by Disentangling Object and Background</span><br>
                <span class="as">Luo, WuyangandYang, SuandZhang, XinjianandZhang, Weishan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_SIEDOB_Semantic_Image_Editing_by_Disentangling_Object_and_Background_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1868-1878.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的语义图像编辑方法将背景和对象作为一个整体处理，限制了其在处理内容丰富的图像上的能力，并可能导致生成不真实的对象和纹理不一致的背景。<br>
                    动机：为了解决这个问题，我们提出了一种新的范式——通过解耦对象和背景进行语义图像编辑（SIEDOB）。<br>
                    方法：SIEDOB将编辑的输入分解为背景区域和实例级对象，然后分别送入专门的生成器中。所有合成的部分都嵌入到它们原来的位置，并使用融合网络得到一个协调的结果。此外，我们还提出了一些创新的设计，包括语义感知的自我传播模块、边界锚定的补丁判别器和风格多样性的对象生成器，并将它们集成到SIEDOB中。<br>
                    效果：我们在Cityscapes和ADE20K-Room数据集上进行了广泛的实验，结果显示我们的方法显著优于基线，特别是在合成真实且多样的对象和纹理一致的背景方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic image editing provides users with a flexible tool to modify a given image guided by a corresponding segmentation map. In this task, the features of the foreground objects and the backgrounds are quite different. However, all previous methods handle backgrounds and objects as a whole using a monolithic model. Consequently, they remain limited in processing content-rich images and suffer from generating unrealistic objects and texture-inconsistent backgrounds. To address this issue, we propose a novel paradigm, Semantic Image Editing by Disentangling Object and Background (SIEDOB), the core idea of which is to explicitly leverages several heterogeneous subnetworks for objects and backgrounds. First, SIEDOB disassembles the edited input into background regions and instance-level objects. Then, we feed them into the dedicated generators. Finally, all synthesized parts are embedded in their original locations and utilize a fusion network to obtain a harmonized result. Moreover, to produce high-quality edited images, we propose some innovative designs, including Semantic-Aware Self-Propagation Module, Boundary-Anchored Patch Discriminator, and Style-Diversity Object Generator, and integrate them into SIEDOB. We conduct extensive experiments on Cityscapes and ADE20K-Room datasets and exhibit that our method remarkably outperforms the baselines, especially in synthesizing realistic and diverse objects and texture-consistent backgrounds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1759.Learning Semantic-Aware Disentangled Representation for Flexible 3D Human Body Editing</span><br>
                <span class="as">Sun, XiaokunandFeng, QiaoandLi, XiongzhengandZhang, JinsongandLai, Yu-KunandYang, JingyuandLi, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Learning_Semantic-Aware_Disentangled_Representation_for_Flexible_3D_Human_Body_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16985-16994.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，3D人体表示学习受到了越来越多的关注。然而，由于语义粗糙和表示能力不足，特别是在缺乏监督数据的情况下，现有的方法无法灵活、可控和准确地表示人体。<br>
                    动机：本文提出了一种在无监督环境中具有细粒度语义和高重建精度的人体表示方法。<br>
                    方法：通过设计一个部分感知的骨架分离解耦策略，建立了潜在向量和身体部位的几何测量之间的对应关系，从而通过修改相应的潜在代码来控制编辑人体。<br>
                    效果：实验结果表明，该方法在公共数据集上具有准确的重建和灵活的编辑能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D human body representation learning has received increasing attention in recent years. However, existing works cannot flexibly, controllably and accurately represent human bodies, limited by coarse semantics and unsatisfactory representation capability, particularly in the absence of supervised data. In this paper, we propose a human body representation with fine-grained semantics and high reconstruction-accuracy in an unsupervised setting. Specifically, we establish a correspondence between latent vectors and geometric measures of body parts by designing a part-aware skeleton-separated decoupling strategy, which facilitates controllable editing of human bodies by modifying the corresponding latent codes. With the help of a bone-guided auto-encoder and an orientation-adaptive weighting strategy, our representation can be trained in an unsupervised manner. With the geometrically meaningful latent space, it can be applied to a wide range of applications, from human body editing to latent code interpolation and shape style transfer. Experimental results on public datasets demonstrate the accurate reconstruction and flexible editing abilities of the proposed method. The code will be available at http://cic.tju.edu.cn/faculty/likun/projects/SemanticHuman.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1760.Paint by Example: Exemplar-Based Image Editing With Diffusion Models</span><br>
                <span class="as">Yang, BinxinandGu, ShuyangandZhang, BoandZhang, TingandChen, XuejinandSun, XiaoyanandChen, DongandWen, Fang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18381-18391.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过利用自我监督训练来分离和重组源图像和范例，以实现更精确的控制。<br>
                    动机：目前的语言引导图像编辑取得了巨大的成功，但直接复制粘贴范例图像会导致明显的融合痕迹。<br>
                    方法：通过设计信息瓶颈和强大的增强技术，避免直接复制粘贴范例图像的平凡解决方案。同时，为了确保编辑过程的可控性，为范例图像设计了一个任意形状的遮罩，并利用无分类器指导来增加与范例图像的相似性。整个框架只涉及一次扩散模型的前向传播，无需任何迭代优化。<br>
                    效果：实验结果表明，该方法在野图像上实现了令人印象深刻的性能，并能够进行高保真的可控编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Language-guided image editing has achieved great success recently. In this paper, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1761.Graphics Capsule: Learning Hierarchical 3D Face Representations From 2D Images</span><br>
                <span class="as">Yu, ChangandZhu, XiangyuandZhang, XiaomeiandZhang, ZhaoxiangandLei, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Graphics_Capsule_Learning_Hierarchical_3D_Face_Representations_From_2D_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20981-20990.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用胶囊网络从大规模无标签图像中学习层次化的三维人脸表示。<br>
                    动机：目前的胶囊网络在描述物体时仅限于二维空间，限制了其模仿人类固有的三维感知能力。<br>
                    方法：提出一种逆向图形胶囊网络（IGC-Net），通过将对象分解为一组语义一致的部分级描述，然后组装成对象级描述来构建层次结构，从而学习层次化的三维人脸表示。<br>
                    效果：实验结果表明，IGC-Net能够揭示神经网络如何以视觉感知为导向理解人脸作为三维模型的层次结构，同时，发现的部分可以用于无监督的人脸分割任务，评估我们的方法的语义一致性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The function of constructing the hierarchy of objects is important to the visual process of the human brain. Previous studies have successfully adopted capsule networks to decompose the digits and faces into parts in an unsupervised manner to investigate the similar perception mechanism of neural networks. However, their descriptions are restricted to the 2D space, limiting their capacities to imitate the intrinsic 3D perception ability of humans. In this paper, we propose an Inverse Graphics Capsule Network (IGC-Net) to learn the hierarchical 3D face representations from large-scale unlabeled images. The core of IGC-Net is a new type of capsule, named graphics capsule, which represents 3D primitives with interpretable parameters in computer graphics (CG), including depth, albedo, and 3D pose. Specifically, IGC-Net first decomposes the objects into a set of semantic-consistent part-level descriptions and then assembles them into object-level descriptions to build the hierarchy. The learned graphics capsules reveal how the neural networks, oriented at visual perception, understand faces as a hierarchy of 3D models. Besides, the discovered parts can be deployed to the unsupervised face segmentation task to evaluate the semantic consistency of our method. Moreover, the part-level descriptions with explicit physical meanings provide insight into the face analysis that originally runs in a black box, such as the importance of shape and texture for face recognition. Experiments on CelebA, BP4D, and Multi-PIE demonstrate the characteristics of our IGC-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1762.Make-a-Story: Visual Memory Conditioned Consistent Story Generation</span><br>
                <span class="as">Rahman, TanzilaandLee, Hsin-YingandRen, JianandTulyakov, SergeyandMahajan, ShwetaandSigal, Leonid</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2493-2502.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用文本描述生成高质量的图像或视频，特别是在复杂的故事情节中处理自然引用和共指关系。<br>
                    动机：现有的基于文本描述生成图像或视频的模型主要依赖于明确的场景和主要角色描述，对于需要根据故事进展判断何时保持一致性、何时不保持一致性的复杂任务，如故事可视化，仍面临挑战。<br>
                    方法：提出一种新的自回归扩散基框架，并加入视觉记忆模块，该模块能隐式捕获生成帧中的角色和背景上下文。通过在记忆中进行句子条件的软注意力，实现有效的引用解析，并在需要时学习保持场景和角色的一致性。<br>
                    效果：在MUGEN、PororoSV和FlintstonesSV数据集上进行故事生成实验，结果显示该方法不仅在生成与故事一致的高视觉质量帧方面优于现有技术，还能在角色和背景之间建立适当的对应关系。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>There has been a recent explosion of impressive generative models that can produce high quality images (or videos) conditioned on text descriptions. However, all such approaches rely on conditional sentences that contain unambiguous descriptions of scenes and main actors in them. Therefore employing such models for more complex task of story visualization, where naturally references and co-references exist, and one requires to reason about when to maintain consistency of actors and backgrounds across frames/scenes, and when not to, based on story progression, remains a challenge. In this work, we address the aforementioned challenges and propose a novel autoregressive diffusion-based framework with a visual memory module that implicitly captures the actor and background context across the generated frames. Sentence-conditioned soft attention over the memories enables effective reference resolution and learns to maintain scene and actor consistency when needed. To validate the effectiveness of our approach, we extend the MUGEN dataset and introduce additional characters, backgrounds and referencing in multi-sentence storylines. Our experiments for story generation on the MUGEN, the PororoSV and the FlintstonesSV dataset show that our method not only outperforms prior state-of-the-art in generating frames with high visual quality, which are consistent with the story, but also models appropriate correspondences between the characters and the background.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1763.StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer</span><br>
                <span class="as">Khwanmuang, SasikarnandPhongthawee, PakkaponandSangkloy, PatsornandSuwajanakorn, Supasorn</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khwanmuang_StyleGAN_Salon_Multi-View_Latent_Optimization_for_Pose-Invariant_Hairstyle_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8609-8618.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在将参考图像的发型转移到输入照片中进行虚拟发型试穿。<br>
                    动机：现有的解决方案使用StyleGAN来生成任何缺失的部分，并通过所谓的GAN反转或投影来产生无缝的面部-头发复合图像。然而，控制这些幻觉以准确转移发型并保留输入的面部形状和身份仍然是一个挑战。<br>
                    方法：我们提出了一个多视图优化框架，该框架使用参考复合物的“两个不同视图”来语义地指导被遮挡或模糊的区域。我们的优化在两种姿势之间共享信息，这使得我们可以从不完整的参考中产生高保真和逼真的结果。<br>
                    效果：我们的框架产生了高质量的结果，并在用户研究中优于先前的工作，该研究包含比之前研究更具挑战性的发型转移场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Our paper seeks to transfer the hairstyle of a reference image to an input photo for virtual hair try-on. We target a variety of challenges scenarios, such as transforming a long hairstyle with bangs to a pixie cut, which requires removing the existing hair and inferring how the forehead would look, or transferring partially visible hair from a hat-wearing person in a different pose. Past solutions leverage StyleGAN for hallucinating any missing parts and producing a seamless face-hair composite through so-called GAN inversion or projection. However, there remains a challenge in controlling the hallucinations to accurately transfer hairstyle and preserve the face shape and identity of the input. To overcome this, we propose a multi-view optimization framework that uses "two different views" of reference composites to semantically guide occluded or ambiguous regions. Our optimization shares information between two poses, which allows us to produce high fidelity and realistic results from incomplete references. Our framework produces high-quality results and outperforms prior work in a user study that consists of significantly more challenging hair transfer scenarios than previously studied. Project page: https://stylegan-salon.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1764.Neural Preset for Color Style Transfer</span><br>
                <span class="as">Ke, ZhanghanandLiu, YuhaoandZhu, LeiandZhao, NanxuanandLau, RynsonW.H.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_Neural_Preset_for_Color_Style_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14173-14182.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有颜色风格转换方法的局限性，包括视觉伪影、巨大的内存需求和慢速的风格切换速度。<br>
                    动机：为了解决这些问题，我们提出了一种基于确定性神经颜色映射（DNCM）和两阶段管道的颜色风格预置技术。<br>
                    方法：首先，我们使用图像自适应颜色映射矩阵对每个像素进行操作，避免伪影并支持高分辨率输入，同时减少内存占用。其次，我们将任务分为颜色归一化和风格化两个阶段，通过提取颜色样式作为预置并在标准化的输入图像上重复使用，实现高效的风格切换。由于无法获取配对数据集，我们描述了如何通过自我监督策略训练神经预置。<br>
                    效果：实验结果表明，神经预置在各种应用中都优于现有方法，包括低光图像增强、水下图像校正、图像去雾和图像和谐化。此外，我们的模型无需微调即可自然支持多种应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multiple applications without fine-tuning, including low-light image enhancement, underwater image correction, image dehazing, and image harmonization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1765.PosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout</span><br>
                <span class="as">Hsu, HsiaoYuanandHe, XiangtengandPeng, YuxinandKong, HaoandZhang, Qing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_PosterLayout_A_New_Benchmark_and_Approach_for_Content-Aware_Visual-Textual_Presentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6018-6026.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在给定的画布上自动布局预定义的元素，包括文本、标志和底纹，以进行无模板的创意图形设计。<br>
                    动机：现有的方法在处理元素间关系和图层间关系时表现不佳，如布局变化性不足或空间对齐不良。<br>
                    方法：我们首先构建了一个名为PKU PosterLayout的新数据集，包含9,974个海报布局对和905张非空画布图像。然后，我们提出了设计序列形成（DSF）来重新组织布局中的元素，模仿人类设计师的设计过程，并提出了一种新的基于CNN-LSTM的条件生成对抗网络（GAN）来生成合适的布局。<br>
                    效果：实验结果验证了新基准的有效性和所提出方法的有效性，该方法通过为多样化的画布生成合适的布局，实现了最佳性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Content-aware visual-textual presentation layout aims at arranging spatial space on the given canvas for pre-defined elements, including text, logo, and underlay, which is a key to automatic template-free creative graphic design. In practical applications, e.g., poster designs, the canvas is originally non-empty, and both inter-element relationships as well as inter-layer relationships should be concerned when generating a proper layout. A few recent works deal with them simultaneously, but they still suffer from poor graphic performance, such as a lack of layout variety or spatial non-alignment. Since content-aware visual-textual presentation layout is a novel task, we first construct a new dataset named PKU PosterLayout, which consists of 9,974 poster-layout pairs and 905 images, i.e., non-empty canvases. It is more challenging and useful for greater layout variety, domain diversity, and content diversity. Then, we propose design sequence formation (DSF) that reorganizes elements in layouts to imitate the design processes of human designers, and a novel CNN-LSTM-based conditional generative adversarial network (GAN) is presented to generate proper layouts. Specifically, the discriminator is design-sequence-aware and will supervise the "design" process of the generator. Experimental results verify the usefulness of the new benchmark and the effectiveness of the proposed approach, which achieves the best performance by generating suitable layouts for diverse canvases. The dataset and the source code are available at https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1766.ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts</span><br>
                <span class="as">Feng, ZhidaandZhang, ZhenyuandYu, XintongandFang, YeweiandLi, LanxinandChen, XuyiandLu, YuxiangandLiu, JiaxiangandYin, WeichongandFeng, ShikunandSun, YuandChen, LiandTian, HaoandWu, HuaandWang, Haifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10135-10145.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的文本到图像生成技术在提高图像保真度和文本相关性方面存在限制。<br>
                    动机：为了解决这些问题，我们提出了ERNIE-ViLG 2.0，一种大规模的中文文本到图像扩散模型。<br>
                    方法：通过结合场景中关键元素的细粒度文本和视觉知识，以及在不同的去噪阶段使用不同的去噪专家。<br>
                    效果：实验结果表明，ERNIE-ViLG 2.0不仅在MS-COCO上取得了新的最先进的成果，而且在图像保真度和图像-文本对齐方面也显著优于最近的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent progress in diffusion models has revolutionized the popular technology of text-to-image generation. While existing approaches could produce photorealistic high-resolution images with text conditions, there are still several open problems to be solved, which limits the further improvement of image fidelity and text relevancy. In this paper, we propose ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by: (1) incorporating fine-grained textual and visual knowledge of key elements in the scene, and (2) utilizing different denoising experts at different denoising stages. With the proposed mechanisms, ERNIE-ViLG 2.0 not only achieves a new state-of-the-art on MS-COCO with zero-shot FID-30k score of 6.75, but also significantly outperforms recent models in terms of image fidelity and image-text alignment, with side-by-side human evaluation on the bilingual prompt set ViLG-300.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1767.Learning To Generate Image Embeddings With User-Level Differential Privacy</span><br>
                <span class="as">Xu, ZhengandCollins, MaxwellandWang, YuxiaoandPanait, LiviuandOh, SewoongandAugenstein, SeanandLiu, TingandSchroff, FlorianandMcMahan, H.Brendan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_To_Generate_Image_Embeddings_With_User-Level_Differential_Privacy_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7969-7980.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在大型图像嵌入特征提取器的训练中实现用户级别的差分隐私（DP）。<br>
                    动机：现有的方法在直接应用于使用具有大类空间的监督训练数据学习嵌入模型时可能会失败。<br>
                    方法：提出DP-FedEmb，一种带有每用户灵敏度控制和噪声添加的联邦学习算法变体，用于从数据中心集中的用户分区数据进行训练。DP-FedEmb结合了虚拟客户端、部分聚合、私有本地微调以及公共预训练，以实现强大的隐私效用权衡。<br>
                    效果：将DP-FedEmb应用于人脸、地标和自然物种的图像嵌入模型训练，并在基准数据集DigiFace、GLD和iNaturalist上证明，在相同的隐私预算下，其效用优越。当数百万用户参与训练时，有可能实现强用户级别DP保证（ε<2），同时控制效用下降在5%以内。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Small on-device models have been successfully trained with user-level differential privacy (DP) for next word prediction and image classification tasks in the past. However, existing methods can fail when directly applied to learn embedding models using supervised training data with a large class space. To achieve user-level DP for large image-to-embedding feature extractors, we propose DP-FedEmb, a variant of federated learning algorithms with per-user sensitivity control and noise addition, to train from user-partitioned data centralized in datacenter. DP-FedEmb combines virtual clients, partial aggregation, private local fine-tuning, and public pretraining to achieve strong privacy utility trade-offs. We apply DP-FedEmb to train image embedding models for faces, landmarks and natural species, and demonstrate its superior utility under same privacy budget on benchmark datasets DigiFace, GLD and iNaturalist. We further illustrate it is possible to achieve strong user-level DP guarantees of epsilon < 2 while controlling the utility drop within 5%, when millions of users can participate in training.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1768.BlendFields: Few-Shot Example-Driven Facial Modeling</span><br>
                <span class="as">Kania, KacperandGarbin, StephanJ.andTagliasacchi, AndreaandEstellers, VirginiaandYi, KwangMooandValentin, JulienandTrzci\&#x27;nski, TomaszandKowalski, Marek</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kania_BlendFields_Few-Shot_Example-Driven_Facial_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/404-415.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成忠实的人脸可视化，同时捕捉到人脸几何和外观的粗粒度和细粒度细节。<br>
                    动机：现有的方法要么需要大量的数据，这些数据对研究社区来说并不公开可获取，要么由于依赖于只能表示粗粒度细节的几何人脸模型而无法捕捉纹理的细粒度细节。<br>
                    方法：我们的方法借鉴了传统的计算机图形技术，通过测量极端表情中的局部体积变化并在当地复制其外观来混合外观，以模拟未见过的表情。<br>
                    效果：我们的方法是通用的，可以在平滑的人脸体积变形上添加细粒度的效果，并且可以推广到人脸之外。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating faithful visualizations of human faces requires capturing both coarse and fine-level details of the face geometry and appearance. Existing methods are either data-driven, requiring an extensive corpus of data not publicly accessible to the research community, or fail to capture fine details because they rely on geometric face models that cannot represent fine-grained details in texture with a mesh discretization and linear deformation designed to model only a coarse face geometry. We introduce a method that bridges this gap by drawing inspiration from traditional computer graphics techniques. Unseen expressions are modeled by blending appearance from a sparse set of extreme poses. This blending is performed by measuring local volumetric changes in those expressions and locally reproducing their appearance whenever a similar expression is performed at test time. We show that our method generalizes to unseen expressions, adding fine-grained effects on top of smooth volumetric deformations of a face, and demonstrate how it generalizes beyond faces.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1769.3D GAN Inversion With Facial Symmetry Prior</span><br>
                <span class="as">Yin, FeiandZhang, YongandWang, XuanandWang, TengfeiandLi, XiaoyuandGong, YuanandFan, YanboandCun, XiaodongandShan, YingandOztireli, CengizandYang, Yujiu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/342-351.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过将真实图像投影到生成器的潜在空间中，实现3D GAN的逆映射，以进行一致的合成和编辑。<br>
                    动机：尽管预训练的3D GAN保留了面部先验，但仅使用单目图像重建3D肖像仍然是一个病态问题。直接应用2D GAN逆映射方法只关注纹理相似性，而忽略了3D几何形状的正确性，可能导致几何塌陷效应。<br>
                    方法：我们提出了一种引入面部对称先验的新方法来改进3D GAN逆映射。我们设计了一个管道和约束，充分利用通过图像翻转获得的伪辅助视图，在逆过程中获得一致且结构良好的几何形状。为了提高未观察到的视角中的纹理保真度，深度引导的3D变形的伪标签可以提供额外的监督。我们还设计了约束，旨在过滤出不对称情况下的冲突区域进行优化。<br>
                    效果：我们在图像重建和编辑方面的全面定量和定性评估表明，我们的方法具有优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, a surge of high-quality 3D-aware GANs have been proposed, which leverage the generative power of neural rendering. It is natural to associate 3D GANs with GAN inversion methods to project a real image into the generator's latent space, allowing free-view consistent synthesis and editing, referred as 3D GAN inversion. Although with the facial prior preserved in pre-trained 3D GANs, reconstructing a 3D portrait with only one monocular image is still an ill-pose problem. The straightforward application of 2D GAN inversion methods focuses on texture similarity only while ignoring the correctness of 3D geometry shapes. It may raise geometry collapse effects, especially when reconstructing a side face under an extreme pose. Besides, the synthetic results in novel views are prone to be blurry. In this work, we propose a novel method to promote 3D GAN inversion by introducing facial symmetry prior. We design a pipeline and constraints to make full use of the pseudo auxiliary view obtained via image flipping, which helps obtain a view-consistent and well-structured geometry shape during the inversion process. To enhance texture fidelity in unobserved viewpoints, pseudo labels from depth-guided 3D warping can provide extra supervision. We design constraints aimed at filtering out conflict areas for optimization in asymmetric situations. Comprehensive quantitative and qualitative evaluations on image reconstruction and editing demonstrate the superiority of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1770.SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation</span><br>
                <span class="as">Cheng, Yen-ChiandLee, Hsin-YingandTulyakov, SergeyandSchwing, AlexanderG.andGui, Liang-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4456-4465.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在为业余用户提供一种简化的3D资产生成框架。<br>
                    动机：为了实现交互式生成，我们的方法支持多种易于人类提供的输入模态，包括图像、文本、部分观察到的形状及其组合，并允许调整每种输入的强度。<br>
                    方法：我们的核心方法是编码器-解码器，将3D形状压缩成紧凑的潜在表示形式，然后学习扩散模型。为了支持多种多模态输入，我们采用了带有丢弃和交叉注意力机制的任务特定编码器。<br>
                    效果：由于其灵活性，我们的模型自然地支持各种任务，并在形状完成、基于图像的3D重建和文本到3D方面优于先前的工作。最有趣的是，我们的模型可以将所有这些任务组合成一个瑞士军刀工具，使用户能够同时使用不完整形状、图像和文本描述进行形状生成，并为每个输入提供相对权重以促进交互性。尽管我们的方法只针对形状，但我们进一步展示了一种利用大规模文本到图像模型高效纹理化生成的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we present a novel framework built to simplify 3D asset generation for amateur users. To enable interactive generation, our method supports a variety of input modalities that can be easily provided by a human, including images, texts, partially observed shapes and combinations of these, further allowing for adjusting the strength of each input. At the core of our approach is an encoder-decoder, compressing 3D shapes into a compact latent representation, upon which a diffusion model is learned. To enable a variety of multi-modal inputs, we employ task-specific encoders with dropout followed by a cross-attention mechanism. Due to its flexibility, our model naturally supports a variety of tasks outperforming prior works on shape completion, image-based 3D reconstruction, and text-to-3D. Most interestingly, our model can combine all these tasks into one swiss-army-knife tool, enabling the user to perform shape generation using incomplete shapes, images, and textual descriptions at the same time, providing the relative weights for each input and facilitating interactivity. Despite our approach being shape-only, we further show an efficient method to texture the generated using large-scale text-to-image models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1771.TryOnDiffusion: A Tale of Two UNets</span><br>
                <span class="as">Zhu, LuyangandYang, DaweiandZhu, TylerandReda, FitsumandChan, WilliamandSaharia, ChitwanandNorouzi, MohammadandKemelmacher-Shlizerman, Ira</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4606-4615.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成逼真的可视化试衣效果，同时适应穿着者的身体姿势和形状变化。<br>
                    动机：现有的方法或关注衣物细节保护，无法有效处理姿势和形状变化，或允许以期望的形状和姿势试穿，但缺乏衣物细节。<br>
                    方法：提出一种基于扩散的架构，该架构统一了两个UNets（称为并行UNet），可以在单个网络中保留衣物细节并适应显著的姿势和身体变化。<br>
                    效果：实验结果表明，TryOnDiffusion在定性和定量上都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given two images depicting a person and a garment worn by another person, our goal is to generate a visualization of how the garment might look on the input person. A key challenge is to synthesize a photorealistic detail-preserving visualization of the garment, while warping the garment to accommodate a significant body pose and shape change across the subjects. Previous methods either focus on garment detail preservation without effective pose and shape variation, or allow try-on with the desired shape and pose but lack garment details. In this paper, we propose a diffusion-based architecture that unifies two UNets (referred to as Parallel-UNet), which allows us to preserve garment details and warp the garment for significant pose and body change in a single network. The key ideas behind Parallel-UNet include: 1) garment is warped implicitly via a cross attention mechanism, 2) garment warp and person blend happen as part of a unified process as opposed to a sequence of two separate tasks. Experimental results indicate that TryOnDiffusion achieves state-of-the-art performance both qualitatively and quantitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1772.Automatic High Resolution Wire Segmentation and Removal</span><br>
                <span class="as">Chiu, MangTikandZhang, XuanerandWei, ZijunandZhou, YuqianandShechtman, EliandBarnes, ConnellyandLin, ZheandKainz, FlorianandAmirghodsi, SohrabandShi, Humphrey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chiu_Automatic_High_Resolution_Wire_Segmentation_and_Removal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2183-2192.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地自动清理照片中的电线，以提升照片的美观度。<br>
                    动机：手动精确分割和移除电线的过程既繁琐又耗时，特别是在高分辨率的照片中，电线可能横跨整个画面，这极大地增加了处理的难度。<br>
                    方法：提出了一种两阶段的方法，首先利用全局和局部上下文准确地在高分辨率图像中分割电线，然后采用基于分块的修复策略根据预测的分割掩码移除电线。同时，还引入了首个电线分割基准数据集WireSegHR。<br>
                    效果：实验证明，这种自动清理电线的系统能够完全自动化地移除各种外观的电线，大大提高了处理效率和准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Wires and powerlines are common visual distractions that often undermine the aesthetics of photographs. The manual process of precisely segmenting and removing them is extremely tedious and may take up to hours, especially on high-resolution photos where wires may span the entire space. In this paper, we present an automatic wire clean-up system that eases the process of wire segmentation and removal/inpainting to within a few seconds. We observe several unique challenges: wires are thin, lengthy, and sparse. These are rare properties of subjects that common segmentation tasks cannot handle, especially in high-resolution images. We thus propose a two-stage method that leverages both global and local context to accurately segment wires in high-resolution images efficiently, and a tile-based inpainting strategy to remove the wires given our predicted segmentation masks. We also introduce the first wire segmentation benchmark dataset, WireSegHR. Finally, we demonstrate quantitatively and qualitatively that our wire clean-up system enables fully automated wire removal for great generalization to various wire appearances.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1773.Multi-Realism Image Compression With a Conditional Generator</span><br>
                <span class="as">Agustsson, EirikurandMinnen, DavidandToderici, GeorgeandMentzer, Fabian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Agustsson_Multi-Realism_Image_Compression_With_a_Conditional_Generator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22324-22333.png><br>
            
            <span class="tt"><span class="t0">研究问题：优化率失真真实性权衡，生成压缩方法可以产生详细、真实的图像，即使在低比特率下，而不是由率失真优化模型产生的模糊重建。<br>
                    动机：先前的方法没有明确控制合成的细节量，这可能导致用户担心生成的重建图像远离输入图像，这是一个常见的批评。<br>
                    方法：我们通过训练一个解码器来缓解这些问题，该解码器可以连接两个领域并导航失真-真实性权衡。从单个压缩表示中，接收者可以选择重建接近输入的低均方误差重建，具有高感知质量的真实重建，或两者之间的任何内容。<br>
                    效果：我们的方法在失真-真实性方面设置了新的最先进的状态，推动了可实现的失真-真实性对的前沿，即我们的方法在高真实性时实现了更好的失真，在低失真时实现了更好的真实性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed representation, the receiver can decide to either reconstruct a low mean squared error reconstruction that is close to the input, a realistic reconstruction with high perceptual quality, or anything in between. With our method, we set a new state-of-the-art in distortion-realism, pushing the frontier of achievable distortion-realism pairs, i.e., our method achieves better distortions at high realism and better realism at low distortion than ever before.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1774.High-Fidelity 3D Face Generation From Natural Language Descriptions</span><br>
                <span class="as">Wu, MenghuaandZhu, HaoandHuang, LinjiaandZhuang, YiyuandLu, YuanxunandCao, Xun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_High-Fidelity_3D_Face_Generation_From_Natural_Language_Descriptions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4521-4530.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从自然语言描述中合成高质量的3D人脸模型。<br>
                    动机：合成高质量的3D人脸模型对于许多应用（包括创建虚拟形象、虚拟现实和远程呈现）非常有价值，但目前对此的研究还很少。<br>
                    方法：构建了DESCRIBE3D数据集，这是第一个用于文本到3D人脸生成任务的具有精细文本描述的大型数据集。然后提出了一个两阶段框架，首先生成与具体描述匹配的3D人脸，然后在3D形状和纹理空间中使用抽象描述优化参数以细化3D人脸模型。<br>
                    效果：大量实验结果表明，该方法可以生成忠实于输入描述的高质量3D人脸，其准确性和质量高于以前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesizing high-quality 3D face models from natural language descriptions is very valuable for many applications, including avatar creation, virtual reality, and telepresence. However, little research ever tapped into this task. We argue the major obstacle lies in 1) the lack of high-quality 3D face data with descriptive text annotation, and 2) the complex mapping relationship between descriptive language space and shape/appearance space. To solve these problems, we build DESCRIBE3D dataset, the first large-scale dataset with fine-grained text descriptions for text-to-3D face generation task. Then we propose a two-stage framework to first generate a 3D face that matches the concrete descriptions, then optimize the parameters in the 3D shape and texture space with abstract description to refine the 3D face model. Extensive experimental results show that our method can produce a faithful 3D face that conforms to the input descriptions with higher accuracy and quality than previous methods. The code and DESCRIBE3D dataset are released at https://github.com/zhuhao-nju/describe3d.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1775.On Distillation of Guided Diffusion Models</span><br>
                <span class="as">Meng, ChenlinandRombach, RobinandGao, RuiqiandKingma, DiederikandErmon, StefanoandHo, JonathanandSalimans, Tim</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14297-14306.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低无分类器引导的扩散模型在推理时的计算成本。<br>
                    动机：无分类器引导的扩散模型虽然在高分辨率图像生成方面效果显著，但计算成本高，需要评估两个扩散模型，即条件模型和无条件模型，数十到数百次。<br>
                    方法：提出一种将无分类器引导的扩散模型蒸馏成快速采样模型的方法。首先学习一个模型来匹配组合的条件和无条件模型的输出，然后逐步将该模型蒸馏成一个扩散模型，该模型需要的采样步骤要少得多。<br>
                    效果：对于在像素空间上训练的标准扩散模型，该方法能够生成与原始模型视觉上相当的图像，在ImageNet 64x64和CIFAR-10上只需要4个采样步骤，FID/IS分数与原始模型相当，但采样速度提高了256倍。对于在潜在空间（如Stable Diffusion）上训练的扩散模型，该方法能够生成高保真图像，在ImageNet 256x256和LAION数据集上只需要1到4个去噪步骤，比现有方法至少快10倍。进一步证明了该方法在文本引导的图像编辑和修复中的有效性，我们的蒸馏模型只需要2到4个去噪步骤就能生成高质量的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALL*E 2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate images visually comparable to that of the original model using as few as 4 sampling steps on ImageNet 64x64 and CIFAR-10, achieving FID/IS scores comparable to that of the original model while being up to 256 times faster to sample from. For diffusion models trained on the latent-space (e.g., Stable Diffusion), our approach is able to generate high-fidelity images using as few as 1 to 4 denoising steps, accelerating inference by at least 10-fold compared to existing methods on ImageNet 256x256 and LAION datasets. We further demonstrate the effectiveness of our approach on text-guided image editing and inpainting, where our distilled model is able to generate high-quality results using as few as 2-4 denoising steps.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1776.Zero-Shot Pose Transfer for Unrigged Stylized 3D Characters</span><br>
                <span class="as">Wang, JiashunandLi, XuetingandLiu, SifeiandDeMello, ShaliniandGallo, OrazioandWang, XiaolongandKautz, Jan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Zero-Shot_Pose_Transfer_for_Unrigged_Stylized_3D_Characters_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8704-8714.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将参考化身的姿态转移到各种形状的样式化3D角色上是计算机图形学中的基本任务。<br>
                    动机：现有的方法要么需要样式化的角色被绑定，要么在训练中使用期望姿态的样式化角色作为真实值。我们提出了一种零射方法，只需要在训练中使用广泛可用的变形非样式化化身，并在推理时使形状显著不同的样式化角色变形。<br>
                    方法：我们引入了一个半监督的形状理解模块来绕过测试时间对显式对应关系的需求，以及一个隐式的姿态变形模块，该模块变形单个表面点以匹配目标姿态。此外，为了鼓励对样式化角色的真实和准确变形，我们引入了一种有效的基于体积的测试时间训练过程。<br>
                    效果：由于我们的模型不需要绑定，也不需要训练时的变形样式化角色，因此它可以推广到类别稀缺注释的情况，如四足动物。大量的实验表明，与使用可比或更多监督的训练状态-of-the-art方法相比，我们提出的方法更有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transferring the pose of a reference avatar to stylized 3D characters of various shapes is a fundamental task in computer graphics. Existing methods either require the stylized characters to be rigged, or they use the stylized character in the desired pose as ground truth at training. We present a zero-shot approach that requires only the widely available deformed non-stylized avatars in training, and deforms stylized characters of significantly different shapes at inference. Classical methods achieve strong generalization by deforming the mesh at the triangle level, but this requires labelled correspondences. We leverage the power of local deformation, but without requiring explicit correspondence labels. We introduce a semi-supervised shape-understanding module to bypass the need for explicit correspondences at test time, and an implicit pose deformation module that deforms individual surface points to match the target pose. Furthermore, to encourage realistic and accurate deformation of stylized characters, we introduce an efficient volume-based test-time training procedure. Because it does not need rigging, nor the deformed stylized character at training time, our model generalizes to categories with scarce annotation, such as stylized quadrupeds. Extensive experiments demonstrate the effectiveness of the proposed method compared to the state-of-the-art approaches trained with comparable or more supervision. Our project page is available at https://jiashunwang.github.io/ZPT</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1777.OTAvatar: One-Shot Talking Face Avatar With Controllable Tri-Plane Rendering</span><br>
                <span class="as">Ma, ZhiyuanandZhu, XiangyuandQi, Guo-JunandLei, ZhenandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16901-16910.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时满足可控性、泛化性和效率，构建由神经隐式场表示的人脸头像。<br>
                    动机：现有的方法无法同时满足这三个要求，或者只关注静态肖像，限制了表示能力，或者计算成本高，限制了灵活性。<br>
                    方法：本文提出了一种一次拍摄的会话人脸头像（OTAvatar），通过一个通用的可控三角面绘制解决方案来构建人脸头像，使得每个个性化的头像都可以仅从一个肖像作为参考进行构建。具体来说，OTAvatar首先将肖像图像转换为无运动的身份代码，然后使用身份代码和运动代码调制一个高效的CNN生成一个编码主题所需运动的三角面形成的体积，最后使用体积渲染在任何视图中生成图像。<br>
                    效果：由于采用了有效的三角面表示，我们实现了以35FPS的速度在A100上对通用人脸头像的可控渲染。实验表明，跨身份重演在训练集外的主题上表现出良好的性能，并且具有更好的3D一致性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Controllability, generalizability and efficiency are the major objectives of constructing face avatars represented by neural implicit field. However, existing methods have not managed to accommodate the three requirements simultaneously. They either focus on static portraits, restricting the representation ability to a specific subject, or suffer from substantial computational cost, limiting their flexibility. In this paper, we propose One-shot Talking face Avatar (OTAvatar), which constructs face avatars by a generalized controllable tri-plane rendering solution so that each personalized avatar can be constructed from only one portrait as the reference. Specifically, OTAvatar first inverts a portrait image to a motion-free identity code. Second, the identity code and a motion code are utilized to modulate an efficient CNN to generate a tri-plane formulated volume, which encodes the subject in the desired motion. Finally, volume rendering is employed to generate an image in any view. The core of our solution is a novel decoupling-by-inverting strategy that disentangles identity and motion in the latent code via optimization-based inversion. Benefiting from the efficient tri-plane representation, we achieve controllable rendering of generalized face avatar at 35 FPS on A100. Experiments show promising performance of cross-identity reenactment on subjects out of the training set and better 3D consistency. The code is available at https://github.com/theEricMa/OTAvatar.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1778.HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images</span><br>
                <span class="as">Karnewar, AnimeshandVedaldi, AndreaandNovotny, DavidandMitra, NiloyJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18423-18433.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将扩散模型扩展到3D图像生成？<br>
                    动机：虽然扩散模型在2D图像生成方面表现出色，但扩展到3D仍面临数据获取复杂和计算内存大的挑战。<br>
                    方法：提出新的训练策略，仅使用2D图像进行监督训练；同时提出一种新的图像形成模型，使模型内存与空间记忆解耦。<br>
                    效果：通过CO3D数据集的实验证明，该方法可扩展性强，训练稳定，并在样本质量和对现有3D生成模型的逼真度上具有竞争力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have emerged as the best approach for generative modeling of 2D images. Part of their success is due to the possibility of training them on millions if not billions of images with a stable learning objective. However, extending these models to 3D remains difficult for two reasons. First, finding a large quantity of 3D training data is much more complex than for 2D images. Second, while it is conceptually trivial to extend the models to operate on 3D rather than 2D grids, the associated cubic growth in memory and compute complexity makes this infeasible. We address the first challenge by introducing a new diffusion setup that can be trained, end-to-end, with only posed 2D images for supervision; and the second challenge by proposing an image formation model that decouples model memory from spatial memory. We evaluate our method on real-world data, using the CO3D dataset which has not been used to train 3D generative models before. We show that our diffusion models are scalable, train robustly, and are competitive in terms of sample quality and fidelity to existing approaches for 3D generative modeling.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1779.NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-Shot Real Image Animation</span><br>
                <span class="as">Yin, YuandGhasedi, KamranandWu, HsiangTaoandYang, JiaolongandTong, XinandFu, Yun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_NeRFInvertor_High_Fidelity_NeRF-GAN_Inversion_for_Single-Shot_Real_Image_Animation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8539-8548.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将NeRF-GAN模型用于生成真实主体的高质量人脸图像。<br>
                    动机：尽管现有的NeRF-GAN模型能够成功合成随机采样的潜在空间中的假身份图像，但在生成真实主体的人脸图像方面仍面临挑战，因为存在所谓的“逆问题”。<br>
                    方法：本文提出了一种通用的方法，通过手术式微调这些NeRF-GAN模型，仅通过单张图像实现真实主体的高保真动画。给定一张非领域的真实图像的优化潜在代码，我们在渲染的图像上使用二维损失函数来缩小身份差距。此外，我们的方法利用优化潜在代码周围的领域内邻域样本进行显性和隐性的3D正则化，以消除几何和视觉伪像。<br>
                    效果：实验证实，我们的方法在多个不同的数据集上的多个NeRF-GAN模型中都能有效地生成真实、高保真且具有3D一致性的真实人脸动画。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Nerf-based Generative models have shown impressive capacity in generating high-quality images with consistent 3D geometry. Despite successful synthesis of fake identity images randomly sampled from latent space, adopting these models for generating face images of real subjects is still a challenging task due to its so-called inversion issue. In this paper, we propose a universal method to surgically fine-tune these NeRF-GAN models in order to achieve high-fidelity animation of real subjects only by a single image. Given the optimized latent code for an out-of-domain real image, we employ 2D loss functions on the rendered image to reduce the identity gap. Furthermore, our method leverages explicit and implicit 3D regularizations using the in-domain neighborhood samples around the optimized latent code to remove geometrical and visual artifacts. Our experiments confirm the effectiveness of our method in realistic, high-fidelity, and 3D consistent animation of real faces on multiple NeRF-GAN models across different datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1780.Disentangling Writer and Character Styles for Handwriting Generation</span><br>
                <span class="as">Dai, GangandZhang, YifanandWang, QingfengandDu, QingandYu, ZhuliangandLiu, ZhuomanandHuang, Shuangping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Disentangling_Writer_and_Character_Styles_for_Handwriting_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5977-5986.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练机器合成多样的手写体是一项有趣的任务，但现有的基于RNN的方法主要关注捕捉一个人的书写风格，忽视了同一人写的字符之间的微妙风格不一致。<br>
                    动机：尽管一个人的笔迹通常表现出一般的统一性（如字形倾斜和宽高比），但在更细微的细节上（如笔画长度和曲率）仍然存在小的风格变化。因此，我们提出从个人笔迹中解耦作者和字符级别的风格表示，以合成真实的在线手写字符。<br>
                    方法：我们提出了风格解耦的Transformer（SDT），它使用两个互补的对比目标来提取参考样本的风格共性，并捕获每个样本的详细风格模式。<br>
                    效果：我们在各种语言脚本上的大量实验表明，SDT的有效性。值得注意的是，我们的实证研究发现，学习到的两个风格表示提供了不同频率大小的信息，强调了单独提取风格的重要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training machines to synthesize diverse handwritings is an intriguing task. Recently, RNN-based methods have been proposed to generate stylized online Chinese characters. However, these methods mainly focus on capturing a person's overall writing style, neglecting subtle style inconsistencies between characters written by the same person. For example, while a person's handwriting typically exhibits general uniformity (e.g., glyph slant and aspect ratios), there are still small style variations in finer details (e.g., stroke length and curvature) of characters. In light of this, we propose to disentangle the style representations at both writer and character levels from individual handwritings to synthesize realistic stylized online handwritten characters. Specifically, we present the style-disentangled Transformer (SDT), which employs two complementary contrastive objectives to extract the style commonalities of reference samples and capture the detailed style patterns of each sample, respectively. Extensive experiments on various language scripts demonstrate the effectiveness of SDT. Notably, our empirical findings reveal that the two learned style representations provide information at different frequency magnitudes, underscoring the importance of separate style extraction. Our source code is public at: https://github.com/dailenson/SDT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1781.StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-Based Generator</span><br>
                <span class="as">Guan, JiazhiandZhang, ZhanwangandZhou, HangandHu, TianshuandWang, KaisiyuanandHe, DongliangandFeng, HaochengandLiu, JingtuoandDing, ErruiandLiu, ZiweiandWang, Jingdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1505-1515.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前的语言表示模型在利用丰富的结构化知识方面存在不足，如何通过结合大规模文本语料库和知识图谱来训练一种增强的语言表示模型。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，而知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：本文提出了一种名为ERNIE的增强语言表示模型，该模型采用大规模文本语料库和知识图谱进行联合训练，能够同时充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite recent advances in syncing lip movements with any audio waves, current methods still struggle to balance generation quality and the model's generalization ability. Previous studies either require long-term data for training or produce a similar movement pattern on all subjects with low quality. In this paper, we propose StyleSync, an effective framework that enables high-fidelity lip synchronization. We identify that a style-based generator would sufficiently enable such a charming property on both one-shot and few-shot scenarios. Specifically, we design a mask-guided spatial information encoding module that preserves the details of the given face. The mouth shapes are accurately modified by audio through modulated convolutions. Moreover, our design also enables personalized lip-sync by introducing style space and generator refinement on only limited frames. Thus the identity and talking style of a target person could be accurately preserved. Extensive experiments demonstrate the effectiveness of our method in producing high-fidelity results on a variety of scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1782.High-Fidelity and Freely Controllable Talking Head Video Generation</span><br>
                <span class="as">Gao, YueandZhou, YuanandWang, JingluandLi, XiaoandMing, XiangandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_High-Fidelity_and_Freely_Controllable_Talking_Head_Video_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5609-5619.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成高质量的、可控的说话人视频。<br>
                    动机：当前方法在生成视频时，面临面部变形和扭曲严重、运动相关信息未明确分离、视频中常出现闪烁等挑战，限制了生成视频的质量与可控性。<br>
                    方法：提出一种新模型，利用自监督学习地标和基于3D人脸模型的地标来模拟运动，并引入一种新颖的运动感知多尺度特征对齐模块，以有效传输运动而不会引起面部扭曲。同时，通过特征上下文适应和传播模块增强合成说话人视频的平滑度。<br>
                    效果：在具有挑战性的数据集上进行评估，实验结果表明该模型在各种指标上都达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Talking head generation is to generate video based on a given source identity and target motion. However, current methods face several challenges that limit the quality and controllability of the generated videos. First, the generated face often has unexpected deformation and severe distortions. Second, the driving image does not explicitly disentangle movement-relevant information, such as poses and expressions, which restricts the manipulation of different attributes during generation. Third, the generated videos tend to have flickering artifacts due to the inconsistency of the extracted landmarks between adjacent frames. In this paper, we propose a novel model that produces high-fidelity talking head videos with free control over head pose and expression. Our method leverages both self-supervised learned landmarks and 3D face model-based landmarks to model the motion. We also introduce a novel motion-aware multi-scale feature alignment module to effectively transfer the motion without face distortion. Furthermore, we enhance the smoothness of the synthesized talking head videos with a feature context adaptation and propagation module. We evaluate our model on challenging datasets and demonstrate its state-of-the-art performance. More information is available at https://yuegao.me/PECHead.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1783.Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization</span><br>
                <span class="as">Huang, MengqiandMao, ZhendongandChen, ZhuoweiandZhang, Yongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22596-22605.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于向量量化的自回归模型在生成图像时，由于将固定大小的图像区域编码为固定长度的代码，忽视了不同区域的信息密度差异，导致重要区域信息不足，不重要区域冗余，最终影响生成质量和速度。<br>
                    动机：为了解决上述问题，提出了一种新的两阶段框架，通过动态量化变分自编码器（DQ-VAE）对图像区域进行基于信息密度的可变长度编码，然后通过DQ-Transformer以粗到细的方式生成图像。<br>
                    方法：首先，使用DQ-VAE对图像区域进行基于信息密度的可变长度编码；其次，通过堆叠变压器架构和共享内容、非共享位置输入层设计，交替地模拟每个粒度的位置和内容代码，从粗到细生成图像。<br>
                    效果：在各种生成任务上的全面实验验证了该方法在有效性和效率上的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing vector quantization (VQ) based autoregressive models follow a two-stage generation paradigm that first learns a codebook to encode images as discrete codes, and then completes generation based on the learned codebook. However, they encode fixed-size image regions into fixed-length codes and ignore their naturally different information densities, which results in insufficiency in important regions and redundancy in unimportant ones, and finally degrades the generation quality and speed. Moreover, the fixed-length coding leads to an unnatural raster-scan autoregressive generation. To address the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization VAE (DQ-VAE) which encodes image regions into variable-length codes based on their information densities for an accurate & compact code representation. (2) DQ-Transformer which thereby generates images autoregressively from coarse-grained (smooth regions with fewer codes) to fine-grained (details regions with more codes) by modeling the position and content of codes in each granularity alternately, through a novel stacked-transformer architecture and shared-content, non-shared position input layers designs. Comprehensive experiments on various generation tasks validate our superiorities in both effectiveness and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1784.ReCo: Region-Controlled Text-to-Image Generation</span><br>
                <span class="as">Yang, ZhengyuanandWang, JianfengandGan, ZheandLi, LinjieandLin, KevinandWu, ChenfeiandDuan, NanandLiu, ZichengandLiu, CeandZeng, MichaelandWang, Lijuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14246-14255.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高文本到图像生成模型的可控性，使其能够根据自由形式的区域描述精确地指定特定区域的具体内容。<br>
                    动机：目前的大规模文本到图像（T2I）模型在生成高保真图像方面表现出色，但在可控性方面存在限制，例如无法根据自由形式的文本描述精确地指定特定区域的内容。<br>
                    方法：提出一种有效的区域控制技术，通过为T2I模型的输入添加额外的位置标记来增强其可控性。每个区域由四个位置标记表示，分别代表左上角和右下角，然后是开放式的自然语言区域描述。然后，使用这种新的输入接口对预训练的T2I模型进行微调。<br>
                    效果：实验结果表明，ReCo（区域控制的T2I）模型在任意对象的开放式区域文本描述下实现了更好的图像质量和更准确的对象放置，相比于使用位置词加强的T2I模型，FID从8.82降低到7.36，SceneFID从15.54降低到6.51（COCO数据集），并且在COCO数据集上实现了20.40%的区域分类精度提升。此外，我们还证明ReCo可以更好地控制对象数量、空间关系和区域属性（如颜色/大小）。在PaintSkill上的人类评估显示，ReCo在生成具有正确对象数量和空间关系的图像方面的准确率比T2I模型高出+19.28%和+17.21%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, large-scale text-to-image (T2I) models have shown impressive performance in generating high-fidelity images, but with limited controllability, e.g., precisely specifying the content in a specific region with a free-form text description. In this paper, we propose an effective technique for such regional control in T2I generation. We augment T2I models' inputs with an extra set of position tokens, which represent the quantized spatial coordinates. Each region is specified by four position tokens to represent the top-left and bottom-right corners, followed by an open-ended natural language regional description. Then, we fine-tune a pre-trained T2I model with such new input interface. Our model, dubbed as ReCo (Region-Controlled T2I), enables the region control for arbitrary objects described by open-ended regional texts rather than by object labels from a constrained category set. Empirically, ReCo achieves better image quality than the T2I model strengthened by positional words (FID: 8.82 -> 7.36, SceneFID: 15.54 -> 6.51 on COCO), together with objects being more accurately placed, amounting to a 20.40% region classification accuracy improvement on COCO. Furthermore, we demonstrate that ReCo can better control the object count, spatial relationship, and region attributes such as color/size, with the free-form regional description. Human evaluation on PaintSkill shows that ReCo is +19.28% and +17.21% more accurate in generating images with correct object count and spatial relationship than the T2I model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1785.Fix the Noise: Disentangling Source Feature for Controllable Domain Translation</span><br>
                <span class="as">Lee, DongyeunandLee, JaeYoungandKim, DoyeonandChoi, JaehyunandYoo, JaejunandKim, Junmo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Fix_the_Noise_Disentangling_Source_Feature_for_Controllable_Domain_Translation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14224-14234.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过单一模型实现高质量的领域翻译，同时更好地控制不同领域的特征。<br>
                    动机：现有的方法需要额外的模型，计算量大且视觉质量不佳，且控制步骤有限，无法实现平滑过渡。<br>
                    方法：在目标特征空间的解耦子空间中保留源特征，仅使用单一模型从全新领域生成图像，从而平滑地控制保留源特征的程度。<br>
                    效果：实验表明，该方法可以生成更一致、更真实的图像，并在不同级别的转换上保持精确的可控性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllability over different levels of transformation. The code is available at LeeDongYeun/FixNoise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1786.FaceLit: Neural 3D Relightable Faces</span><br>
                <span class="as">Ranjan, AnuragandYi, KwangMooandChang, Jen-HaoRickandTuzel, Oncel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ranjan_FaceLit_Neural_3D_Relightable_Faces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8619-8628.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅从2D图像中生成可以在各种用户定义的光照条件和视角下渲染的3D人脸。<br>
                    动机：现有的方法需要仔细的捕捉设置或人工劳动，而我们的方法依赖于现成的姿态和照明估计器，无需手动标注。<br>
                    方法：我们提出了一个名为FaceLit的生成框架，该框架能够从野外的2D图像中学习生成3D人脸，并结合了Phong反射模型在神经体积渲染框架中。<br>
                    效果：我们的方法能够在多个数据集——FFHQ、MetFaces和CelebA-HQ上实现具有明确照明和视图控制的逼真人脸生成。在FFHQ数据集上，我们在3D感知的GANs中实现了最先进的照片写实主义，FID得分为3.5。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a generative framework, FaceLit, capable of generating a 3D face that can be rendered at various user-defined lighting conditions and views, learned purely from 2D images in-the-wild without any manual annotation. Unlike existing works that require careful capture setup or human labor, we rely on off-the-shelf pose and illumination estimators. With these estimates, we incorporate the Phong reflectance model in the neural volume rendering framework. Our model learns to generate shape and material properties of a face such that, when rendered according to the natural statistics of pose and illumination, produces photorealistic face images with multiview 3D and illumination consistency. Our method enables photorealistic generation of faces with explicit illumination and view controls on multiple datasets -- FFHQ, MetFaces and CelebA-HQ. We show state-of-the-art photorealism among 3D aware GANs on FFHQ dataset achieving an FID score of 3.5.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1787.StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis</span><br>
                <span class="as">Li, HaoandHou, XianxuandHuang, ZepengandShen, Linlin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20960-20969.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模、高质量的亲属关系数据合成高质量的后代面部图像。<br>
                    动机：由于缺乏大规模的高质量标注的亲属关系数据，合成具有遗传关系的高质量后代面部图像具有挑战性。<br>
                    方法：提出区域级面部基因（RFG）提取框架，通过图像基基因编码器（IGE）、潜在基基因编码器（LGE）和基因解码器学习给定面部图像的RFG及其与StyleGAN2的潜在空间的关系。设计循环损失来测量基因解码器和图像编码器的输出以及LGE和IGE的输出之间的L_2距离，从而仅需要面部图像就可以训练我们的框架。<br>
                    效果：在FIW、TSKinFace和FF数据库上的定性、定量和主观实验清楚地表明，我们的方法生成的亲属关系面部图像的质量和多样性明显优于现有的最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-fidelity kinship face synthesis has many potential applications, such as kinship verification, missing child identification, and social media analysis. However, it is challenging to synthesize high-quality descendant faces with genetic relations due to the lack of large-scale, high-quality annotated kinship data. This paper proposes RFG (Region-level Facial Gene) extraction framework to address this issue. We propose to use IGE (Image-based Gene Encoder), LGE (Latent-based Gene Encoder) and Gene Decoder to learn the RFGs of a given face image, and the relationships between RFGs and the latent space of StyleGAN2. As cycle-like losses are designed to measure the L_2 distances between the output of Gene Decoder and image encoder, and that between the output of LGE and IGE, only face images are required to train our framework, i.e. no paired kinship face data is required. Based upon the proposed RFGs, a crossover and mutation module is further designed to inherit the facial parts of parents. A Gene Pool has also been used to introduce the variations into the mutation of RFGs. The diversity of the faces of descendants can thus be significantly increased. Qualitative, quantitative, and subjective experiments on FIW, TSKinFace, and FF-Databases clearly show that the quality and diversity of kinship faces generated by our approach are much better than the existing state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1788.3D Cinemagraphy From a Single Image</span><br>
                <span class="as">Li, XingyiandCao, ZhiguoandSun, HuiqiangandZhang, JianmingandXian, KeandLin, Guosheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_3D_Cinemagraphy_From_a_Single_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4595-4605.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将2D图像动画与3D摄影相结合，以生成同时包含视觉内容动画和相机运动的视频。<br>
                    动机：现有的2D图像动画和3D摄影方法直接结合会产生明显的伪影或不一致的动画效果。<br>
                    方法：首先，将输入图像转换为基于特征的分层深度图像，然后将其投影到特征点云。为了动画场景，进行运动估计并将2D运动提升为3D场景流。最后，通过根据场景流双向移动点云并分别投影到目标图像平面并混合结果来解决点向前移动时出现的孔洞问题。<br>
                    效果：大量实验表明该方法的有效性。用户研究也验证了该方法的引人注目的渲染结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and blending the results. Extensive experiments demonstrate the effectiveness of our method. A user study is also conducted to validate the compelling rendering results of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1789.Inversion-Based Style Transfer With Diffusion Models</span><br>
                <span class="as">Zhang, YuxinandHuang, NishaandTang, FanandHuang, HaibinandMa, ChongyangandDong, WeimingandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10146-10156.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过一种模型，直接从一幅画中学习艺术风格，并无需复杂的文本描述即可指导图像的合成。<br>
                    动机：现有的艺术风格生成方法往往无法有效控制形状变化或传达元素，而预训练的文本到图像合成扩散概率模型虽然质量出色，但通常需要详细的文本描述才能准确描绘特定画作的属性。<br>
                    方法：提出一种基于反转的风格转移方法（InST），该方法可以高效准确地学习图像的关键信息，从而捕捉和转移画作的艺术风格。<br>
                    效果：在多种艺术家和风格的众多画作上展示了该方法的质量和效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The artistic style within a painting is the means of expression, which includes not only the painting material, colors, and brushstrokes, but also the high-level attributes, including semantic elements and object shapes. Previous arbitrary example-guided artistic image generation methods often fail to control shape changes or convey elements. Pre-trained text-to-image synthesis diffusion probabilistic models have achieved remarkable quality but often require extensive textual descriptions to accurately portray the attributes of a particular painting.The uniqueness of an artwork lies in the fact that it cannot be adequately explained with normal language. Our key idea is to learn the artistic style directly from a single painting and then guide the synthesis without providing complex textual descriptions. Specifically, we perceive style as a learnable textual description of a painting.We propose an inversion-based style transfer method (InST), which can efficiently and accurately learn the key information of an image, thus capturing and transferring the artistic style of a painting. We demonstrate the quality and efficiency of our method on numerous paintings of various artists and styles. Codes are available at https://github.com/zyxElsa/InST.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1790.StyleRes: Transforming the Residuals for Real Image Editing With StyleGAN</span><br>
                <span class="as">Pehlivan, HamzaandDalva, YusufandDundar, Aysegul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pehlivan_StyleRes_Transforming_the_Residuals_for_Real_Image_Editing_With_StyleGAN_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1828-1837.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现高保真图像反转和高质量属性编辑。<br>
                    动机：在将真实图像转换为StyleGAN的潜空间时，如何在图像重建保真度和图像编辑质量之间取得平衡仍然是一个开放的挑战。<br>
                    方法：通过学习较高潜码中较低潜码无法编码的残差特征来实现高保真反转，并通过学习如何转换残差特征以适应潜在代码中的操作来实现高质量编辑。<br>
                    效果：通过训练框架提取残差特征并使用新颖的架构管道和循环一致性损失进行转换，实验结果和与最先进的反转方法的比较表明了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel image inversion framework and a training pipeline to achieve high-fidelity image inversion with high-quality attribute editing. Inverting real images into StyleGAN's latent space is an extensively studied problem, yet the trade-off between the image reconstruction fidelity and image editing quality remains an open challenge. The low-rate latent spaces are limited in their expressiveness power for high-fidelity reconstruction. On the other hand, high-rate latent spaces result in degradation in editing quality. In this work, to achieve high-fidelity inversion, we learn residual features in higher latent codes that lower latent codes were not able to encode. This enables preserving image details in reconstruction. To achieve high-quality editing, we learn how to transform the residual features for adapting to manipulations in latent codes. We train the framework to extract residual features and transform them via a novel architecture pipeline and cycle consistency losses. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1791.Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding</span><br>
                <span class="as">Kim, GyeongmanandShim, HajinandKim, HyunsuandChoi, YunjeyandKim, JunhoandYang, Eunho</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Diffusion_Video_Autoencoders_Toward_Temporally_Consistent_Face_Video_Editing_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6091-6100.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将最新的人脸图像编辑方法扩展到人脸视频编辑任务，并解决编辑帧之间的时间一致性问题。<br>
                    动机：现有的人脸视频编辑方法无法有效解决编辑帧之间的时间一致性问题。<br>
                    方法：提出一种基于扩散自编码器的新型人脸视频编辑框架，该模型能够从给定的视频中提取身份和运动的特征，并通过操纵这些特征来实现视频编辑。<br>
                    效果：实验结果表明，该方法在各种情况下都能实现优秀的人脸视频编辑效果，并且优于现有的基于GAN的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1792.Conditional Text Image Generation With Diffusion Models</span><br>
                <span class="as">Zhu, YuanzhiandLi, ZhaohaiandWang, TianweiandHe, MengchaoandYao, Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Conditional_Text_Image_Generation_With_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14235-14245.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决文本图像生成的问题，通过利用扩散模型的强大能力，在给定条件下生成逼真且多样化的图像样本。<br>
                    动机：目前的文本识别系统严重依赖图像合成和增强，因为收集和注释足够的真实文本图像以实现现实世界的复杂性和多样性是困难的。<br>
                    方法：本文提出了一种名为条件文本图像生成的扩散模型（CTIG-DM）的方法，该方法利用扩散模型在给定条件下生成逼真且多样化的图像样本的能力，并设计了三种条件：图像条件、文本条件和风格条件，以控制图像生成过程中样本的属性、内容和样式。<br>
                    效果：实验结果表明，提出的CTIG-DM能够生成模拟现实世界复杂性和多样性的图像样本，从而提高现有文本识别器的性能。此外，CTIG-DM在领域适应和生成包含未登录词的图像方面显示出其吸引人的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current text recognition systems, including those for handwritten scripts and scene text, have relied heavily on image synthesis and augmentation, since it is difficult to realize real-world complexity and diversity through collecting and annotating enough real text images. In this paper, we explore the problem of text image generation, by taking advantage of the powerful abilities of Diffusion Models in generating photo-realistic and diverse image samples with given conditions, and propose a method called Conditional Text Image Generation with Diffusion Models (CTIG-DM for short). To conform to the characteristics of text images, we devise three conditions: image condition, text condition, and style condition, which can be used to control the attributes, contents, and styles of the samples in the image generation process. Specifically, four text image generation modes, namely: (1) synthesis mode, (2) augmentation mode, (3) recovery mode, and (4) imitation mode, can be derived by combining and configuring these three conditions. Extensive experiments on both handwritten and scene text demonstrate that the proposed CTIG-DM is able to produce image samples that simulate real-world complexity and diversity, and thus can boost the performance of existing text recognizers. Besides, CTIG-DM shows its appealing potential in domain adaptation and generating images containing Out-Of-Vocabulary (OOV) words.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1793.Transforming Radiance Field With Lipschitz Network for Photorealistic 3D Scene Stylization</span><br>
                <span class="as">Zhang, ZichengandLiu, YingluandHan, CongyingandPan, YingweiandGuo, TiandeandYao, Ting</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Transforming_Radiance_Field_With_Lipschitz_Network_for_Photorealistic_3D_Scene_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20712-20721.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用神经辐射场（NeRF）进行真实感3D场景风格化。<br>
                    动机：虽然神经辐射场（NeRFs）在3D场景表示和新颖视图合成方面取得了显著进步，但直接将其用于生成视觉一致、真实的风格化场景仍具有挑战性。<br>
                    方法：研究者提出了LipRF框架，该框架首先预训练一个辐射场来重建3D场景，然后通过2D PST模拟每个视图的风格，学习一个Lipschitz网络来风格化预先训练的外观。为了平衡重建和风格化，设计了一种自适应正则化策略，并引入了渐进式梯度聚合策略以优化LipRF。<br>
                    效果：实验表明，LipRF在真实感3D风格化和物体外观编辑方面表现出高质量和稳健的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not trivial to exploit NeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic stylized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses. Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When transforming the appearance representation of a pre-trained NeRF with Lipschitz mapping, the consistency and photorealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a radiance field to reconstruct the 3D scene, and then emulates the style on each view by 2D PST as the prior to learn a Lipschitz network to stylize the pre-trained appearance. In view of that Lipschitz condition highly impacts the expressivity of the neural network, we devise an adaptive regularization to balance the reconstruction and stylization. A gradual gradient aggregation strategy is further introduced to optimize LipRF in a cost-efficient manner. We conduct extensive experiments to show the high quality and robust performance of LipRF on both photorealistic 3D stylization and object appearance editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1794.DiffCollage: Parallel Generation of Large Content With Diffusion Models</span><br>
                <span class="as">Zhang, QinshengandSong, JiamingandHuang, XunandChen, YongxinandLiu, Ming-Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10188-10198.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种组合扩散模型DiffCollage，利用在生成大段内容的训练中训练的扩散模型来生成大量内容。<br>
                    动机：现有的生成模型需要通过自回归过程来生成内容，效率较低。扩散模型可以并行生成任意大小和形状的内容，但需要对每个部分分别进行训练。<br>
                    方法：DiffCollage采用因子图表示，其中每个因子节点代表内容的一部分，变量节点代表它们的重叠部分。这种表示允许我们聚合在单个节点上定义的扩散模型的中间输出，以并行生成任意大小和形状的内容，而无需依赖自回归生成过程。<br>
                    效果：我们将DiffCollage应用于各种任务，包括无限图像生成、全景图像生成和长篇文本引导的运动生成等。与强大的自回归基线相比，大量的实验结果验证了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present DiffCollage, a compositional diffusion model that can generate large content by leveraging diffusion models trained on generating pieces of the large content. Our approach is based on a factor graph representation where each factor node represents a portion of the content and a variable node represents their overlap. This representation allows us to aggregate intermediate outputs from diffusion models defined on individual nodes to generate content of arbitrary size and shape in parallel without resorting to an autoregressive generation procedure. We apply DiffCollage to various tasks, including infinite image generation, panorama image generation, and long-duration text-guided motion generation. Extensive experimental results with a comparison to strong autoregressive baselines verify the effectiveness of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1795.Mofusion: A Framework for Denoising-Diffusion-Based Motion Synthesis</span><br>
                <span class="as">Dabral, RishabhandMughal, MuhammadHamzaandGolyanik, VladislavandTheobalt, Christian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9760-9770.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的人类运动合成方法要么确定性太强，要么在运动多样性和质量之间难以取舍。<br>
                    动机：为了解决这些问题，我们提出了MoFusion，一种基于去噪扩散的新型高质量条件人类运动合成框架。<br>
                    方法：通过去噪扩散框架，根据各种条件（如音乐和文本）合成长时间、时间上合理且语义准确的运动。并通过我们的计划加权策略在运动扩散框架中引入众所周知的运动合理性损失。<br>
                    效果：通过全面的定量评估和感知用户研究，我们证明了MoFusion在文献中的现有基准上优于最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Conventional methods for human motion synthesis have either been deterministic or have had to struggle with the trade-off between motion diversity vs motion quality. In response to these limitations, we introduce MoFusion, i.e., a new denoising-diffusion-based framework for high-quality conditional human motion synthesis that can synthesise long, temporally plausible, and semantically accurate motions based on a range of conditioning contexts (such as music and text). We also present ways to introduce well-known kinematic losses for motion plausibility within the motion-diffusion framework through our scheduled weighting strategy. The learned latent space can be used for several interactive motion-editing applications like in-betweening, seed-conditioning, and text-based editing, thus, providing crucial abilities for virtual-character animation and robotics. Through comprehensive quantitative evaluations and a perceptual user study, we demonstrate the effectiveness of MoFusion compared to the state-of-the-art on established benchmarks in the literature. We urge the reader to watch our supplementary video. The source code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1796.Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation</span><br>
                <span class="as">Chai, JackyChenLongandNg, Tiong-SikandLow, Cheng-YawandPark, JaewooandTeoh, AndrewBengJin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_Recognizability_Embedding_Enhancement_for_Very_Low-Resolution_Face_Recognition_and_Quality_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9957-9967.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决极低分辨率人脸识别（VLRFR）中的挑战，如兴趣区域小和由于采集设备极端的远离距离或广视角导致的低分辨率。<br>
                    动机：现有的方法主要关注提高视觉质量，而忽视了在嵌入空间中提升面部识别性的方法。<br>
                    方法：本文提出了一种基于学习的面部识别性测量方法，即识别性指数（RI）。同时设计了一种索引转移损失函数，以推动具有低RI的难以识别的面部嵌入远离不可识别的面部簇，从而提高RI。此外，还引入了一种感知注意力机制，以关注显著可识别的面部区域，为嵌入学习提供更好的解释性和判别性内容。<br>
                    效果：通过在三个具有挑战性的低分辨率数据集上进行广泛的评估，并与最先进的方法进行比较，证明了所提出模型在极低分辨率人脸识别任务上的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Very low-resolution face recognition (VLRFR) poses unique challenges, such as tiny regions of interest and poor resolution due to extreme standoff distance or wide viewing angle of the acquisition device. In this paper, we study principled approaches to elevate the recognizability of a face in the embedding space instead of the visual quality. We first formulate a robust learning-based face recognizability measure, namely recognizability index (RI), based on two criteria: (i) proximity of each face embedding against the unrecognizable faces cluster center and (ii) closeness of each face embedding against its positive and negative class prototypes. We then devise an index diversion loss to push the hard-to-recognize face embedding with low RI away from unrecognizable faces cluster to boost the RI, which reflects better recognizability. Additionally, a perceptibility-aware attention mechanism is introduced to attend to the salient recognizable face regions, which offers better explanatory and discriminative content for embedding learning. Our proposed model is trained end-to-end and simultaneously serves recognizability-aware embedding learning and face quality estimation. To address VLRFR, extensive evaluations on three challenging low-resolution datasets and face quality assessment demonstrate the superiority of the proposed model over the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1797.Shape-Aware Text-Driven Layered Video Editing</span><br>
                <span class="as">Lee, Yao-ChihandJang, Ji-ZeGenevieveandChen, Yi-TingandQiu, ElizabethandHuang, Jia-Bin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14317-14326.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频编辑中形状变化的问题，现有的方法只能编辑对象外观，无法处理形状变化。<br>
                    动机：由于使用固定UV映射场进行纹理图集的限制，现有的视频编辑方法无法处理形状变化。<br>
                    方法：我们提出了一种形状感知的、基于文本的视频编辑方法。首先，我们将输入和编辑的关键帧之间的形变场传播到所有帧。然后，我们利用预训练的文本条件扩散模型作为指导，对形状失真进行细化并完成未见过的区域。<br>
                    效果：实验结果表明，我们的方法可以实现形状感知的一致视频编辑，并与最先进的技术相比具有优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal consistency is essential for video editing applications. Existing work on layered representation of videos allows propagating edits consistently to each frame. These methods, however, can only edit object appearance rather than object shape changes due to the limitation of using a fixed UV mapping field for texture atlas. We present a shape-aware, text-driven video editing method to tackle this challenge. To handle shape changes in video editing, we first propagate the deformation field between the input and edited keyframe to all frames. We then leverage a pre-trained text-conditioned diffusion model as guidance for refining shape distortion and completing unseen regions. The experimental results demonstrate that our method can achieve shape-aware consistent video editing and compare favorably with the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1798.QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity</span><br>
                <span class="as">Huang, SiyuandAn, JieandWei, DonglaiandLuo, JieboandPfister, Hanspeter</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_QuantArt_Quantizing_Image_Style_Transfer_Towards_High_Visual_Fidelity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5947-5956.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高预训练语言模型对结构化知识的利用，以提升语言理解能力。<br>
                    动机：现有的预训练语言模型在处理知识驱动任务时，缺乏对结构化知识的充分利用。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，该模型通过联合训练大规模文本语料库和知识图谱，能够同时捕捉词汇、句法和知识信息。<br>
                    效果：实验结果显示，ERNIE在各种知识驱动任务上表现优秀，且在其他常见的NLP任务上与BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The mechanism of existing style transfer algorithms is by minimizing a hybrid loss function to push the generated image toward high similarities in both content and style. However, this type of approach cannot guarantee visual fidelity, i.e., the generated artworks should be indistinguishable from real ones. In this paper, we devise a new style transfer framework called QuantArt for high visual-fidelity stylization. QuantArt pushes the latent representation of the generated artwork toward the centroids of the real artwork distribution with vector quantization. By fusing the quantized and continuous latent representations, QuantArt allows flexible control over the generated artworks in terms of content preservation, style similarity, and visual fidelity. Experiments on various style transfer settings show that our QuantArt framework achieves significantly higher visual fidelity compared with the existing style transfer methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1799.Neural Transformation Fields for Arbitrary-Styled Font Generation</span><br>
                <span class="as">Fu, BinandHe, JunjunandWang, JianjunandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Neural_Transformation_Fields_for_Arbitrary-Styled_Font_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22438-22447.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量样本生成字体图像。<br>
                    动机：由于学术和商业价值，近年来少样本字体生成（FFG）成为新兴话题。<br>
                    方法：将字体生成模型化为从源字符图像到目标字体图像的连续转换过程，通过创建和消散字体像素将相应的转换嵌入神经变换场。<br>
                    效果：实验表明该方法在少样本字体生成任务上取得了最先进的性能，证明了提出的模型的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot font generation (FFG), aiming at generating font images with a few samples, is an emerging topic in recent years due to the academic and commercial values. Typically, the FFG approaches follow the style-content disentanglement paradigm, which transfers the target font styles to characters by combining the content representations of source characters and the style codes of reference samples. Most existing methods attempt to increase font generation ability via exploring powerful style representations, which may be a sub-optimal solution for the FFG task due to the lack of modeling spatial transformation in transferring font styles. In this paper, we model font generation as a continuous transformation process from the source character image to the target font image via the creation and dissipation of font pixels, and embed the corresponding transformations into a neural transformation field. With the estimated transformation path, the neural transformation field generates a set of intermediate transformation results via the sampling process, and a font rendering formula is developed to accumulate them into the target font image. Extensive experiments show that our method achieves state-of-the-art performance on few-shot font generation task, which demonstrates the effectiveness of our proposed model. Our implementation is available at: https://github.com/fubinfb/NTF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1800.EDICT: Exact Diffusion Inversion via Coupled Transformations</span><br>
                <span class="as">Wallace, BramandGokul, AkashandNaik, Nikhil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22532-22541.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何找到一种初始噪声向量，当输入到扩散过程中时，可以产生输入图像（称为反转），这是去噪扩散模型（DDMs）中的一个重要问题，对于真实图像编辑有应用。<br>
                    动机：在真实图像的编辑和反转中，标准的方法是使用去噪扩散隐式模型（DDIMs）确定性地对图像进行噪声处理，使其达到中间状态，这是给定原始条件的情况下，去噪将遵循的路径。然而，这种方法对于真实图像的反转是不稳定的，因为它依赖于局部线性化假设，这会导致误差的传播，从而导致错误的图像重建和内容的丢失。<br>
                    方法：我们提出了通过耦合变换的精确扩散反转（EDICT）方法，这是一种从仿射耦合层中获得灵感的反转方法。EDICT通过保持两个耦合的噪声向量来数学上精确地反转真实图像和模型生成的图像，这两个噪声向量以交替的方式相互反转。<br>
                    效果：我们在最先进的潜在扩散模型Stable Diffusion上进行实验，证明EDICT能够高保真地重建真实图像。在像MS-COCO这样的复杂图像数据集上，EDICT的重建效果显著优于DDIM，将重建的均方误差提高了一倍。使用从真实图像反转的噪声向量，EDICT可以实现广泛的图像编辑——从局部和全局语义编辑到图像风格化——同时保持与原始图像结构的准确性。EDICT不需要模型训练/微调、提示调整或额外的数据，并且可以与任何预训练的DDM结合使用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Finding an initial noise vector that produces an input image when fed into the diffusion process (known as inversion) is an important problem in denoising diffusion models (DDMs), with applications for real image editing. The standard approach for real image editing with inversion uses denoising diffusion implicit models (DDIMs) to deterministically noise the image to the intermediate state along the path that the denoising would follow given the original conditioning. However, DDIM inversion for real images is unstable as it relies on local linearization assumptions, which result in the propagation of errors, leading to incorrect image reconstruction and loss of content. To alleviate these problems, we propose Exact Diffusion Inversion via Coupled Transformations (EDICT), an inversion method that draws inspiration from affine coupling layers. EDICT enables mathematically exact inversion of real and model-generated images by maintaining two coupled noise vectors which are used to invert each other in an alternating fashion. Using Stable Diffusion [25], a state-of-the-art latent diffusion model, we demonstrate that EDICT successfully reconstructs real images with high fidelity. On complex image datasets like MS-COCO, EDICT reconstruction significantly outperforms DDIM, improving the mean square error of reconstruction by a factor of two. Using noise vectors inverted from real images, EDICT enables a wide range of image edits--from local and global semantic edits to image stylization--while maintaining fidelity to the original image structure. EDICT requires no model training/finetuning, prompt tuning, or extra data and can be combined with any pretrained DDM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1801.Image Super-Resolution Using T-Tetromino Pixels</span><br>
                <span class="as">Grosche, SimonandRegensky, AndyandSeiler, J\&quot;urgenandKaup, Andr\&#x27;e</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Grosche_Image_Super-Resolution_Using_T-Tetromino_Pixels_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9989-9998.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高现代高分辨率成像传感器在低光照条件下和需要高帧率时的性能？<br>
                    动机：为了恢复原始的空间分辨率，可以在低光照条件和需要高帧率的情况下进行像素合并。为了在放大后获得更高的图像质量，我们提出了一种新的像素合并概念，即使用T型形状的像素。<br>
                    方法：我们将这种新的像素合并概念嵌入到压缩传感领域，并计算其一致性以激发使用的传感器布局。然后，我们首次在文献中研究了使用T型像素进行重建的质量。<br>
                    效果：我们使用局部全连接重建（LFCR）网络以及压缩传感领域的两种经典重建方法进行重建。与使用非常深的超分辨率（VDSR）网络进行的传统单图像超分辨率相比，使用提出的T型像素布局和LFCR网络可以实现更好的图像质量，例如在PSNR、SSIM方面，并且视觉上也有优势。对于PSNR，可以获得高达+1.92 dB的增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For modern high-resolution imaging sensors, pixel binning is performed in low-lighting conditions and in case high frame rates are required. To recover the original spatial resolution, single-image super-resolution techniques can be applied for upscaling. To achieve a higher image quality after upscaling, we propose a novel binning concept using tetromino-shaped pixels. It is embedded into the field of compressed sensing and the coherence is calculated to motivate the sensor layouts used. Next, we investigate the reconstruction quality using tetromino pixels for the first time in literature. Instead of using different types of tetrominoes as proposed elsewhere, we show that using a small repeating cell consisting of only four T-tetrominoes is sufficient. For reconstruction, we use a locally fully connected reconstruction (LFCR) network as well as two classical reconstruction methods from the field of compressed sensing. Using the LFCR network in combination with the proposed tetromino layout, we achieve superior image quality in terms of PSNR, SSIM, and visually compared to conventional single-image super-resolution using the very deep super-resolution (VDSR) network. For PSNR, a gain of up to +1.92 dB is achieved.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1802.VIVE3D: Viewpoint-Independent Video Editing Using 3D-Aware GANs</span><br>
                <span class="as">Fr\&quot;uhst\&quot;uck, AnnaandSarafianos, NikolaosandXu, YuanluandWonka, PeterandTung, Tony</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fruhstuck_VIVE3D_Viewpoint-Independent_Video_Editing_Using_3D-Aware_GANs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4446-4455.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将基于图像的3D GANs的能力扩展到视频编辑，并以身份保护和时间一致的方式表示输入视频。<br>
                    动机：目前的3D GANs在视频编辑方面的能力有限，需要一种新方法来扩展其功能。<br>
                    方法：提出了一种新的GAN逆技术，通过联合嵌入多帧并优化相机参数，专门针对3D GANs进行优化。同时，除了传统的语义面部编辑（如年龄和表情），还首次展示了由3D GANs的内在属性和光流引导的合成技术实现的新头部视图编辑。<br>
                    效果：实验证明，VIVE3D可以从各种相机视角生成高保真度的面部编辑，并以时间和空间一致的方式与原始视频合成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new building blocks. First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent properties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video. Our experiments demonstrate that VIVE3D generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially-consistent manner.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1803.StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields</span><br>
                <span class="as">Liu, KunhaoandZhan, FangnengandChen, YiwenandZhang, JiahuiandYu, YingchenandElSaddik, AbdulmotalebandLu, ShijianandXing, EricP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8338-8348.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D风格转换技术在准确重建几何形状、高质量风格化和对任意新风格的泛化性上存在困境。<br>
                    动机：提出一种创新的3D风格转换技术，通过在辐射场的特征空间中进行风格转换，解决这个三难困境。<br>
                    方法：采用显式的高级特征网格来表示3D场景，通过体积渲染可以可靠地恢复高精度几何形状。同时，根据参考样式转换网格特征，直接实现高质量的零样本风格转换。<br>
                    效果：实验表明，StyleRF在精确重建几何形状的同时实现了高质量的3D风格化，并能以零样本的方式推广到各种新风格。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1804.DPE: Disentanglement of Pose and Expression for General Video Portrait Editing</span><br>
                <span class="as">Pang, YouxinandZhang, YongandQuan, WeizeandFan, YanboandCun, XiaodongandShan, YingandYan, Dong-Ming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_DPE_Disentanglement_of_Pose_and_Expression_for_General_Video_Portrait_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/427-436.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决面部运动中头部姿态和面部表情的耦合问题，特别是在视频肖像编辑中需要修改表情而保持姿态不变的场景。<br>
                    动机：目前的面部运动转移方法由于头部姿态和面部表情的耦合，无法直接应用于视频肖像编辑。此外，缺乏配对数据（如相同姿态但不同表情）也增加了挑战。<br>
                    方法：本文提出了一种新颖的无监督解耦框架，无需3DMMs和配对数据即可分离姿态和表情。该框架包括一个运动编辑模块、一个姿态生成器和一个表情生成器。编辑模块将面部投影到潜在空间中，在潜在空间中可以解耦姿态运动和表情运动，通过添加操作可以在潜在空间中方便地进行姿态或表情转移。两个生成器分别将修改后的潜在代码渲染为图像。<br>
                    效果：实验结果表明，该方法可以独立控制姿态或表情，并可用于一般的视频编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>One-shot video-driven talking face generation aims at producing a synthetic talking video by transferring the facial motion from a video to an arbitrary portrait image. Head pose and facial expression are always entangled in facial motion and transferred simultaneously. However, the entanglement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged. One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blendshapes, which has side effects on motion transfer. In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints. Evaluations demonstrate our method can control pose or expression independently and be used for general video editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1805.Implicit Diffusion Models for Continuous Super-Resolution</span><br>
                <span class="as">Gao, SichengandLiu, XuhuiandZeng, BohanandXu, ShengandLi, YanjingandLuo, XiaoyanandLiu, JianzhuangandZhen, XiantongandZhang, Baochang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10021-10030.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决当前图像超分辨率方法普遍存在的过度平滑和伪影问题，以及仅能处理固定放大倍数的问题。<br>
                    动机：由于其广泛的应用，图像超分辨率（SR）引起了越来越多的关注。然而，现有的SR方法通常存在过度平滑和伪影的问题，且大多数只能处理固定倍数的放大。<br>
                    方法：本文提出了一种隐式扩散模型（IDM）用于高保真连续图像超分辨率。IDM在一个统一的端到端框架中集成了一个隐式的神经表示和一个去噪扩散模型，其中在解码过程中采用隐式的神经表示来学习连续分辨率表示。此外，我们还设计了一个可控制尺度的调节机制，该机制由一个低分辨率（LR）条件网络和一个缩放因子组成。缩放因子调整分辨率，并相应地调节LR信息和生成的特征在最终输出中的比例，使模型能够满足连续分辨率的需求。<br>
                    效果：大量的实验验证了我们IDM的有效性，并证明其在现有技术中具有优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image super-resolution (SR) has attracted increasing attention due to its wide applications. However, current SR methods generally suffer from over-smoothing and artifacts, and most work only with fixed magnifications. This paper introduces an Implicit Diffusion Model (IDM) for high-fidelity continuous image super-resolution. IDM integrates an implicit neural representation and a denoising diffusion model in a unified end-to-end framework, where the implicit neural representation is adopted in the decoding process to learn continuous-resolution representation. Furthermore, we design a scale-controllable conditioning mechanism that consists of a low-resolution (LR) conditioning network and a scaling factor. The scaling factor regulates the resolution and accordingly modulates the proportion of the LR information and generated features in the final output, which enables the model to accommodate the continuous-resolution requirement. Extensive experiments validate the effectiveness of our IDM and demonstrate its superior performance over prior arts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1806.VGFlow: Visibility Guided Flow Network for Human Reposing</span><br>
                <span class="as">Jain, RishabhandSingh, KrishnaKumarandHemani, MayurandLu, JingwanandSarkar, MausoomandCeylan, DuyguandKrishnamurthy, Balaji</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VGFlow_Visibility_Guided_Flow_Network_for_Human_Reposing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21088-21097.png><br>
            
            <span class="tt"><span class="t0">研究问题：生成具有任意可想象姿势的人体模型的真实图像存在多个困难，包括保留纹理、保持图案连贯性、尊重衣物边界、处理遮挡和操作皮肤生成等。<br>
                    动机：现有的方法在保持纹理、维护图案连贯性、尊重衣物边界、处理遮挡和操作皮肤生成等方面存在限制，而且人体姿势的可能性空间大且多变，衣物的性质高度非刚性，人群的身体形状差异大。<br>
                    方法：我们提出了VGFlow模型，该模型使用可见性引导流模块将流分离为目标的可见部分和不可见部分，以实现纹理保留和风格操纵的同时进行。此外，为了解决不同的身体形状并避免网络伪影，我们还引入了一种自我监督的逐片“真实感”损失来进一步提高输出质量。<br>
                    效果：VGFlow模型在SSIM、LPIPS、FID等不同的图像质量指标上取得了最先进的结果，无论是定性还是定量观察。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of human reposing involves generating a realistic image of a model standing in an arbitrary conceivable pose. There are multiple difficulties in generating perceptually accurate images and existing methods suffers from limitations in preserving texture, maintaining pattern coherence, respecting cloth boundaries, handling occlusions, manipulating skin generation etc. These difficulties are further exacerbated by the fact that the possible space of pose orientation for humans is large and variable, the nature of clothing items are highly non-rigid and the diversity in body shape differ largely among the population. To alleviate these difficulties and synthesize perceptually accurate images, we propose VGFlow, a model which uses a visibility guided flow module to disentangle the flow into visible and invisible parts of the target for simultaneous texture preservation and style manipulation. Furthermore, to tackle distinct body shapes and avoid network artifacts, we also incorporate an a self-supervised patch-wise "realness" loss to further improve the output. VGFlow achieves state-of-the-art results as observed qualitatively and quantitatively on different image quality metrics(SSIM, LPIPS, FID).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1807.CoralStyleCLIP: Co-Optimized Region and Layer Selection for Image Editing</span><br>
                <span class="as">Revanur, AmbareeshandBasu, DebrajandAgrawal, ShradhaandAgarwal, DhwanitandPai, Deepak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Revanur_CoralStyleCLIP_Co-Optimized_Region_and_Layer_Selection_for_Image_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12695-12704.png><br>
            
            <span class="tt"><span class="t0">研究问题：在开放世界的可控生成图像编辑中，编辑保真度是一个重要问题。<br>
                    动机：最近，基于CLIP的方法通过在StyleGAN的手动选择层引入空间注意力来缓解这些问题，但牺牲了简单性。<br>
                    方法：本文提出了CoralStyleCLIP，该方法在StyleGAN2的特征空间中引入了多层注意力引导的混合策略，以获得高保真的编辑效果。<br>
                    效果：我们的方法在保持使用简便的同时，实现了高质量的编辑效果。实验结果表明，CoralStyleCLIP在各种架构复杂性下都能保持较低的时间复杂度，同时保持了高质量的编辑效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Edit fidelity is a significant issue in open-world controllable generative image editing. Recently, CLIP-based approaches have traded off simplicity to alleviate these problems by introducing spatial attention in a handpicked layer of a StyleGAN. In this paper, we propose CoralStyleCLIP, which incorporates a multi-layer attention-guided blending strategy in the feature space of StyleGAN2 for obtaining high-fidelity edits. We propose multiple forms of our co-optimized region and layer selection strategy to demonstrate the variation of time complexity with the quality of edits over different architectural intricacies while preserving simplicity. We conduct extensive experimental analysis and benchmark our method against state-of-the-art CLIP-based methods. Our findings suggest that CoralStyleCLIP results in high-quality edits while preserving the ease of use.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1808.GLeaD: Improving GANs With a Generator-Leading Task</span><br>
                <span class="as">Bai, QingyanandYang, CeyuanandXu, YinghaoandLiu, XihuiandYang, YujiuandShen, Yujun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_GLeaD_Improving_GANs_With_a_Generator-Leading_Task_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12094-12104.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使生成对抗网络（GAN）的训练过程更公平，以提高模型性能。<br>
                    动机：目前的GAN训练过程中，判别器（D）往往占据主导地位，导致训练结果可能不理想。<br>
                    方法：提出一种新的对抗训练范式，让生成器（G）为判别器（D）分配任务，即从图像中提取出代表性特征，并被G解码以重建输入图像。<br>
                    效果：在多个数据集上的实验结果表明，这种方法比基线方法有显著优势，例如，将StyleGAN2在LSUN Bedroom和LSUN Church上的FID得分分别从4.30降低到2.55和从4.04降低到2.82。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative adversarial network (GAN) is formulated as a two-player game between a generator (G) and a discriminator (D), where D is asked to differentiate whether an image comes from real data or is produced by G. Under such a formulation, D plays as the rule maker and hence tends to dominate the competition. Towards a fairer game in GANs, we propose a new paradigm for adversarial training, which makes G assign a task to D as well. Specifically, given an image, we expect D to extract representative features that can be adequately decoded by G to reconstruct the input. That way, instead of learning freely, D is urged to align with the view of G for domain classification. Experimental results on various datasets demonstrate the substantial superiority of our approach over the baselines. For instance, we improve the FID of StyleGAN2 from 4.30 to 2.55 on LSUN Bedroom and from 4.04 to 2.82 on LSUN Church. We believe that the pioneering attempt present in this work could inspire the community with better designed generator-leading tasks for GAN improvement. Project page is at https://ezioby.github.io/glead/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1809.GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</span><br>
                <span class="as">Tao, MingandBao, Bing-KunandTang, HaoandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14214-14223.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从文本中合成高保真复杂图像？<br>
                    动机：现有的大型预训练自动回归和扩散模型虽然在图像合成方面取得了显著进展，但仍存在需要大量训练数据和参数、多步骤生成设计导致速度慢以及难以控制合成视觉特征的问题。<br>
                    方法：提出一种名为GALIP的生成对抗性CLIP模型，利用强大的预训练CLIP模型作为判别器和生成器。具体包括一个基于CLIP的判别器和一个由CLIP通过桥接特征和提示引发视觉概念的生成器。<br>
                    效果：GALIP仅需要约3%的训练数据和6%的可学习参数，就能达到与大型预训练自动回归和扩散模型相当的结果。此外，该模型的合成速度提高了120倍，并继承了GAN的平滑潜在空间。实验结果证明了GALIP的优秀性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesizing high-fidelity complex images from text is challenging. Based on large pretraining, the autoregressive and diffusion models can synthesize photo-realistic images. Although these large models have shown notable progress, there remain three flaws. 1) These models require tremendous training data and parameters to achieve good performance. 2) The multi-step generation design slows the image synthesis process heavily. 3) The synthesized visual features are challenging to control and require delicately designed prompts. To enable high-quality, efficient, fast, and controllable text-to-image synthesis, we propose Generative Adversarial CLIPs, namely GALIP. GALIP leverages the powerful pretrained CLIP model both in the discriminator and generator. Specifically, we propose a CLIP-based discriminator. The complex scene understanding ability of CLIP enables the discriminator to accurately assess the image quality. Furthermore, we propose a CLIP-empowered generator that induces the visual concepts from CLIP through bridge features and prompts. The CLIP-integrated generator and discriminator boost training efficiency, and as a result, our model only requires about 3% training data and 6% learnable parameters, achieving comparable results to large pretrained autoregressive and diffusion models. Moreover, our model achieves 120 times faster synthesis speed and inherits the smooth latent space from GAN. The extensive experimental results demonstrate the excellent performance of our GALIP. Code is available at https://github.com/tobran/GALIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1810.3DAvatarGAN: Bridging Domains for Personalized Editable Avatars</span><br>
                <span class="as">Abdal, RameenandLee, Hsin-YingandZhu, PeihaoandChai, MengleiandSiarohin, AliaksandrandWonka, PeterandTulyakov, Sergey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4552-4562.png><br>
            
            <span class="tt"><span class="t0">研究问题：我们能否在艺术数据上训练一个3D-GAN，同时保持多视图一致性和纹理质量？<br>
                    动机：现有的3D-GAN模型主要在具有一致结构的大型数据集上进行训练，而在风格化、艺术性的数据上进行训练，尤其是那些几何形状未知且高度变化的艺术性数据，以及相机信息，尚未被证明是可能的。<br>
                    方法：我们提出了一个适应框架，其中源领域是一个预训练的3D-GAN，而目标领域是一个在艺术数据集上训练的2D-GAN。然后，我们将2D生成器的知识提炼到源3D生成器中。为此，我们首先提出了一种优化方法来对齐不同领域的相机参数分布。其次，我们提出了必要的正则化方法来学习高质量的纹理，同时避免退化的几何解决方案，如平面形状。第三，我们展示了一种基于变形的技术，用于模拟艺术领域的夸张几何形状，从而实现个性化的几何编辑。最后，我们提出了一种新的3D-GAN逆映射方法，将源领域和目标领域的隐空间联系起来。<br>
                    效果：我们的贡献首次实现了在艺术数据集上生成、编辑和动画个性化艺术3D头像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling---as a byproduct---personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions---for the first time---allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1811.Person Image Synthesis via Denoising Diffusion Model</span><br>
                <span class="as">Bhunia, AnkanKumarandKhan, SalmanandCholakkal, HishamandAnwer, RaoMuhammadandLaaksonen, JormaandShah, MubarakandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhunia_Person_Image_Synthesis_via_Denoising_Diffusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5968-5976.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成任意姿态的人像图像？<br>
                    动机：现有的方法在处理复杂变形和严重遮挡时，无法保持真实的纹理或需要密集的对应关系。<br>
                    方法：利用去噪扩散模型进行高保真度的人像图像合成，通过一系列简单的前向-后向去噪步骤分解复杂的转换问题。<br>
                    效果：在两个大规模基准测试和一个用户研究中，该方法在具有挑战性的场景下表现出了高度的真实性，并可以用于下游任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The pose-guided person image generation task requires synthesizing photorealistic images of humans in arbitrary poses. The existing approaches use generative adversarial networks that do not necessarily maintain realistic textures or need dense correspondences that struggle to handle complex deformations and severe occlusions. In this work, we show how denoising diffusion models can be applied for high-fidelity person image synthesis with strong sample diversity and enhanced mode coverage of the learnt data distribution. Our proposed Person Image Diffusion Model (PIDM) disintegrates the complex transfer problem into a series of simpler forward-backward denoising steps. This helps in learning plausible source-to-target transformation trajectories that result in faithful textures and undistorted appearance details. We introduce a 'texture diffusion module' based on cross-attention to accurately model the correspondences between appearance and pose information available in source and target images. Further, we propose 'disentangled classifier-free guidance' to ensure close resemblance between the conditional inputs and the synthesized output in terms of both pose and appearance information. Our extensive results on two large-scale benchmarks and a user study demonstrate the photorealism of our proposed approach under challenging scenarios. We also show how our generated images can help in downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1812.Implicit Neural Head Synthesis via Controllable Local Deformation Fields</span><br>
                <span class="as">Chen, ChuhanandO{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Implicit_Neural_Head_Synthesis_via_Controllable_Local_Deformation_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/416-426.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从2D视频中高质量重建可控的3D头部化身，以实现电影、游戏和远程会议等虚拟人应用。<br>
                    动机：现有的方法无法精细地控制面部部分，或者从单目视频推断非对称表情，而且大多数方法只依赖于3DMM参数，缺乏局部性。<br>
                    方法：我们基于部分基于隐式形状模型，将全局形变场分解为局部形变场。我们的新方法通过基于3DMM的参数和代表性面部地标，用局部语义装配式控制来建模多个隐式形变场。此外，我们还提出了局部控制损失和注意力掩码机制，以促进每个学习到的形变场的稀疏性。<br>
                    效果：我们的方法比之前的隐式单目方法渲染出更锐利的局部可控非线性形变，特别是在口腔内部、非对称表情和面部细节方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-quality reconstruction of controllable 3D head avatars from 2D videos is highly desirable for virtual human applications in movies, games, and telepresence. Neural implicit fields provide a powerful representation to model 3D head avatars with personalized shape, expressions, and facial parts, e.g., hair and mouth interior, that go beyond the linear 3D morphable model (3DMM). However, existing methods do not model faces with fine-scale facial features, or local control of facial parts that extrapolate asymmetric expressions from monocular videos. Further, most condition only on 3DMM parameters with poor(er) locality, and resolve local features with a global neural field. We build on part-based implicit shape models that decompose a global deformation field into local ones. Our novel formulation models multiple implicit deformation fields with local semantic rig-like control via 3DMM-based parameters, and representative facial landmarks. Further, we propose a local control loss and attention mask mechanism that promote sparsity of each learned deformation field. Our formulation renders sharper locally controllable nonlinear deformations than previous implicit monocular approaches, especially mouth interior, asymmetric expressions, and facial details. Project page:https://imaging.cs.cmu.edu/local_deformation_fields/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1813.GANHead: Towards Generative Animatable Neural Head Avatars</span><br>
                <span class="as">Wu, SijingandYan, YichaoandLi, YunhaoandCheng, YuhaoandZhu, WenhanandGao, KeandLi, XiaoboandZhai, Guangtao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_GANHead_Towards_Generative_Animatable_Neural_Head_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/437-447.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成完整、真实且可动画的头部虚拟形象。<br>
                    动机：现有的方法难以同时满足生成完整、真实和可动画的头部虚拟形象的要求。<br>
                    方法：提出GANHead，一种利用显式表达参数的精细控制和隐式表示的真实渲染结果的新型生成头部模型。具体来说，GANHead通过三个网络在标准空间中表示粗糙的几何形状、细致的细节和纹理，以获得生成完整和真实的头部虚拟形象的能力。为了实现灵活的动画，我们定义了由标准的线性混合蒙皮（LBS）确定的变形字段，结合学习到的连续姿态和表情基础以及LBS权重。这使得虚拟形象可以直接通过FLAME参数进行动画化，并能很好地推广到未见过的姿态和表情。<br>
                    效果：与最先进的方法相比，GANHead在头部虚拟形象生成和原始扫描拟合方面表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To bring digital avatars into people's lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel generative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of implicit representations. Specifically, GANHead represents coarse geometry, fine-gained details and texture via three networks in canonical space to obtain the ability to generate complete and realistic head avatars. To achieve flexible animation, we define the deformation filed by standard linear blend skinning (LBS), with the learned continuous pose and expression bases and LBS weights. This allows the avatars to be directly animated by FLAME parameters and generalize well to unseen poses and expressions. Compared to state-of-the-art (SOTA) methods, GANHead achieves superior performance on head avatar generation and raw scan fitting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1814.NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models</span><br>
                <span class="as">Kim, SeungWookandBrown, BradleyandYin, KangxueandKreis, KarstenandSchwarz, KatjaandLi, DaiqingandRombach, RobinandTorralba, AntonioandFidler, Sanja</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8496-8506.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何自动生成高质量的真实世界3D场景，以应用于虚拟现实和机器人仿真等领域。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as virtual reality and robotics simulation. Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. We leverage Latent Diffusion Models that have been successfully utilized for efficient high-quality 2D content creation. We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of latent representations. A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline. We achieve a substantial improvement over existing state-of-the-art scene generation models. Additionally, we show how NeuralField-LDM can be used for a variety of 3D content creation applications, including conditional scene generation, scene inpainting and scene style manipulation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1815.NUWA-LIP: Language-Guided Image Inpainting With Defect-Free VQGAN</span><br>
                <span class="as">Ni, MinhengandLi, XiaomingandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_NUWA-LIP_Language-Guided_Image_Inpainting_With_Defect-Free_VQGAN_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14183-14192.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用文本指导进行图像修复，同时保持非缺陷区域不变。<br>
                    动机：直接编码缺陷图像会对非缺陷区域产生负面影响，导致非缺陷部分的变形结构。<br>
                    方法：提出NUWA-LIP模型，包括无损VQGAN（DF-VQGAN）和多视角序列到序列模块（MP-S2S）。DF-VQGAN引入相对估计以谨慎控制感受野扩散，并使用对称连接保护结构细节不变。MP-S2S通过聚合来自低级别像素、高级别令牌和文本描述的互补视角，将文本指导和谐地嵌入局部缺陷区域。<br>
                    效果：实验表明，我们的DF-VQGAN有效地辅助了修复过程，同时避免了非缺陷区域的意外变化。在三个开放领域的基准测试中，我们的方法优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Language-guided image inpainting aims to fill the defective regions of an image under the guidance of text while keeping the non-defective regions unchanged. However, directly encoding the defective images is prone to have an adverse effect on the non-defective regions, giving rise to distorted structures on non-defective parts. To better adapt the text guidance to the inpainting task, this paper proposes NUWA-LIP, which involves defect-free VQGAN (DF-VQGAN) and a multi-perspective sequence-to-sequence module (MP-S2S). To be specific, DF-VQGAN introduces relative estimation to carefully control the receptive spreading, as well as symmetrical connections to protect structure details unchanged. For harmoniously embedding text guidance into the locally defective regions, MP-S2S is employed by aggregating the complementary perspectives from low-level pixels, high-level tokens as well as the text description. Experiments show that our DF-VQGAN effectively aids the inpainting process while avoiding unexpected changes in non-defective regions. Results on three open-domain benchmarks demonstrate the superior performance of our method against state-of-the-arts. Our code, datasets, and model will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1816.MARLIN: Masked Autoencoder for Facial Video Representation LearnINg</span><br>
                <span class="as">Cai, ZhixiandGhosh, ShreyaandStefanov, KalinandDhall, AbhinavandCai, JianfeiandRezatofighi, HamidandHaffari, RezaandHayat, Munawar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1493-1504.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种自我监督的方法，从视频中学习通用的面部表示，可以跨各种面部分析任务进行转移。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：提出了一个名为MARLIN的面部视频掩码自动编码器框架，通过大量非注释的网络爬取面部视频学习高度稳健和通用的面部嵌入。<br>
                    效果：实验结果表明，MARLIN在各种下游任务上表现出色，包括面部属性识别（FAR）、面部表情识别（FER）、深度伪造检测（DFD）和唇同步（LS），并在低数据量的情况下也有良好的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a self-supervised approach to learn universal facial representations from videos, that can transfer across a variety of facial analysis tasks such as Facial Attribute Recognition (FAR), Facial Expression Recognition (FER), DeepFake Detection (DFD), and Lip Synchronization (LS). Our proposed framework, named MARLIN, is a facial video masked autoencoder, that learns highly robust and generic facial embeddings from abundantly available non-annotated web crawled facial videos. As a challenging auxiliary task, MARLIN reconstructs the spatio-temporal details of the face from the densely masked facial regions which mainly include eyes, nose, mouth, lips, and skin to capture local and global aspects that in turn help in encoding generic and transferable features. Through a variety of experiments on diverse downstream tasks, we demonstrate MARLIN to be an excellent facial video encoder as well as feature extractor, that performs consistently well across a variety of downstream tasks including FAR (1.13% gain over supervised benchmark), FER (2.64% gain over unsupervised benchmark), DFD (1.86% gain over unsupervised benchmark), LS (29.36% gain for Frechet Inception Distance), and even in low data regime. Our code and models are available at https://github.com/ControlNet/MARLIN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1817.3D-Aware Face Swapping</span><br>
                <span class="as">Li, YixuanandMa, ChaoandYan, YichaoandZhu, WenhanandYang, Xiaokang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_3D-Aware_Face_Swapping_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12705-12714.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有人脸交换方法在面对大姿态变化时，会在交换后的人脸上产生不期望的人工痕迹的问题。<br>
                    动机：由于现有的人脸交换方法直接学习交换二维人脸图像，没有考虑到人脸的几何信息，因此在源和目标脸部之间存在大的姿态变化时，交换后的人脸上总会存在不期望的人工痕迹。<br>
                    方法：本文提出了一种新的3D感知人脸交换方法，该方法从单视图的源和目标图像生成高保真度和多视图一致的交换人脸。为了实现这一目标，我们利用了3D人脸的强大几何和纹理先验，将2D人脸投影到3D生成模型的潜在空间中。通过在潜在空间中解耦身份和属性特征，我们成功地以3D感知的方式交换了人脸，对姿态变化具有鲁棒性，同时传递了精细的面部细节。<br>
                    效果：大量的实验表明，我们的3D感知人脸交换框架在视觉质量、身份相似性和多视图一致性方面都具有优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face swapping is an important research topic in computer vision with wide applications in entertainment and privacy protection. Existing methods directly learn to swap 2D facial images, taking no account of the geometric information of human faces. In the presence of large pose variance between the source and the target faces, there always exist undesirable artifacts on the swapped face. In this paper, we present a novel 3D-aware face swapping method that generates high-fidelity and multi-view-consistent swapped faces from single-view source and target images. To achieve this, we take advantage of the strong geometry and texture prior of 3D human faces, where the 2D faces are projected into the latent space of a 3D generative model. By disentangling the identity and attribute features in the latent space, we succeed in swapping faces in a 3D-aware manner, being robust to pose variations while transferring fine-grained facial details. Extensive experiments demonstrate the superiority of our 3D-aware face swapping framework in terms of visual quality, identity similarity, and multi-view consistency. Code is available at https://lyx0208.github.io/3dSwap.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1818.RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion</span><br>
                <span class="as">Wang, TengfeiandZhang, BoandZhang, TingandGu, ShuyangandBao, JianminandBaltrusaitis, TadasandShen, JingjingandChen, DongandWen, FangandChen, QifengandGuo, Baining</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RODIN_A_Generative_Model_for_Sculpting_3D_Digital_Avatars_Using_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4563-4573.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成高质量的3D数字头像。<br>
                    动机：现有的3D扩散模型由于内存和处理成本的限制，难以生成具有丰富细节的高质量结果。<br>
                    方法：提出一种卷出扩散网络（RODIN）模型，将3D NeRF模型表示为多个2D特征图，并将其展开到一个单一的2D特征平面上进行3D感知扩散。<br>
                    效果：RODIN模型在保持3D扩散完整性的同时，通过使用3D感知卷积来关注2D平面中投影特征的原始3D关系，大大提高了计算效率。此外，我们还利用潜在条件来协调特征生成，使生成的头像具有高度的真实性，并能够根据文本提示进行语义编辑。最后，我们使用分层合成来进一步增强细节。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a 3D diffusion model that automatically generates 3D digital avatars represented as neural radiance fields (NeRFs). A significant challenge for 3D diffusion is that the memory and processing costs are prohibitive for producing high-quality results with rich details. To tackle this problem, we propose the roll-out diffusion network (RODIN), which takes a 3D NeRF model represented as multiple 2D feature maps and rolls out them onto a single 2D feature plane within which we perform 3D-aware diffusion. The RODIN model brings much-needed computational efficiency while preserving the integrity of 3D diffusion by using 3D-aware convolution that attends to projected features in the 2D plane according to their original relationships in 3D. We also use latent conditioning to orchestrate the feature generation with global coherence, leading to high-fidelity avatars and enabling semantic editing based on text prompts. Finally, we use hierarchical synthesis to further enhance details. The 3D avatars generated by our model compare favorably with those produced by existing techniques. We can generate highly detailed avatars with realistic hairstyles and facial hair. We also demonstrate 3D avatar generation from image or text, as well as text-guided editability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1819.High-Fidelity Guided Image Synthesis With Latent Diffusion Models</span><br>
                <span class="as">Singh, JaskiratandGould, StephenandZheng, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_High-Fidelity_Guided_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5997-6006.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于用户涂鸦的可控图像合成方法存在内在领域偏移问题，生成的输出结果常常缺乏细节，类似于目标领域的简化表示。<br>
                    动机：为了解决这个问题，本文提出了一种新的引导式图像合成框架，通过将输出图像建模为约束优化问题的解来解决问题。<br>
                    方法：虽然计算优化问题的精确解是不可行的，但只需要反向扩散过程的一次传递就可以实现相同解的近似。此外，通过定义输入文本令牌和用户描边绘画之间的交叉注意力对应关系，用户可以在不需要任何条件训练或微调的情况下控制不同绘画区域的语义。<br>
                    效果：人类用户研究结果表明，该方法在整体用户满意度得分上超过先前最先进的方法85.32%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Controllable image synthesis with user scribbles has gained huge public interest with the recent advent of text-conditioned latent diffusion models. The user scribbles control the color composition while the text prompt provides control over the overall image semantics. However, we find that prior works suffer from an intrinsic domain shift problem wherein the generated outputs often lack details and resemble simplistic representations of the target domain. In this paper, we propose a novel guided image synthesis framework, which addresses this problem by modeling the output image as the solution of a constrained optimization problem. We show that while computing an exact solution to the optimization is infeasible, an approximation of the same can be achieved while just requiring a single pass of the reverse diffusion process. Additionally, we show that by simply defining a cross-attention based correspondence between the input text tokens and the user stroke-painting, the user is also able to control the semantics of different painted regions without requiring any conditional training or finetuning. Human user study results show that the proposed approach outperforms the previous state-of-the-art by over 85.32% on the overall user satisfaction scores. Project page for our paper is available at https://1jsingh.github.io/gradop.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1820.CodeTalker: Speech-Driven 3D Facial Animation With Discrete Motion Prior</span><br>
                <span class="as">Xing, JinboandXia, MenghanandZhang, YuechenandCun, XiaodongandWang, JueandWong, Tien-Tsin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12780-12790.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过语音驱动的三维面部动画实现更真实、生动的效果。<br>
                    动机：由于音频-视觉数据的稀缺性和高度病态性质，现有的工作在实现真实和生动的面部动画方面仍存在差距。<br>
                    方法：将语音驱动的面部动画转化为学习代码簿有限代理空间中的代码查询任务，通过减少跨模态映射的不确定性来有效提升生成动画的生动性。<br>
                    效果：实验结果表明，该方法在定性和定量上都优于当前最先进的方法，用户研究进一步证明了其在感知质量上的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Speech-driven 3D facial animation has been widely studied, yet there is still a gap to achieving realism and vividness due to the highly ill-posed nature and scarcity of audio-visual data. Existing works typically formulate the cross-modal mapping into a regression task, which suffers from the regression-to-mean problem leading to over-smoothed facial motions. In this paper, we propose to cast speech-driven facial animation as a code query task in a finite proxy space of the learned codebook, which effectively promotes the vividness of the generated motions by reducing the cross-modal mapping uncertainty. The codebook is learned by self-reconstruction over real facial motions and thus embedded with realistic facial motion priors. Over the discrete motion space, a temporal autoregressive model is employed to sequentially synthesize facial motions from the input speech signal, which guarantees lip-sync as well as plausible facial expressions. We demonstrate that our approach outperforms current state-of-the-art methods both qualitatively and quantitatively. Also, a user study further justifies our superiority in perceptual quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1821.Semi-Supervised Parametric Real-World Image Harmonization</span><br>
                <span class="as">Wang, KeandGharbi, Micha\&quot;elandZhang, HeandXia, ZhihaoandShechtman, Eli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Semi-Supervised_Parametric_Real-World_Image_Harmonization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5927-5936.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于学习的图片和谐化技术通常只能处理单一的全局变换，无法很好地处理真实图像中的复杂局部变化。<br>
                    动机：为了解决现有模型在真实图像中无法很好地进行复杂局部变化的处理问题，我们提出了一种新的半监督训练策略。<br>
                    方法：我们的模型是完全参数化的，使用RGB曲线来修正全局颜色和色调，并使用阴影图来模拟局部变化。<br>
                    效果：实验结果表明，我们的方法在已建立的基准测试和真实图像上都优于先前的工作，并且在用户研究中表现出色，能够交互式地处理高分辨率图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based image harmonization techniques are usually trained to undo synthetic global transformations, applied to a masked foreground in a single ground truth photo. This simulated data does not model many important appearance mismatches (illumination, object boundaries, etc.) between foreground and background in real composites, leading to models that do not generalize well and cannot model complex local changes. We propose a new semi-supervised training strategy that addresses this problem and lets us learn complex local appearance harmonization from unpaired real composites, where foreground and background come from different images. Our model is fully parametric. It uses RGB curves to correct the global colors and tone and a shading map to model local variations. Our approach outperforms previous work on established benchmarks and real composites, as shown in a user study, and processes high-resolution images interactively. The code and project page is available at https://kewang0622.github.io/sprih/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1822.VecFontSDF: Learning To Reconstruct and Synthesize High-Quality Vector Fonts via Signed Distance Functions</span><br>
                <span class="as">Xia, ZeqingandXiong, BojunandLian, Zhouhui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_VecFontSDF_Learning_To_Reconstruct_and_Synthesize_High-Quality_Vector_Fonts_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1848-1857.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种自动合成矢量字体的算法，以显著简化字体设计过程。<br>
                    动机：现有的方法主要关注光栅图像生成，只有少数方法可以直接合成矢量字体。<br>
                    方法：本文提出了一种可端到端训练的方法VecFontSDF，使用有符号距离函数（SDFs）重建和合成高质量的矢量字体。具体来说，基于提出的基于SDF的隐式形状表示，VecFontSDF学习将每个字形建模为由几个抛物线围成的几何形状原语，这些抛物线可以精确转换为广泛使用的二次贝塞尔曲线，这是矢量字体产品中常用的。<br>
                    效果：在公开可用的数据集上进行的定性和定量实验表明，我们的方法在几个任务上获得了高质量的结果，包括矢量字体重建、插值和少样本矢量字体合成，明显优于最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Font design is of vital importance in the digital content design and modern printing industry. Developing algorithms capable of automatically synthesizing vector fonts can significantly facilitate the font design process. However, existing methods mainly concentrate on raster image generation, and only a few approaches can directly synthesize vector fonts. This paper proposes an end-to-end trainable method, VecFontSDF, to reconstruct and synthesize high-quality vector fonts using signed distance functions (SDFs). Specifically, based on the proposed SDF-based implicit shape representation, VecFontSDF learns to model each glyph as shape primitives enclosed by several parabolic curves, which can be precisely converted to quadratic Bezier curves that are widely used in vector font products. In this manner, most image generation methods can be easily extended to synthesize vector fonts. Qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality results on several tasks, including vector font reconstruction, interpolation, and few-shot vector font synthesis, markedly outperforming the state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1823.Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation</span><br>
                <span class="as">Huang, MengqiandMao, ZhendongandWang, QuanandZhang, Yongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2002-2011.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自动回归模型在图像重建和生成中存在冗余，限制了模型的结构和效率。<br>
                    动机：为了解决现有模型的问题，提高图像生成的效率和质量。<br>
                    方法：提出了一种新颖的两阶段框架，包括Masked Quantization VAE（MQ-VAE）和Stackformer。MQ-VAE通过引入自适应掩码模块来消除冗余区域特征，然后通过自适应解掩码模块恢复原始网格图像特征图以忠实地重建量化后的原始图像。Stackformer则学习预测下一个代码及其在特征图中的位置。<br>
                    效果：实验证明，该方法在各种图像生成任务上具有高效性和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing autoregressive models follow the two-stage generation paradigm that first learns a codebook in the latent space for image reconstruction and then completes the image generation autoregressively based on the learned codebook. However, existing codebook learning simply models all local region information of images without distinguishing their different perceptual importance, which brings redundancy in the learned codebook that not only limits the next stage's autoregressive model's ability to model important structure but also results in high training cost and slow generation speed. In this study, we borrow the idea of importance perception from classical image coding theory and propose a novel two-stage framework, which consists of Masked Quantization VAE (MQ-VAE) and Stackformer, to relieve the model from modeling redundancy. Specifically, MQ-VAE incorporates an adaptive mask module for masking redundant region features before quantization and an adaptive de-mask module for recovering the original grid image feature map to faithfully reconstruct the original images after quantization. Then, Stackformer learns to predict the combination of the next code and its position in the feature map. Comprehensive experiments on various image generation validate our effectiveness and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1824.Identity-Preserving Talking Face Generation With Landmark and Appearance Priors</span><br>
                <span class="as">Zhong, WeizhiandFang, ChaoweiandCai, YinqiandWei, PengxuandZhao, GangmingandLin, LiangandLi, Guanbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9729-9738.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从音频生成逼真、唇形同步且保持身份特征的说话人视频。<br>
                    动机：现有的针对特定人的方法和通用方法在生成真实和唇形同步的视频时存在困难，或需要目标说话人的录像进行训练或微调。<br>
                    方法：提出了一个两阶段框架，包括音频到地标生成和地标到视频渲染过程。首先，设计了一个基于Transformer的地标生成器，从音频中推断嘴唇和下颌的地标。然后，构建了一个视频渲染模型，将生成的地标转换为面部图像。在此过程中，提取了下半部分遮挡的目标面部和静态参考图像的外观信息，以生成真实和保留身份特征的视觉内容。<br>
                    效果：大量实验证明，该方法比现有的通用说话人脸生成方法能产生更真实、唇形同步且保持身份特征的视频。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating talking face videos from audio attracts lots of research interest. A few person-specific methods can generate vivid videos but require the target speaker's videos for training or fine-tuning. Existing person-generic methods have difficulty in generating realistic and lip-synced videos while preserving identity information. To tackle this problem, we propose a two-stage framework consisting of audio-to-landmark generation and landmark-to-video rendering procedures. First, we devise a novel Transformer-based landmark generator to infer lip and jaw landmarks from the audio. Prior landmark characteristics of the speaker's face are employed to make the generated landmarks coincide with the facial outline of the speaker. Then, a video rendering model is built to translate the generated landmarks into face images. During this stage, prior appearance information is extracted from the lower-half occluded target face and static reference images, which helps generate realistic and identity-preserving visual content. For effectively exploring the prior information of static reference images, we align static reference images with the target face's pose and expression based on motion fields. Moreover, auditory features are reused to guarantee that the generated face images are well synchronized with the audio. Extensive experiments demonstrate that our method can produce more realistic, lip-synced, and identity-preserving videos than existing person-generic talking face generation methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1825.MetaPortrait: Identity-Preserving Talking Head Generation With Fast Personalized Adaptation</span><br>
                <span class="as">Zhang, BowenandQi, ChenyangandZhang, PanandZhang, BoandWu, HsiangTaoandChen, DongandChen, QifengandWang, YongandWen, Fang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22096-22105.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种ID保留的头像生成框架，以改进先前的方法。<br>
                    动机：与稀疏流插值不同，我们认为密集地标对于实现准确的几何感知流场至关重要。同时，受到人脸交换方法的启发，我们在合成过程中自适应地融合源身份，使网络更好地保留图像肖像的关键特征。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we propose an ID-preserving talking head generation framework, which advances previous methods in two aspects. First, as opposed to interpolating from sparse flow, we claim that dense landmarks are crucial to achieving accurate geometry-aware flow fields. Second, inspired by face-swapping methods, we adaptively fuse the source identity during synthesis, so that the network better preserves the key characteristics of the image portrait. Although the proposed model surpasses prior generation fidelity on established benchmarks, personalized fine-tuning is still needed to further make the talking head generation qualified for real usage. However, this process is rather computationally demanding that is unaffordable to standard users. To alleviate this, we propose a fast adaptation model using a meta-learning approach. The learned model can be adapted to a high-quality personalized model as fast as 30 seconds. Last but not least, a spatial-temporal enhancement module is proposed to improve the fine details while ensuring temporal coherency. Extensive experiments prove the significant superiority of our approach over the state of the arts in both one-shot and personalized settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1826.CelebV-Text: A Large-Scale Facial Text-Video Dataset</span><br>
                <span class="as">Yu, JianhuiandZhu, HaoandJiang, LimingandLoy, ChenChangeandCai, WeidongandWu, Wayne</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14805-14814.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决人脸文本驱动的视频生成任务，由于缺乏包含高质量视频和高度相关文本的合适数据集，这一任务仍然具有挑战性。<br>
                    动机：目前，以文本驱动的视频生成模型在视频生成和编辑方面得到了蓬勃发展，但以人脸为中心的文本到视频生成仍然是一个挑战。<br>
                    方法：本文提出了CelebV-Text，这是一个大型、多样化、高质量的面部文本视频对数据集，用于促进面部文本到视频生成任务的研究。CelebV-Text包含70,000个野外拍摄的人脸视频片段，每个片段都有20个通过提出的半自动文本生成策略生成的相关文本。<br>
                    效果：通过对视频、文本和文本-视频相关性进行综合统计分析，证明了CelebV-Text优于其他数据集。通过大量的自我评估，进一步展示了CelebV-Text的有效性和潜力。通过构建代表性方法的基准测试，标准化了面部文本到视频生成任务的评估。所有数据和模型都是公开的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-driven generation models are flourishing in video generation and editing. However, face-centric text-to-video generation remains a challenge due to the lack of a suitable dataset containing high-quality videos and highly relevant texts. This paper presents CelebV-Text, a large-scale, diverse, and high-quality dataset of facial text-video pairs, to facilitate research on facial text-to-video generation tasks. CelebV-Text comprises 70,000 in-the-wild face video clips with diverse visual content, each paired with 20 texts generated using the proposed semi-automatic text generation strategy. The provided texts are of high quality, describing both static and dynamic attributes precisely. The superiority of CelebV-Text over other datasets is demonstrated via comprehensive statistical analysis of the videos, texts, and text-video relevance. The effectiveness and potential of CelebV-Text are further shown through extensive self-evaluation. A benchmark is constructed with representative methods to standardize the evaluation of the facial text-to-video generation task. All data and models are publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1827.Diffusion-SDF: Text-To-Shape via Voxelized Diffusion</span><br>
                <span class="as">Li, MuhengandDuan, YueqiandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12642-12651.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何根据指定的条件（如文本）生成新的3D内容。<br>
                    动机：随着工业界对3D虚拟建模技术的关注日益增加，基于特定条件生成新的3D内容已成为一个热门问题。<br>
                    方法：提出了一种新的生成性3D建模框架Diffusion-SDF，用于具有挑战性的文本到形状合成任务。该框架包括一个SDF自动编码器和体素化的扩散模型，用于学习和生成3D形状的体素化有符号距离场（SDFs）表示。<br>
                    效果：实验结果表明，与以往的方法相比，Diffusion-SDF能生成更高质量、更具多样性的3D形状，且这些形状能很好地符合给定的文本描述。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation. Experimental results show that Diffusion-SDF generates both higher quality and more diversified 3D shapes that conform well to given text descriptions when compared to previous approaches. Code is available at: https://github.com/ttlmh/Diffusion-SDF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1828.Semantic-Conditional Diffusion Networks for Image Captioning</span><br>
                <span class="as">Luo, JianjieandLi, YehaoandPan, YingweiandYao, TingandFeng, JianlinandChao, HongyangandMei, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Semantic-Conditional_Diffusion_Networks_for_Image_Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23359-23368.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用扩散模型在图像描述任务中捕捉离散词的依赖关系并实现复杂的视觉语言对齐。<br>
                    动机：现有的基于Transformer的编码器-解码器模型在图像描述任务中存在挑战，需要新的模型来改进。<br>
                    方法：提出一种新的基于扩散模型的框架，命名为Semantic-Conditional Diffusion Networks (SCD-Net)。通过跨模态检索模型找到与输入图像语义相关的语句，将其丰富的语义信息作为语义先验，触发扩散Transformer的学习，生成输出语句。<br>
                    效果：在COCO数据集上的大量实验表明，扩散模型在具有挑战性的图像描述任务中有潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances on text-to-image generation have witnessed the rise of diffusion models which act as powerful generative models. Nevertheless, it is not trivial to exploit such latent variable models to capture the dependency among discrete words and meanwhile pursue complex visual-language alignment in image captioning. In this paper, we break the deeply rooted conventions in learning Transformer-based encoder-decoder, and propose a new diffusion model based paradigm tailored for image captioning, namely Semantic-Conditional Diffusion Networks (SCD-Net). Technically, for each input image, we first search the semantically relevant sentences via cross-modal retrieval model to convey the comprehensive semantic information. The rich semantics are further regarded as semantic prior to trigger the learning of Diffusion Transformer, which produces the output sentence in a diffusion process. In SCD-Net, multiple Diffusion Transformer structures are stacked to progressively strengthen the output sentence with better visional-language alignment and linguistical coherence in a cascaded manner. Furthermore, to stabilize the diffusion process, a new self-critical sequence training strategy is designed to guide the learning of SCD-Net with the knowledge of a standard autoregressive Transformer model. Extensive experiments on COCO dataset demonstrate the promising potential of using diffusion models in the challenging image captioning task. Source code is available at https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1829.Unite and Conquer: Plug \&amp; Play Multi-Modal Synthesis Using Diffusion Models</span><br>
                <span class="as">Nair, NithinGopalakrishnanandBandara, WeleGedaraChamindaandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nair_Unite_and_Conquer_Plug__Play_Multi-Modal_Synthesis_Using_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6070-6079.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成满足多种约束条件的照片，以解决内容创作行业中的问题。<br>
                    动机：目前的方法需要所有模态的配对数据和相应的输出，且引入新条件需要重新训练所有模态的配对数据。<br>
                    方法：提出一种基于去噪扩散概率模型（DDPMs）的解决方案。由于扩散模型具有灵活的内部结构，每个采样步骤都遵循高斯分布，因此存在一种闭型解，可以根据各种约束条件生成图像。该方法可以利用单个扩散模型进行多任务训练，并通过提出的采样策略提高组合任务的效果。<br>
                    效果：在各种标准多模态任务上进行实验，证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating photos satisfying multiple constraints finds broad utility in the content creation industry. A key hurdle to accomplishing this task is the need for paired data consisting of all modalities (i.e., constraints) and their corresponding output. Moreover, existing methods need retraining using paired data across all modalities to introduce a new condition. This paper proposes a solution to this problem based on denoising diffusion probabilistic models (DDPMs). Our motivation for choosing diffusion models over other generative models comes from the flexible internal structure of diffusion models. Since each sampling step in the DDPM follows a Gaussian distribution, we show that there exists a closed-form solution for generating an image given various constraints. Our method can utilize a single diffusion model trained on multiple sub-tasks and improve the combined task through our proposed sampling strategy. We also introduce a novel reliability parameter that allows using different off-the-shelf diffusion models trained across various datasets during sampling time alone to guide it to the desired outcome satisfying multiple constraints. We perform experiments on various standard multimodal tasks to demonstrate the effectiveness of our approach. More details can be found at: https://nithin-gk.github.io/projectpages/Multidiff</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1830.Magic3D: High-Resolution Text-to-3D Content Creation</span><br>
                <span class="as">Lin, Chen-HsuanandGao, JunandTang, LumingandTakikawa, TowakiandZeng, XiaohuiandHuang, XunandKreis, KarstenandFidler, SanjaandLiu, Ming-YuandLin, Tsung-Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/300-309.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化Neural Radiance Fields (NeRF)的表示，并解决研究问题：如何优化Neural Radiance Fields (NeRF)的表示，并解决DreamFusion方法中存在的优化速度慢和低分辨率图像导致的3D模型质量低的问题。<br>
                    动机：目前的文本到3D合成方法在优化NeRF表示时速度慢且需要长时间等待，同时由于使用低分辨率图像进行监督，生成的3D模型质量较低。<br>
                    方法：本文提出了一种两阶段粗到精的优化框架。首先，使用稀疏的3D神经表示来加速优化，同时使用低分辨率的扩散先验。然后，使用从粗神经表示初始化的纹理网格模型，通过与高分辨率图像交互的高效可微渲染器进行优化。该方法被称为Magic3D。<br>
                    效果：Magic3D可以在40分钟内生成一个3D网格模型，比DreamFusion快2倍，同时生成的模型分辨率高出8倍。用户研究中，61.7%的评分者更喜欢Magic3D的方法。此外，结合图像条件生成能力，我们为用户提供了新的控制3D合成的方式，为各种创意应用开辟了新途径。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, DreamFusion demonstrated the utility of a pretrained text-to-image diffusion model to optimize Neural Radiance Fields (NeRF), achieving remarkable text-to-3D synthesis results. However, the method has two inherent limitations: 1) optimization of the NeRF representation is extremely slow, 2) NeRF is supervised by images at a low resolution (64x64), thus leading to low-quality 3D models with a long wait time. In this paper, we address these limitations by utilizing a two-stage coarse-to-fine optimization framework. In the first stage, we use a sparse 3D neural representation to accelerate optimization while using a low-resolution diffusion prior. In the second stage, we use a textured mesh model initialized from the coarse neural representation, allowing us to perform optimization with a very efficient differentiable renderer interacting with high-resolution images. Our method, dubbed Magic3D, can create a 3D mesh model in 40 minutes, 2x faster than DreamFusion (reportedly taking 1.5 hours on average), while achieving 8x higher resolution. User studies show 61.7% raters to prefer our approach than DreamFusion. Together with the image-conditioned generation capabilities, we provide users with new ways to control 3D synthesis, opening up new avenues to various creative applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1831.SINE: Semantic-Driven Image-Based NeRF Editing With Prior-Guided Editing Field</span><br>
                <span class="as">Bao, ChongandZhang, YindaandYang, BangbangandFan, TianxingandYang, ZesongandBao, HujunandZhang, GuofengandCui, Zhaopeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20919-20929.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管在二维编辑方面已经取得了巨大的成功，但在三维领域的类似功能仍然受限。<br>
                    动机：本文提出了一种新的基于语义的NeRF编辑方法，使用户能够通过单张图像编辑神经辐射场，并忠实地生成具有高保真度和多视图一致性的编辑新视图。<br>
                    方法：为了实现这一目标，我们提出了一种先验引导的编辑场来编码3D空间中的细粒度几何和纹理编辑，并开发了一系列技术来辅助编辑过程，包括使用代理网格的循环约束以促进几何监督、颜色合成机制以稳定语义驱动的纹理编辑以及基于特征聚类的正则化以保持无关内容不变。<br>
                    效果：大量的实验和编辑示例证明，我们的方法仅使用一张编辑过的图像就能实现照片级的3D编辑，推动了在现实世界场景中基于语义的3D编辑的界限。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the great success in 2D editing using user-friendly tools, such as Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D areas are still limited, either relying on 3D modeling skills or allowing editing within only a few categories. In this paper, we present a novel semantic-driven NeRF editing approach, which enables users to edit a neural radiance field with a single image, and faithfully delivers edited novel views with high fidelity and multi-view consistency. To achieve this goal, we propose a prior-guided editing field to encode fine-grained geometric and texture editing in 3D space, and develop a series of techniques to aid the editing process, including cyclic constraints with a proxy mesh to facilitate geometric supervision, a color compositing mechanism to stabilize semantic-driven texture editing, and a feature-cluster-based regularization to preserve the irrelevant content unchanged. Extensive experiments and editing examples on both real-world and synthetic data demonstrate that our method achieves photo-realistic 3D editing using only a single edited image, pushing the bound of semantic-driven editing in 3D real-world scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1832.Fine-Grained Face Swapping via Regional GAN Inversion</span><br>
                <span class="as">Liu, ZhianandLi, MaomaoandZhang, YongandWang, CairongandZhang, QiandWang, JueandNie, Yongwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8578-8587.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现高保真人脸交换，同时保留所需的微妙几何和纹理细节。<br>
                    动机：从细粒度人脸编辑的角度重新思考人脸交换，即编辑用于交换（E4S），并提出一个基于面部组件形状和纹理显式解耦的框架。<br>
                    方法：提出了一种基于显式解耦形状和纹理的新颖区域生成对抗网络反转（RGI）方法，并在StyleGAN的潜在空间中进行人脸交换。设计了一个多尺度掩码引导编码器将每个面部组件的纹理投影到区域风格代码中，并设计了一个掩码引导注入模块用风格代码操纵特征图。<br>
                    效果：通过大量的实验和与当前最先进的方法的比较，证明了该方法在保留纹理和形状细节以及处理高分辨率图像方面的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel paradigm for high-fidelity face swapping that faithfully preserves the desired subtle geometry and texture details. We rethink face swapping from the perspective of fine-grained face editing, i.e., editing for swapping (E4S), and propose a framework that is based on the explicit disentanglement of the shape and texture of facial components. Following the E4S principle, our framework enables both global and local swapping of facial features, as well as controlling the amount of partial swapping specified by the user. Furthermore, the E4S paradigm is inherently capable of handling facial occlusions by means of facial masks. At the core of our system lies a novel Regional GAN Inversion (RGI) method, which allows the explicit disentanglement of shape and texture. It also allows face swapping to be performed in the latent space of StyleGAN. Specifically, we design a multi-scale mask-guided encoder to project the texture of each facial component into regional style codes. We also design a mask-guided injection module to manipulate the feature maps with the style codes. Based on the disentanglement, face swapping is reformulated as a simplified problem of style and mask swapping. Extensive experiments and comparisons with current state-of-the-art methods demonstrate the superiority of our approach in preserving texture and shape details, as well as working with high resolution images. The project page is https://e4s2022.github.io</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1833.Where Is My Spot? Few-Shot Image Generation via Latent Subspace Optimization</span><br>
                <span class="as">Zheng, ChenxiandLiu, BangzhenandZhang, HuaidongandXu, XuemiaoandHe, Shengfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Where_Is_My_Spot_Few-Shot_Image_Generation_via_Latent_Subspace_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3272-3281.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像生成模型在面对少量未见过的类别样本时，难以产生多样化的图像的问题。<br>
                    动机：目前的图像生成模型需要大量的训练数据才能生成多样化的未见过的类别图像，这在实际中是不现实的。因此，本文试图通过将稀疏的少数样本映射到潜在的连续潜在空间来解决这个问题。<br>
                    方法：本文提出的方法是将少数样本投影到一个潜在的连续潜在空间中，这个空间可以生成无限的未见过的样本。具体来说，我们首先找到条件StyleGAN中的一个质心潜在位置，该位置对应的输出图像与给定样本的相似度最大。然后，我们假设质心周围的潜在子空间属于新的类别，并引入两个潜在子空间优化目标。第一个目标是使用少数样本作为新类别的正锚点，调整StyleGAN以产生带有新类别标签的结果。第二个目标是从另一个方向控制生成过程，通过改变质心及其周围的潜在子空间来更精确地生成新的类别。这两个互惠的优化目标将新类别注入StyleGAN的潜在子空间，从而可以通过从该空间采样图像来轻松生成新的未见过的样本。<br>
                    效果：实验结果表明，该方法在少数样本生成性能上优于最先进的方法，特别是在多样性和生成质量方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image generation relies on massive training data that can hardly produce diverse images of an unseen category according to a few examples. In this paper, we address this dilemma by projecting sparse few-shot samples into a continuous latent space that can potentially generate infinite unseen samples. The rationale behind is that we aim to locate a centroid latent position in a conditional StyleGAN, where the corresponding output image on that centroid can maximize the similarity with the given samples. Although the given samples are unseen for the conditional StyleGAN, we assume the neighboring latent subspace around the centroid belongs to the novel category, and therefore introduce two latent subspace optimization objectives. In the first one we use few-shot samples as positive anchors of the novel class, and adjust the StyleGAN to produce the corresponding results with the new class label condition. The second objective is to govern the generation process from the other way around, by altering the centroid and its surrounding latent subspace for a more precise generation of the novel class. These reciprocal optimization objectives inject a novel class into the StyleGAN latent subspace, and therefore new unseen samples can be easily produced by sampling images from it. Extensive experiments demonstrate superior few-shot generation performances compared with state-of-the-art methods, especially in terms of diversity and generation quality. Code is available at https://github.com/chansey0529/LSO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1834.Discrete Point-Wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition</span><br>
                <span class="as">Li, QianandHu, YuxiaoandLiu, YeandZhang, DongxiaoandJin, XinandChen, Yuntian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Discrete_Point-Wise_Attack_Is_Not_Enough_Generalized_Manifold_Adversarial_Attack_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20575-20584.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高人脸识别模型对抗攻击的效果和泛化能力。<br>
                    动机：现有的针对人脸识别的对抗攻击方法在未知身份状态下表现不佳，且容易被防御。<br>
                    方法：提出了一种广义流形对抗攻击（GMAA）的新方法，通过将目标从单一扩展到多个，并将攻击点从离散扩展到流形，以扩大攻击范围并提高攻击效果。同时，设计了一种局部和全局约束的双重监督机制，以提高生成对抗样本的视觉质量。<br>
                    效果：实验证明，GMAA能在语义连续的对抗空间中实现更高的泛化能力和视觉质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Classical adversarial attacks for Face Recognition (FR) models typically generate discrete examples for target identity with a single state image. However, such paradigm of point-wise attack exhibits poor generalization against numerous unknown states of identity and can be easily defended. In this paper, by rethinking the inherent relationship between the face of target identity and its variants, we introduce a new pipeline of Generalized Manifold Adversarial Attack (GMAA) to achieve a better attack performance by expanding the attack range. Specifically, this expansion lies on two aspects -- GMAA not only expands the target to be attacked from one to many to encourage a good generalization ability for the generated adversarial examples, but it also expands the latter from discrete points to manifold by leveraging the domain knowledge that face expression change can be continuous, which enhances the attack effect as a data augmentation mechanism did. Moreover, we further design a dual supervision with local and global constraints as a minor contribution to improve the visual quality of the generated adversarial examples. We demonstrate the effectiveness of our method based on extensive experiments, and reveal that GMAA promises a semantic continuous adversarial space with a higher generalization ability and visual quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1835.Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation</span><br>
                <span class="as">Wang, HaochenandDu, XiaodanandLi, JiahaoandYeh, RaymondA.andShakhnarovich, Greg</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12619-12629.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练的2D模型用于3D数据生成？<br>
                    动机：通过应用扩散模型和链式规则，将2D得分聚合为3D得分，以实现3D数据的生成。<br>
                    方法：在可微渲染器（如体素辐射场）上应用扩散模型的梯度，并通过雅可比矩阵进行反向传播。<br>
                    效果：解决了分布不匹配的技术挑战，并在多个现有的扩散图像生成模型上进行了实验，取得了良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A diffusion model learns to predict a vector field of gradients. We propose to apply chain rule on the learned gradients, and back-propagate the score of a diffusion model through the Jacobian of a differentiable renderer, which we instantiate to be a voxel radiance field. This setup aggregates 2D scores at multiple camera viewpoints into a 3D score, and repurposes a pretrained 2D model for 3D data generation. We identify a technical challenge of distribution mismatch that arises in this application, and propose a novel estimation mechanism to resolve it. We run our algorithm on several off-the-shelf diffusion image generative models, including the recently released Stable Diffusion trained on the large-scale LAION dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1836.Generating Part-Aware Editable 3D Shapes Without 3D Supervision</span><br>
                <span class="as">Tertikas, KonstantinosandPaschalidou, DespoinaandPan, BoxiaoandPark, JeongJoonandUy, MikaelaAngelinaandEmiris, IoannisandAvrithis, YannisandGuibas, Leonidas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tertikas_Generating_Part-Aware_Editable_3D_Shapes_Without_3D_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4466-4478.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成具有局部控制和编辑能力的高质量3D形状。<br>
                    动机：现有的方法虽然可以生成高质量的3D形状，但缺乏对形状的局部控制和编辑能力，限制了其在内容创作应用中的使用。<br>
                    方法：本文提出了一种新的部分感知生成模型PartNeRF，该模型不需要任何显式的3D监督，而是将物体生成为一组局部定义的NeRFs，并附加仿射变换。通过强制将光线分配给特定部分，确保每个光线的颜色只由一个NeRF决定，从而实现对不同部分的独立操作和编辑。<br>
                    效果：在各种ShapeNet类别上的评估表明，与需要3D监督或依赖NeRF的先前部分基础生成方法相比，PartNeRF能够生成具有改进保真度的可编辑3D对象。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Impressive progress in generative models and implicit representations gave rise to methods that can generate 3D shapes of high quality. However, being able to locally control and edit shapes is another essential property that can unlock several content creation applications. Local control can be achieved with part-aware models, but existing methods require 3D supervision and cannot produce textures. In this work, we devise PartNeRF, a novel part-aware generative model for editable 3D shape synthesis that does not require any explicit 3D supervision. Our model generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables several editing operations such as applying transformations on parts, mixing parts from different objects etc. To ensure distinct, manipulable parts we enforce a hard assignment of rays to parts that makes sure that the color of each ray is only determined by a single NeRF. As a result, altering one part does not affect the appearance of the others. Evaluations on various ShapeNet categories demonstrate the ability of our model to generate editable 3D objects of improved fidelity, compared to previous part-based generative approaches that require 3D supervision or models relying on NeRFs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1837.High-Fidelity Facial Avatar Reconstruction From Monocular Video With Generative Priors</span><br>
                <span class="as">Bai, YunpengandFan, YanboandWang, XuanandZhang, YongandSun, JingxiangandYuan, ChunandShan, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_High-Fidelity_Facial_Avatar_Reconstruction_From_Monocular_Video_With_Generative_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4541-4551.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目视频中重建高保真度的人脸化身。<br>
                    动机：由于人脸的复杂动态性和单目视频中的3D信息缺失，使得从单目视频中重建高保真度的人脸化身成为一项重大的研究挑战。<br>
                    方法：提出一种基于Neural Radiance Field (NeRF)的新方法来进行人脸化身重建，该方法利用了3D感知的生成先验。与现有依赖条件变形场进行动态建模的工作不同，我们提出了学习个性化生成先验的方法，该方法被构造为3D-GAN潜在空间中的一个局部低维子空间。<br>
                    效果：通过少量的给定个体面部图像，可以有效地构建个性化的生成先验。学习后，它可以进行照片级的新颖视图渲染，并且通过在潜在空间中导航可以实现面部再演算。与现有的工作相比，我们获得了优越的新颖视图合成结果和忠实的面部再演算性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-fidelity facial avatar reconstruction from a monocular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar reconstruction. However, the complex facial dynamics and missing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views, and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audio. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance. The code is available here https://github.com/bbaaii/HFA-GP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1838.Restoration of Hand-Drawn Architectural Drawings Using Latent Space Mapping With Degradation Generator</span><br>
                <span class="as">Choi, NakkwanandLee, SeungjaeandLee, YongsikandYang, Seungjoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Restoration_of_Hand-Drawn_Architectural_Drawings_Using_Latent_Space_Mapping_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14164-14172.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何恢复木结构遗产的手绘图纸。<br>
                    动机：手绘图纸包含最重要的原始信息，但随着时间的推移，这些信息往往会严重退化。<br>
                    方法：提出了一种基于矢量量化变分自动编码器的新型恢复方法。学习图纸和噪声的隐空间表示，用于将有噪声的图纸映射到清洁的图纸进行恢复，并生成真实的有噪声的图纸进行数据增强。<br>
                    效果：该方法应用于文化遗产管理局归档的图纸，恢复后的图纸质量显著提高，可以更准确地解释信息。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work presents the restoration of drawings of wooden built heritage. Hand-drawn drawings contain the most important original information but are often severely degraded over time. A novel restoration method based on the vector quantized variational autoencoders is presented. Latent space representations of drawings and noise are learned, which are used to map noisy drawings to clean drawings for restoration and to generate authentic noisy drawings for data augmentation. The proposed method is applied to the drawings archived in the Cultural Heritage Administration. Restored drawings show significant quality improvement and allow more accurate interpretations of information.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1839.DiffusionRig: Learning Personalized Priors for Facial Appearance Editing</span><br>
                <span class="as">Ding, ZhengandZhang, XuanerandXia, ZhihaoandJebe, LarsandTu, ZhuowenandZhang, Xiuming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_DiffusionRig_Learning_Personalized_Priors_for_Facial_Appearance_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12736-12746.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从少量（例如20张）同一人物的肖像照片中学习人脸特定先验知识。<br>
                    动机：通过这种方式，我们可以编辑特定人物的面部外观，如表情和照明，同时保留其身份和高频面部细节。<br>
                    方法：我们的方法称为DiffusionRig，主要是通过一个扩散模型对从单张野外图像中估计出的粗略3D人脸模型进行“装配”。<br>
                    效果：定性和定量实验表明，DiffusionRig在身份保持和照片真实感方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of learning person-specific facial priors from a small number (e.g., 20) of portrait photos of the same person. This enables us to edit this specific person's facial appearance, such as expression and lighting, while preserving their identity and high-frequency facial details. Key to our approach, which we dub DiffusionRig, is a diffusion model conditioned on, or "rigged by," crude 3D face models estimated from single in-the-wild images by an off-the-shelf estimator. On a high level, DiffusionRig learns to map simplistic renderings of 3D face models to realistic photos of a given person. Specifically, DiffusionRig is trained in two stages: It first learns generic facial priors from a large-scale face dataset and then person-specific priors from a small portrait photo collection of the person of interest. By learning the CGI-to-photo mapping with such personalized priors, DiffusionRig can "rig" the lighting, facial expression, head pose, etc. of a portrait photo, conditioned only on coarse 3D models while preserving this person's identity and other high-frequency characteristics. Qualitative and quantitative experiments show that DiffusionRig outperforms existing approaches in both identity preservation and photorealism. Please see the project website: https://diffusionrig.github.io for the supplemental material, video, code, and data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1840.Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint</span><br>
                <span class="as">Liu, HongyuandSong, YibingandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_StyleGAN_Inversion_for_Image_Editing_A_Foundation_Latent_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10072-10082.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过StyleGAN映射将输入图像嵌入到W、W+和F的嵌入空间中，以同时保持图像保真度和有意义的操作。<br>
                    动机：现有的GAN逆行方法主要探索W+和F空间以提高重建保真度，但忽视了作为StyleGAN基础的潜在空间W。<br>
                    方法：首先在基础潜在空间W中找到适当的潜在码，然后引入对比学习对齐W和图像空间以发现适当的潜在码。接着利用跨注意力编码器将获得的W中的潜在码转换为W+和F。<br>
                    效果：实验表明，对基础潜在空间W的探索提高了W+和F中潜在码和特征的表现能力，从而在标准基准上实现了最先进的重建保真度和可编辑性结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>GAN inversion and editing via StyleGAN maps an input image into the embedding spaces (W, W^+, and F) to simultaneously maintain image fidelity and meaningful manipulation. From latent space W to extended latent space W^+ to feature space F in StyleGAN, the editability of GAN inversion decreases while its reconstruction quality increases. Recent GAN inversion methods typically explore W^+ and F rather than W to improve reconstruction fidelity while maintaining editability. As W^+ and F are derived from W that is essentially the foundation latent space of StyleGAN, these GAN inversion methods focusing on W^+ and F spaces could be improved by stepping back to W. In this work, we propose to first obtain the proper latent code in foundation latent space W. We introduce contrastive learning to align W and the image space for proper latent code discovery. Then, we leverage a cross-attention encoder to transform the obtained latent code in W into W^+ and F, accordingly. Our experiments show that our exploration of the foundation latent space W improves the representation ability of latent codes in W^+ and features in F, which yields state-of-the-art reconstruction fidelity and editability results on the standard benchmarks. Project page: https://kumapowerliu.github.io/CLCAE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1841.GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling</span><br>
                <span class="as">Plesh, RichardandPeer, PeterandStruc, Vitomir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Plesh_GlassesGAN_Eyewear_Personalization_Using_Synthetic_Appearance_Discovery_and_Targeted_Subspace_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16847-16857.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种新颖的眼镜图像编辑框架，以实现眼镜的定制设计。<br>
                    动机：目前的图像编辑框架在输出图像质量、编辑真实性和连续多风格编辑能力方面存在不足。<br>
                    方法：提出了GlassesGAN，这是一种新的图像编辑框架，用于自定义眼镜设计。同时，还提出了Targeted Subspace Modelling（TSM）过程，通过在预训练的GAN生成器的潜空间中进行（合成）外观发现的新机制，构建了一个专门针对眼镜的（潜在）子空间，供编辑框架使用。此外，还引入了外观约束子空间初始化（SI）技术，将给定输入图像的潜在表示定位在构造的子空间的明确部分，以提高学习的编辑的可靠性。<br>
                    效果：在两个高分辨率数据集（CelebA-HQ和SiblingsDB-HQf）上测试GlassesGAN，并将其与三种最先进的基线方法（即InterfaceGAN、GANSpace和MaskGAN）进行了比较。结果显示，GlassesGAN在所有竞争技术中表现优异，同时提供了竞争对手无法提供的功能（如细粒度的多风格编辑）。GlassesGAN的源代码已公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present GlassesGAN, a novel image editing framework for custom design of glasses, that sets a new standard in terms of output-image quality, edit realism, and continuous multi-style edit capability. To facilitate the editing process with GlassesGAN, we propose a Targeted Subspace Modelling (TSM) procedure that, based on a novel mechanism for (synthetic) appearance discovery in the latent space of a pre-trained GAN generator, constructs an eyeglasses-specific (latent) subspace that the editing framework can utilize. Additionally, we also introduce an appearance-constrained subspace initialization (SI) technique that centers the latent representation of the given input image in the well-defined part of the constructed subspace to improve the reliability of the learned edits. We test GlassesGAN on two (diverse) high-resolution datasets (CelebA-HQ and SiblingsDB-HQf) and compare it to three state-of-the-art baselines, i.e., InterfaceGAN, GANSpace, and MaskGAN. The reported results show that GlassesGAN convincingly outperforms all competing techniques, while offering functionality (e.g., fine-grained multi-style editing) not available with any of the competitors. The source code for GlassesGAN is made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1842.Parametric Implicit Face Representation for Audio-Driven Facial Reenactment</span><br>
                <span class="as">Huang, RicongandLai, PeiwenandQin, YipengandLi, Guanbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Parametric_Implicit_Face_Representation_for_Audio-Driven_Facial_Reenactment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12759-12768.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决音频驱动的面部重演技术中存在的解释性和表现力之间的权衡问题。<br>
                    动机：现有的工作要么使用明确的中间面部表示（如2D面部地标或3D面部模型），要么使用隐式的表示（如神经辐射场），因此在结果的控制性和质量方面存在权衡。<br>
                    方法：本文提出了一种新的参数化隐式面部表示，并提出了一种新的音频驱动的面部重演框架，该框架既可控又可以生成高质量的说话头部。具体来说，我们的参数化隐式表示使用3D面部模型的解释性参数对隐式表示进行参数化，从而兼顾了显式和隐式方法的优点。此外，我们还提出了几种新技巧来改进我们框架的三个组件，包括将上下文信息纳入音频到表达参数编码、使用条件图像合成来参数化隐式表示并实施创新的三平面结构以实现有效学习，以及将面部重演公式化为条件图像修复问题并提出一种新的数据增强技术以提高模型的泛化能力。<br>
                    效果：大量实验证明，与以往的方法相比，我们的方法可以生成更真实的结果，更能忠实于说话者的身份和讲话风格。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Audio-driven facial reenactment is a crucial technique that has a range of applications in film-making, virtual avatars and video conferences. Existing works either employ explicit intermediate face representations (e.g., 2D facial landmarks or 3D face models) or implicit ones (e.g., Neural Radiance Fields), thus suffering from the trade-offs between interpretability and expressive power, hence between controllability and quality of the results. In this work, we break these trade-offs with our novel parametric implicit face representation and propose a novel audio-driven facial reenactment framework that is both controllable and can generate high-quality talking heads. Specifically, our parametric implicit representation parameterizes the implicit representation with interpretable parameters of 3D face models, thereby taking the best of both explicit and implicit methods. In addition, we propose several new techniques to improve the three components of our framework, including i) incorporating contextual information into the audio-to-expression parameters encoding; ii) using conditional image synthesis to parameterize the implicit representation and implementing it with an innovative tri-plane structure for efficient learning; iii) formulating facial reenactment as a conditional image inpainting problem and proposing a novel data augmentation technique to improve model generalizability. Extensive experiments demonstrate that our method can generate more realistic results than previous methods with greater fidelity to the identities and talking styles of speakers.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1843.Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild</span><br>
                <span class="as">Saha, AvinabandMishra, SandeepandBovik, AlanC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saha_Re-IQA_Unsupervised_Learning_for_Image_Quality_Assessment_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5846-5855.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动感知图像质量评估是一个影响数十亿互联网和社交媒体用户的日常挑战。<br>
                    动机：为了推进这一领域的研究，我们提出了一种混合专家的方法，在无监督的设置中训练两个独立的编码器来学习高层次的内容和低层次的图像质量特征。<br>
                    方法：我们的方法的独特新颖之处在于其能够生成与表示图像内容的高层特征互补的图像质量的低层表示。我们将用于训练这两个编码器的框架称为Re-IQA。<br>
                    效果：我们在多个包含真实和合成失真的大规模图像质量评估数据库上部署了从Re-IQA框架获得的互补的低层和高层图像表示，以训练一个线性回归模型，该模型用于将图像表示映射到地面真实质量分数。我们的方法在多个大规模图像质量评估数据库上实现了最先进的性能，展示了深度神经网络如何在无监督的设置中产生感知相关的表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic Perceptual Image Quality Assessment is a challenging problem that impacts billions of internet, and social media users daily. To advance research in this field, we propose a Mixture of Experts approach to train two separate encoders to learn high-level content and low-level image quality features in an unsupervised setting. The unique novelty of our approach is its ability to generate low-level representations of image quality that are complementary to high-level features representing image content. We refer to the framework used to train the two encoders as Re-IQA. For Image Quality Assessment in the Wild, we deploy the complementary low and high-level image representations obtained from the Re-IQA framework to train a linear regression model, which is used to map the image representations to the ground truth quality scores, refer Figure 1. Our method achieves state-of-the-art performance on multiple large-scale image quality assessment databases containing both real and synthetic distortions, demonstrating how deep neural networks can be trained in an unsupervised setting to produce perceptually relevant representations. We conclude from our experiments that the low and high-level features obtained are indeed complementary and positively impact the performance of the linear regressor. A public release of all the codes associated with this work will be made available on GitHub.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1844.Catch Missing Details: Image Reconstruction With Frequency Augmented Variational Autoencoder</span><br>
                <span class="as">Lin, XinmiaoandLi, YikangandHsiao, JenhaoandHo, ChiumanandKong, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Catch_Missing_Details_Image_Reconstruction_With_Frequency_Augmented_Variational_Autoencoder_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1736-1745.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的VQ-VAE模型在图像重建中，随着压缩率的提高，图像质量快速下降。<br>
                    动机：高压缩率导致高频谱视觉信号丢失，这是由于高频谱反映了像素空间的细节信息。<br>
                    方法：提出频率补偿模块（FCM）架构来捕获丢失的频率信息以提升重建质量，并将其整合到VQ-VAE结构中，形成频率增强VAE（FA-VAE）。同时引入动态频谱损失（DSL）来动态平衡各种频率。<br>
                    效果：在多个基准数据集上进行的大量重建实验表明，与现有方法相比，FA-VAE能更准确地恢复细节。此外，通过使用交叉注意力自回归变压器（CAT），在图像-文本生成任务上也实现了更好的语义对齐和生成质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The popular VQ-VAE models reconstruct images through learning a discrete codebook but suffer from a significant issue in the rapid quality degradation of image reconstruction as the compression rate rises. One major reason is that a higher compression rate induces more loss of visual signals on the higher frequency spectrum, which reflect the details on pixel space. In this paper, a Frequency Complement Module (FCM) architecture is proposed to capture the missing frequency information for enhancing reconstruction quality. The FCM can be easily incorporated into the VQ-VAE structure, and we refer to the new model as Frequancy Augmented VAE (FA-VAE). In addition, a Dynamic Spectrum Loss (DSL) is introduced to guide the FCMs to balance between various frequencies dynamically for optimal reconstruction. FA-VAE is further extended to the text-to-image synthesis task, and a Cross-attention Autoregressive Transformer (CAT) is proposed to obtain more precise semantic attributes in texts. Extensive reconstruction experiments with different compression rates are conducted on several benchmark datasets, and the results demonstrate that the proposed FA-VAE is able to restore more faithfully the details compared to SOTA methods. CAT also shows improved generation quality with better image-text semantic alignment.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1845.RaBit: Parametric Modeling of 3D Biped Cartoon Characters With a Topological-Consistent Dataset</span><br>
                <span class="as">Luo, ZhongjinandCai, ShengcaiandDong, JinguoandMing, RuiboandQiu, LiangdongandZhan, XiaohangandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_RaBit_Parametric_Modeling_of_3D_Biped_Cartoon_Characters_With_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12825-12835.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成视觉上可信的3D卡通人物。<br>
                    动机：虽然现有的学习型方法在3D真实人类数字化方面取得了前所未有的准确性和效率，但没有一个关注于3D双足卡通人物模型化，这在游戏和电影制作中也有大量需求。<br>
                    方法：介绍了第一个大规模的3D双足卡通人物数据集3DBiCar和相应的参数化模型RaBit。该数据集包含1500个高质量的、由专业艺术家手工制作的拓扑一致的3D纹理模型。基于这些数据，RaBit被设计成一个类似SMPL的线性混合形状模型和一个基于StyleGAN的神经UV纹理生成器，同时表达形状、姿态和纹理。<br>
                    效果：通过各种应用，包括单视图重建、草图建模和3D卡通动画，展示了3DBiCar和RaBit的实用性。实验进一步证明了我们的方法在定性和定量上的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Assisting people in efficiently producing visually plausible 3D characters has always been a fundamental research topic in computer vision and computer graphics. Recent learning-based approaches have achieved unprecedented accuracy and efficiency in the area of 3D real human digitization. However, none of the prior works focus on modeling 3D biped cartoon characters, which are also in great demand in gaming and filming. In this paper, we introduce 3DBiCar, the first large-scale dataset of 3D biped cartoon characters, and RaBit, the corresponding parametric model. Our dataset contains 1,500 topologically consistent high-quality 3D textured models which are manually crafted by professional artists. Built upon the data, RaBit is thus designed with a SMPL-like linear blend shape model and a StyleGAN-based neural UV-texture generator, simultaneously expressing the shape, pose, and texture. To demonstrate the practicality of 3DBiCar and RaBit, various applications are conducted, including single-view reconstruction, sketch-based modeling, and 3D cartoon animation. For the single-view reconstruction setting, we find a straightforward global mapping from input images to the output UV-based texture maps tends to lose detailed appearances of some local parts (e.g., nose, ears). Thus, a part-sensitive texture reasoner is adopted to make all important local areas perceived. Experiments further demonstrate the effectiveness of our method both qualitatively and quantitatively. 3DBiCar and RaBit are available at gaplab.cuhk.edu.cn/projects/RaBit.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1846.Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</span><br>
                <span class="as">Sun, JingxiangandWang, XuanandWang, LizhenandLi, XiaoyuandZhang, YongandZhang, HongwenandLiu, Yebin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20991-21002.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从无结构的2D图像中，使用3D变形模型（3DMM）进行高保真和多视角一致的面部图像合成。<br>
                    动机：目前的3D-GAN方法在处理面部属性的细节控制时，要么可以精确控制面部表情但不能处理由头发和配饰引起的拓扑变化，要么能处理不同的拓扑结构但因未受约束的形变场而泛化能力有限。<br>
                    方法：提出一种新的3D-GAN框架，用于从无结构的2D图像中进行无监督学习，生成高质量且3D一致的面部头像。为了同时实现形变准确性和拓扑灵活性，提出了一种称为“生成纹理光栅化三平面”的3D表示法。该方法在学习参数化网格模板上的生成神经纹理后，通过光栅化将它们投影到三个正交的视角特征平面上，形成用于体积渲染的三平面特征表示。<br>
                    效果：实验表明，该方法在3D感知的合成质量和动画能力方面均达到最先进的水平。此外，作为3D先验，这种可动画化的3D表示法提高了包括单次面部头像和3D感知风格化在内的多种应用的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D-aware generative adversarial networks (GANs) synthesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to describe deformation in generative radiance fields either explicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized Tri-planes. The proposed representation learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we combine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumetric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3Daware synthesis quality and animation ability through extensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware stylization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1847.Linking Garment With Person via Semantically Associated Landmarks for Virtual Try-On</span><br>
                <span class="as">Yan, KeyuandGao, TingweiandZhang, HuiandXie, Chengjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Linking_Garment_With_Person_via_Semantically_Associated_Landmarks_for_Virtual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17194-17204.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的虚拟试穿算法，称为SAL-VTON，通过语义关联的地标将服装与人体连接起来，以减轻错位问题。<br>
                    动机：现有的虚拟试穿技术在处理服装和人体的整体变形时存在错位问题。<br>
                    方法：SAL-VTON算法通过在店内服装图像和试穿图像上找到具有相同局部语义的一系列地标对来链接服装和人。然后，利用这些地标对有效地模拟服装和人体的局部语义关联，弥补了整体变形中的错位。<br>
                    效果：实验结果表明，SAL-VTON能够处理错位问题，并在定性和定量上都优于现有方法。同时，我们还提出了一个新的地标数据集，为各种风格的服装提供了统一的地标标注规则。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, a novel virtual try-on algorithm, dubbed SAL-VTON, is proposed, which links the garment with the person via semantically associated landmarks to alleviate misalignment. The semantically associated landmarks are a series of landmark pairs with the same local semantics on the in-shop garment image and the try-on image. Based on the semantically associated landmarks, SAL-VTON effectively models the local semantic association between garment and person, making up for the misalignment in the overall deformation of the garment. The outcome is achieved with a three-stage framework: 1) the semantically associated landmarks are estimated using the landmark localization model; 2) taking the landmarks as input, the warping model explicitly associates the corresponding parts of the garment and person for obtaining the local flow, thus refining the alignment in the global flow; 3) finally, a generator consumes the landmarks to better capture local semantics and control the try-on results.Moreover, we propose a new landmark dataset with a unified labelling rule of landmarks for diverse styles of garments. Extensive experimental results on popular datasets demonstrate that SAL-VTON can handle misalignment and outperform state-of-the-art methods both qualitatively and quantitatively. The dataset is available on https://modelscope.cn/datasets/damo/SAL-HG/summary.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1848.ConZIC: Controllable Zero-Shot Image Captioning by Sampling-Based Polishing</span><br>
                <span class="as">Zeng, ZequnandZhang, HaoandLu, RuiyingandWang, DongshengandChen, BoandWang, Zhengjue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_ConZIC_Controllable_Zero-Shot_Image_Captioning_by_Sampling-Based_Polishing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23465-23476.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高零样本图像描述（IC）的多样性和推理速度，并解决其控制性问题。<br>
                    动机：现有的零样本图像描述方法虽然有效，但其自回归生成和梯度导向搜索机制限制了描述的多样性和推理速度，同时也未考虑控制性问题。<br>
                    方法：提出一种名为ConZIC的控制性零样本图像描述框架，核心是一种新颖的基于采样的非自回归语言模型GibbsBERT，可以生成并连续优化每个词。<br>
                    效果：实验结果表明，ConZIC在零样本图像描述和控制性零样本图像描述方面均表现出优越性能，特别是与ZeroCap相比，生成速度提高了约5倍，多样性得分提高了约1.5倍，并能根据不同的控制信号进行准确生成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Zero-shot capability has been considered as a new revolution of deep learning, letting machines work on tasks without curated training data. As a good start and the only existing outcome of zero-shot image captioning (IC), ZeroCap abandons supervised training and sequentially searching every word in the caption using the knowledge of large-scale pre-trained models. Though effective, its autoregressive generation and gradient-directed searching mechanism limit the diversity of captions and inference speed, respectively. Moreover, ZeroCap does not consider the controllability issue of zero-shot IC. To move forward, we propose a framework for Controllable Zero-shot IC, named ConZIC. The core of ConZIC is a novel sampling-based non-autoregressive language model named GibbsBERT, which can generate and continuously polish every word. Extensive quantitative and qualitative results demonstrate the superior performance of our proposed ConZIC for both zero-shot IC and controllable zero-shot IC. Especially, ConZIC achieves about 5x faster generation speed than ZeroCap, and about 1.5x higher diversity scores, with accurate generation given different control signals.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1849.EDGE: Editable Dance Generation From Music</span><br>
                <span class="as">Tseng, JonathanandCastellon, RodrigoandLiu, Karen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/448-458.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成与音乐相匹配的、物理上可信的舞蹈动作？<br>
                    动机：现有的舞蹈生成方法往往难以生成具有物理可信度的舞蹈，同时编辑功能也相对有限。<br>
                    方法：本文提出了一种基于转换器的扩散模型EDGE，结合强大的音乐特征提取器Jukebox，实现了对舞蹈动作的精细编辑和生成。<br>
                    效果：通过大量实验和用户研究，证明EDGE在生成物理上可信、与音乐匹配的舞蹈方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1850.HumanGen: Generating Human Radiance Fields With Explicit Priors</span><br>
                <span class="as">Jiang, SuyiandJiang, HaoranandWang, ZiyuandLuo, HaiminandChen, WenzhengandXu, Lan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12543-12554.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成具有详细几何形状和360度真实自由视角渲染的高质量人类辐射场。<br>
                    动机：现有的方法由于采用的人类相关先验知识有限，导致高质量的人类辐射场生成仍然具有挑战性。<br>
                    方法：提出了一种新的3D人类生成方案HumanGen，通过设计“锚定图像”将3D人类生成与2D生成器和3D重建器的多种先验知识明确结合起来。引入了使用锚定图像的混合特征表示，以连接HumanGen的潜在空间和现有的2D生成器。然后采用了一种分阶段的设计方案，对几何形状和外观的生成进行解耦。借助锚定图像，适应了一个用于细粒度细节合成的3D重建器，并提出了两级混合方案来提升外观生成。<br>
                    效果：广泛的实验表明，在几何细节、纹理质量和自由视角性能方面，HumanGen在最先进的3D人类生成方面非常有效。值得注意的是，HumanGen还可以整合各种现成的2D潜在编辑方法，无缝地将它们提升到3D。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent years have witnessed the tremendous progress of 3D GANs for generating view-consistent radiance fields with photo-realism. Yet, high-quality generation of human radiance fields remains challenging, partially due to the limited human-related priors adopted in existing methods. We present HumanGen, a novel 3D human generation scheme with detailed geometry and 360deg realistic free-view rendering. It explicitly marries the 3D human generation with various priors from the 2D generator and 3D reconstructor of humans through the design of "anchor image". We introduce a hybrid feature representation using the anchor image to bridge the latent space of HumanGen with the existing 2D generator. We then adopt a pronged design to disentangle the generation of geometry and appearance. With the aid of the anchor image, we adapt a 3D reconstructor for fine-grained details synthesis and propose a two-stage blending scheme to boost appearance generation. Extensive experiments demonstrate our effectiveness for state-of-the-art 3D human generation regarding geometry details, texture quality, and free-view performance. Notably, HumanGen can also incorporate various off-the-shelf 2D latent editing methods, seamlessly lifting them into 3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1851.Towards Practical Plug-and-Play Diffusion Models</span><br>
                <span class="as">Go, HyojunandLee, YunsungandKim, Jin-YoungandLee, SeunghyunandJeong, MyeonghoandLee, HyunSeungandChoi, Seungtaek</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Go_Towards_Practical_Plug-and-Play_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1962-1971.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散基生成模型在图像生成方面取得了显著的成功，但其对噪声输入的直接使用效果不佳。<br>
                    动机：目前的实践中，为了解决这一问题，通常的做法是使用标注数据进行微调，但这存在两个问题：一是单个指导模型难以处理各种噪声输入；二是收集标注数据集阻碍了任务的扩展。<br>
                    方法：本文提出了一种新的策略，即利用多个专家来指导反向扩散过程，每个专家专门处理特定的噪声范围。同时，为了避免管理多个网络和利用标注数据的困难，我们提出了一种实用的指导框架——实用即插即用（PPAP），它利用参数高效的微调和无数据的知识转移。<br>
                    效果：通过ImageNet类别条件生成实验，我们发现该方法可以成功地指导扩散，且只需少量可训练参数和无需标注数据。此外，我们还展示了图像分类器、深度估计器和语义分割模型可以通过我们的框架以即插即用的方式指导公开的GLIDE模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion-based generative models have achieved remarkable success in image generation. Their guidance formulation allows an external model to plug-and-play control the generation process for various tasks without fine-tuning the diffusion model. However, the direct use of publicly available off-the-shelf models for guidance fails due to their poor performance on noisy inputs. For that, the existing practice is to fine-tune the guidance models with labeled data corrupted with noises. In this paper, we argue that this practice has limitations in two aspects: (1) performing on inputs with extremely various noises is too hard for a single guidance model; (2) collecting labeled datasets hinders scaling up for various tasks. To tackle the limitations, we propose a novel strategy that leverages multiple experts where each expert is specialized in a particular noise range and guides the reverse process of the diffusion at its corresponding timesteps. However, as it is infeasible to manage multiple networks and utilize labeled data, we present a practical guidance framework termed Practical Plug-And-Play (PPAP), which leverages parameter-efficient fine-tuning and data-free knowledge transfer. We exhaustively conduct ImageNet class conditional generation experiments to show that our method can successfully guide diffusion with small trainable parameters and no labeled data. Finally, we show that image classifiers, depth estimators, and semantic segmentation models can guide publicly available GLIDE through our framework in a plug-and-play manner. Our code is available at https://github.com/riiid/PPAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1852.Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models</span><br>
                <span class="as">Wu, QiuchengandLiu, YujianandZhao, HandongandKale, AjinkyaandBui, TrungandYu, TongandLin, ZheandZhang, YangandChang, Shiyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1900-1910.png><br>
            
            <span class="tt"><span class="t0">研究问题：探索扩散模型是否具有固有的解耦能力，即能否在不改变语义内容的情况下修改图像风格。<br>
                    动机：解耦能力是图像生成模型的重要特性，可以对不同图像进行通用的修改参数操作。<br>
                    方法：通过改变输入文本嵌入的风格描述，同时固定去噪过程中引入的高斯随机噪声，观察生成的图像是否能被修改为目标风格而不影响语义内容。<br>
                    效果：研究发现稳定的扩散模型具有这种解耦能力，进一步提出了一种简单、轻量级的图像编辑算法，该算法不需要微调扩散模型本身，只需优化两个文本嵌入的混合权重即可实现风格匹配和内容保留。实验证明该方法能修改多种属性，且性能优于需要微调的扩散模型基础的图像编辑算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., "a photo of person") to one with style (e.g., "a photo of person with smile") while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1853.SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation</span><br>
                <span class="as">Zhang, WenxuanandCun, XiaodongandWang, XuanandZhang, YongandShen, XiandGuo, YuandShan, YingandWang, Fei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8652-8661.png><br>
            
            <span class="tt"><span class="t0">研究问题：通过面部图像和语音音频生成谈话头部视频仍存在许多挑战，如不自然的头部运动、扭曲的表情和身份修改。<br>
                    动机：这些问题主要是由于从耦合的二维运动场中学习引起的。另一方面，显式使用三维信息也会导致表情僵硬和视频不一致的问题。<br>
                    方法：我们提出了SadTalker，它从音频中生成3DMM的三维运动系数（头部姿势、表情），并隐式地调整一种新的3D感知人脸渲染器进行谈话头部生成。为了学习真实的运动系数，我们显式地分别建模了音频与不同类型的运动系数之间的连接。具体来说，我们提出了ExpNet，通过蒸馏系数和3D渲染的人脸来从音频中学习准确的面部表情。对于头部姿势，我们设计了PoseVAE，通过条件变分自动编码器合成不同风格的头部运动。最后，生成的三维运动系数映射到所提出的面部渲染器的无监督三维关键点空间，以合成最终的视频。<br>
                    效果：我们进行了广泛的实验，结果显示我们的方法在运动和视频质量方面具有优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating talking head videos through a face image and a piece of speech audio still contains many challenges. i.e., unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly caused by learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render to synthesize the final video. We conducted extensive experiments to show the superior of our method in terms of motion and video quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1854.Scaling Up GANs for Text-to-Image Synthesis</span><br>
                <span class="as">Kang, MingukandZhu, Jun-YanandZhang, RichardandPark, JaesikandShechtman, EliandParis, SylvainandPark, Taesung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10124-10134.png><br>
            
            <span class="tt"><span class="t0">研究问题：我们能否扩大GAN的规模，使其从像LAION这样的大型数据集中受益？<br>
                    动机：最近文本到图像合成的成功引起了公众的广泛关注，这标志着设计生成图像模型的首选架构发生了根本性的变化。<br>
                    方法：我们引入了GigaGAN，一种新的GAN架构，它远远超过了这个限制，证明了GAN是文本到图像合成的一个可行选项。<br>
                    效果：GigaGAN在推理时间上快了几个数量级，只需0.13秒就能合成一张512px的图像。其次，它可以合成高分辨率的图像，例如，在3.66秒内合成16百万像素的图像。最后，GigaGAN支持各种潜在空间编辑应用，如潜在插值、风格混合和向量算术运算。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL-E 2, auto-regressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that naively increasing the capacity of the StyleGAN architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1855.A Soma Segmentation Benchmark in Full Adult Fly Brain</span><br>
                <span class="as">Liu, XiaoyuandHu, BoandLi, MingxingandHuang, WeiandZhang, YueyiandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_A_Soma_Segmentation_Benchmark_in_Full_Adult_Fly_Brain_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7402-7411.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从高分辨率电子显微镜（EM）数据中重建果蝇全脑的神经元，特别是对于神经细胞体（somas）的准确分布和形态。<br>
                    动机：由于缺乏专门注释了somas的EM数据集，现有的深度学习方法无法直接提供准确的soma分布和形态。同时，由于EM数据的庞大规模，全脑神经元重建过程非常耗时。<br>
                    方法：首先制作了一个具有精细3D手动注释somas的高分辨率EM数据集。然后，基于此数据集，提出了一种高效、两阶段的深度学习算法，用于预测3D soma实例的精确位置和边界。最后，为全脑执行上述算法部署了一个并行化的高吞吐量数据处理管道。<br>
                    效果：通过在测试集上进行定量和定性基准比较，验证了该方法的优越性。同时，从生物学角度对重建的果蝇全脑中的somas进行了初步统计。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neuron reconstruction in a full adult fly brain from high-resolution electron microscopy (EM) data is regarded as a cornerstone for neuroscientists to explore how neurons inspire intelligence. As the central part of neurons, somas in the full brain indicate the origin of neurogenesis and neural functions. However, due to the absence of EM datasets specifically annotated for somas, existing deep learning-based neuron reconstruction methods cannot directly provide accurate soma distribution and morphology. Moreover, full brain neuron reconstruction remains extremely time-consuming due to the unprecedentedly large size of EM data. In this paper, we develop an efficient soma reconstruction method for obtaining accurate soma distribution and morphology information in a full adult fly brain. To this end, we first make a high-resolution EM dataset with fine-grained 3D manual annotations on somas. Relying on this dataset, we propose an efficient, two-stage deep learning algorithm for predicting accurate locations and boundaries of 3D soma instances. Further, we deploy a parallelized, high-throughput data processing pipeline for executing the above algorithm on the full brain. Finally, we provide quantitative and qualitative benchmark comparisons on the testset to validate the superiority of the proposed method, as well as preliminary statistics of the reconstructed somas in the full adult fly brain from the biological perspective. We release our code and dataset at https://github.com/liuxy1103/EMADS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1856.DiffPose: Toward More Reliable 3D Pose Estimation</span><br>
                <span class="as">Gong, JiaandFoo, LinGengandFan, ZhipengandKe, QiuhongandRahmani, HosseinandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_DiffPose_Toward_More_Reliable_3D_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13041-13051.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目三维人体姿态估计由于其固有的模糊性和遮挡性，常常导致高度的不确定性和不明确性。<br>
                    动机：受扩散模型在从噪声中生成高质量图像方面的有效性启发，我们探索了一种新颖的姿态估计框架（DiffPose），将3D姿态估计表述为反向扩散过程。<br>
                    方法：我们在DiffPose中融入了新的设计以促进3D姿态估计的扩散过程：特定于姿态的姿态不确定性分布初始化、基于高斯混合模型的前向扩散过程以及上下文条件反向扩散过程。<br>
                    效果：我们提出的DiffPose在广泛使用的姿态估计基准Human3.6M和MPI-INF-3DHP上显著优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular 3D human pose estimation is quite challenging due to the inherent ambiguity and occlusion, which often lead to high uncertainty and indeterminacy. On the other hand, diffusion models have recently emerged as an effective tool for generating high-quality images from noise. Inspired by their capability, we explore a novel pose estimation framework (DiffPose) that formulates 3D pose estimation as a reverse diffusion process. We incorporate novel designs into our DiffPose to facilitate the diffusion process for 3D pose estimation: a pose-specific initialization of pose uncertainty distributions, a Gaussian Mixture Model-based forward diffusion process, and a context-conditioned reverse diffusion process. Our proposed DiffPose significantly outperforms existing methods on the widely used pose estimation benchmarks Human3.6M and MPI-INF-3DHP. Project page: https://gongjia0208.github.io/Diffpose/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1857.Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field</span><br>
                <span class="as">Li, LehengandLian, QingandWang, LuozhouandMa, NingningandChen, Ying-Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Lift3D_Synthesize_3D_Training_Data_by_Lifting_2D_GAN_to_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/332-341.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用3D生成模型合成训练数据以提升3D视觉任务的性能？<br>
                    动机：现有的基于NeRF的3D GANs由于其设计生成流程和缺乏明确的3D监督，难以生成与真实世界场景相匹配的高分辨率照片般真实的数据。<br>
                    方法：提出Lift3D，一种逆向2D到3D的生成框架，通过提升良好解缠的2D GAN到3D对象NeRF，为生成的对象提供明确的3D信息，从而为后续任务提供准确的3D标注。<br>
                    效果：通过在自动驾驶数据集上进行增强实验，结果显示该数据生成框架能有效提升3D物体检测器的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods: (1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output. (2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Code: len-li.github.io/lift3d-web</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1858.LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation</span><br>
                <span class="as">Zheng, GuangcongandZhou, XianpanandLi, XueweiandQi, ZhongangandShan, YingandLi, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22490-22499.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对复杂的多对象场景进行布局到图像的生成，实现全局布局图和每个详细对象的强控制。<br>
                    动机：现有的扩散模型在图像合成上取得了成功，但在处理具有复杂多对象场景的布局到图像生成任务时，如何实现更强的控制仍是一个挑战。<br>
                    方法：提出了一种名为LayoutDiffusion的扩散模型，通过构建带有区域信息的结构化图像补丁，并将其转换为特殊的布局以与正常布局进行统一融合，克服了图像和布局的多模态融合难题。同时，设计了布局融合模块（LFM）和对象感知交叉注意力（OaCA），以精确控制空间相关信息。<br>
                    效果：实验表明，我们的LayoutDiffusion在FID、CAS等指标上比先前的方法提高了46.35%、26.70%，在COCO-stuff和VG数据集上分别提高了44.29%、41.82%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, diffusion models have achieved great success in image synthesis. However, when it comes to the layout-to-image generation where an image often has a complex scene of multiple objects, how to make strong control over both the global layout map and each detailed object remains a challenging task. In this paper, we propose a diffusion model named LayoutDiffusion that can obtain higher generation quality and greater controllability than the previous works. To overcome the difficult multimodal fusion of image and layout, we propose to construct a structural image patch with region information and transform the patched image into a special layout to fuse with the normal layout in a unified form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention (OaCA) are proposed to model the relationship among multiple objects and designed to be object-aware and position-sensitive, allowing for precisely controlling the spatial related information. Extensive experiments show that our LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by relatively 46.35%, 26.70% on COCO-stuff and 44.29%, 41.82% on VG. Code is available at https://github.com/ZGCTroy/LayoutDiffusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1859.BBDM: Image-to-Image Translation With Brownian Bridge Diffusion Models</span><br>
                <span class="as">Li, BoandXue, KaitaoandLiu, BinandLai, Yu-Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BBDM_Image-to-Image_Translation_With_Brownian_Bridge_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1952-1961.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉和图像处理中的重要且具有挑战性的问题——图像到图像的转换。<br>
                    动机：现有的扩散模型在高质量的图像合成和图像到图像的转换任务上表现出了强大的潜力，但在不同领域的应用中存在明显的鸿沟。<br>
                    方法：提出了一种基于布朗桥扩散模型（BBDM）的新型图像到图像的转换方法，将图像到图像的转换建模为随机布朗桥过程，并通过双向扩散过程直接学习两个领域之间的转换，而不是通过条件生成过程。<br>
                    效果：实验结果表明，所提出的BBDM模型在各种基准测试上都取得了有竞争力的性能，无论是通过视觉检查还是可测量的指标。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-to-image translation is an important and challenging problem in computer vision and image processing. Diffusion models(DM) have shown great potentials for high-quality image synthesis, and have gained competitive performance on the task of image-to-image translation. However, most of the existing diffusion models treat image-to-image translation as conditional generation processes, and suffer heavily from the gap between distinct domains. In this paper, a novel image-to-image translation method based on the Brownian Bridge Diffusion Model(BBDM) is proposed, which models image-to-image translation as a stochastic Brownian Bridge process, and learns the translation between two domains directly through the bidirectional diffusion process rather than a conditional generation process. To the best of our knowledge, it is the first work that proposes Brownian Bridge diffusion process for image-to-image translation. Experimental results on various benchmarks demonstrate that the proposed BBDM model achieves competitive performance through both visual inspection and measurable metrics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1860.Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting</span><br>
                <span class="as">Wang, SuandSaharia, ChitwanandMontgomery, CesleeandPont-Tuset, JordiandNoy, ShaiandPellegrini, StefanoandOnoe, YasumasaandLaszlo, SarahandFleet, DavidJ.andSoricut, RaduandBaldridge, JasonandNorouzi, MohammadandAnderson, PeterandChan, William</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18359-18369.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成忠实于文本提示且与输入图像一致的编辑？<br>
                    动机：为了支持创新应用，需要一种能够对文本引导的图像进行编辑的技术。<br>
                    方法：通过在训练过程中引入对象检测器来提出修复掩模，构建了一个级联扩散模型Imagen Editor。<br>
                    效果：实验结果表明，Imagen Editor的编辑结果忠实于文本提示，并且在处理物体、属性和场景等方面表现出色，优于DALL-E 2和Stable Diffusion等模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-guided image editing can have a transformative impact in supporting creative applications. A key challenge is to generate edits that are faithful to the input text prompt, while consistent with the input image. We present Imagen Editor, a cascaded diffusion model, built by fine-tuning Imagen on text-guided image inpainting. Imagen Editor's edits are faithful to the text prompts, which is accomplished by incorporating object detectors for proposing inpainting masks during training. In addition, text-guided image inpainting captures fine details in the input image by conditioning the cascaded pipeline on the original high resolution image. To improve qualitative and quantitative evaluation, we introduce EditBench, a systematic benchmark for text-guided image inpainting. EditBench evaluates inpainting edits on natural and generated images exploring objects, attributes, and scenes. Through extensive human evaluation on EditBench, we find that object-masking during training leads to across-the-board improvements in text-image alignment -- such that Imagen Editor is preferred over DALL-E 2 and Stable Diffusion -- and, as a cohort, these models are better at object-rendering than text-rendering, and handle material/color/size attributes better than count/shape attributes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1861.OSRT: Omnidirectional Image Super-Resolution With Distortion-Aware Transformer</span><br>
                <span class="as">Yu, FanghuaandWang, XintaoandCao, MingdengandLi, GenandShan, YingandDong, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_OSRT_Omnidirectional_Image_Super-Resolution_With_Distortion-Aware_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13283-13292.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高全景图像的分辨率，以获取更丰富的沉浸式体验。<br>
                    动机：现有的方法通过等距投影图像的超分辨率来解决全景图像分辨率不足的问题，但忽视了等距投影在退化过程中的几何特性，且其模型难以泛化到真实的等距投影图像上。<br>
                    方法：我们提出了一种模仿真实世界成像过程的鱼眼降采样方法，合成了更真实的低分辨率样本。然后设计了一种失真感知的Transformer（OSRT）来连续和自适应地调整等距投影失真。<br>
                    效果：无需繁琐的过程，OSRT就能比之前的方法在PSNR上提高约0.2dB。此外，我们还提出了一种方便的数据增强策略，即从普通图像合成伪等距投影图像。这种简单的策略可以缓解大网络的过拟合问题，并显著提高全景图像超分辨率的性能。大量的实验表明，我们的OSRT具有最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Omnidirectional images (ODIs) have obtained lots of research interest for immersive experiences. Although ODIs require extremely high resolution to capture details of the entire scene, the resolutions of most ODIs are insufficient. Previous methods attempt to solve this issue by image super-resolution (SR) on equirectangular projection (ERP) images. However, they omit geometric properties of ERP in the degradation process, and their models can hardly generalize to real ERP images. In this paper, we propose Fisheye downsampling, which mimics the real-world imaging process and synthesizes more realistic low-resolution samples. Then we design a distortion-aware Transformer (OSRT) to modulate ERP distortions continuously and self-adaptively. Without a cumbersome process, OSRT outperforms previous methods by about 0.2dB on PSNR. Moreover, we propose a convenient data augmentation strategy, which synthesizes pseudo ERP images from plain images. This simple strategy can alleviate the over-fitting problem of large networks and significantly boost the performance of ODI SR. Extensive experiments have demonstrated the state-of-the-art performance of our OSRT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1862.KD-DLGAN: Data Limited Image Generation via Knowledge Distillation</span><br>
                <span class="as">Cui, KaiwenandYu, YingchenandZhan, FangnengandLiao, ShengcaiandLu, ShijianandXing, EricP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3872-3882.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用有限的训练数据，提高生成对抗网络（GAN）在图像生成任务上的效果。<br>
                    动机：当训练数据有限时，GAN的判别器往往会出现严重的过拟合现象，导致生成效果下降，尤其是生成多样性降低。<br>
                    方法：提出一种基于知识蒸馏的生成框架KD-GAN，引入预训练的视觉语言模型来训练有效的数据受限的图像生成模型。KD-GAN包含两个创新设计：一是聚合生成的知识蒸馏，通过给判别器提供更难的学习任务和从预训练模型中提炼更具有泛化性的知识，以减轻判别器的过拟合；二是相关的生成知识蒸馏，通过提炼和保留预训练模型中的多样化图像文本关联性，以提高生成的多样性。<br>
                    效果：大量实验表明，KD-GAN在多个基准测试中实现了优秀的图像生成效果，且与最先进的技术相比，其性能有稳定且显著的提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-GAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited image generation models. KD-GAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-GAN achieves superior image generation with limited training data. In addition, KD-GAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1863.HouseDiffusion: Vector Floorplan Generation via a Diffusion Model With Discrete and Continuous Denoising</span><br>
                <span class="as">Shabani, MohammadAminandHosseini, SepidehsadatandFurukawa, Yasutaka</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5466-5475.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新颖的矢量平面图生成方法，通过去噪模型对房间/门角的二维坐标进行去噪。<br>
                    动机：现有的矢量平面图生成方法无法精确地反转连续前向过程，也无法建立几何关联关系，如平行、垂直和共享角等。<br>
                    方法：本文提出的去噪模型采用Transformer架构作为核心，根据输入的图形约束控制注意力掩码，并通过离散和连续的去噪过程直接生成矢量图形平面图。<br>
                    效果：在RPLAN数据集上进行的评估表明，该方法在所有指标上都取得了显著改进，与最先进的方法相比具有显著的优势，同时能够生成非曼哈顿结构和控制每个房间的角的确切数量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The paper presents a novel approach for vector-floorplan generation via a diffusion model, which denoises 2D coordinates of room/door corners with two inference objectives: 1) a single-step noise as the continuous quantity to precisely invert the continuous forward process; and 2) the final 2D coordinate as the discrete quantity to establish geometric incident relationships such as parallelism, orthogonality, and corner-sharing. Our task is graph-conditioned floorplan generation, a common workflow in floorplan design. We represent a floorplan as 1D polygonal loops, each of which corresponds to a room or a door. Our diffusion model employs a Transformer architecture at the core, which controls the attention masks based on the input graph-constraint and directly generates vector-graphics floorplans via a discrete and continuous denoising process. We have evaluated our approach on RPLAN dataset. The proposed approach makes significant improvements in all the metrics against the state-of-the-art with significant margins, while being capable of generating non-Manhattan structures and controlling the exact number of corners per room. We will share all our code and models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1864.Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation</span><br>
                <span class="as">Zhao, RuiandLi, WeiandHu, ZhipengandLi, LinchengandZou, ZhengxiaandShi, ZhenweiandFan, Changjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Zero-Shot_Text-to-Parameter_Translation_for_Game_Character_Auto-Creation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21013-21023.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现零射击的文字驱动游戏角色自动创建。<br>
                    动机：现有的游戏角色自动创建系统大多基于图像，需要优化面部参数使渲染的角色与参考照片相似，缺乏个性化和自定义功能。<br>
                    方法：提出一种新的文本到参数转换方法（T2P），利用大规模预训练的多模态CLIP和神经渲染技术，在统一框架中搜索连续面部参数和离散面部参数。<br>
                    效果：实验结果表明，T2P可以生成高质量的生动游戏角色，并在客观评估和主观评估上都优于其他最先进的文本到3D生成方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent popular Role-Playing Games (RPGs) saw the great success of character auto-creation systems. The bone-driven face model controlled by continuous parameters (like the position of bones) and discrete parameters (like the hairstyles) makes it possible for users to personalize and customize in-game characters. Previous in-game character auto-creation systems are mostly image-driven, where facial parameters are optimized so that the rendered character looks similar to the reference face photo. This paper proposes a novel text-to-parameter translation method (T2P) to achieve zero-shot text-driven game character auto-creation. With our method, users can create a vivid in-game character with arbitrary text description without using any reference photo or editing hundreds of parameters manually. In our method, taking the power of large-scale pre-trained multi-modal CLIP and neural rendering, T2P searches both continuous facial parameters and discrete facial parameters in a unified framework. Due to the discontinuous parameter representation, previous methods have difficulty in effectively learning discrete facial parameters. T2P, to our best knowledge, is the first method that can handle the optimization of both discrete and continuous parameters. Experimental results show that T2P can generate high-quality and vivid game characters with given text prompts. T2P outperforms other SOTA text-to-3D generation methods on both objective evaluations and subjective evaluations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1865.Generating Holistic 3D Human Motion From Speech</span><br>
                <span class="as">Yi, HongweiandLiang, HualinandLiu, YifeiandCao, QiongandWen, YandongandBolkart, TimoandTao, DachengandBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/469-480.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从人类语音中生成3D全身运动。<br>
                    动机：目前的模型在生成语音对应的全身运动时，无法同时生成真实且多样化的面部表情、身体姿态和手势。<br>
                    方法：构建了一个高质量的同步语音3D全身网格数据集，并定义了一种新的语音到运动的生成框架，将面部、身体和手部分别建模。面部采用自动编码器进行建模，而身体姿态和手势则采用组合式向量量化变分自编码器（VQ-VAE）。此外，还提出了一种跨条件自回归模型来生成身体姿态和手势，以产生连贯且真实的运动。<br>
                    效果：实验和用户研究表明，该方法在定性和定量上都达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work addresses the problem of generating 3D holistic body motions from human speech. Given a speech recording, we synthesize sequences of 3D body poses, hand gestures, and facial expressions that are realistic and diverse. To achieve this, we first build a high-quality dataset of 3D holistic body meshes with synchronous speech. We then define a novel speech-to-motion generation framework in which the face, body, and hands are modeled separately. The separated modeling stems from the fact that face articulation strongly correlates with human speech, while body poses and hand gestures are less correlated. Specifically, we employ an autoencoder for face motions, and a compositional vector-quantized variational autoencoder (VQ-VAE) for the body and hand motions. The compositional VQ-VAE is key to generating diverse results. Additionally, we propose a cross conditional autoregressive model that generates body poses and hand gestures, leading to coherent and realistic motions. Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively. Our novel dataset and code will be released for research purposes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1866.Unifying Layout Generation With a Decoupled Diffusion Model</span><br>
                <span class="as">Hui, MudeandZhang, ZhizhengandZhang, XiaoyiandXie, WenxuanandWang, YuwangandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Unifying_Layout_Generation_With_a_Decoupled_Diffusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1942-1951.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决布局生成任务中的各种子任务的统一问题，包括有条件和无条件的生成。<br>
                    动机：布局生成是减轻繁重的图形设计工作负担的重要任务，但不同的应用场景对布局生成提出了挑战，需要统一各种子任务。<br>
                    方法：本文提出了一种布局扩散生成模型（LDGM），通过单一的解耦扩散模型实现子任务的统一。LDGM将任意缺失或粗糙的元素属性布局视为从完整布局开始的中间扩散状态。由于不同的属性具有各自的语义和特性，我们提出解耦它们的扩散过程，以提高训练样本的多样性，并联合学习反向过程以利用全局范围的上下文来促进生成。<br>
                    效果：实验结果表明，我们的LDGM在功能和性能上都优于现有的布局生成模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Layout generation aims to synthesize realistic graphic scenes consisting of elements with different attributes including category, size, position, and between-element relation. It is a crucial task for reducing the burden on heavy-duty graphic design works for formatted scenes, e.g., publications, documents, and user interfaces (UIs). Diverse application scenarios impose a big challenge in unifying various layout generation subtasks, including conditional and unconditional generation. In this paper, we propose a Layout Diffusion Generative Model (LDGM) to achieve such unification with a single decoupled diffusion model. LDGM views a layout of arbitrary missing or coarse element attributes as an intermediate diffusion status from a completed layout. Since different attributes have their individual semantics and characteristics, we propose to decouple the diffusion processes for them to improve the diversity of training samples and learn the reverse process jointly to exploit global-scope contexts for facilitating generation. As a result, our LDGM can generate layouts either from scratch or conditional on arbitrary available attributes. Extensive qualitative and quantitative experiments demonstrate our proposed LDGM outperforms existing layout generation models in both functionality and performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1867.Human Guided Ground-Truth Generation for Realistic Image Super-Resolution</span><br>
                <span class="as">Chen, DuandLiang, JieandZhang, XindongandLiu, MingandZeng, HuiandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Human_Guided_Ground-Truth_Generation_for_Realistic_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14082-14091.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成真实的图像超分辨率（Real-ISR）模型的地面实况（GT）图像是一个关键问题。<br>
                    动机：现有的方法主要采用一组高分辨率（HR）图像作为GT，并应用各种降级技术来模拟其低分辨率（LR）对应物。虽然取得了很大的进展，但这种LR-HR对生成方案存在几个限制。首先，HR图像的感知质量可能不够高，限制了Real-ISR输出的质量。其次，现有的方案在GT生成时没有充分考虑人的感知，训练的模型往往产生过度平滑的结果或令人不愉快的伪影。<br>
                    方法：我们提出了一种人引导的GT生成方案。我们首先训练多个图像增强模型以提高HR图像的感知质量，并使一张LR图像具有多个HR对应物。然后让人类参与者标注增强的HR图像中高质量的区域作为GT，并将带有不愉快伪影的区域标记为负样本。然后构建一个包含正负样本的人引导GT图像数据集，并提出一个损失函数来训练Real-ISR模型。<br>
                    效果：实验表明，在我们数据集上训练的Real-ISR模型可以产生更具感知真实性且伪影更少的结果。数据集和代码可在https://github.com/ChrisDud0257/HGGT找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How to generate the ground-truth (GT) image is a critical issue for training realistic image super-resolution (Real-ISR) models. Existing methods mostly take a set of high-resolution (HR) images as GTs and apply various degradations to simulate their low-resolution (LR) counterparts. Though great progress has been achieved, such an LR-HR pair generation scheme has several limitations. First, the perceptual quality of HR images may not be high enough, limiting the quality of Real-ISR outputs. Second, existing schemes do not consider much human perception in GT generation, and the trained models tend to produce over-smoothed results or unpleasant artifacts. With the above considerations, we propose a human guided GT generation scheme. We first elaborately train multiple image enhancement models to improve the perceptual quality of HR images, and enable one LR image having multiple HR counterparts. Human subjects are then involved to annotate the high quality regions among the enhanced HR images as GTs, and label the regions with unpleasant artifacts as negative samples. A human guided GT image dataset with both positive and negative samples is then constructed, and a loss function is proposed to train the Real-ISR models. Experiments show that the Real-ISR models trained on our dataset can produce perceptually more realistic results with less artifacts. Dataset and codes can be found at https://github.com/ChrisDud0257/HGGT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1868.SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene</span><br>
                <span class="as">Son, MinjungandPark, JeongJoonandGuibas, LeonidasandWetzstein, Gordon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Son_SinGRAF_Learning_a_3D_Generative_Radiance_Field_for_a_Single_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8507-8517.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用少量输入图像训练一种3D感知的生成模型SinGRAF，以生成不同场景布局的3D场景。<br>
                    动机：现有的3D生成模型需要大量训练数据，而SinGRAF通过使用单一场景的几张输入图像进行训练，可以更好地利用有限的数据资源。<br>
                    方法：基于最新的3D GAN架构，引入一种新的渐进尺度补丁判别方法进行训练。<br>
                    效果：实验结果表明，SinGRAF在质量和多样性方面都优于最相关的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative models have shown great promise in synthesizing photorealistic 3D objects, but they require large amounts of training data. We introduce SinGRAF, a 3D-aware generative model that is trained with a few input images of a single scene. Once trained, SinGRAF generates different realizations of this 3D scene that preserve the appearance of the input while varying scene layout. For this purpose, we build on recent progress in 3D GAN architectures and introduce a novel progressive-scale patch discrimination approach during training. With several experiments, we demonstrate that the results produced by SinGRAF outperform the closest related works in both quality and diversity by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1869.Dimensionality-Varying Diffusion Process</span><br>
                <span class="as">Zhang, HanandFeng, RuiliandYang, ZhantaoandHuang, LianghuaandLiu, YuandZhang, YifeiandShen, YujunandZhao, DeliandZhou, JingrenandCheng, Fan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Dimensionality-Varying_Diffusion_Process_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14307-14316.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型在生成新数据时需要每一步的信号维度相同，但作者认为对于图像信号的空间冗余性，无需在演变过程中保持高维度。<br>
                    动机：考虑到图像信号的空间冗余性，特别是在早期生成阶段，不需要维持高维度。<br>
                    方法：通过信号分解对前向扩散过程进行理论推广。具体来说，将图像分解为多个正交分量，并在干扰图像时控制每个分量的衰减。随着噪声强度的增加，可以减小那些无关紧要的分量，从而使用较低维的信号来表示源，几乎不丢失信息。<br>
                    效果：这种方法大大减少了计算成本，并在一系列数据集上实现了与基线方法相当甚至更好的合成性能。同时，该方法也有助于高分辨率图像合成，并提高了在1024x1024分辨率下训练的FFHQ扩散模型的FID值（从52.40降至10.46）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models, which learn to reverse a signal destruction process to generate new data, typically require the signal at each step to have the same dimension. We argue that, considering the spatial redundancy in image signals, there is no need to maintain a high dimensionality in the evolution process, especially in the early generation phase. To this end, we make a theoretical generalization of the forward diffusion process via signal decomposition. Concretely, we manage to decompose an image into multiple orthogonal components and control the attenuation of each component when perturbing the image. That way, along with the noise strength increasing, we are able to diminish those inconsequential components and thus use a lower-dimensional signal to represent the source, barely losing information. Such a reformulation allows to vary dimensions in both training and inference of diffusion models. Extensive experiments on a range of datasets suggest that our approach substantially reduces the computational cost and achieves on-par or even better synthesis performance compared to baseline methods. We also show that our strategy facilitates high-resolution image synthesis and improves FID of diffusion model trained on FFHQ at 1024x1024 resolution from 52.40 to 10.46. Code is available at https://github.com/damo-vilab/dvdp.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1870.RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation</span><br>
                <span class="as">Anciukevi\v{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12608-12618.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的图像扩散模型在条件和无条件的图像生成上取得了最先进的性能，但不支持3D理解所需的任务，如视图一致的3D生成或单视图对象重建。<br>
                    动机：本文提出了第一个用于3D生成和推理的扩散模型RenderDiffusion，该模型仅使用单目2D监督进行训练。<br>
                    方法：我们的方法的核心是一个新颖的图像去噪架构，它在每个去噪步骤中生成并渲染场景的中间三维表示。这在扩散过程中强制了一个强烈的归纳结构，提供了一种3D一致的表示，同时只需要2D监督。<br>
                    效果：我们在FFHQ、AFHQ、ShapeNet和CLEVR数据集上评估了RenderDiffusion，结果显示其在3D场景生成和从2D图像推理3D场景方面具有竞争力。此外，我们的基于扩散的方法允许我们使用2D修复来编辑3D场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Additionally, our diffusion-based approach allows us to use 2D inpainting to edit 3D scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1871.Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures</span><br>
                <span class="as">Metzer, GalandRichardson, EladandPatashnik, OrandGiryes, RajaandCohen-Or, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12663-12673.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用文本引导3D模型生成？<br>
                    动机：近年来，文本引导图像生成取得了快速进展，激发了在文本引导形状生成方面的主要突破。<br>
                    方法：将得分蒸馏应用于计算效率高的预训练自动编码器的潜伏扩散模型，并将NeRF模型引入潜伏空间，形成潜伏-NeRF。同时，使用草图形状作为约束直接指导潜伏-NeRF的3D生成过程。<br>
                    效果：实验证明，通过文本和形状的双重引导，可以增加对生成过程的控制力。此外，得分蒸馏也可以直接应用于3D网格，从而在给定几何体上生成高质量的纹理。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-guided image generation has progressed rapidly in recent years, inspiring major breakthroughs in text-guided shape generation. Recently, it has been shown that using score distillation, one can successfully text-guide a NeRF model to generate a 3D object. We adapt the score distillation to the publicly available, and computationally efficient, Latent Diffusion Models, which apply the entire diffusion process in a compact latent space of a pretrained autoencoder. As NeRFs operate in image space, a naive solution for guiding them with latent score distillation would require encoding to the latent space at each guidance step. Instead, we propose to bring the NeRF to the latent space, resulting in a Latent-NeRF. Analyzing our Latent-NeRF, we show that while Text-to-3D models can generate impressive results, they are inherently unconstrained and may lack the ability to guide or enforce a specific 3D structure. To assist and direct the 3D generation, we propose to guide our Latent-NeRF using a Sketch-Shape: an abstract geometry that defines the coarse structure of the desired object. Then, we present means to integrate such a constraint directly into a Latent-NeRF. This unique combination of text and shape guidance allows for increased control over the generation process. We also show that latent score distillation can be successfully applied directly on 3D meshes. This allows for generating high-quality textures on a given geometry. Our experiments validate the power of our different forms of guidance and the efficiency of using latent rendering.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1872.Learning Generative Structure Prior for Blind Text Image Super-Resolution</span><br>
                <span class="as">Li, XiaomingandZuo, WangmengandLoy, ChenChange</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_Generative_Structure_Prior_for_Blind_Text_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10103-10113.png><br>
            
            <span class="tt"><span class="t0">研究问题：盲文图像超分辨率（SR）在处理不同字体风格和未知退化时具有挑战性。<br>
                    动机：现有的方法通过损失约束或中间特征条件来并行执行字符识别以规范SR任务，但在遇到严重退化时，高级先验可能会失败。<br>
                    方法：我们提出了一种新的先验方法，该方法更关注字符结构。具体来说，我们在StyleGAN中学习封装丰富多样的结构，并利用这种生成结构先验进行恢复。为了限制StyleGAN的生成空间，使其遵守字符的结构，同时保持对不同字体风格的灵活性，我们将每个字符的离散特征存储在一个代码簿中。该代码随后驱动StyleGAN生成高分辨率的结构细节以帮助文本SR。<br>
                    效果：与基于字符识别的先验相比，所提出的结构先验对指定字符的忠实和精确的笔画施加更强的字符特定指导。在合成和真实数据集上的大量实验表明，所提出的生成结构先验在促进鲁棒的文本SR方面具有引人注目的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Blind text image super-resolution (SR) is challenging as one needs to cope with diverse font styles and unknown degradation. To address the problem, existing methods perform character recognition in parallel to regularize the SR task, either through a loss constraint or intermediate feature condition. Nonetheless, the high-level prior could still fail when encountering severe degradation. The problem is further compounded given characters of complex structures, e.g., Chinese characters that combine multiple pictographic or ideographic symbols into a single character. In this work, we present a novel prior that focuses more on the character structure. In particular, we learn to encapsulate rich and diverse structures in a StyleGAN and exploit such generative structure priors for restoration. To restrict the generative space of StyleGAN so that it obeys the structure of characters yet remains flexible in handling different font styles, we store the discrete features for each character in a  codebook . The code subsequently drives the StyleGAN to generate high-resolution structural details to aid text SR. Compared to priors based on character recognition, the proposed structure prior exerts stronger character-specific guidance to restore faithful and precise strokes of a designated character. Extensive experiments on synthetic and real datasets demonstrate the compelling performance of the proposed generative structure prior in facilitating robust text SR. Our code is available at https://github.com/csxmli2016/MARCONet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1873.LayoutDM: Discrete Diffusion Model for Controllable Layout Generation</span><br>
                <span class="as">Inoue, NaotoandKikuchi, KotaroandSimo-Serra, EdgarandOtani, MayuandYamaguchi, Kota</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Inoue_LayoutDM_Discrete_Diffusion_Model_for_Controllable_Layout_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10167-10176.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决通过离散状态空间扩散模型在单一模型中解决广泛的布局生成任务的问题。<br>
                    动机：目前的布局生成任务需要处理各种可选的约束条件，如特定元素的类型或位置。因此，需要一个能够处理结构化布局数据的模型。<br>
                    方法：本文提出了一种基于离散状态空间扩散模型的布局生成模型LayoutDM，该模型可以自然地处理结构化布局数据，并通过逐步推理从初始输入中推断出无噪声的布局。<br>
                    效果：实验结果表明，LayoutDM能够成功生成高质量的布局，并在多个布局任务上优于特定任务和通用任务的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Controllable layout generation aims at synthesizing plausible arrangement of element bounding boxes with optional constraints, such as type or position of a specific element. In this work, we try to solve a broad range of layout generation tasks in a single model that is based on discrete state-space diffusion models. Our model, named LayoutDM, naturally handles the structured layout data in the discrete representation and learns to progressively infer a noiseless layout from the initial input, where we model the layout corruption process by modality-wise discrete diffusion. For conditional generation, we propose to inject layout constraints in the form of masking or logit adjustment during inference. We show in the experiments that our LayoutDM successfully generates high-quality layouts and outperforms both task-specific and task-agnostic baselines on several layout tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1874.PREIM3D: 3D Consistent Precise Image Attribute Editing From a Single Image</span><br>
                <span class="as">Li, JianhuiandLi, JianminandZhang, HaojiandLiu, ShilongandWang, ZhengyiandXiao, ZihaoandZheng, KaiwenandZhu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_PREIM3D_3D_Consistent_Precise_Image_Attribute_Editing_From_a_Single_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8549-8558.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了3D感知的图像属性编辑问题，该问题在实际应用中具有广泛的应用。<br>
                    动机：尽管现有的方法在输入视图附近取得了有希望的结果，但在大相机姿态下生成的图像仍然存在3D不一致性和图像属性编辑不精确的问题。<br>
                    方法：我们训练了一个共享编码器来映射所有图像，并提出了两种新的方法来解决大相机姿态下的3D不一致性和主体身份问题。同时，我们将GAN模型的隐空间和逆映射进行比较，发现在逆映射中进行编辑可以获得更好的结果。<br>
                    效果：实验结果表明，我们的方法生成的图像更具3D一致性，并且比之前的工作实现了更精确的图像编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the 3D-aware image attribute editing problem in this paper, which has wide applications in practice. Recent methods solved the problem by training a shared encoder to map images into a 3D generator's latent space or by per-image latent code optimization and then edited images in the latent space. Despite their promising results near the input view, they still suffer from the 3D inconsistency of produced images at large camera poses and imprecise image attribute editing, like affecting unspecified attributes during editing. For more efficient image inversion, we train a shared encoder for all images. To alleviate 3D inconsistency at large camera poses, we propose two novel methods, an alternating training scheme and a multi-view identity loss, to maintain 3D consistency and subject identity. As for imprecise image editing, we attribute the problem to the gap between the latent space of real images and that of generated images. We compare the latent space and inversion manifold of GAN models and demonstrate that editing in the inversion manifold can achieve better results in both quantitative and qualitative evaluations. Extensive experiments show that our method produces more 3D consistent images and achieves more precise image editing than previous work. Source code and pretrained models can be found on our project page: https://mybabyyh.github.io/Preim3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1875.MaskSketch: Unpaired Structure-Guided Masked Image Generation</span><br>
                <span class="as">Bashkirova, DinaandLezama, Jos\&#x27;eandSohn, KihyukandSaenko, KateandEssa, Irfan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bashkirova_MaskSketch_Unpaired_Structure-Guided_Masked_Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1879-1889.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的图像生成方法主要通过标签或文本提示进行条件化，限制了对生成结果的控制。<br>
                    动机：提出一种名为MaskSketch的图像生成方法，使用指导草图作为额外的条件信号进行采样，实现空间条件化的生成结果。<br>
                    方法：利用预训练的掩蔽生成变压器，无需模型训练或配对监督，可处理不同抽象程度的输入草图。通过观察发现，掩蔽生成变压器的中间自我注意力映射编码了输入图像的重要结构信息，如场景布局和物体形状。基于此观察，提出了一种新的基于结构的采样方法以实现结构化引导的生成。<br>
                    效果：实验结果表明，MaskSketch能够实现高度真实的图像生成，并对指导结构具有良好的保真度。在标准基准数据集上评估，MaskSketch在草图到图像转换以及无配对图像到图像转换方面优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent conditional image generation methods produce images of remarkable diversity, fidelity and realism. However, the majority of these methods allow conditioning only on labels or text prompts, which limits their level of control over the generation result. In this paper, we introduce MaskSketch, an image generation method that allows spatial conditioning of the generation result using a guiding sketch as an extra conditioning signal during sampling. MaskSketch utilizes a pre-trained masked generative transformer, requiring no model training or paired supervision, and works with input sketches of different levels of abstraction. We show that intermediate self-attention maps of a masked generative transformer encode important structural information of the input image, such as scene layout and object shape, and we propose a novel sampling method based on this observation to enable structure-guided generation. Our results show that MaskSketch achieves high image realism and fidelity to the guiding structure. Evaluated on standard benchmark datasets, MaskSketch outperforms state-of-the-art methods for sketch-to-image translation, as well as unpaired image-to-image translation approaches. The code can be found on our project website: https://masksketch.github.io/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1876.Sequential Training of GANs Against GAN-Classifiers Reveals Correlated &#x27;&#x27;Knowledge Gaps&#x27;&#x27; Present Among Independently Trained GAN Instances</span><br>
                <span class="as">Pathak, ArkanathandDufour, Nicholas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pathak_Sequential_Training_of_GANs_Against_GAN-Classifiers_Reveals_Correlated_Knowledge_Gaps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24460-24469.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在探索生成对抗网络（GANs）训练过程中的知识差距，以及通过研究问题：本研究旨在探索生成对抗网络（GANs）训练过程中的知识差距，以及通过迭代训练GAN分类器和“欺骗”分类器的GAN来填补这些差距的效果。<br>
                    动机：先前的研究表明，在GAN训练中存在知识差距（样本间的分布不一致现象），并且可以通过训练与判别器分开的GAN分类器来证明这一点。本研究希望通过迭代训练GAN分类器和“欺骗”分类器的GAN来填补这些知识差距。<br>
                    方法：本研究在两个设置下进行实验，一个是在低维图像（MNIST）上训练的小尺寸DCGAN架构，另一个是在高维图像（FFHQ）上训练的最新GAN架构StyleGAN2。通过迭代训练GAN分类器和“欺骗”分类器的GAN，观察其对GAN训练动态、输出质量和GAN分类器泛化的影响。<br>
                    效果：研究发现，DCGAN无法在不影响输出质量的情况下有效地“欺骗”保留的GAN分类器。然而，StyleGAN2可以在不影响输出质量的情况下“欺骗”保留的分类器，并且这种效果在多个回合的GAN/分类器训练中持续存在，这似乎揭示了生成器参数空间中的优化顺序。最后，研究了不同的分类器架构，并表明GAN分类器的架构对其学习到的artifacts有强烈影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern Generative Adversarial Networks (GANs) generate realistic images remarkably well. Previous work has demonstrated the feasibility of "GAN-classifiers" that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of "knowledge gaps" (out-of-distribution artifacts across samples) present in GAN training. We iteratively train GAN-classifiers and train GANs that "fool" the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ). We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality. However, StyleGAN2 can fool held-out classifiers with no change in output quality, and this effect persists over multiple rounds of GAN/classifier training which appears to reveal an ordering over optima in the generator parameter space. Finally, we study different classifier architectures and show that the architecture of the GAN-classifier has a strong influence on the set of its learned artifacts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1877.Lookahead Diffusion Probabilistic Models for Refining Mean Estimation</span><br>
                <span class="as">Zhang, GuoqiangandNiwa, KentaandKleijn, W.Bastiaan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Lookahead_Diffusion_Probabilistic_Models_for_Refining_Mean_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1421-1429.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高扩散概率模型中条件高斯分布均值估计的准确性。<br>
                    动机：现有的扩散概率模型在处理深度神经网络输出的后续时间步长上的相关性时，无法精确地提炼条件高斯分布的均值估计。<br>
                    方法：提出一种前瞻性扩散概率模型（LA-DPM），通过在扩散概率模型中引入额外的连接，对最近的状态和索引进行前向传播，以获取更准确的数据样本x的估计。<br>
                    效果：实验表明，将此额外连接插入DDPM、DDIM、DEIS、S-PNDM和高阶DPM-Solvers等模型中，可以显著提高Frechet inception距离（FID）得分，从而改善模型性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose lookahead diffusion probabilistic models (LA-DPMs) to exploit the correlation in the outputs of the deep neural networks (DNNs) over subsequent timesteps in diffusion probabilistic models (DPMs) to refine the mean estimation of the conditional Gaussian distributions in the backward process. A typical DPM first obtains an estimate of the original data sample x by feeding the most recent state z_i and index i into the DNN model and then computes the mean vector of the conditional Gaussian distribution for z_ i-1 . We propose to calculate a more accurate estimate for x by performing extrapolation on the two estimates of x that are obtained by feeding (z_ i+1 , i+1) and (z_i, i) into the DNN model. The extrapolation can be easily integrated into the backward process of existing DPMs by introducing an additional connection over two consecutive timesteps, and fine-tuning is not required. Extensive experiments showed that plugging in the additional connection into DDPM, DDIM, DEIS, S-PNDM, and high-order DPM-Solvers leads to a significant performance gain in terms of Frechet inception distance (FID) score. Our implementation is available at https://github.com/guoqiangzhang-x/LA-DPM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1878.PyramidFlow: High-Resolution Defect Contrastive Localization Using Pyramid Normalizing Flow</span><br>
                <span class="as">Lei, JiaruiandHu, XiaoboandWang, YueandLiu, Dong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_PyramidFlow_High-Resolution_Defect_Contrastive_Localization_Using_Pyramid_Normalizing_Flow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14143-14152.png><br>
            
            <span class="tt"><span class="t0">研究问题：工业生产过程中，由于不可控因素，产品可能出现无法预见的缺陷。尽管无监督方法在缺陷定位上取得了成功，但常用的预训练模型会导致低分辨率的输出，影响视觉性能。<br>
                    动机：为了解决这个问题，我们提出了PyramidFlow，这是第一个没有预训练模型的全归一化流方法，可以实现高分辨率的缺陷定位。<br>
                    方法：我们提出了一种基于潜在模板的缺陷对比定位范式，以减少类内方差，这与预训练模型的做法不同。此外，PyramidFlow利用类似金字塔的归一化流进行多尺度融合和体积归一化，以帮助泛化。<br>
                    效果：我们在MVTecAD上的全面研究表明，所提出的方法优于不使用外部先验的可比算法，甚至在更具挑战性的BTAD场景中实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>During industrial processing, unforeseen defects may arise in products due to uncontrollable factors. Although unsupervised methods have been successful in defect localization, the usual use of pre-trained models results in low-resolution outputs, which damages visual performance. To address this issue, we propose PyramidFlow, the first fully normalizing flow method without pre-trained models that enables high-resolution defect localization. Specifically, we propose a latent template-based defect contrastive localization paradigm to reduce intra-class variance, as the pre-trained models do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for multi-scale fusing and volume normalization to help generalization. Our comprehensive studies on MVTecAD demonstrate the proposed method outperforms the comparable algorithms that do not use external priors, even achieving state-of-the-art performance in more challenging BTAD scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1879.DF-Platter: Multi-Face Heterogeneous Deepfake Dataset</span><br>
                <span class="as">Narayan, KartikandAgarwal, HarshandThakral, KartikandMittal, SurbhiandVatsa, MayankandSingh, Richa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9739-9748.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度伪造检测在学术界的重要性日益增长，尤其是在高质量图像和视频方面的研究。然而，目前的生成算法已经能够生成低分辨率视频、遮挡的深度伪造以及多主体深度伪造。<br>
                    动机：本研究模拟了深度伪造生成和传播的真实世界场景，并提出了DF-Platter数据集，该数据集包含使用多种生成技术生成的低分辨率和高分辨率深度伪造，以及单主体和多主体深度伪造，面部图像为印度裔。<br>
                    方法：我们使用32个GPU连续工作116天，累计使用了1800GB内存来准备这个数据库。数据集大小超过500GB，总共包含133,260个视频，分为三组。我们还提供了在多个评估设置下使用流行和最先进的深度伪造检测模型的基准结果。<br>
                    效果：实验结果表明，现有的技术在低分辨率深度伪造和多主体深度伪造上的性能大幅下降。我们断言，这个数据库将通过将深度伪造检测算法的能力扩展到真实世界场景来提高现有技术水平。该数据库可在http://iab-rubric.org/df-platter-database获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deepfake detection is gaining significant importance in the research community. While most of the research efforts are focused around high-quality images and videos, deepfake generation algorithms today have the capability to generate low-resolution videos, occluded deepfakes, and multiple-subject deepfakes. In this research, we emulate the real-world scenario of deepfake generation and spreading, and propose the DF-Platter dataset, which contains (i) both low-resolution and high-resolution deepfakes generated using multiple generation techniques and (ii) single-subject and multiple-subject deepfakes, with face images of Indian ethnicity. Faces in the dataset are annotated for various attributes such as gender, age, skin tone, and occlusion. The database is prepared in 116 days with continuous usage of 32 GPUs accounting to 1,800 GB cumulative memory. With over 500 GBs in size, the dataset contains a total of 133,260 videos encompassing three sets. To the best of our knowledge, this is one of the largest datasets containing vast variability and multiple challenges. We also provide benchmark results under multiple evaluation settings using popular and state-of-the-art deepfake detection models. Further, benchmark results under c23 and c40 compression are provided. The results demonstrate a significant performance reduction in the deepfake detection task on low-resolution deepfakes and show that the existing techniques fail drastically on multiple-subject deepfakes. It is our assertion that this database will improve the state-of-the-art by extending the capabilities of deepfake detection algorithms to real-world scenarios. The database is available at: http://iab-rubric.org/df-platter-database.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1880.Robust Unsupervised StyleGAN Image Restoration</span><br>
                <span class="as">Poirier-Ginter, YohanandLalonde, Jean-Fran\c{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Poirier-Ginter_Robust_Unsupervised_StyleGAN_Image_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22292-22301.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有无监督方法在图像恢复任务中需要对每个任务和退化级别进行仔细调整的问题。<br>
                    动机：为了提高图像恢复的鲁棒性，使一个超参数集能适用于广泛的退化级别，并处理多种退化的组合，而无需重新调整。<br>
                    方法：提出了一种基于StyleGAN的图像恢复方法，该方法依赖于3阶段的渐进潜在空间扩展和保守优化器，避免了任何额外的正则化项的需求。<br>
                    效果：通过大量的实验表明，该方法在各种退化级别的图像修复、上采样、去噪和去伪影等任务上都表现出了强大的鲁棒性，优于其他基于StyleGAN的反转技术。与扩散基恢复相比，该方法也产生了更真实的反转结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>GAN-based image restoration inverts the generative process to repair images corrupted by known degradations. Existing unsupervised methods must carefully be tuned for each task and degradation level. In this work, we make StyleGAN image restoration robust: a single set of hyperparameters works across a wide range of degradation levels. This makes it possible to handle combinations of several degradations, without the need to retune. Our proposed approach relies on a 3-phase progressive latent space extension and a conservative optimizer, which avoids the need for any additional regularization terms. Extensive experiments demonstrate robustness on inpainting, upsampling, denoising, and deartifacting at varying degradations levels, outperforming other StyleGAN-based inversion techniques. Our approach also favorably compares to diffusion-based restoration by yielding much more realistic inversion results. Code will be released upon publication.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1881.Blemish-Aware and Progressive Face Retouching With Limited Paired Data</span><br>
                <span class="as">Xie, LianxinandXue, WenandXu, ZhenandWu, SiandYu, ZhiwenandWong, HauSan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Blemish-Aware_and_Progressive_Face_Retouching_With_Limited_Paired_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5599-5608.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决面部修饰中的主要挑战，即在去除面部瑕疵的同时保持图像的文本细节。<br>
                    动机：现有的面部修饰方法需要昂贵的配对训练数据，且耗时耗力。因此，本文提出了一种能够区分瑕疵和面部特征（如痣）的新颖方法。<br>
                    方法：本文提出的模型分为两个阶段进行逐步的瑕疵去除。首先，编码器-解码器模块学习粗略地去除瑕疵；然后，将得到的中间特征注入生成器以丰富局部细节。同时，通过引入注意力模块来抑制瑕疵，进一步提高模型性能。<br>
                    效果：实验结果表明，该方法在各种面部修饰任务上取得了显著的性能提升。并且，通过对无配对样本施加有效的正则化，降低了对配对训练样本的依赖。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face retouching aims to remove facial blemishes, while at the same time maintaining the textual details of a given input image. The main challenge lies in distinguishing blemishes from the facial characteristics, such as moles. Training an image-to-image translation network with pixel-wise supervision suffers from the problem of expensive paired training data, since professional retouching needs specialized experience and is time-consuming. In this paper, we propose a Blemish-aware and Progressive Face Retouching model, which is referred to as BPFRe. Our framework can be partitioned into two manageable stages to perform progressive blemish removal. Specifically, an encoder-decoder-based module learns to coarsely remove the blemishes at the first stage, and the resulting intermediate features are injected into a generator to enrich local detail at the second stage. We find that explicitly suppressing the blemishes can contribute to an effective collaboration among the components. Toward this end, we incorporate an attention module, which learns to infer a blemish-aware map and further determine the corresponding weights, which are then used to refine the intermediate features transferred from the encoder to the decoder, and from the decoder to the generator. Therefore, BPFRe is able to deliver significant performance gains on a wide range of face retouching tasks. It is worth noting that we reduce the dependence of BPFRe on paired training samples by imposing effective regularization on unpaired ones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1882.Synthesizing Photorealistic Virtual Humans Through Cross-Modal Disentanglement</span><br>
                <span class="as">Ravichandran, SiddarthandTexler, Ond\v{r</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ravichandran_Synthesizing_Photorealistic_Virtual_Humans_Through_Cross-Modal_Disentanglement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4585-4594.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成高质量的虚拟人脸，并实现准确的唇部运动和实时运行。<br>
                    动机：随着虚拟领域的增强，如数字助手和元宇宙，需要生成逼真的人类视觉描绘，但目前的深度伪造和头部生成方法在质量、唇同步、分辨率等方面存在不足，且无法实时运行。<br>
                    方法：提出一种端到端的框架，利用音节作为中间音频表示，采用分层图像合成策略进行数据增强，以分离控制全局头部运动的模态。<br>
                    效果：该方法能够实时运行，并在质量、唇部运动等方面优于当前最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Over the last few decades, many aspects of human life have been enhanced with virtual domains, from the advent of digital assistants such as Amazon's Alexa and Apple's Siri to the latest metaverse efforts of the rebranded Meta. These trends underscore the importance of generating photorealistic visual depictions of humans. This has led to the rapid growth of so-called deepfake and talking-head generation methods in recent years. Despite their impressive results and popularity, they usually lack certain qualitative aspects such as texture quality, lips synchronization, or resolution, and practical aspects such as the ability to run in real-time. To allow for virtual human avatars to be used in practical scenarios, we propose an end-to-end framework for synthesizing high-quality virtual human faces capable of speaking with accurate lip motion with a special emphasis on performance. We introduce a novel network utilizing visemes as an intermediate audio representation and a novel data augmentation strategy employing a hierarchical image synthesis approach that allows disentanglement of the different modalities used to control the global head motion. Our method runs in real-time, and is able to deliver superior results compared to the current state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1883.LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook</span><br>
                <span class="as">Wang, JiayuandZhao, KangandZhang, ShiweiandZhang, YingyaandShen, YujunandZhao, DeliandZhou, Jingren</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13844-13853.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从输入音频序列生成说话人视频，这是一个实际但具有挑战性的任务。<br>
                    动机：大多数现有方法无法捕捉到精细的面部细节，或者需要为每个身份训练一个特定的模型。我们认为，预先在高质量的人脸图像上学习代码簿可以作为有用的先验知识，有助于高保真和可泛化的头部说话合成。<br>
                    方法：我们提出了LipFormer，一种基于变压器的框架，通过模拟音频视觉一致性并基于输入音频特征预测唇码序列来生成说话人视频。我们还引入了一个自适应人脸变形模块，帮助将参考人脸变形为目标姿势的特征空间，以减轻在不同姿势下预测唇码的难度。<br>
                    效果：实验表明，与之前的方法相比，LipFormer能够生成更真实的说话人视频，并能忠实地推广到未见的身份。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating a talking face video from the input audio sequence is a practical yet challenging task. Most existing methods either fail to capture fine facial details or need to train a specific model for each identity. We argue that a codebook pre-learned on high-quality face images can serve as a useful prior that facilitates high-fidelity and generalizable talking head synthesis. Thanks to the strong capability of the codebook in representing face textures, we simplify the talking face generation task as finding proper lip-codes to characterize the variation of lips during a portrait talking. To this end, we propose LipFormer, a transformer-based framework, to model the audio-visual coherence and predict the lip-codes sequence based on the input audio features. We further introduce an adaptive face warping module, which helps warp the reference face to the target pose in the feature space, to alleviate the difficulty of lip-code prediction under different poses. By this means, LipFormer can make better use of the pre-learned priors in images and is robust to posture change. Extensive experiments show that LipFormer can produce more realistic talking face videos compared to previous methods and faithfully generalize to unseen identities.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1884.UTM: A Unified Multiple Object Tracking Model With Identity-Aware Feature Enhancement</span><br>
                <span class="as">You, SisiandYao, HantaoandBao, Bing-KunandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/You_UTM_A_Unified_Multiple_Object_Tracking_Model_With_Identity-Aware_Feature_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21876-21886.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多目标跟踪方法在对象检测、特征嵌入和身份关联三个步骤中，身份关联是独立的，导致身份相关的知识没有被用于提升检测和嵌入模块。<br>
                    动机：为了克服现有方法的局限性，提出了一种新的统一跟踪模型（UTM），通过建立正反馈循环，使这三个组件相互受益。<br>
                    方法：UTM的关键思想是身份感知特征增强（IAFE），通过利用身份相关的知识来提升检测和嵌入，连接并促进这三个组件。具体来说，IAFE包括身份感知增强注意力（IABA）和身份感知擦除注意力（IAEA），其中IABA增强了当前帧特征与身份相关知识的一致性区域，IAEA抑制了当前帧特征中的干扰区域。<br>
                    效果：通过在三个基准测试集上进行大量实验，证明了UTM的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, Multiple Object Tracking has achieved great success, which consists of object detection, feature embedding, and identity association. Existing methods apply the three-step or two-step paradigm to generate robust trajectories, where identity association is independent of other components. However, the independent identity association results in the identity-aware knowledge contained in the tracklet not be used to boost the detection and embedding modules. To overcome the limitations of existing methods, we introduce a novel Unified Tracking Model (UTM) to bridge those three components for generating a positive feedback loop with mutual benefits. The key insight of UTM is the Identity-Aware Feature Enhancement (IAFE), which is applied to bridge and benefit these three components by utilizing the identity-aware knowledge to boost detection and embedding. Formally, IAFE contains the Identity-Aware Boosting Attention (IABA) and the Identity-Aware Erasing Attention (IAEA), where IABA enhances the consistent regions between the current frame feature and identity-aware knowledge, and IAEA suppresses the distracted regions in the current frame feature. With better detections and embeddings, higher-quality tracklets can also be generated. Extensive experiments of public and private detections on three benchmarks demonstrate the robustness of UTM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1885.High-Resolution Image Reconstruction With Latent Diffusion Models From Human Brain Activity</span><br>
                <span class="as">Takagi, YuandNishimoto, Shinji</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14453-14463.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过人脑活动重建视觉体验，并理解计算机视觉模型与视觉系统之间的联系。<br>
                    动机：虽然深度生成模型已被用于此任务，但用高语义保真度重建真实图像仍是一个挑战。<br>
                    方法：提出一种基于扩散模型（DM）的新方法，从通过功能磁共振成像（fMRI）获取的人脑活动中重建图像。具体来说，我们依赖于称为稳定扩散的潜伏扩散模型（LDM）。<br>
                    效果：该方法能直接以高保真度重建高分辨率图像，无需复杂深度学习模型的任何额外训练和微调。同时，我们还从神经科学的角度对LDM的不同组件进行了定量解释。总的来说，这项研究提出了一种从人脑活动重建图像的有前景的方法，并为理解DMs提供了一个新的框架。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstructing visual experiences from human brain activity offers a unique way to understand how the brain represents the world, and to interpret the connection between computer vision models and our visual system. While deep generative models have recently been employed for this task, reconstructing realistic images with high semantic fidelity is still a challenging problem. Here, we propose a new method based on a diffusion model (DM) to reconstruct images from human brain activity obtained via functional magnetic resonance imaging (fMRI). More specifically, we rely on a latent diffusion model (LDM) termed Stable Diffusion. This model reduces the computational cost of DMs, while preserving their high generative performance. We also characterize the inner mechanisms of the LDM by studying how its different components (such as the latent vector Z, conditioning inputs C, and different elements of the denoising U-Net) relate to distinct brain functions. We show that our proposed method can reconstruct high-resolution images with high fidelity in straightforward fashion, without the need for any additional training and fine-tuning of complex deep-learning models. We also provide a quantitative interpretation of different LDM components from a neuroscientific perspective. Overall, our study proposes a promising method for reconstructing images from human brain activity, and provides a new framework for understanding DMs. Please check out our webpage at https://sites.google.com/view/stablediffusion-withbrain/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1886.Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences Between Pretrained Generative Models</span><br>
                <span class="as">Olson, MatthewL.andLiu, ShusenandAnirudh, RushilandThiagarajan, JayaramanJ.andBremer, Peer-TimoandWong, Weng-Keen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Olson_Cross-GAN_Auditing_Unsupervised_Identification_of_Attribute_Level_Similarities_and_Differences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7981-7990.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练生成对抗网络（GANs）具有挑战性，特别是在复杂分布和有限数据的情况下。<br>
                    动机：为了审计已训练的网络，例如识别偏见或确保公平性，需要可解释的工具。<br>
                    方法：提出了一种新的GAN比较方法——Cross-GAN Auditing (xGA)，该方法通过比较新开发的GAN和已有的基线GAN，共同识别两者共有、客户特有的或客户GAN中缺失的语义属性。<br>
                    效果：定量分析表明，xGA优于基线方法。同时，通过对各种图像数据集上训练的GAN进行定性分析，展示了xGA识别出的共有、独特和缺失的属性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative Adversarial Networks (GANs) are notoriously difficult to train especially for complex distributions and with limited data. This has driven the need for interpretable tools to audit trained networks, for example, to identify biases or ensure fairness. Existing GAN audit tools are restricted to coarse-grained, model-data comparisons based on summary statistics such as FID or recall. In this paper, we propose an alternative approach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Auditing (xGA) that, given an established "reference" GAN and a newly proposed "client" GAN, jointly identifies semantic attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN. This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these metrics to demonstrate quantitatively that xGA outperforms baseline approaches. We also include qualitative results that illustrate the common, novel and missing attributes identified by xGA from GANs trained on a variety of image datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1887.SINE: SINgle Image Editing With Text-to-Image Diffusion Models</span><br>
                <span class="as">Zhang, ZhixingandHan, LigongandGhosh, ArnabandMetaxas, DimitrisN.andRen, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6027-6037.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练的扩散模型进行单图像编辑。<br>
                    动机：现有的基于扩散模型的图像生成方法在处理单图像编辑任务时，由于信息泄露问题，无法保持与给定图像相同的内容并创建新的语言指导特征。<br>
                    方法：提出一种基于无分类器指导的新型模型，将单个图像训练的知识提炼到预训练的扩散模型中，即使只有一张给定的图像也能进行内容创作。同时，提出一种基于补丁的微调方法，能有效帮助模型生成任意分辨率的图像。<br>
                    效果：通过大量实验验证了该方法的设计选择，展示了其在改变风格、添加内容和操作对象等方面的优秀编辑能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem--real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1888.Diffusion-Based Signed Distance Fields for 3D Shape Generation</span><br>
                <span class="as">Shim, JaehyeokandKang, ChangwooandJoo, Kyungdon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20887-20897.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种使用去噪扩散模型和通过有符号距离场（SDF）进行连续3D表示的3D形状生成框架。<br>
                    动机：大多数现有方法依赖于点云等不连续形式，而我们的SDF-Diffusion框架可以生成高分辨率的3D形状，同时通过将生成过程分为两个阶段来缓解内存问题。<br>
                    方法：我们的方法首先使用基于扩散的生成模型生成3D形状的低分辨率SDF，然后使用估计的低分辨率SDF作为条件，第二个阶段的扩散模型执行超分辨率以生成高分辨率SDF。<br>
                    效果：在ShapeNet数据集上，我们的模型表现出与最先进的方法相当的性能，并在无需修改的情况下显示出在形状完成任务上的适用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a 3D shape generation framework (SDF-Diffusion in short) that uses denoising diffusion models with continuous 3D representation via signed distance fields (SDF). Unlike most existing methods that depend on discontinuous forms, such as point clouds, SDF-Diffusion generates high-resolution 3D shapes while alleviating memory issues by separating the generative process into two-stage: generation and super-resolution. In the first stage, a diffusion-based generative model generates a low-resolution SDF of 3D shapes. Using the estimated low-resolution SDF as a condition, the second stage diffusion model performs super-resolution to generate high-resolution SDF. Our framework can generate a high-fidelity 3D shape despite the extreme spatial complexity. On the ShapeNet dataset, our model shows competitive performance to the state-of-the-art methods and shows applicability on the shape completion task without modification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1889.CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer</span><br>
                <span class="as">Wen, LinfengandGao, ChengyingandZou, Changqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_CAP-VSTNet_Content_Affinity_Preserved_Versatile_Style_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18300-18309.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决内容亲和性损失（包括特征和像素亲和性）在照片级真实感和视频风格转换中导致伪影的主要问题。<br>
                    动机：传统的可逆网络在保留内容亲和性的同时，会引入冗余信息，而本文提出的新框架CAP-VSTNet则能解决这个问题，从而更好地进行风格转换。<br>
                    方法：本文提出了一种新的可逆残差网络和一种无偏线性变换模块，形成了一个名为CAP-VSTNet的新框架，用于实现多样化的风格转换。<br>
                    效果：实验结果表明，与传统的先进方法相比，CAP-VSTNet在多样化的风格转换上能够产生更好的定性和定量结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Content affinity loss including feature and pixel affinity is a main problem which leads to artifacts in photorealistic and video style transfer. This paper proposes a new framework named CAP-VSTNet, which consists of a new reversible residual network and an unbiased linear transform module, for versatile style transfer. This reversible residual network can not only preserve content affinity but not introduce redundant information as traditional reversible networks, and hence facilitate better stylization. Empowered by Matting Laplacian training loss which can address the pixel affinity loss problem led by the linear transform, the proposed framework is applicable and effective on versatile style transfer. Extensive experiments show that CAP-VSTNet can produce better qualitative and quantitative results in comparison with the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1890.FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits</span><br>
                <span class="as">Karpikova, PolinaandRadionova, EkaterinaandYaschenko, AnastasiaandSpiridonov, AndreiandKostyushko, LeonidandFabbricatore, RiccardoandIvakhnenko, Aleksei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karpikova_FIANCEE_Faster_Inference_of_Adversarial_Networks_via_Conditional_Early_Exits_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12032-12043.png><br>
            
            <span class="tt"><span class="t0">研究问题：生成对抗网络（DNNs）在图像合成方面功能强大，但计算负载大，且对于不同特性的图像，输出质量不均。<br>
                    动机：为了解决这一问题，我们提出了一种通过添加早期退出分支来降低计算负载的方法，并根据渲染输出的难度动态切换计算路径。<br>
                    方法：我们将这种方法应用于两种不同的SOTA模型，一种是从语义地图生成，另一种是面部表情的交叉重演。结果显示，该方法能够输出具有自定义较低质量阈值的图像。<br>
                    效果：对于LPIPS <=0.1的阈值，我们可以将计算负载减少多达一半，这对于需要控制质量损失同时减少大部分输入计算的实时应用（如人脸合成）尤其重要。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative DNNs are a powerful tool for image synthesis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output image quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the model's complexity on some instances, maintaining a high quality. We propose a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross reenactment of face expressions; showing it is able to output images with custom lower quality thresholds. For a threshold of LPIPS <=0.1, we diminish their computations by up to a half. This is especially relevant for real-time applications such as synthesis of faces, when quality loss needs to be contained, but most of the inputs need fewer computations than the complex instances.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1891.DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</span><br>
                <span class="as">Ruiz, NatanielandLi, YuanzhenandJampani, VarunandPritch, YaelandRubinstein, MichaelandAberman, Kfir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22500-22510.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的大规模文本到图像模型缺乏模仿给定参考集中主体外观并在不同的上下文中合成它们的新颖版本的能力。<br>
                    动机：为了解决这一问题，我们提出了一种个性化的文本到图像扩散模型的新方法。<br>
                    方法：我们的方法首先使用少量主题图像对预训练的文本到图像模型进行微调，使模型学习将唯一的标识符与特定主题绑定。然后，一旦主题被嵌入模型的输出域，就可以使用该唯一标识符合成主题在不同场景、姿态、视角和光照条件下的新颖照片般真实的图像。<br>
                    效果：通过利用模型中嵌入的语义先验以及一种新的自主类特定先验保留损失，我们的技术能够在参考图像中不存在的场景、姿态、视角和光照条件下合成主题，同时保持主题的关键特征。我们在几个先前无法解决的任务上应用了我们的方法，包括主题再语境化、文本引导的视图合成和艺术渲染。我们还为这个新的主题驱动生成任务提供了一个新的数据集和评估协议。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, and artistic rendering, all while preserving the subject's key features. We also provide a new dataset and evaluation protocol for this new task of subject-driven generation. Project page: https://dreambooth.github.io/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1892.Inferring and Leveraging Parts From Object Shape for Improving Semantic Image Synthesis</span><br>
                <span class="as">Wei, YuxiangandJi, ZhilongandWu, XiaoheandBai, JinfengandZhang, LeiandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Inferring_and_Leveraging_Parts_From_Object_Shape_for_Improving_Semantic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11248-11258.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从输入的语义地图生成逼真的部分图像。<br>
                    动机：尽管在语义图像合成方面取得了进展，但从输入的语义地图生成逼真的部分仍然是一个挑战。<br>
                    方法：本文提出了一种从物体形状推断部分（iPOSE）的方法，并将其用于改进语义图像合成。通过使用预定义的支持部分地图，我们可以学习一个PartNet来预测对象部分地图。<br>
                    效果：实验表明，我们的iPOSE不仅能够生成具有丰富部分细节的对象，而且可以灵活地控制图像合成。此外，我们的iPOSE在定量和定性评估方面都优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the progress in semantic image synthesis, it remains a challenging problem to generate photo-realistic parts from input semantic map. Integrating part segmentation map can undoubtedly benefit image synthesis, but is bothersome and inconvenient to be provided by users. To improve part synthesis, this paper presents to infer Parts from Object ShapE (iPOSE) and leverage it for improving semantic image synthesis. However, albeit several part segmentation datasets are available, part annotations are still not provided for many object categories in semantic image synthesis. To circumvent it, we resort to few-shot regime to learn a PartNet for predicting the object part map with the guidance of pre-defined support part maps. PartNet can be readily generalized to handle a new object category when a small number (e.g., 3) of support part maps for this category are provided. Furthermore, part semantic modulation is presented to incorporate both inferred part map and semantic map for image synthesis. Experiments show that our iPOSE not only generates objects with rich part details, but also enables to control the image synthesis flexibly. And our iPOSE performs favorably against the state-of-the-art methods in terms of quantitative and qualitative evaluation. Our code will be publicly available at https://github.com/csyxwei/iPOSE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1893.Pose-Disentangled Contrastive Learning for Self-Supervised Facial Representation</span><br>
                <span class="as">Liu, YuanyuanandWang, WenbinandZhan, YibingandFeng, ShaozeandLiu, KejunandChen, Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Pose-Disentangled_Contrastive_Learning_for_Self-Supervised_Facial_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9717-9728.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对比学习在面部表示学习中存在姿态细节无法描绘的问题。<br>
                    动机：为了解决现有对比学习的局限性，提高面部理解性能。<br>
                    方法：提出一种新颖的Pose-disentangled Contrastive Learning（PCL）方法，通过设计的姿态解耦解码器和姿态相关对比学习方案进行面部表示学习。<br>
                    效果：实验结果表明，PCL显著优于先进的自监督学习方法，并在面部表情识别、人脸识别、AU检测和头部姿态估计等任务上表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised facial representation has recently attracted increasing attention due to its ability to perform face understanding without relying on large-scale annotated datasets heavily. However, analytically, current contrastive-based self-supervised learning (SSL) still performs unsatisfactorily for learning facial representation. More specifically, existing contrastive learning (CL) tends to learn pose-invariant features that cannot depict the pose details of faces, compromising the learning performance. To conquer the above limitation of CL, we propose a novel Pose-disentangled Contrastive Learning (PCL) method for general self-supervised facial representation. Our PCL first devises a pose-disentangled decoder (PDD) with a delicately designed orthogonalizing regulation, which disentangles the pose-related features from the face-aware features; therefore, pose-related and other pose-unrelated facial information could be performed in individual subnetworks and do not affect each other's training. Furthermore, we introduce a pose-related contrastive learning scheme that learns pose-related information based on data augmentation of the same image, which would deliver more effective face-aware representation for various downstream tasks. We conducted linear evaluation on four challenging downstream facial understanding tasks, i.e., facial expression recognition, face recognition, AU detection and head pose estimation.Experimental results demonstrate that PCL significantly outperforms cutting-edge SSL methods. Our Code is available at https://github.com/DreamMr/PCL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1894.Attribute-Preserving Face Dataset Anonymization via Latent Code Optimization</span><br>
                <span class="as">Barattin, SimoneandTzelepis, ChristosandPatras, IoannisandSebe, Nicu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Barattin_Attribute-Preserving_Face_Dataset_Anonymization_via_Latent_Code_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8001-8010.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在保护被描绘人脸的隐私的同时，如何有效地对图像数据集进行匿名化处理，以便用于训练机器学习模型等下游任务。<br>
                    动机：现有的最先进方法存在两个主要缺点，一是需要额外训练专用神经网络，成本高昂；二是无法在匿名化图像中保留原始图像的人脸属性，这对于其在下游任务中的使用至关重要。<br>
                    方法：我们提出了一种与任务无关的匿名化程序，直接优化预训练生成对抗网络（GAN）的潜在表示空间中的图像潜在表示。通过直接优化潜在代码，我们确保身份与原始身份的距离符合要求（具有身份混淆损失），同时保留面部属性（使用FaRL深度特征空间中的新型特征匹配损失）。<br>
                    效果：通过一系列定性和定量实验，我们的方法能够有效地对图像的身份进行匿名化，同时更好地保留面部属性。我们将代码和预训练模型公开发布在 https://github.com/chi0tzp/FALCO。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work addresses the problem of anonymizing the identity of faces in a dataset of images, such that the privacy of those depicted is not violated, while at the same time the dataset is useful for downstream task such as for training machine learning models. To the best of our knowledge, we are the first to explicitly address this issue and deal with two major drawbacks of the existing state-of-the-art approaches, namely that they (i) require the costly training of additional, purpose-trained neural networks, and/or (ii) fail to retain the facial attributes of the original images in the anonymized counterparts, the preservation of which is of paramount importance for their use in downstream tasks. We accordingly present a task-agnostic anonymization procedure that directly optimises the images' latent representation in the latent space of a pre-trained GAN. By optimizing the latent codes directly, we ensure both that the identity is of a desired distance away from the original (with an identity obfuscation loss), whilst preserving the facial attributes (using a novel feature-matching loss in FaRL's deep feature space). We demonstrate through a series of both qualitative and quantitative experiments that our method is capable of anonymizing the identity of the images whilst--crucially--better-preserving the facial attributes. We make the code and the pre-trained models publicly available at: https://github.com/chi0tzp/FALCO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1895.Delving Into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling</span><br>
                <span class="as">Liu, YulinandLiu, HaoranandYin, YingdaandWang, YangandChen, BaoquanandWang, He</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Discrete_Normalizing_Flows_on_SO3_Manifold_for_Probabilistic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21264-21273.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在三维旋转流形SO(3)上构建概率模型。<br>
                    动机：由于遮挡和对称性的存在，旋转在计算机视觉、图形学和机器人学中常常存在模糊性，需要概率模型来处理。<br>
                    方法：提出一种新的基于莫比乌斯变换耦合层和四元数仿射变换的SO(3)上的正则化流。<br>
                    效果：实验表明，这种旋转正则化流不仅能有效表达SO(3)上的任意分布，还能根据输入观察结果有条件地构建目标分布，并在无条件和有条件任务上都显著优于基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Normalizing flows (NFs) provide a powerful tool to construct an expressive distribution by a sequence of trackable transformations of a base distribution and form a probabilistic model of underlying data.Rotation, as an important quantity in computer vision, graphics, and robotics, can exhibit many ambiguities when occlusion and symmetry occur and thus demands such probabilistic models. Though much progress has been made for NFs in Euclidean space, there are no effective normalizing flows without discontinuity or many-to-one mapping tailored for SO(3) manifold. Given the unique non-Euclidean properties of the rotation manifold, adapting the existing NFs to SO(3) manifold is non-trivial. In this paper, we propose a novel normalizing flow on SO(3) by combining a Mobius transformation-based coupling layer and a quaternion affine transformation. With our proposed rotation normalizing flows, one can not only effectively express arbitrary distributions on SO(3), but also conditionally build the target distribution given input observations. Extensive experiments show that our rotation normalizing flows significantly outperform the baselines on both unconditional and conditional tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1896.DeepVecFont-v2: Exploiting Transformers To Synthesize Vector Fonts With Higher Quality</span><br>
                <span class="as">Wang, YuqingandWang, YizhiandYu, LonghuiandZhu, YueshengandLian, Zhouhui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DeepVecFont-v2_Exploiting_Transformers_To_Synthesize_Vector_Fonts_With_Higher_Quality_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18320-18328.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉和计算机图形领域中的矢量字体合成问题，特别是处理长序列数据和依赖图像引导轮廓细化后处理的问题。<br>
                    动机：虽然最近提出的DeepVecFont通过利用矢量字体的图像和序列信息实现了最先进的性能，但其在处理长序列数据方面的能力有限，且严重依赖于图像引导的轮廓细化后处理。因此，由DeepVecFont合成的矢量字形仍然经常包含一些畸变和人工痕迹，无法与人类设计的结果相媲美。<br>
                    方法：本文提出了一种增强版的DeepVecFont，主要通过以下三个新颖的技术贡献来实现：首先，采用Transformers代替RNNs来处理序列数据，并设计了一种放松表示法用于矢量轮廓，显著提高了模型合成长而复杂轮廓的能力和稳定性；其次，除了控制点外，还建议采样辅助点以精确对齐生成的目标贝塞尔曲线或线；最后，为了减轻序列生成过程中的错误累积，开发了一个基于另一Transformer-based解码器的上下文自修正模块，以消除初步合成字形中的人工痕迹。<br>
                    效果：定性和定量的结果表明，所提出的方法有效地解决了原始DeepVecFont的内在问题，并在生成具有复杂结构和多样风格的英文和中文矢量字体方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vector font synthesis is a challenging and ongoing problem in the fields of Computer Vision and Computer Graphics. The recently-proposed DeepVecFont achieved state-of-the-art performance by exploiting information of both the image and sequence modalities of vector fonts. However, it has limited capability for handling long sequence data and heavily relies on an image-guided outline refinement post-processing. Thus, vector glyphs synthesized by DeepVecFont still often contain some distortions and artifacts and cannot rival human-designed results. To address the above problems, this paper proposes an enhanced version of DeepVecFont mainly by making the following three novel technical contributions. First, we adopt Transformers instead of RNNs to process sequential data and design a relaxation representation for vector outlines, markedly improving the model's capability and stability of synthesizing long and complex outlines. Second, we propose to sample auxiliary points in addition to control points to precisely align the generated and target Bezier curves or lines. Finally, to alleviate error accumulation in the sequential generation process, we develop a context-based self-refinement module based on another Transformer-based decoder to remove artifacts in the initially synthesized glyphs. Both qualitative and quantitative results demonstrate that the proposed method effectively resolves those intrinsic problems of the original DeepVecFont and outperforms existing approaches in generating English and Chinese vector fonts with complicated structures and diverse styles.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1897.Continuous Landmark Detection With 3D Queries</span><br>
                <span class="as">Chandran, PrashanthandZoss, GaspardandGotardo, PauloandBradley, Derek</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chandran_Continuous_Landmark_Detection_With_3D_Queries_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16858-16867.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的面部地标检测神经网络受限于固定的一组地标和指定的布局，且需要手动标注相应的地标配置进行训练。<br>
                    动机：提出首个能预测连续、无限制地标的面部地标检测网络，允许在推理时指定所需地标的数量和位置。<br>
                    方法：将简单的图像特征提取器与查询地标预测器结合，用户可以指定任意相对于3D模板人脸网格的连续查询点作为输入。<br>
                    效果：由于不局限于固定的一组地标，该方法能够利用所有现有的2D地标数据集进行训练，即使它们具有不一致的地标配置。因此，提出了一种非常强大的面部地标检测器，可以一次性训练，并可轻松用于诸如3D人脸重建、任意人脸分割等众多应用，甚至与头盔式摄像头兼容，从而大大简化媒体和娱乐应用中的人脸跟踪工作流程。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural networks for facial landmark detection are notoriously limited to a fixed set of landmarks in a dedicated layout, which must be specified at training time. Dedicated datasets must also be hand-annotated with the corresponding landmark configuration for training. We propose the first facial landmark detection network that can predict continuous, unlimited landmarks, allowing to specify the number and location of the desired landmarks at inference time. Our method combines a simple image feature extractor with a queried landmark predictor, and the user can specify any continuous query points relative to a 3D template face mesh as input. As it is not tied to a fixed set of landmarks, our method is able to leverage all pre-existing 2D landmark datasets for training, even if they have inconsistent landmark configurations. As a result, we present a very powerful facial landmark detector that can be trained once, and can be used readily for numerous applications like 3D face reconstruction, arbitrary face segmentation, and is even compatible with helmeted mounted cameras, and therefore could vastly simplify face tracking workflows for media and entertainment applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1898.PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360deg</span><br>
                <span class="as">An, SizheandXu, HongyiandShi, YichunandSong, GuoxianandOgras, UmitY.andLuo, Linjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360deg_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20950-20959.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模无结构图像训练一种3D人头生成模型，实现高质量、全方位、具有多样性外观和详细几何形状的头部图像合成。<br>
                    动机：现有的最先进的3D GANs在3D人头合成方面存在限制，如只能生成近正面视图，或在大视角下难以保持3D一致性。<br>
                    方法：提出PanoHead，这是一种首个3D感知的生成模型，它使用仅在野外采集的无结构图像进行训练，能够实现全方位、具有多样性外观和详细几何形状的头部图像合成。我们的核心方法是提升最新的3D GANs的表示能力，并弥合从野外广泛分布的视角进行训练时的数据对齐差距。<br>
                    效果：我们的模型显著优于先前的3D GANs，能生成具有准确几何形状和多样外观的高质量3D头部图像，即使对于长发卷曲和非洲式发型也能处理。此外，我们的系统还能从单个输入图像重建完整的3D头部，为个性化的真实3D头像提供可能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesis and reconstruction of 3D human head has gained increasing interests in computer vision and computer graphics recently. Existing state-of-the-art 3D generative adversarial networks (GANs) for 3D human head synthesis are either limited to near-frontal views or hard to preserve 3D consistency in large view angles. We propose PanoHead, the first 3D-aware generative model that enables high-quality view-consistent image synthesis of full heads in 360deg with diverse appearance and detailed geometry using only in-the-wild unstructured images for training. At its core, we lift up the representation power of recent 3D GANs and bridge the data alignment gap when training from in-the-wild images with widely distributed views. Specifically, we propose a novel two-stage self-adaptive image alignment for robust 3D GAN training. We further introduce a tri-grid neural volume representation that effectively addresses front-face and back-head feature entanglement rooted in the widely-adopted tri-plane formulation. Our method instills prior knowledge of 2D image segmentation in adversarial learning of 3D neural scene structures, enabling compositable head synthesis in diverse backgrounds. Benefiting from these designs, our method significantly outperforms previous 3D GANs, generating high-quality 3D heads with accurate geometry and diverse appearances, even with long wavy and afro hairstyles, renderable from arbitrary poses. Furthermore, we show that our system can reconstruct full 3D heads from single input images for personalized realistic 3D avatars.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1899.Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details</span><br>
                <span class="as">Hwang, InwooandKim, HyeonwooandKim, YoungMin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hwang_Text2Scene_Text-Driven_Indoor_Scene_Stylization_With_Part-Aware_Details_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1890-1899.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为由多个物体组成的虚拟场景自动创建逼真的纹理？<br>
                    动机：现有的方法在为虚拟场景添加纹理时，通常在整个场景上应用单一的风格化处理，无法保持结构上下文。<br>
                    方法：提出Text2Scene方法，通过参考图像和文本描述，对房间中的3D几何形状添加详细的纹理，使生成的颜色尊重由相似材料组成的分层结构或语义部分。首先从几何分割中获得弱语义线索，然后为单个物体添加纹理细节，使其在图像空间上的投影展示出与输入嵌入对齐的特征嵌入。<br>
                    效果：该方法能够为具有多个物体的场景创建详细且逼真的纹理，同时保持结构上下文。这是第一个实用且可扩展的方法，不需要由熟练艺术家设计的高质量纹理的专用数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Text2Scene, a method to automatically create realistic textures for virtual scenes composed of multiple objects. Guided by a reference image and text descriptions, our pipeline adds detailed texture on labeled 3D geometries in the room such that the generated colors respect the hierarchical structure or semantic parts that are often composed of similar materials. Instead of applying flat stylization on the entire scene at a single step, we obtain weak semantic cues from geometric segmentation, which are further clarified by assigning initial colors to segmented parts. Then we add texture details for individual objects such that their projections on image space exhibit feature embedding aligned with the embedding of the input. The decomposition makes the entire pipeline tractable to a moderate amount of computation resources and memory. As our framework utilizes the existing resources of image and text embedding, it does not require dedicated datasets with high-quality textures designed by skillful artists. To the best of our knowledge, it is the first practical and scalable approach that can create detailed and realistic textures of the desired style that maintain structural context for scenes with multiple objects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1900.TAPS3D: Text-Guided 3D Textured Shape Generation From Pseudo Supervision</span><br>
                <span class="as">Wei, JiachengandWang, HaoandFeng, JiashiandLin, GuoshengandYap, Kim-Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_TAPS3D_Text-Guided_3D_Textured_Shape_Generation_From_Pseudo_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16805-16815.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从给定的文本描述生成可控的3D纹理形状。<br>
                    动机：现有的方法需要真实标签或大量的优化时间，我们提出一个新的框架TAPS3D来解决这些问题。<br>
                    方法：我们基于渲染的2D图像，从CLIP词汇表中检索相关单词并使用模板构造伪标题，为生成的3D形状提供高级语义监督。同时，为了产生精细纹理和增加几何多样性，我们采用低级别的图像正则化使假渲染的图像与真实的图像对齐。在推理阶段，我们的模型可以从给定的文本生成3D纹理形状，无需任何额外的优化。<br>
                    效果：通过广泛的实验，我们分析了我们提出的每个组件，并展示了我们的框架在生成高保真度3D纹理和与文本相关的形状方面的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we investigate an open research task of generating controllable 3D textured shapes from the given textual descriptions. Previous works either require ground truth caption labeling or extensive optimization time. To resolve these issues, we present a novel framework, TAPS3D, to train a text-guided 3D shape generator with pseudo captions. Specifically, based on rendered 2D images, we retrieve relevant words from the CLIP vocabulary and construct pseudo captions using templates. Our constructed captions provide high-level semantic supervision for generated 3D shapes. Further, in order to produce fine-grained textures and increase geometry diversity, we propose to adopt low-level image regularization to enable fake-rendered images to align with the real ones. During the inference phase, our proposed model can generate 3D textured shapes from the given text without any additional optimization. We conduct extensive experiments to analyze each of our proposed components and show the efficacy of our framework in generating high-fidelity 3D textured and text-relevant shapes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1901.Learning Personalized High Quality Volumetric Head Avatars From Monocular RGB Videos</span><br>
                <span class="as">Bai, ZiqianandTan, FeitongandHuang, ZengandSarkar, KripasindhuandTang, DanhangandQiu, DiandMeka, AbhimitraandDu, RuofeiandDou, MingsongandOrts-Escolano, SergioandPandey, RohitandTan, PingandBeeler, ThaboandFanello, SeanandZhang, Yinda</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Learning_Personalized_High_Quality_Volumetric_Head_Avatars_From_Monocular_RGB_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16890-16900.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从野外捕获的单目RGB视频中学习高质量的隐式3D头部化身。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：提出一种从野外捕获的单目RGB视频中学习高质量隐式3D头部化身的方法，该方法结合了3DMM的几何先验和动态追踪以及神经辐射场，以实现精细控制和照片级真实感。<br>
                    效果：实验结果表明，该方法能够重建高质量的化身，具有更准确的表情相关细节，良好的泛化能力，并能产生比其他最先进的方法更优秀的渲染效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method to learn a high-quality implicit 3D head avatar from a monocular RGB video captured in the wild. The learnt avatar is driven by a parametric face model to achieve user-controlled facial expressions and head poses. Our hybrid pipeline combines the geometry prior and dynamic tracking of a 3DMM with a neural radiance field to achieve fine-grained control and photorealism. To reduce over-smoothing and improve out-of-model expressions synthesis, we propose to predict local features anchored on the 3DMM geometry. These learnt features are driven by 3DMM deformation and interpolated in 3D space to yield the volumetric radiance at a designated query point. We further show that using a Convolutional Neural Network in the UV space is critical in incorporating spatial context and producing representative local features. Extensive experiments show that we are able to reconstruct high-quality avatars, with more accurate expression-dependent details, good generalization to out-of-training expressions, and quantitatively superior renderings compared to other state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1902.LayoutFormer++: Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction</span><br>
                <span class="as">Jiang, ZhaoyunandGuo, JiaqiandSun, ShizhaoandDeng, HuayuandWu, ZhongkaiandMijovic, VuksanandYang, ZijiangJamesandLou, Jian-GuangandZhang, Dongmei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_LayoutFormer_Conditional_Graphic_Layout_Generation_via_Constraint_Serialization_and_Decoding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18403-18412.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何灵活、统一地处理多样的用户约束，同时在满足用户约束的前提下提高布局生成质量。<br>
                    动机：现有的布局生成模型在处理用户约束时灵活性不足，且常常牺牲生成质量以满足约束。<br>
                    方法：提出LayoutFormer++模型，通过约束序列化方案将不同用户约束表示为预设格式的令牌序列；将条件布局生成建模为序列到序列转换任务，并采用基于Transformer的编码器-解码器框架进行实现；提出解码空间限制策略，通过剔除明显违反用户约束和可能导致低质量布局的选项，对预测分布进行剪枝，使模型从受限分布中采样。<br>
                    效果：实验表明，LayoutFormer++在生成质量和约束违反方面均优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Conditional graphic layout generation, which generates realistic layouts according to user constraints, is a challenging task that has not been well-studied yet. First, there is limited discussion about how to handle diverse user constraints flexibly and uniformly. Second, to make the layouts conform to user constraints, existing work often sacrifices generation quality significantly. In this work, we propose LayoutFormer++ to tackle the above problems. First, to flexibly handle diverse constraints, we propose a constraint serialization scheme, which represents different user constraints as sequences of tokens with a predefined format. Then, we formulate conditional layout generation as a sequence-to-sequence transformation, and leverage encoder-decoder framework with Transformer as the basic architecture. Furthermore, to make the layout better meet user requirements without harming quality, we propose a decoding space restriction strategy. Specifically, we prune the predicted distribution by ignoring the options that definitely violate user constraints and likely result in low-quality layouts, and make the model samples from the restricted distribution. Experiments demonstrate that LayoutFormer++ outperforms existing approaches on all the tasks in terms of both better generation quality and less constraint violation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1903.Implicit Identity Driven Deepfake Face Swapping Detection</span><br>
                <span class="as">Huang, BaojinandWang, ZhongyuanandYang, JifanandAi, JiaxinandZou, QinandWang, QianandYe, Dengpan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4490-4499.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文从人脸身份的角度考虑人脸交换检测。<br>
                    动机：人脸交换的目的是将目标人脸替换为源人脸，生成人类无法区分真假的假人脸。我们认为假人脸包含显性和隐性两种身份，分别对应于人脸交换过程中源人脸和目标人脸的身份。<br>
                    方法：我们提出了一种基于隐性身份的新的人脸交换检测框架。具体来说，我们设计了一个显性身份对比（EIC）损失和一个隐性身份探索（IIE）损失，这两个损失监督CNN主干将人脸图像嵌入到隐性身份空间中。在EIC的指导下，真实样本被拉向其显性身份，而假样本则被推离其显性身份。此外，IIE来源于基于间隔的分类损失函数，它鼓励已知目标身份的假人脸具有类内紧凑性和类间多样性。<br>
                    效果：我们在几个数据集上进行了广泛的实验和可视化，证明了我们的方法相对于最先进的对手具有良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we consider the face swapping detection from the perspective of face identity. Face swapping aims to replace the target face with the source face and generate the fake face that the human cannot distinguish between real and fake. We argue that the fake face contains the explicit identity and implicit identity, which respectively corresponds to the identity of the source face and target face during face swapping. Note that the explicit identities of faces can be extracted by regular face recognizers. Particularly, the implicit identity of real face is consistent with the its explicit identity. Thus the difference between explicit and implicit identity of face facilitates face swapping detection. Following this idea, we propose a novel implicit identity driven framework for face swapping detection. Specifically, we design an explicit identity contrast (EIC) loss and an implicit identity exploration (IIE) loss, which supervises a CNN backbone to embed face images into the implicit identity space. Under the guidance of EIC, real samples are pulled closer to their explicit identities, while fake samples are pushed away from their explicit identities. Moreover, IIE is derived from the margin-based classification loss function, which encourages the fake faces with known target identities to enjoy intra-class compactness and inter-class diversity. Extensive experiments and visualizations on several datasets demonstrate the generalization of our method against the state-of-the-art counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1904.Logical Consistency and Greater Descriptive Power for Facial Hair Attribute Learning</span><br>
                <span class="as">Wu, HaiyuandBezold, GraceandBhatta, AmanandBowyer, KevinW.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Logical_Consistency_and_Greater_Descriptive_Power_for_Facial_Hair_Attribute_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8588-8597.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决面部属性研究中的一些问题，如对面部毛发属性的简单二元分类，缺乏逻辑一致性和完整性等。<br>
                    动机：目前的面部属性研究只使用了简单的二进制属性进行面部毛发分类，如胡须/无胡须，且在处理逻辑一致性和完整性方面存在问题。<br>
                    方法：作者创建了一种新的、更具描述性的面部毛发注释方案，并应用于创建一个新的面部毛发属性数据集FH37K。同时，作者提出了一种逻辑一致预测损失（LCPLoss）来帮助学习属性间的逻辑一致性，以及一种标签补偿训练策略来解决相关属性中没有正预测的问题。<br>
                    效果：通过在FH37K数据集上训练的面部毛发属性分类器，作者研究了面部毛发如何影响面部识别的准确性，包括不同人口统计群体之间的变化。结果显示，面部发型的相似性和差异性对面部识别中的冒名顶替者和真实得分分布有重要影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face attribute research has so far used only simple binary attributes for facial hair; e.g., beard / no beard. We have created a new, more descriptive facial hair annotation scheme and applied it to create a new facial hair attribute dataset, FH37K. Face attribute research also so far has not dealt with logical consistency and completeness. For example, in prior research, an image might be classified as both having no beard and also having a goatee (a type of beard). We show that the test accuracy of previous classification methods on facial hair attribute classification drops significantly if logical consistency of classifications is enforced. We propose a logically consistent prediction loss, LCPLoss, to aid learning of logical consistency across attributes, and also a label compensation training strategy to eliminate the problem of no positive prediction across a set of related attributes. Using an attribute classifier trained on FH37K, we investigate how facial hair affects face recognition accuracy, including variation across demographics. Results show that similarity and difference in facial hairstyle have important effects on the impostor and genuine score distributions in face recognition. The code is at https:// github.com/ HaiyuWu/ facial hair logical.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1905.Diffusion Probabilistic Model Made Slim</span><br>
                <span class="as">Yang, XingyiandZhou, DaquanandFeng, JiashiandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22552-22562.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散概率模型（DPM）在视觉上取得了令人满意的结果，但其巨大的计算成本一直是其长期存在的缺陷，这极大地限制了其在资源有限平台上的应用。<br>
                    动机：尽管现有的方法试图提高DPM的效率，但大多数都集中在加速测试阶段，而忽视了其复杂的结构和庞大的规模。<br>
                    方法：本文提出了一种名为Spectral Diffusion（SD）的轻量级DPM设计。SD在其架构中引入了小波门控，以在每一步反向步骤中提取频率动态特征，并通过基于频谱幅度的反向加权目标进行频谱感知蒸馏，以促进高频恢复。<br>
                    效果：实验结果表明，SD在一系列有条件和无条件的图像生成任务上，与潜在的扩散模型相比，实现了8-18倍的计算复杂度降低，同时保持了具有竞争力的图像保真度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the visually-pleasing results achieved, the massive computational cost has been a long-standing flaw for diffusion probabilistic models (DPMs), which, in turn, greatly limits their applications on resource-limited platforms. Prior methods towards efficient DPM, however, have largely focused on accelerating the testing yet overlooked their huge complexity and size. In this paper, we make a dedicated attempt to lighten DPM while striving to preserve its favourable performance. We start by training a small-sized latent diffusion model (LDM) from scratch but observe a significant fidelity drop in the synthetic images. Through a thorough assessment, we find that DPM is intrinsically biased against high-frequency generation, and learns to recover different frequency components at different time-steps. These properties make compact networks unable to represent frequency dynamics with accurate high-frequency estimation. Towards this end, we introduce a customized design for slim DPM, which we term as Spectral Diffusion (SD), for lightweight image synthesis. SD incorporates wavelet gating in its architecture to enable frequency dynamic feature extraction at every reverse steps, and conducts spectrum-aware distillation to promote high-frequency recovery by inverse weighting the objective based on spectrum magnitudes. Experimental results demonstrate that, SD achieves 8-18x computational complexity reduction as compared to the latent diffusion models on a series of conditional and unconditional image generation tasks while retaining competitive image fidelity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1906.Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style</span><br>
                <span class="as">Lu, HaomingandTunanyan, HazarapetandWang, KaiandNavasardyan, ShantandWang, ZhangyangandShi, Humphrey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14267-14276.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过少量图像（例如少于10张）对预训练的扩散模型进行微调，以生成任意对象的高质量特定风格图像。<br>
                    动机：现有的扩散模型在文本条件图像合成方面表现出色，通过个性化预训练的扩散模型可以生成特定的目标对象或风格的图像，具有广泛的应用前景。<br>
                    方法：提出了一种新的微调技术工具包，包括文本到图像的定制数据增强、促进内容和风格分离的内容损失以及只关注少数时间步长的稀疏更新。通过这些技术，可以在少量的图像上微调预训练的扩散模型，使其能够生成任意对象的高质量特定风格图像。<br>
                    效果：实验结果表明，该方法在学习高度复杂的风格方面优于现有的几种少数样本扩散模型，如Textual Inversion和DreamBooth，具有超样本高效的微调性能。此外，还可以将该方法集成到Textual Inversion之上，进一步提高性能，甚至在非常不寻常的风格上也能达到良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization alternatives of diffusion models such as Textual Inversion and DreamBooth, in terms of learning highly sophisticated styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of textual inversion to boost performance further, even on highly unusual styles. Our codes are available at: https://github.com/Picsart-AI-Research/Specialist-Diffusion</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1907.HyperCUT: Video Sequence From a Single Blurry Image Using Unsupervised Ordering</span><br>
                <span class="as">Pham, Bang-DangandTran, PhongandTran, AnhandPham, CuongandNguyen, RangandHoai, Minh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pham_HyperCUT_Video_Sequence_From_a_Single_Blurry_Image_Using_Unsupervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9843-9852.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练图像到视频去模糊模型，解决模糊图像输入对应的清晰图像序列恢复问题。<br>
                    动机：图像到视频去模糊模型的训练存在帧排序的模糊性问题，前向和后向序列都可能是合理的解决方案。<br>
                    方法：提出一种有效的自我监督排序方案，通过在高维潜在空间中为每个视频序列映射向量并定义超平面，使正向和反向序列的向量位于超平面的两侧，从而确定序列的顺序。<br>
                    效果：实验结果证实了该方法的有效性，同时提出了一个覆盖人脸、手部和街道等多种流行领域的真实图像数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the challenging task of training models for image-to-video deblurring, which aims to recover a sequence of sharp images corresponding to a given blurry image input. A critical issue disturbing the training of an image-to-video model is the ambiguity of the frame ordering since both the forward and backward sequences are plausible solutions. This paper proposes an effective self-supervised ordering scheme that allows training high-quality image-to-video deblurring models. Unlike previous methods that rely on order-invariant losses, we assign an explicit order for each video sequence, thus avoiding the order-ambiguity issue. Specifically, we map each video sequence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video sequence, the vectors extracted from it and its reversed sequence are on different sides of the hyperplane. The side of the vectors will be used to define the order of the corresponding sequence. Last but not least, we propose a real-image dataset for the image-to-video deblurring problem that covers a variety of popular domains, including face, hand, and street. Extensive experimental results confirm the effectiveness of our method. Code and data are available at https://github.com/VinAIResearch/HyperCUT.git</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1908.Document Image Shadow Removal Guided by Color-Aware Background</span><br>
                <span class="as">Zhang, LingandHe, YinghaoandZhang, QingandLiu, ZhengandZhang, XiaolongandXiao, Chunxia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Document_Image_Shadow_Removal_Guided_by_Color-Aware_Background_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1818-1827.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的文档图像阴影移除方法主要依赖于从图像中学习和利用恒定的背景（纸的颜色），但这种方法忽视了其他背景颜色，如印刷颜色，导致结果失真。<br>
                    动机：提出一种颜色感知的背景提取网络（CBENet）来准确描绘文档的背景颜色，并使用预测的随空间变化的背景作为辅助信息，设计了一种背景引导的文档图像阴影移除网络（BGShadowNet）。<br>
                    方法：BGShadowNet由两个阶段组成。在第一阶段，设计了一个背景约束的解码器以促进粗略的结果。然后在第二阶段，通过背景基于的注意力模块（BAModule）和细节改进模块（DEModule）对粗略结果进行细化，以保持外观一致性并增强纹理细节。<br>
                    效果：实验证明，该方法在两个基准数据集上均优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing works on document image shadow removal mostly depend on learning and leveraging a constant background (the color of the paper) from the image. However, the constant background is less representative and frequently ignores other background colors, such as the printed colors, resulting in distorted results. In this paper, we present a color-aware background extraction network (CBENet) for extracting a spatially varying background image that accurately depicts the background colors of the document. Furthermore, we propose a background-guided document images shadow removal network (BGShadowNet) using the predicted spatially varying background as auxiliary information, which consists of two stages. At Stage I, a background-constrained decoder is designed to promote a coarse result. Then, the coarse result is refined with a background-based attention module (BAModule) to maintain a consistent appearance and a detail improvement module (DEModule) to enhance the texture details at Stage II. Experiments on two benchmark datasets qualitatively and quantitatively validate the superiority of the proposed approach over state-of-the-arts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1909.CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes From Natural Language</span><br>
                <span class="as">Sanghi, AdityaandFu, RaoandLiu, VivianandWillis, KarlD.D.andShayani, HoomanandKhasahmadi, AmirH.andSridhar, SrinathandRitchie, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sanghi_CLIP-Sculptor_Zero-Shot_Generation_of_High-Fidelity_and_Diverse_Shapes_From_Natural_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18339-18348.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过自然语言生成和编辑高保真度和多样性的3D形状。<br>
                    动机：现有的方法在生成3D形状时，其保真度和多样性有限。<br>
                    方法：提出CLIP-Sculptor方法，该方法采用多分辨率方式，首先在低维潜在空间中生成，然后升级到更高分辨率以提高形状保真度。为了提高形状多样性，使用了一个由转换器建模的离散潜在空间，该转换器以CLIP的图像-文本嵌入空间为条件。<br>
                    效果：实验结果表明，CLIP-Sculptor优于最先进的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1910.VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</span><br>
                <span class="as">Jain, AjayandXie, AmberandAbbeel, Pieter</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1911-1920.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用扩散模型生成可导出为SVG格式的矢量图形。<br>
                    动机：设计师常使用SVG等矢量图形进行数字图标、图像和贴纸的设计，但现有的方法需要大量带字幕的SVG数据集，缺乏可行性。<br>
                    方法：本文提出一种基于文本条件扩散模型的方法，通过优化可微分的矢量图形栅格化器，将抽象语义知识从预训练的扩散模型中提炼出来，生成连贯的像素艺术和草图。<br>
                    效果：实验结果表明，该方法在生成矢量图形方面优于现有方法CLIP，并能生成连贯的像素艺术和草图。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have shown impressive results in text-to-image synthesis. Using massive datasets of captioned images, diffusion models learn to generate raster images of highly diverse objects and scenes. However, designers frequently use vector representations of images like Scalable Vector Graphics (SVGs) for digital icons, graphics and stickers. Vector graphics can be scaled to any size, and are compact. In this work, we show that a text-conditioned diffusion model trained on pixel representations of images can be used to generate SVG-exportable vector graphics. We do so without access to large datasets of captioned SVGs. Instead, inspired by recent work on text-to-3D synthesis, we vectorize a text-to-image diffusion sample and fine-tune with a Score Distillation Sampling loss. By optimizing a differentiable vector graphics rasterizer, our method distills abstract semantic knowledge out of a pretrained diffusion model. By constraining the vector representation, we can also generate coherent pixel art and sketches. Our approach, VectorFusion, produces more coherent graphics than prior works that optimize CLIP, a contrastive image-text model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1911.Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models</span><br>
                <span class="as">Somepalli, GowthamiandSingla, VasuandGoldblum, MicahandGeiping, JonasandGoldstein, Tom</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6048-6058.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型生成的图像是否具有原创性，还是直接复制了训练集的内容？<br>
                    动机：为了探究扩散模型在商业艺术和图形设计方面的应用潜力。<br>
                    方法：通过图像检索框架比较生成的图像与训练样本，检测内容复制情况。<br>
                    效果：发现扩散模型会复制训练数据，且训练集大小会影响内容复制率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1912.AltFreezing for More General Video Face Forgery Detection</span><br>
                <span class="as">Wang, ZhendongandBao, JianminandZhou, WengangandWang, WeilunandLi, Houqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AltFreezing_for_More_General_Video_Face_Forgery_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4129-4138.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的人脸伪造检测模型主要通过检测空间或时间伪影进行判别，但面临跨领域伪影时性能会显著下降。<br>
                    动机：为了解决这一问题，本文提出在一个模型中同时捕获空间和时间伪影进行人脸伪造检测。<br>
                    方法：采用一种名为AltFreezing的新颖训练策略，将一个时空网络的权重分为两组：空间相关和时间相关。在训练过程中交替冻结这两组权重，使模型能够学习空间和时间特征以区分真实或伪造的视频。此外，还引入了各种视频级数据增强方法以提高伪造检测模型的泛化能力。<br>
                    效果：大量实验表明，我们的框架在对未见过的操作和数据集进行泛化方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing face forgery detection models try to discriminate fake images by detecting only spatial artifacts (e.g., generative artifacts, blending) or mainly temporal artifacts (e.g., flickering, discontinuity). They may experience significant performance degradation when facing out-domain artifacts. In this paper, we propose to capture both spatial and temporal artifacts in one model for face forgery detection. A simple idea is to leverage a spatiotemporal model (3D ConvNet). However, we find that it may easily rely on one type of artifact and ignore the other. To address this issue, we present a novel training strategy called AltFreezing for more general face forgery detection. The AltFreezing aims to encourage the model to detect both spatial and temporal artifacts. It divides the weights of a spatiotemporal network into two groups: spatial- and temporal-related. Then the two groups of weights are alternately frozen during the training process so that the model can learn spatial and temporal features to distinguish real or fake videos. Furthermore, we introduce various video-level data augmentation methods to improve the generalization capability of the forgery detection model. Extensive experiments show that our framework outperforms existing methods in terms of generalization to unseen manipulations and datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1913.MoDi: Unconditional Motion Synthesis From Diverse Data</span><br>
                <span class="as">Raab, SigalandLeibovitch, InbalandLi, PeizhuoandAberman, KfirandSorkine-Hornung, OlgaandCohen-Or, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Raab_MoDi_Unconditional_Motion_Synthesis_From_Diverse_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13873-13883.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从给定的分布中无条件地合成多样化的高质量运动。<br>
                    动机：尽管神经网络的出现革新了运动合成领域，但学习从给定的分布中无条件地合成多样化的运动仍然具有挑战性。<br>
                    方法：我们提出了MoDi，这是一种在无监督设置中从极度多样化、无结构和未标记的数据集中训练的生成模型。在推理过程中，MoDi可以合成高质量的多样化运动。<br>
                    效果：我们的模型产生了一个表现良好且高度结构化的潜在空间，可以进行语义聚类，构成强大的运动先验，有助于各种应用，包括语义编辑和人群动画。此外，我们还展示了一种编码器，可以将真实运动转换为MoDi的自然运动流形，解决了各种病态挑战，如前缀完成和空间编辑。我们的定性和定量实验取得了超越最新SOTA技术的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The emergence of neural networks has revolutionized the field of motion synthesis. Yet, learning to unconditionally synthesize motions from a given distribution remains challenging, especially when the motions are highly diverse. In this work, we present MoDi -- a generative model trained in an unsupervised setting from an extremely diverse, unstructured and unlabeled dataset. During inference, MoDi can synthesize high-quality, diverse motions. Despite the lack of any structure in the dataset, our model yields a well-behaved and highly structured latent space, which can be semantically clustered, constituting a strong motion prior that facilitates various applications including semantic editing and crowd animation. In addition, we present an encoder that inverts real motions into MoDi's natural motion manifold, issuing solutions to various ill-posed challenges such as completion from prefix and spatial editing. Our qualitative and quantitative experiments achieve state-of-the-art results that outperform recent SOTA techniques. Code and trained models are available at https://sigal-raab.github.io/MoDi.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1914.Generative Diffusion Prior for Unified Image Restoration and Enhancement</span><br>
                <span class="as">Fei, BenandLyu, ZhaoyangandPan, LiangandZhang, JunzheandYang, WeidongandLuo, TianyueandZhang, BoandDai, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fei_Generative_Diffusion_Prior_for_Unified_Image_Restoration_and_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9935-9946.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像恢复方法大多利用自然图像的后验分布，但它们通常假设已知的退化过程，并且需要监督训练，这限制了它们在复杂真实应用中的适应性。<br>
                    动机：为了解决这些问题，我们提出了生成扩散先验（GDP），以无监督采样的方式有效地对后验分布进行建模。<br>
                    方法：GDP利用预训练的去噪扩散生成模型（DDPM）来解决线性、非线性或盲问题。具体来说，GDP系统地探索了一种条件引导协议，该协议被证明比常用的引导方式更实用。此外，GDP还在去噪过程中优化了退化模型的参数，实现了盲图像恢复。我们还设计了分层引导和基于补丁的方法，使GDP能够生成任意分辨率的图像。<br>
                    效果：实验表明，GDP在多个图像数据集上具有通用性，可以处理线性问题如超分辨率、去模糊、修复和着色，以及非线性和盲问题如低光增强和HDR图像恢复。在重建质量和感知质量的各种基准测试中，GDP优于当前领先的无监督方法。此外，GDP还很好地泛化到自然图像或来自ImageNet训练集分布外的各种任务的任意大小合成图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing image restoration methods mostly leverage the posterior distribution of natural images. However, they often assume known degradation and also require supervised training, which restricts their adaptation to complex real applications. In this work, we propose the Generative Diffusion Prior (GDP) to effectively model the posterior distributions in an unsupervised sampling manner. GDP utilizes a pre-train denoising diffusion generative model (DDPM) for solving linear inverse, non-linear, or blind problems. Specifically, GDP systematically explores a protocol of conditional guidance, which is verified more practical than the commonly used guidance way. Furthermore, GDP is strength at optimizing the parameters of degradation model during denoising process, achieving blind image restoration. Besides, we devise hierarchical guidance and patch-based methods, enabling the GDP to generate images of arbitrary resolutions. Experimentally, we demonstrate GDP's versatility on several image datasets for linear problems, such as super-resolution, deblurring, inpainting, and colorization, as well as non-linear and blind issues, such as low-light enhancement and HDR image recovery. GDP outperforms the current leading unsupervised methods on the diverse benchmarks in reconstruction quality and perceptual quality. Moreover, GDP also generalizes well for natural images or synthesized images with arbitrary sizes from various tasks out of the distribution of the ImageNet training set.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1915.CF-Font: Content Fusion for Few-Shot Font Generation</span><br>
                <span class="as">Wang, ChiandZhou, MinandGe, TiezhengandJiang, YuningandBao, HujunandXu, Weiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_CF-Font_Content_Fusion_for_Few-Shot_Font_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1858-1867.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地实现少数字体生成？<br>
                    动机：现有的内容和风格解耦方法在提取代表性字体的内容特征时可能不是最优的。<br>
                    方法：提出一种内容融合模块（CFM），将内容特征投影到由基本字体内容特征定义的线性空间中，以考虑不同字体引起的内容特征变化。同时，通过轻量级迭代样式向量优化（ISR）策略优化参考图像的风格表示向量。<br>
                    效果：在包含300种字体、每种字体6500个字符的数据集上进行评估，实验结果表明，该方法比现有的最先进的少数字体生成方法有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style defined with a few reference images in a target domain. However, the content feature extracted using a representative font might not be optimal. In light of this, we propose a content fusion module (CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also allows to optimize the style representation vector of reference images through a lightweight iterative style-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruction loss (namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at https://github.com/wangchi95/CF-Font.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1916.3D-Aware Multi-Class Image-to-Image Translation With NeRFs</span><br>
                <span class="as">Li, SenmaoandvandeWeijer, JoostandWang, YaxingandKhan, FahadShahbazandLiu, MeiqinandYang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_3D-Aware_Multi-Class_Image-to-Image_Translation_With_NeRFs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12652-12662.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现3D一致的多类别图像到图像转换（3D-aware I2I）翻译。<br>
                    动机：现有的二维图像到图像转换方法在处理形状和身份变化时会产生不真实的效果，而直接使用这些方法进行三维感知的多类别图像到图像转换则无法保证视图一致性。<br>
                    方法：将学习过程分为两个步骤，首先通过提出新的条件架构和有效的训练策略，训练一个能够保持视图一致性的多类别3D-aware GAN；然后基于训练好的GAN架构，构建一个3D-aware I2I翻译系统，并进一步提出U型网络适配器设计、分层表示约束和相对正则化损失等新方法来减少视图一致性问题。<br>
                    效果：在两个数据集上的大量实验表明，该方法能成功实现具有多视角一致性的3D-aware I2I翻译。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in 3D-aware generative models (3D-aware GANs) combined with Neural Radiance Fields (NeRF) have achieved impressive results. However no prior works investigate 3D-aware GANs for 3D consistent multi-class image-to-image (3D-aware I2I) translation. Naively using 2D-I2I translation methods suffers from unrealistic shape/identity change. To perform 3D-aware multi-class I2I translation, we decouple this learning process into a multi-class 3D-aware GAN step and a 3D-aware I2I translation step. In the first step, we propose two novel techniques: a new conditional architecture and an effective training strategy. In the second step, based on the well-trained multi-class 3D-aware GAN architecture, that preserves view-consistency, we construct a 3D-aware I2I translation system. To further reduce the view-consistency problems, we propose several new techniques, including a U-net-like adaptor network design, a hierarchical representation constrain and a relative regularization loss. In extensive experiments on two datasets, quantitative and qualitative results demonstrate that we successfully perform 3D-aware I2I translation with multi-view consistency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1917.Seeing Beyond the Brain: Conditional Diffusion Model With Sparse Masked Modeling for Vision Decoding</span><br>
                <span class="as">Chen, ZijiaoandQing, JiaxinandXiang, TiangeandYue, WanLinandZhou, JuanHelen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22710-22720.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从大脑记录中解码视觉刺激，以深化对人类视觉系统的理解，并通过脑机接口建立人与计算机视觉的联系。<br>
                    动机：由于大脑信号的复杂底层表示和数据标注的稀缺性，从大脑记录中重建具有正确语义的高质图像是一个挑战。<br>
                    方法：我们提出了MinD-Vis，这是一种用于人类视觉解码的稀疏掩蔽脑模型，使用双重条件潜扩散模型。首先，我们在大的潜在空间中通过掩蔽建模学习有效的功能性磁共振成像数据的自监督表示，这受到初级视觉皮层信息稀疏编码的启发。然后，通过增强潜扩散模型的双重条件，我们证明MinD-Vis可以使用非常少的配对标注从大脑记录重建高度可信的、具有语义匹配细节的图像。<br>
                    效果：我们的模型在定性和定量上进行了基准测试；实验结果表明，我们的方法在语义映射（100类语义分类）和生成质量（FID）方面分别比最先进的方法提高了66%和41%。我们还进行了详尽的分析来分析我们的框架。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present MinD-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of fMRI data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that MinD-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality (FID) by 66% and 41% respectively. An exhaustive ablation study was also conducted to analyze our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1918.High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning</span><br>
                <span class="as">Xu, ChaoandZhu, JunweiandZhang, JiangningandHan, YueandChu, WenqingandTai, YingandWang, ChengjieandXie, ZhifengandLiu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6609-6619.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的情感说话人脸生成方法在实际应用中缺乏灵活性，无法处理未见过的情感风格。<br>
                    动机：为了解决上述问题，本文提出了一个更灵活、更通用的框架。<br>
                    方法：通过在文本提示中补充情感风格，并使用对齐的多模态情感编码器将文本、图像和音频情感模态嵌入统一空间，从而继承来自CLIP的丰富语义先验。此外，还提出了情感感知的音频到3DMM转换器来连接情感条件和音频序列的结构表示。最后设计了一个基于风格的高保真情感人脸生成器来生成任意高分辨率的真实身份。<br>
                    效果：实验结果表明，该方法在情感控制方面具有灵活性和泛化性，并且在高质量人脸合成方面具有有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, emotional talking face generation has received considerable attention. However, existing methods only adopt one-hot coding, image, or audio as emotion conditions, thus lacking flexible control in practical applications and failing to handle unseen emotion styles due to limited semantics. They either ignore the one-shot setting or the quality of generated faces. In this paper, we propose a more flexible and generalized framework. Specifically, we supplement the emotion style in text prompts and use an Aligned Multi-modal Emotion encoder to embed the text, image, and audio emotion modality into a unified space, which inherits rich semantic prior from CLIP. Consequently, effective multi-modal emotion space learning helps our method support arbitrary emotion modality during testing and could generalize to unseen emotion styles. Besides, an Emotion-aware Audio-to-3DMM Convertor is proposed to connect the emotion condition and the audio sequence to structural representation. A followed style-based High-fidelity Emotional Face generator is designed to generate arbitrary high-resolution realistic identities. Our texture generator hierarchically learns flow fields and animated faces in a residual manner. Extensive experiments demonstrate the flexibility and generalization of our method in emotion control and the effectiveness of high-quality face synthesis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1919.Masked and Adaptive Transformer for Exemplar Based Image Translation</span><br>
                <span class="as">Jiang, ChangandGao, FeiandMa, BiaoandLin, YuhaoandWang, NannanandXu, Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Masked_and_Adaptive_Transformer_for_Exemplar_Based_Image_Translation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22418-22427.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的基于范例的图像翻译框架，以解决跨领域语义匹配的挑战。<br>
                    动机：当前先进的图像翻译方法主要关注建立跨领域的语义对应关系，但这在图像生成中会导致局部风格控制的问题。<br>
                    方法：我们提出了一种遮蔽和自适应的变压器（MAT）来学习准确的跨领域对应关系，并执行上下文感知的特征增强。同时，我们使用输入源特征和范例的全局风格代码作为补充信息进行图像解码。此外，我们还设计了一种新的对比风格学习方法，以获取质量判别的风格表示，从而有利于高质量的图像生成。<br>
                    效果：实验结果表明，我们的MATEBIT方法在各种图像翻译任务上的表现明显优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel framework for exemplar based image translation. Recent advanced methods for this task mainly focus on establishing cross-domain semantic correspondence, which sequentially dominates image generation in the manner of local style control. Unfortunately, cross domain semantic matching is challenging; and matching errors ultimately degrade the quality of generated images. To overcome this challenge, we improve the accuracy of matching on the one hand, and diminish the role of matching in image generation on the other hand. To achieve the former, we propose a masked and adaptive transformer (MAT) for learning accurate cross-domain correspondence, and executing context-aware feature augmentation. To achieve the latter, we use source features of the input and global style codes of the exemplar, as supplementary information, for decoding an image. Besides, we devise a novel contrastive style learning method, for acquire quality-discriminative style representations, which in turn benefit high-quality image generation. Experimental results show that our method, dubbed MATEBIT, performs considerably better than state-of-the-art methods, in diverse image translation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1920.Imagic: Text-Based Real Image Editing With Diffusion Models</span><br>
                <span class="as">Kawar, BahjatandZada, ShiranandLang, OranandTov, OmerandChang, HuiwenandDekel, TaliandMosseri, InbarandIrani, Michal</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6007-6017.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将复杂的基于文本的语义编辑应用于单个真实图像。<br>
                    动机：目前的大部分方法仅限于特定的编辑类型、合成生成的图像或需要多个相同对象的输入图像，缺乏对真实图像进行复杂语义编辑的能力。<br>
                    方法：本文提出了一种名为Imagic的方法，利用预训练的文本到图像扩散模型，通过产生与输入图像和目标文本对齐的文本嵌入，并微调扩散模型以捕获图像特定的外观，实现对单个真实图像进行复杂语义编辑。<br>
                    效果：在各种领域的多个输入上展示了Imagic的高质量和多功能性，并在引入的TEdBench图像编辑基准测试中，用户研究结果显示人类评价者更喜欢Imagic而不是先前的领先编辑方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. -- each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1921.LightPainter: Interactive Portrait Relighting With Freehand Scribble</span><br>
                <span class="as">Mei, YiqunandZhang, HeandZhang, XuanerandZhang, JianmingandShu, ZhixinandWang, YilinandWei, ZijunandYan, ShiandJung, HyunJoonandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_LightPainter_Interactive_Portrait_Relighting_With_Freehand_Scribble_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/195-205.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的肖像重光照方法在实现理想的照明效果时缺乏用户交互和精确的照明控制。<br>
                    动机：提出一种基于涂鸦的重光照系统LightPainter，使用户能够轻松地交互式操纵肖像照明效果。<br>
                    方法：通过两个条件神经网络，一个根据肤色恢复几何和漫反射的快乐模块，和一个用于重光照的基于涂鸦的模块。<br>
                    效果：通过定量和定性实验展示了高质量和灵活的肖像照明编辑能力。与商业照明编辑工具的用户研究比较也显示了用户对该方法的一致偏好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent portrait relighting methods have achieved realistic results of portrait lighting effects given a desired lighting representation such as an environment map. However, these methods are not intuitive for user interaction and lack precise lighting control. We introduce LightPainter, a scribble-based relighting system that allows users to interactively manipulate portrait lighting effect with ease. This is achieved by two conditional neural networks, a delighting module that recovers geometry and albedo optionally conditioned on skin tone, and a scribble-based module for relighting. To train the relighting module, we propose a novel scribble simulation procedure to mimic real user scribbles, which allows our pipeline to be trained without any human annotations. We demonstrate high-quality and flexible portrait lighting editing capability with both quantitative and qualitative experiments. User study comparisons with commercial lighting editing tools also demonstrate consistent user preference for our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1922.Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model</span><br>
                <span class="as">Aoshima, TakehiroandMatsubara, Takashi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Aoshima_Deep_Curvilinear_Editing_Commutative_and_Nonlinear_Image_Manipulation_for_Pretrained_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5957-5967.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对生成的图像进行语义编辑。<br>
                    动机：尽管深度学习方法如生成对抗网络（GANs）能够产生高质量的图像，但它们通常没有固有的方式来对生成的图像进行语义编辑。<br>
                    方法：本研究提出了一种新的深度曲线编辑（DeCurvEd）方法，在潜在空间上确定语义交换向量场。<br>
                    效果：由于交换性，多个属性的编辑仅取决于数量而不是顺序。实验证明，与以往的方法相比，DeCurvEd的非线性和交换性提供了更高质量的编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate that compared to previous methods, the nonlinear and commutative nature of DeCurvEd provides higher-quality editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1923.Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement</span><br>
                <span class="as">Wu, YuhuiandPan, ChenandWang, GuoqingandYang, YangandWei, JiweiandLi, ChongyiandShen, HengTao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Learning_Semantic-Aware_Knowledge_Guidance_for_Low-Light_Image_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1662-1671.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过改进照明条件和生成正常光照图像来提高低光图像质量。<br>
                    动机：现有的大部分方法通过全局统一的方式改善低光图像，没有考虑到不同区域的语义信息。<br>
                    方法：提出了一种新的语义感知知识引导框架（SKF），该框架可以在学习丰富的、多样的先验知识的同时，帮助低光增强模型进行学习。主要从三个方面引入语义知识：一是在特征表示空间中明智地集成语义先验知识的语义感知嵌入模块；二是保留各种实例颜色一致性的语义引导颜色直方图损失；三是通过语义先验知识产生更自然纹理的语义引导对抗性损失。<br>
                    效果：实验表明，配备了SKF的模型在多个数据集上显著优于基线，并且SKF可以很好地推广到不同的模型和场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Low-light image enhancement (LLIE) investigates how to improve illumination and produce normal-light images. The majority of existing methods improve low-light images via a global and uniform manner, without taking into account the semantic information of different regions. Without semantic priors, a network may easily deviate from a region's original color. To address this issue, we propose a novel semantic-aware knowledge-guided framework (SKF) that can assist a low-light enhancement model in learning rich and diverse priors encapsulated in a semantic segmentation model. We concentrate on incorporating semantic knowledge from three key aspects: a semantic-aware embedding module that wisely integrates semantic priors in feature representation space, a semantic-guided color histogram loss that preserves color consistency of various instances, and a semantic-guided adversarial loss that produces more natural textures by semantic priors. Our SKF is appealing in acting as a general framework in LLIE task. Extensive experiments show that models equipped with the SKF significantly outperform the baselines on multiple datasets and our SKF generalizes to different models and scenes well. The code is available at Semantic-Aware-Low-Light-Image-Enhancement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1924.Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process</span><br>
                <span class="as">Li, YuhanandDou, YishunandChen, XuanhongandNi, BingbingandSun, YilinandLiu, YutianandWang, Fuzhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Generalized_Deep_3D_Shape_Prior_via_Part-Discretized_Diffusion_Process_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16784-16794.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种适用于多种3D任务的通用3D形状生成先验模型，包括无条件形状生成、点云补全和跨模态形状生成等。<br>
                    动机：为了精确捕捉局部精细的形状信息，并建立不同标记之间的固有结构依赖关系，同时抑制高频率形状特征波动。<br>
                    方法：利用基于广泛任务训练数据的紧凑代码簿的向量量化变分自编码器（VQ-VAE）来索引局部几何形状，引入离散扩散生成器来建模不同标记之间的固有结构依赖关系，并开发多频融合模块（MFM）来抑制高频率形状特征波动。<br>
                    效果：通过上述设计，所提出的3D形状先验模型具有高保真度、多样化特征以及跨模态对齐的能力，在各种3D形状生成任务上表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We develop a generalized 3D shape generation prior model, tailored for multiple 3D tasks including unconditional shape generation, point cloud completion, and cross-modality shape generation, etc. On one hand, to precisely capture local fine detailed shape information, a vector quantized variational autoencoder (VQ-VAE) is utilized to index local geometry from a compactly learned codebook based on a broad set of task training data. On the other hand, a discrete diffusion generator is introduced to model the inherent structural dependencies among different tokens. In the meantime, a multi-frequency fusion module (MFM) is developed to suppress high-frequency shape feature fluctuations, guided by multi-frequency contextual information. The above designs jointly equip our proposed 3D shape prior model with high-fidelity, diverse features as well as the capability of cross-modality alignment, and extensive experiments have demonstrated superior performances on various 3D shape generation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1925.CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search</span><br>
                <span class="as">Shamshad, FahadandNaseer, MuzammalandNandakumar, Karthik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_CLIP2Protect_Protecting_Facial_Privacy_Using_Text-Guided_Makeup_via_Adversarial_Latent_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20595-20605.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习在人脸识别领域的成功引发了隐私问题，如何在保护面部隐私的同时不影响用户体验。<br>
                    动机：现有的增强隐私的方法无法生成自然图像来保护面部隐私。<br>
                    方法：提出一种新的两步法进行面部隐私保护，该方法依赖于在预训练的生成模型的低维流形中找到对抗性潜在代码。第一步将给定的人脸图像反转到潜在空间中，并微调生成模型以从其潜在代码中准确重建给定的图像。这一步产生了良好的初始状态，有助于生成与给定身份相似的高质量人脸。随后，使用用户定义的化妆文本提示和身份保护正则化来引导在潜在空间中搜索对抗性代码。<br>
                    效果：实验表明，我们的方法生成的人脸具有更强的黑盒转移能力，在面部验证任务上比最先进的面部隐私保护方法绝对提高了12.06%。最后，我们展示了该方法在商业人脸识别系统上的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of deep learning based face recognition systems has given rise to serious privacy concerns due to their ability to enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic' images that can protect facial privacy without compromising user experience. We propose a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The first step inverts the given face image into the latent space and finetunes the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. Subsequently, user defined makeup text prompts and identity-preserving regularization are used to guide the search for adversarial codes in the latent space. Extensive experiments demonstrate that faces generated by our approach have stronger black-box transferability with an absolute gain of 12.06% over the state-of-the-art facial privacy protection approach under the face verification task. Finally, we demonstrate the effectiveness of the proposed approach for commercial face recognition systems. Our code is available at https://github.com/fahadshamshad/Clip2Protect.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1926.PAniC-3D: Stylized Single-View 3D Reconstruction From Portraits of Anime Characters</span><br>
                <span class="as">Chen, ShuhongandZhang, KevinandShi, YichunandWang, HengandZhu, YihengandSong, GuoxianandAn, SizheandKristjansson, JanusandYang, XiaoandZwicker, Matthias</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_PAniC-3D_Stylized_Single-View_3D_Reconstruction_From_Portraits_of_Anime_Characters_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21068-21077.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接从插图化的动漫角色肖像中重建风格化的3D角色头部。<br>
                    动机：动漫风格的领域对单视图重建提出了独特的挑战，与人类头部的自然图像相比，角色肖像插图的头发和配饰具有更复杂和多样化的几何形状，并且用非照片真实的轮廓线进行着色。此外，缺乏适合训练和评估这种模糊的风格化重建任务的3D模型和肖像插图数据。<br>
                    方法：提出的PAniC-3D架构通过线条填充模型跨越插图到3D领域的鸿沟，并用体积辐射场表示复杂的几何形状。使用两个大型新数据集（11.2k Vroid 3D模型，1k Vtuber肖像插图）训练系统，并在新的动漫重建基准测试集中进行评估。<br>
                    效果：PAniC-3D显著优于基线方法，并为从肖像插图中重建风格化的任务提供了数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose PAniC-3D, a system to reconstruct stylized 3D character heads directly from illustrated (p)ortraits of (ani)me (c)haracters. Our anime-style domain poses unique challenges to single-view reconstruction; compared to natural images of human heads, character portrait illustrations have hair and accessories with more complex and diverse geometry, and are shaded with non-photorealistic contour lines. In addition, there is a lack of both 3D model and portrait illustration data suitable to train and evaluate this ambiguous stylized reconstruction task. Facing these challenges, our proposed PAniC-3D architecture crosses the illustration-to-3D domain gap with a line-filling model, and represents sophisticated geometries with a volumetric radiance field. We train our system with two large new datasets (11.2k Vroid 3D models, 1k Vtuber portrait illustrations), and evaluate on a novel AnimeRecon benchmark of illustration-to-3D pairs. PAniC-3D significantly outperforms baseline methods, and provides data to establish the task of stylized reconstruction from portrait illustrations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1927.DCFace: Synthetic Face Generation With Dual Condition Diffusion Model</span><br>
                <span class="as">Kim, MinchulandLiu, FengandJain, AnilandLiu, Xiaoming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12715-12725.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练人脸识别模型的合成数据集生成具有挑战性，因为不仅需要创建高保真图像，还需要在不同因素（如姿态、照明、表情、老化和遮挡）下生成同一主题的多张图像。<br>
                    动机：先前的研究使用GAN或3D模型来生成合成数据集，本研究从结合主题外观（ID）和外部因素（风格）条件的角度解决这个问题。<br>
                    方法：提出了一种基于扩散模型的双重条件人脸生成器（DCFace），其新颖的Patch-wise风格提取器和Time-step依赖的ID损失使DCFace能够精确控制同一主题在不同风格下的面部图像生成。<br>
                    效果：在LFW、CFP-FP、CPLFW、AgeDB和CALFW等5个测试数据集中，有4个数据集上，用提出的DCFace生成的合成图像训练的人脸识别模型相比之前的工作平均提高了6.11%的验证准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating synthetic datasets for training face recognition models is challenging because dataset generation entails more than creating high fidelity images. It involves generating multiple images of same subjects under different factors (e.g., variations in pose, illumination, expression, aging and occlusion) which follows the real image conditional distribution. Previous works have studied the generation of synthetic datasets using GAN or 3D models. In this work, we approach the problem from the aspect of combining subject appearance (ID) and external factor (style) conditions. These two conditions provide a direct way to control the inter-class and intra-class variations. To this end, we propose a Dual Condition Face Generator (DCFace) based on a diffusion model. Our novel Patch-wise style extractor and Time-step dependent ID loss enables DCFace to consistently produce face images of the same subject under different styles with precise control. Face recognition models trained on synthetic images from the proposed DCFace provide higher verification accuracies compared to previous works by 6.11% on average in 4 out of 5 test datasets, LFW, CFP-FP, CPLFW, AgeDB and CALFW. Model, code, and synthetic dataset are available at https://github.com/mk-minchul/dcface</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1928.Perception-Oriented Single Image Super-Resolution Using Optimal Objective Estimation</span><br>
                <span class="as">Park, SeungHoandMoon, YoungSuandCho, NamIk</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Perception-Oriented_Single_Image_Super-Resolution_Using_Optimal_Objective_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1725-1735.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化使用单一图像超分辨率（SISR）网络进行高对比度输出。<br>
                    动机：虽然现有的SISR网络通过感知损失和对抗损失训练可以提供高对比度的输出，但仅使用单一的感知损失无法准确恢复图像中局部变化的形状，常常产生不自然的纹理或细节。<br>
                    方法：本文提出了一种新的SISR框架，该框架为每个区域应用最优目标以生成整体高分辨率输出的合理结果。具体来说，该框架包含两个模型：一个预测模型，用于为给定的低分辨率（LR）输入推断最优目标图；一个生成模型，用于应用目标目标图来生成相应的SR输出。生成模型在代表一组基本目标的目标轨迹上进行训练，使单个网络能够学习对应于轨迹上组合损失的各种SR结果。预测模型则使用成对的LR图像和从目标轨迹中搜索到的相应最优目标图进行训练。<br>
                    效果：在五个基准测试集上的实验结果表明，该方法在LPIPS、DISTS、PSNR和SSIM指标上都优于最先进的感知驱动的SR方法。视觉结果也证明了我们的方法在感知导向重建方面的优越性。代码可在https://github.com/seungho-snu/SROOE获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single-image super-resolution (SISR) networks trained with perceptual and adversarial losses provide high-contrast outputs compared to those of networks trained with distortion-oriented losses, such as L1 or L2. However, it has been shown that using a single perceptual loss is insufficient for accurately restoring locally varying diverse shapes in images, often generating undesirable artifacts or unnatural details. For this reason, combinations of various losses, such as perceptual, adversarial, and distortion losses, have been attempted, yet it remains challenging to find optimal combinations. Hence, in this paper, we propose a new SISR framework that applies optimal objectives for each region to generate plausible results in overall areas of high-resolution outputs. Specifically, the framework comprises two models: a predictive model that infers an optimal objective map for a given low-resolution (LR) input and a generative model that applies a target objective map to produce the corresponding SR output. The generative model is trained over our proposed objective trajectory representing a set of essential objectives, which enables the single network to learn various SR results corresponding to combined losses on the trajectory. The predictive model is trained using pairs of LR images and corresponding optimal objective maps searched from the objective trajectory. Experimental results on five benchmarks show that the proposed method outperforms state-of-the-art perception-driven SR methods in LPIPS, DISTS, PSNR, and SSIM metrics. The visual results also demonstrate the superiority of our method in perception-oriented reconstruction. The code is available at https://github.com/seungho-snu/SROOE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1929.GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning</span><br>
                <span class="as">Xie, ZhenyuandHuang, ZaiyuandDong, XinandZhao, FuweiandDong, HaoyeandZhang, XijinandZhu, FeidaandLiang, Xiaodan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23550-23559.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于图像的虚拟试穿技术在处理复杂输入（如复杂的人体姿势、困难的服装）时，无法保留不同部分的语义信息，且直接进行变形会导致纹理失真。<br>
                    动机：为了解决上述问题，并推动虚拟试穿技术在实际中的应用。<br>
                    方法：提出了一种名为GP-VTON的通用虚拟试穿框架，通过开发局部流动全局解析（LFGP）变形模块和动态梯度截断（DGT）训练策略。LFGP采用局部流动对服装各部分进行单独变形，并通过全局服装解析组装局部变形结果，即使在复杂输入下也能生成合理的变形部分和语义正确的完整服装。DGT训练策略动态截断重叠区域和变形服装的梯度，有效避免了纹理压缩问题。<br>
                    效果：实验证明，GP-VTON在两个高分辨率基准测试中优于现有最先进的方法，且可以容易地扩展到多类别场景，并使用来自不同服装类别的数据进行联合训练。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-based Virtual Try-ON aims to transfer an in-shop garment onto a specific person. Existing methods employ a global warping module to model the anisotropic deformation for different garment parts, which fails to preserve the semantic information of different parts when receiving challenging inputs (e.g, intricate human poses, difficult garments). Moreover, most of them directly warp the input garment to align with the boundary of the preserved region, which usually requires texture squeezing to meet the boundary shape constraint and thus leads to texture distortion. The above inferior performance hinders existing methods from real-world applications. To address these problems and take a step towards real-world virtual try-on, we propose a General-Purpose Virtual Try-ON framework, named GP-VTON, by developing an innovative Local-Flow Global-Parsing (LFGP) warping module and a Dynamic Gradient Truncation (DGT) training strategy. Specifically, compared with the previous global warping mechanism, LFGP employs local flows to warp garments parts individually, and assembles the local warped results via the global garment parsing, resulting in reasonable warped parts and a semantic-correct intact garment even with challenging inputs.On the other hand, our DGT training strategy dynamically truncates the gradient in the overlap area and the warped garment is no more required to meet the boundary constraint, which effectively avoids the texture squeezing problem. Furthermore, our GP-VTON can be easily extended to multi-category scenario and jointly trained by using data from different garment categories. Extensive experiments on two high-resolution benchmarks demonstrate our superiority over the existing state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1930.Video Probabilistic Diffusion Models in Projected Latent Space</span><br>
                <span class="as">Yu, SihyunandSohn, KihyukandKim, SubinandShin, Jinwoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18456-18466.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成高分辨率、时间连贯的视频？<br>
                    动机：尽管深度生成模型取得了显著的进步，但由于视频的高维度和复杂的时空动态以及大的空间变化，合成高分辨率和时间连贯的视频仍然是一个挑战。<br>
                    方法：提出了一种新的视频生成模型——投影潜在视频扩散模型（PVDM），这是一种概率扩散模型，可以在低维潜在空间中学习视频分布，因此可以在有限的资源下高效地训练高分辨率的视频。具体来说，PVDM由两个组件组成：（a）一个自动编码器，将给定的视频投影为二维形状的潜在向量，分解视频像素的复杂立方结构；（b）一种专门针对我们的新因子化潜在空间和训练/采样过程的扩散模型架构，以单模型合成任意长度的视频。<br>
                    效果：在流行的视频生成数据集上的实验表明，PVDM优于以前的视频合成方法；例如，PVDM在UCF-101长视频（128帧）生成基准测试中获得了639.7的FVD分数，比之前最先进的提高了1773.4。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized latent space and the training/sampling procedure to synthesize videos of arbitrary length with a single model. Experiments on popular video generation datasets demonstrate the superiority of PVDM compared with previous video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of the prior state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1931.NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation</span><br>
                <span class="as">Wang, ZiyanandNam, GiljooandStuyck, TuurandLombardi, StephenandCao, ChenandSaragih, JasonandZollh\&quot;ofer, MichaelandHodgins, JessicaandLassner, Christoph</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_NeuWigs_A_Neural_Dynamic_Model_for_Volumetric_Hair_Capture_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8641-8651.png><br>
            
            <span class="tt"><span class="t0">研究问题：在虚拟现实中创建逼真的头像时，如何捕捉和动画化人类的头发是两个主要挑战。<br>
                    动机：由于头发具有复杂的几何形状、外观和运动特性，因此这两个问题都非常具有挑战性。<br>
                    方法：本文提出了一种两阶段的方法，将头发与头部独立建模，以数据驱动的方式解决这些挑战。第一阶段，状态压缩，通过一种新型的自动编码器跟踪策略，学习包含运动和外观的3D头发状态的低维潜在空间。为了在外观学习中更好地分离头发和头部，我们结合了多视图头发分割掩码和一个可微体积渲染器。第二阶段学习一种新的头发动力学模型，根据发现的潜在代码进行时间上的头发转移。为了在我们的动力学模型中强制更高的稳定性，我们使用压缩阶段的3D点云自动编码器对头发状态进行去噪。<br>
                    效果：我们的模型在新颖视图合成方面优于现有技术，并且能够在不依赖头发观察作为驱动信号的情况下创建新的发型动画。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The capture and animation of human hair are two of the major challenges in the creation of realistic avatars for the virtual reality. Both problems are highly challenging, because hair has complex geometry and appearance, as well as exhibits challenging motion. In this paper, we present a two-stage approach that models hair independently from the head to address these challenges in a data-driven manner. The first stage, state compression, learns a low-dimensional latent space of 3D hair states containing motion and appearance, via a novel autoencoder-as-a-tracker strategy. To better disentangle the hair and head in appearance learning, we employ multi-view hair segmentation masks in combination with a differentiable volumetric renderer. The second stage learns a novel hair dynamics model that performs temporal hair transfer based on the discovered latent codes. To enforce higher stability while driving our dynamics model, we employ the 3D point-cloud autoencoder from the compression stage for de-noising of the hair state. Our model outperforms the state of the art in novel view synthesis and is capable of creating novel hair animations without having to rely on hair observations as a driving signal</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1932.One-Shot High-Fidelity Talking-Head Synthesis With Deformable Neural Radiance Field</span><br>
                <span class="as">Li, WeichuangandZhang, LonghaoandWang, DongandZhao, BinandWang, ZhigangandChen, MulinandZhang, BangandWang, ZhongjianandBo, LiefengandLi, Xuelong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17969-17978.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成能保持源图像身份信息并模仿驱动图像动作的头部图像？<br>
                    动机：大多数先前的方法主要依赖2D表示，因此在遇到大头旋转时会不可避免地出现面部扭曲。<br>
                    方法：本文提出了HiDe-NeRF，它通过将3D动态场景表示为一个标准外观场和一个隐含的形变场来实现高保真度和自由视角的说话人合成。<br>
                    效果：实验结果表明，我们提出的方法比之前的工作能产生更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Talking head generation aims to generate faces that maintain the identity information of the source image and imitate the motion of the driving image. Most pioneering methods rely primarily on 2D representations and thus will inevitably suffer from face distortion when large head rotations are encountered. Recent works instead employ explicit 3D structural representations or implicit neural rendering to improve performance under large pose changes. Nevertheless, the fidelity of identity and expression is not so desirable, especially for novel-view synthesis. In this paper, we propose HiDe-NeRF, which achieves high-fidelity and free-view talking-head synthesis. Drawing on the recently proposed Deformable Neural Radiance Fields, HiDe-NeRF represents the 3D dynamic scene into a canonical appearance field and an implicit deformation field, where the former comprises the canonical source face and the latter models the driving pose and expression. In particular, we improve fidelity from two aspects: (i) to enhance identity expressiveness, we design a generalized appearance module that leverages multi-scale volume features to preserve face shape and details; (ii) to improve expression preciseness, we propose a lightweight deformation module that explicitly decouples the pose and expression to enable precise expression modeling. Extensive experiments demonstrate that our proposed approach can generate better results than previous works. Project page: https://www.waytron.net/hidenerf/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1933.Conditional Image-to-Video Generation With Latent Flow Diffusion Models</span><br>
                <span class="as">Ni, HaomiaoandShi, ChanghaoandLi, KaiandHuang, SharonX.andMin, MartinRenqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18444-18455.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决条件图像到视频（cI2V）生成任务中，如何从给定的图像和条件（如动作类别标签）出发，同时生成逼真的空间外观和时间动态的新视频。<br>
                    动机：条件图像到视频生成任务的主要挑战在于如何同时生成与给定图像和条件相对应的真实空间外观和时间动态。现有的直接合成方法无法充分利用给定图像的空间内容进行合成。<br>
                    方法：本文提出了一种使用新颖的潜在流扩散模型（LFDM）进行条件图像到视频生成的方法。LFDM在潜在空间中根据给定的条件合成光流序列，以扭曲给定的图像。相比于直接合成的方法，LFDM能更好地利用给定图像的空间内容，并在潜在空间中根据生成的时间连贯流进行扭曲，从而更好地合成空间细节和时间运动。<br>
                    效果：实验结果表明，LFDM在多个数据集上的表现均优于现有方法。此外，通过简单地微调图像解码器，LFDM可以很容易地适应新的领域。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Conditional image-to-video (cI2V) generation aims to synthesize a new plausible video starting from an image (e.g., a person's face) and a condition (e.g., an action class label like smile). The key challenge of the cI2V task lies in the simultaneous generation of realistic spatial appearance and temporal dynamics corresponding to the given image and condition. In this paper, we propose an approach for cI2V using novel latent flow diffusion models (LFDM) that synthesize an optical flow sequence in the latent space based on the given condition to warp the given image. Compared to previous direct-synthesis-based works, our proposed LFDM can better synthesize spatial details and temporal motion by fully utilizing the spatial content of the given image and warping it in the latent space according to the generated temporally-coherent flow. The training of LFDM consists of two separate stages: (1) an unsupervised learning stage to train a latent flow auto-encoder for spatial content generation, including a flow predictor to estimate latent flow between pairs of video frames, and (2) a conditional learning stage to train a 3D-UNet-based diffusion model (DM) for temporal latent flow generation. Unlike previous DMs operating in pixel space or latent feature space that couples spatial and temporal information, the DM in our LFDM only needs to learn a low-dimensional latent flow space for motion generation, thus being more computationally efficient. We conduct comprehensive experiments on multiple datasets, where LFDM consistently outperforms prior arts. Furthermore, we show that LFDM can be easily adapted to new domains by simply finetuning the image decoder. Our code is available at https://github.com/nihaomiao/CVPR23_LFDM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1934.Towards Universal Fake Image Detectors That Generalize Across Generative Models</span><br>
                <span class="as">Ojha, UtkarshandLi, YuhengandLee, YongJae</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24480-24489.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的生成模型检测器无法有效识别新出现的生成模型产生的假图像。<br>
                    动机：目前的假图像检测方法主要通过训练深度网络进行真实与伪造分类，但这种方法在面对新的生成模型时效果不佳。<br>
                    方法：提出一种无需学习的真实与伪造分类方法，即不直接在特征空间中区分真实和假图像。具体实施方式包括最近邻分类和线性探测。<br>
                    效果：在大型预训练的视觉语言模型的特征空间中，即使使用非常简单的最近邻分类作为基线，也能在检测各种生成模型产生的假图像上表现出惊人的泛化能力，例如，在未见过的传播和自回归模型上，其性能比当前最佳方法提高了+15.07 mAP和+25.90%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With generative models proliferating at a rapid rate, there is a growing need for general purpose fake image detectors. In this work, we first show that the existing paradigm, which consists of training a deep network for real-vs-fake classification, fails to detect fake images from newer breeds of generative models when trained to detect GAN fake images. Upon analysis, we find that the resulting classifier is asymmetrically tuned to detect patterns that make an image fake. The real class becomes a 'sink' class holding anything that is not fake, including generated images from models not accessible during training. Building upon this discovery, we propose to perform real-vs-fake classification without learning; i.e., using a feature space not explicitly trained to distinguish real from fake images. We use nearest neighbor and linear probing as instantiations of this idea. When given access to the feature space of a large pretrained vision-language model, the very simple baseline of nearest neighbor classification has surprisingly good generalization ability in detecting fake images from a wide variety of generative models; e.g., it improves upon the SoTA by +15.07 mAP and +25.90% acc when tested on unseen diffusion and autoregressive models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1935.DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model</span><br>
                <span class="as">Kim, GwanghyunandChun, SeYoung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DATID-3D_Diversity-Preserved_Domain_Adaptation_Using_Text-to-Image_Diffusion_for_3D_Generative_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14203-14213.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image correspondence and poor image quality. Here we propose DATID-3D, a domain adaptation method tailored for 3D generative models using text-to-image diffusion models that can synthesize diverse images per text prompt without collecting additional images and camera information for the target domain. Unlike 3D extensions of prior text-guided domain adaptation methods, our novel pipeline was able to fine-tune the state-of-the-art 3D generator of the source domain to synthesize high resolution, multi-view consistent images in text-guided targeted domains without additional data, outperforming the existing text-guided domain adaptation methods in diversity and text-image correspondence. Furthermore, we propose and demonstrate diverse 3D image manipulations such as one-shot instance-selected adaptation and single-view manipulated 3D reconstruction to fully enjoy diversity in text.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1936.ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal</span><br>
                <span class="as">Guo, LanqingandWang, ChongandYang, WenhanandHuang, SiyuandWang, YufeiandPfister, HanspeterandWen, Bihan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_ShadowDiffusion_When_Degradation_Prior_Meets_Diffusion_Model_for_Shadow_Removal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14049-14058.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习方法在图像阴影移除上取得了良好的效果，但恢复的图像边界伪影问题仍然严重。<br>
                    动机：由于缺乏退化先验和模型能力不足，导致恢复的图像边界伪影问题严重。<br>
                    方法：提出了一种统一的扩散框架，整合了图像和退化先验，用于高效的影子移除。首先提出一个影子退化模型，然后构建了一个名为ShadowDiffusion的新型展开扩散模型，通过逐步改进期望输出来提高模型在阴影移除上的能力。<br>
                    效果：在三个流行的公共数据集上进行了大量的实验，结果显示，与最先进的方法相比，我们的模型在PSNR上有了显著的提高，从31.69dB提高到34.73dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent deep learning methods have achieved promising results in image shadow removal. However, their restored images still suffer from unsatisfactory boundary artifacts, due to the lack of degradation prior and the deficiency in modeling capacity. Our work addresses these issues by proposing a unified diffusion framework that integrates both the image and degradation priors for highly effective shadow removal. In detail, we first propose a shadow degradation model, which inspires us to build a novel unrolling diffusion model, dubbed ShandowDiffusion. It remarkably improves the model's capacity in shadow removal via progressively refining the desired output with both degradation prior and diffusive generative prior, which by nature can serve as a new strong baseline for image restoration. Furthermore, ShadowDiffusion progressively refines the estimated shadow mask as an auxiliary task of the diffusion generator, which leads to more accurate and robust shadow-free image generation. We conduct extensive experiments on three popular public datasets, including ISTD, ISTD+, and SRD, to validate our method's effectiveness. Compared to the state-of-the-art methods, our model achieves a significant improvement in terms of PSNR, increasing from 31.69dB to 34.73dB over SRD dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1937.FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction</span><br>
                <span class="as">Bai, HaoranandKang, DiandZhang, HaoxianandPan, JinshanandBao, Linchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_FFHQ-UV_Normalized_Facial_UV-Texture_Dataset_for_3D_Face_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/362-371.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模人脸图像数据集生成高质量的面部UV纹理地图，以用于在各种光照条件下渲染真实的3D人脸模型。<br>
                    动机：现有的面部UV纹理数据集质量参差不齐，且多样性不足。因此，需要一种能够自动、稳健地从大规模人脸图像数据集中生成高质量面部UV纹理地图的方法。<br>
                    方法：通过使用基于StyleGAN的人脸图像编辑方法从单张人脸图像中生成多视图标准化人脸图像，然后应用详细的UV纹理提取、校正和完成过程从标准化人脸图像中生成高质量的UV映射。<br>
                    效果：实验表明，该方法提高了重建精度，超过了最先进的方法，并且生成的高质量纹理地图已准备好进行逼真的渲染。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a large-scale facial UV-texture dataset that contains over 50,000 high-quality texture UV-maps with even illuminations, neutral expressions, and cleaned facial regions, which are desired characteristics for rendering realistic 3D face models under different lighting conditions. The dataset is derived from a large-scale face image dataset namely FFHQ, with the help of our fully automatic and robust UV-texture production pipeline. Our pipeline utilizes the recent advances in StyleGAN-based facial image editing approaches to generate multi-view normalized face images from single-image inputs. An elaborated UV-texture extraction, correction, and completion procedure is then applied to produce high-quality UV-maps from the normalized face images. Compared with existing UV-texture datasets, our dataset has more diverse and higher-quality texture maps. We further train a GAN-based texture decoder as the nonlinear texture basis for parametric fitting based 3D face reconstruction. Experiments show that our method improves the reconstruction accuracy over state-of-the-art approaches, and more importantly, produces high-quality texture maps that are ready for realistic renderings. The dataset, code, and pre-trained texture decoder are publicly available at https://github.com/csbhr/FFHQ-UV.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1938.Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer</span><br>
                <span class="as">Tang, HaoandLiu, SonghuaandLin, TianweiandHuang, ShaoliandLi, FuandHe, DongliangandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Master_Meta_Style_Transformer_for_Controllable_Zero-Shot_and_Few-Shot_Artistic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18329-18338.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用Transformer模型进行艺术风格转移，并解决参数过多和内容失真的问题。<br>
                    动机：Transformer模型在艺术风格转移任务上表现良好，但其多层结构导致参数过多，训练负担重，且直接融合内容和风格特征易导致内容失真。<br>
                    方法：提出一种名为Master的新型Transformer模型，通过共享参数、引入可学习的缩放操作以及元学习机制，以降低参数数量、提高训练稳定性、保留内容特征的相似性并确保风格化质量。<br>
                    效果：实验证明，Master模型在零样本和少样本风格转移设置下均表现出优越性，实现了文本引导的少样本风格转移。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformer-based models achieve favorable performance in artistic style transfer recently thanks to its global receptive field and powerful multi-head/layer attention operations. Nevertheless, the over-paramerized multi-layer structure increases parameters significantly and thus presents a heavy burden for training. Moreover, for the task of style transfer, vanilla Transformer that fuses content and style features by residual connections is prone to content-wise distortion. In this paper, we devise a novel Transformer model termed as Master specifically for style transfer. On the one hand, in the proposed model, different Transformer layers share a common group of parameters, which (1) reduces the total number of parameters, (2) leads to more robust training convergence, and (3) is readily to control the degree of stylization via tuning the number of stacked layers freely during inference. On the other hand, different from the vanilla version, we adopt a learnable scaling operation on content features before content-style feature interaction, which better preserves the original similarity between a pair of content features while ensuring the stylization quality. We also propose a novel meta learning scheme for the proposed model so that it can not only work in the typical setting of arbitrary style transfer, but also adaptable to the few-shot setting, by only fine-tuning the Transformer encoder layer in the few-shot stage for one specific style. Text-guided few-shot style transfer is firstly achieved with the proposed framework. Extensive experiments demonstrate the superiority of Master under both zero-shot and few-shot style transfer settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1939.Affordance Diffusion: Synthesizing Hand-Object Interactions</span><br>
                <span class="as">Ye, YufeiandLi, XuetingandGupta, AbhinavandDeMello, ShaliniandBirchfield, StanandSong, JiamingandTulsiani, ShubhamandLiu, Sifei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22479-22489.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像合成中复杂交互（如手部与物体的互动）的问题。<br>
                    动机：目前的图像合成方法大多局限于文本或图像条件生成，对于复杂的手部与物体互动合成能力有限。<br>
                    方法：提出一种两步生成方法，首先利用LayoutNet预测一个与关节无关的手-物体交互布局，然后利用ContentNet根据预测的布局合成手抓取物体的图像。这两个网络都建立在大规模预训练的扩散模型之上，以利用其潜在表示。<br>
                    效果：实验结果表明，该方法在新颖物体上具有更好的泛化能力，并且在分布外的自然场景上也表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two step generative approach that leverages a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1940.Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method</span><br>
                <span class="as">Yi, RanandTian, HaoyuanandGu, ZhihaoandLai, Yu-KunandRosin, PaulL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Towards_Artistic_Image_Aesthetics_Assessment_A_Large-Scale_Dataset_and_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22388-22397.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何评估艺术图像的审美质量，特别是在现有数据集只包含相对较少艺术作品的情况下。<br>
                    动机：当前的艺术图像审美评估（AIAA）研究主要依赖于大规模数据集，但现有的数据集对于艺术图像的评估存在缺陷，因此需要一种新的方法来解决这个问题。<br>
                    方法：提出了一种新方法SAAN（Style-specific Art Assessment Network），该方法可以有效地提取和使用特定风格和通用的审美信息来评估艺术图像。同时，还引入了一个大规模的AIAA数据集：Boldbrush Artistic Image Dataset（BAID）。<br>
                    效果：实验结果表明，相比于现有的IAA方法，SAAN在提出的BAID数据集上的表现更好。这种方法和数据集可以为未来的AIAA工作提供基础，并激发该领域的更多研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image aesthetics assessment (IAA) is a challenging task due to its highly subjective nature. Most of the current studies rely on large-scale datasets (e.g., AVA and AADB) to learn a general model for all kinds of photography images. However, little light has been shed on measuring the aesthetic quality of artistic images, and the existing datasets only contain relatively few artworks. Such a defect is a great obstacle to the aesthetic assessment of artistic images. To fill the gap in the field of artistic image aesthetics assessment (AIAA), we first introduce a large-scale AIAA dataset: Boldbrush Artistic Image Dataset (BAID), which consists of 60,337 artistic images covering various art forms, with more than 360,000 votes from online users. We then propose a new method, SAAN (Style-specific Art Assessment Network), which can effectively extract and utilize style-specific and generic aesthetic information to evaluate artistic images. Experiments demonstrate that our proposed approach outperforms existing IAA methods on the proposed BAID dataset according to quantitative comparisons. We believe the proposed dataset and method can serve as a foundation for future AIAA works and inspire more research in this field.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1941.Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</span><br>
                <span class="as">Tumanyan, NarekandGeyer, MichalandBagon, ShaiandDekel, Tali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1921-1930.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本到图像生成模型进行真实世界的内容创作，并为用户提供对生成内容的精细控制。<br>
                    动机：虽然大规模的文本到图像生成模型在合成复杂视觉概念的多样化图像方面取得了突破性进展，但在将其用于真实世界的内容创作时，如何让用户对生成内容有精细的控制仍是一个关键挑战。<br>
                    方法：本文提出了一种新的框架，将文本到图像的合成推向了图像到图像转换的领域。给定一个指导图像和目标文本提示作为输入，该方法利用预训练的文本到图像扩散模型的力量，生成符合目标文本的新图像，同时保留指导图像的语义布局。具体来说，通过操纵模型内的空间特征及其自我注意力，实现了对生成结构的精细控制。<br>
                    效果：在多种文本引导的图像转换任务上，包括将草图、粗糙的图画和动画转换为真实的图像，改变给定图像中物体的类别和外观，以及修改全局属性如光照和颜色等，该方法都展示了高质量的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale text-to-image generative models have been a revolutionary breakthrough in the evolution of generative AI, synthesizing diverse images with highly complex visual concepts. However, a pivotal challenge in leveraging such models for real-world content creation is providing users with control over the generated content. In this paper, we present a new framework that takes text-to-image synthesis to the realm of image-to-image translation -- given a guidance image and a target text prompt as input, our method harnesses the power of a pre-trained text-to-image diffusion model to generate a new image that complies with the target text, while preserving the semantic layout of the guidance image. Specifically, we observe and empirically demonstrate that fine-grained control over the generated structure can be achieved by manipulating spatial features and their self-attention inside the model. This results in a simple and effective approach, where features extracted from the guidance image are directly injected into the generation process of the translated image, requiring no training or fine-tuning. We demonstrate high-quality results on versatile text-guided image translation tasks, including translating sketches, rough drawings and animations into realistic images, changing the class and appearance of objects in a given image, and modifying global qualities such as lighting and color.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1942.Local 3D Editing via 3D Distillation of CLIP Knowledge</span><br>
                <span class="as">Hyung, JunhaandHwang, SungwonandKim, DaejinandLee, HyunjiandChoo, Jaegul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hyung_Local_3D_Editing_via_3D_Distillation_of_CLIP_Knowledge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12674-12684.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行3D内容的编辑和修改，特别是在保持视觉质量的同时进行局部化操作。<br>
                    动机：现有的3D GANs在生成逼真的3D内容方面表现出色，但在编辑和修改这些内容时，如使用语义地图等控制手段，可能会导致视觉质量下降。同时，虽然文本引导的编辑方法显示出潜力，但它们往往缺乏局部性。<br>
                    方法：提出了Local Editing NeRF（LENeRF）模型，该模型只需要文本输入就可以进行精细和局部化的编辑。具体来说，我们引入了三个附加模块：潜在残余映射器、注意力场网络和变形网络，共同用于通过估计3D注意力场来局部编辑3D特征。3D注意力场是通过将CLIP的零样本掩码生成能力提炼到3D并结合多视图指导进行无监督学习的。<br>
                    效果：实验结果表明，LENeRF在各种任务上都取得了显著改进，无论是定量还是定性评估，都证明了其在3D内容编辑方面的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D GANs can generate diverse photo-realistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome the problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly used for local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the CLIP's zero-shot mask generation capability to 3D with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1943.3D-Aware Conditional Image Synthesis</span><br>
                <span class="as">Deng, KangleandYang, GengshanandRamanan, DevaandZhu, Jun-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_3D-Aware_Conditional_Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4434-4445.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种3D感知的条件生成模型pix2pix3D，用于可控的逼真图像合成。<br>
                    动机：现有的条件生成模型无法从不同视角合成对应的图像，需要改进以实现3D用户控制。<br>
                    方法：通过扩展条件生成模型与神经辐射场，使模型能为每个3D点分配标签、颜色和密度，同时渲染图像和像素对齐的标签图。<br>
                    效果：构建了一个交互系统，用户可以从不同视角编辑标签图并生成相应的输出，实现了可控的逼真图像合成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model learns to synthesize a corresponding image from different viewpoints. To enable explicit 3D user control, we extend conditional generative models with neural radiance fields. Given widely-available posed monocular image and label map pairs, our model learns to assign a label to every 3D point in addition to color and density, which enables it to render the image and pixel-aligned label map simultaneously. Finally, we build an interactive system that allows users to edit the label map from different viewpoints and generate outputs accordingly.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1944.Spider GAN: Leveraging Friendly Neighbors To Accelerate GAN Training</span><br>
                <span class="as">Asokan, SiddarthandSeelamantula, ChandraSekhar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Asokan_Spider_GAN_Leveraging_Friendly_Neighbors_To_Accelerate_GAN_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3883-3893.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练生成对抗网络（GANs）的稳定性是一个挑战。<br>
                    动机：图像比噪声更有结构，生成器可以利用这一点学习更稳健的转换。<br>
                    方法：提出一种新的方法，使用图像作为输入来训练GANs，而不强制任何成对约束。通过识别密切相关的数据集或目标分布的“友好邻域”来定义友好邻域，激发了“蜘蛛GAN”的绰号。<br>
                    效果：实验结果表明，蜘蛛GAN的公式可以实现更快的收敛，因为生成器可以在看似无关的数据集之间发现对应关系，例如在Tiny-ImageNet和CelebA人脸之间。此外，还展示了级联蜘蛛GAN，其中预训练GAN生成器的输出分布用作后续网络的输入。有效地，以级联的方式将一个分布传输到另一个分布，直到目标被学习——这是转移学习的一种新风味。在DCGAN、条件GAN、PGGAN、StyleGAN2和StyleGAN3上证明了蜘蛛方法的有效性。与他们在高分辨率小数据集（如MetFaces、Ukiyo-E Faces和AFHQ-Cats）上的基线相比，所提出的方法实现了最先进的Frechet Inception Distance（FID）值，训练迭代次数仅为五分之一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a "friendly neighborhood" of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet and CelebA faces. Further, we demonstrate cascading Spider GAN, where the output distribution from a pre-trained GAN generator is used as the input to the subsequent network. Effectively, transporting one distribution to another in a cascaded fashion until the target is learnt -- a new flavor of transfer learning. We demonstrate the efficacy of the Spider approach on DCGAN, conditional GAN, PGGAN, StyleGAN2 and StyleGAN3. The proposed approach achieves state-of-the-art Frechet inception distance (FID) values, with one-fifth of the training iterations, in comparison to their baseline counterparts on high-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1945.DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation</span><br>
                <span class="as">Shen, ShuaiandZhao, WenliangandMeng, ZibinandLi, WanhuaandZhu, ZhengandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1982-1991.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时提高生成质量和模型泛化能力，以实现高质量的人脸说话视频生成。<br>
                    动机：尽管现有的人脸说话视频生成技术在改善生成质量或增强模型泛化方面已有所努力，但能同时解决这两个问题的研究却寥寥无几，这对于实际应用至关重要。<br>
                    方法：本文引入了新兴的强大的潜在扩散模型，并将人脸说话视频生成视为一个与源音频同步的音频驱动的时间连贯去噪过程（DiffTalk）。具体来说，我们不仅使用音频信号作为驱动因素，还研究了控制说话人脸的机制，并将参考人脸图像和地标作为个性化泛化合成的条件。<br>
                    效果：所提出的DiffTalk能够产生与源音频同步的高质量人脸说话视频，更重要的是，它可以自然地泛化到不同的个体，无需任何额外的微调。此外，我们的DiffTalk可以很容易地适应更高分辨率的合成，而计算成本几乎可以忽略不计。大量实验表明，所提出的DiffTalk能有效合成高保真度的音频驱动的人脸说话视频，适用于泛化的新颖个体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Talking head synthesis is a promising approach for the video production industry. Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications. To this end, in this paper, we turn attention to the emerging powerful Latent Diffusion Models, and model the Talking head generation as an audio-driven temporally coherent denoising process (DiffTalk). More specifically, instead of employing audio signals as the single driving factor, we investigate the control mechanism of the talking face, and incorporate reference face images and landmarks as conditions for personality-aware generalized synthesis. In this way, the proposed DiffTalk is capable of producing high-quality talking head videos in synchronization with the source audio, and more importantly, it can be naturally generalized across different identities without any further fine-tuning. Additionally, our DiffTalk can be gracefully tailored for higher-resolution synthesis with negligible extra computational cost. Extensive experiments show that the proposed DiffTalk efficiently synthesizes high-fidelity audio-driven talking head videos for generalized novel identities. For more video results, please refer to https://sstzal.github.io/DiffTalk/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1946.SceneComposer: Any-Level Semantic Image Synthesis</span><br>
                <span class="as">Zeng, YuandLin, ZheandZhang, JianmingandLiu, QingandCollomosse, JohnandKuen, JasonandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22468-22478.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的条件图像合成框架，用于从任何精度级别的语义布局进行生成。<br>
                    动机：现有的方法在处理具有不同精度级别的语义布局时存在局限性，需要一种灵活且高效的框架来解决这个问题。<br>
                    方法：该框架支持从纯文本到具有精确形状的2D语义画布的各种精度级别。通过引入一系列新技术，如训练数据收集管道、精度编码的掩码金字塔和文本特征图表示等，实现了对精度级别、语义和构图信息的联合编码，以及多尺度引导扩散模型用于图像合成。<br>
                    效果：实验结果表明，该方法能够根据给定的布局精度生成高质量的图像，并在与现有方法的比较中表现优越。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a new framework for conditional image synthesis from semantic layouts of any precision levels, ranging from pure text to a 2D semantic canvas with precise shapes. More specifically, the input layout consists of one or more semantic regions with free-form text descriptions and adjustable precision levels, which can be set based on the desired controllability. The framework naturally reduces to text-to-image (T2I) at the lowest level with no shape information, and it becomes segmentation-to-image (S2I) at the highest level. By supporting the levels in-between, our framework is flexible in assisting users of different drawing expertise and at different stages of their creative workflow. We introduce several novel techniques to address the challenges coming with this new setup, including a pipeline for collecting training data; a precision-encoded mask pyramid and a text feature map representation to jointly encode precision level, semantics, and composition information; and a multi-scale guided diffusion model to synthesize images. To evaluate the proposed method, we collect a test dataset containing user-drawn layouts with diverse scenes and styles. Experimental results show that the proposed method can generate high-quality images following the layout at given precision, and compares favorably against existing methods. Project page https://zengxianyu.github.io/scenec/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1947.Unsupervised Domain Adaption With Pixel-Level Discriminator for Image-Aware Layout Generation</span><br>
                <span class="as">Xu, ChenchenandZhou, MinandGe, TiezhengandJiang, YuningandXu, Weiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Unsupervised_Domain_Adaption_With_Pixel-Level_Discriminator_for_Image-Aware_Layout_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10114-10123.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习模型生成广告海报的图形布局？<br>
                    动机：现有的数据集存在源领域数据（修复后的海报）和目标领域数据（清洁产品图像）之间的领域差距，影响了图形布局的质量。<br>
                    方法：提出了一种基于GAN和无监督领域适应技术的新模型PDA-GAN，通过连接浅层特征图的像素级判别器来计算输入图像像素的GAN损失，以生成与图像内容相符的广告海报图形布局。<br>
                    效果：定量和定性评估表明，PDA-GAN能够实现最先进的性能，并生成高质量的图像感知广告海报图形布局。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Layout is essential for graphic design and poster generation. Recently, applying deep learning models to generate layouts has attracted increasing attention. This paper focuses on using the GAN-based model conditioned on image contents to generate advertising poster graphic layouts, which requires an advertising poster layout dataset with paired product images and graphic layouts. However, the paired images and layouts in the existing dataset are collected by inpainting and annotating posters, respectively. There exists a domain gap between inpainted posters (source domain data) and clean product images (target domain data). Therefore, this paper combines unsupervised domain adaption techniques to design a GAN with a novel pixel-level discriminator (PD), called PDA-GAN, to generate graphic layouts according to image contents. The PD is connected to the shallow level feature map and computes the GAN loss for each input-image pixel. Both quantitative and qualitative evaluations demonstrate that PDA-GAN can achieve state-of-the-art performances and generate high-quality image-aware graphic layouts for advertising posters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1948.Real-Time 6K Image Rescaling With Rate-Distortion Optimization</span><br>
                <span class="as">Qi, ChenyangandYang, XinandCheng, KaLeongandChen, Ying-CongandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qi_Real-Time_6K_Image_Rescaling_With_Rate-Distortion_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14092-14101.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将高分辨率图像嵌入到低分辨率图像中，以实现对高分辨率图像的重建。<br>
                    动机：现有的图像缩放方法没有优化低分辨率图像的文件大小，并且最新的基于流的缩放方法对于高分辨率图像的重建（如6K）来说并不是实时的。<br>
                    方法：我们提出了一种新的框架（HyperThumbnail），用于实时6K率失真感知的图像缩放。我们的HyperThumbnail首先通过一个带有我们提出的可学习JPEG量化模块的编码器将高分辨率图像嵌入到一个JPEG低分辨率图像（缩略图）中，该模块优化了嵌入的低分辨率JPEG图像的文件大小。然后，一个高效的解码器从低分辨率图像实时重建高保真度（6K）的HR图像。<br>
                    效果：大量的实验表明，我们的框架在率失真性能方面优于以前的图像缩放基线，并且在高分辨率图像重建速度上比先前的工作快得多。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of image rescaling aims at embedding an high-resolution (HR) image into a low-resolution (LR) one that can contain embedded information for HR image reconstruction. Existing image rescaling methods do not optimize the LR image file size and recent flow-based rescaling methods are not real-time yet for HR image reconstruction (e.g., 6K). To address these two challenges, we propose a novel framework (HyperThumbnail) for real-time 6K rate-distortion-aware image rescaling. Our HyperThumbnail first embeds an HR image into a JPEG LR image (thumbnail) by an encoder with our proposed learnable JPEG quantization module, which optimizes the file size of the embedding LR JPEG image. Then, an efficient decoder reconstructs a high-fidelity HR (6K) image from the LR one in real time. Extensive experiments demonstrate that our framework outperforms previous image rescaling baselines in both rate-distortion performance and is much faster than prior work in HR image reconstruction speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1949.OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis</span><br>
                <span class="as">Xu, HongyiandSong, GuoxianandJiang, ZihangandZhang, JianfengandShi, YichunandLiu, JingandMa, WanchunandFeng, JiashiandLuo, Linjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_OmniAvatar_Geometry-Guided_Controllable_3D_Head_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12814-12824.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种新颖的几何引导3D人头合成模型，能够从非结构化图像中训练，生成具有动态细节的不同身份保持的3D人头。<br>
                    动机：现有的3D人头合成模型无法在摄像机姿态、面部表情、头部形状、颈部和下颌关节姿势等方面进行完全解耦控制，因此需要开发新的模型来解决这个问题。<br>
                    方法：首先定义了一种围绕头部几何的新型语义有符号距离函数（SDF），然后利用3D感知的GAN框架（EG3D）在规范空间中合成详细的3D全头形状和外观，最后通过体积渲染步骤输出到观察空间。<br>
                    效果：实验结果表明，新模型可以生成更优的身份保持的3D人头，其动态细节比现有方法更具吸引力，无论是定性还是定量比较。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1950.LayoutDM: Transformer-Based Diffusion Model for Layout Generation</span><br>
                <span class="as">Chai, ShangandZhuang, LianshengandYan, Fengying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18349-18358.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用扩散模型在条件布局生成中实现高质量的布局生成？<br>
                    动机：尽管现有的基于生成对抗网络（GANs）和变分自编码器（VAEs）的方法在布局生成上有所进步，但在质量和多样性方面仍有改进空间。<br>
                    方法：受最近扩散模型在高质量图像生成上的成功启发，本文提出了一种基于Transformer的布局扩散模型（LayoutDM）。该模型通过将条件去噪扩散概率模型（DDPM）与纯Transformer架构相结合，使用Transformer-based条件布局去噪器从噪声布局数据中学习反向扩散过程以生成样本。<br>
                    效果：与GANs和VAEs相比，LayoutDM具有高质量生成、强大的样本多样性、忠实的分布覆盖和稳定的训练等理想特性。实验结果表明，该方法在质量和多样性方面优于现有的最佳生成模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic layout generation that can synthesize high-quality layouts is an important tool for graphic design in many applications. Though existing methods based on generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) have progressed, they still leave much room for improving the quality and diversity of the results. Inspired by the recent success of diffusion models in generating high-quality images, this paper explores their potential for conditional layout generation and proposes Transformer-based Layout Diffusion Model (LayoutDM) by instantiating the conditional denoising diffusion probabilistic model (DDPM) with a purely transformer-based architecture. Instead of using convolutional neural networks, a transformer-based conditional Layout Denoiser is proposed to learn the reverse diffusion process to generate samples from noised layout data. Benefitting from both transformer and DDPM, our LayoutDM is of desired properties such as high-quality generation, strong sample diversity, faithful distribution coverage, and stationary training in comparison to GANs and VAEs. Quantitative and qualitative experimental results show that our method outperforms state-of-the-art generative models in terms of quality and diversity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1951.DualVector: Unsupervised Vector Font Synthesis With Dual-Part Representation</span><br>
                <span class="as">Liu, Ying-TianandZhang, ZhifeiandGuo, Yuan-ChenandFisher, MatthewandWang, ZhaowenandZhang, Song-Hai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_DualVector_Unsupervised_Vector_Font_Synthesis_With_Dual-Part_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14193-14202.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高字体设计的自动生成效果？<br>
                    动机：目前的字体设计方法存在像素化图像的弊端，并且在向量化过程中会出现质量损失。同时，现有的矢量字体合成方法在形状简洁表示和训练过程中的向量监督方面存在问题。<br>
                    方法：提出一种新的矢量字形双部分表示法，将每个字形建模为一组封闭的“正”和“负”路径对，通过布尔运算获取字形轮廓。首先仅从字形图像中学习这种表示法，然后设计一个轮廓细化步骤，使轮廓与图像表示对齐，以进一步改善细节。<br>
                    效果：该方法（命名为DualVector）在矢量字体合成方面优于现有方法，无论是定量还是定性评估。生成的矢量字体可以方便地转换为TrueType Font等常见数字字体格式，具有实际应用价值。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic generation of fonts can be an important aid to typeface design. Many current approaches regard glyphs as pixelated images, which present artifacts when scaling and inevitable quality losses after vectorization. On the other hand, existing vector font synthesis methods either fail to represent the shape concisely or require vector supervision during training. To push the quality of vector font synthesis to the next level, we propose a novel dual-part representation for vector glyphs, where each glyph is modeled as a collection of closed "positive" and "negative" path pairs. The glyph contour is then obtained by boolean operations on these paths. We first learn such a representation only from glyph images and devise a subsequent contour refinement step to align the contour with an image representation to further enhance details. Our method, named DualVector, outperforms state-of-the-art methods in vector font synthesis both quantitatively and qualitatively. Our synthesized vector fonts can be easily converted to common digital font formats like TrueType Font for practical use. The code is released at https://github.com/thuliu-yt16/dualvector.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1952.GazeNeRF: 3D-Aware Gaze Redirection With Neural Radiance Fields</span><br>
                <span class="as">Ruzzi, AlessandroandShi, XiangweiandWang, XiandLi, GengyanandDeMello, ShaliniandChang, HyungJinandZhang, XucongandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruzzi_GazeNeRF_3D-Aware_Gaze_Redirection_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9676-9685.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种3D感知的注视重定向方法。<br>
                    动机：现有的注视重定向方法在二维图像上操作，难以生成3D一致的结果。<br>
                    方法：我们的方法建立在面部区域和眼球是独立移动的分离3D结构的基础上，利用最新的条件图像基神经辐射场的进步，提出了分别预测面部和眼部体积特征的双分支架构。通过3D旋转矩阵刚性变换眼球特征，可以精细控制期望的注视角度。最后，通过可微分体积合成获得重定向的图像。<br>
                    效果：我们的实验表明，这种架构在重定向准确性和身份保持方面优于简单地条件化的NeRF基线以及先前最先进的2D注视重定向方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose GazeNeRF, a 3D-aware method for the task of gaze redirection. Existing gaze redirection methods operate on 2D images and struggle to generate 3D consistent results. Instead, we build on the intuition that the face region and eye balls are separate 3D structures that move in a coordinated yet independent fashion. Our method leverages recent advancements in conditional image-based neural radiance fields and proposes a two-branch architecture that predicts volumetric features for the face and eye regions separately. Rigidly transforming the eye features via a 3D rotation matrix provides fine-grained control over the desired gaze angle. The final, redirected image is then attained via differentiable volume compositing. Our experiments show that this architecture outperforms naively conditioned NeRF baselines as well as previous state-of-the-art 2D gaze redirection methods in terms of redirection accuracy and identity preservation. Code and models will be released for research purposes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1953.Realistic Saliency Guided Image Enhancement</span><br>
                <span class="as">Miangoleh, S.MahdiH.andBylinskii, ZoyaandKee, EricandShechtman, EliandAksoy, Ya\u{g</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Miangoleh_Realistic_Saliency_Guided_Image_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/186-194.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过增强语言表示模型（ERNIE）充分利用词汇、句法和知识信息，同时提高各种知识驱动任务的性能。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，而知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出ERNIE模型，以更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1954.Collaborative Diffusion for Multi-Modal Face Generation and Editing</span><br>
                <span class="as">Huang, ZiqiandChan, KelvinC.K.andJiang, YumingandLiu, Ziwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6080-6090.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的扩散模型主要关注单模态控制，即扩散过程仅由一种模态的条件驱动。为了进一步释放用户的创造力，需要模型能够同时由多个模态控制，例如通过描述年龄（文本驱动）生成和编辑面部（掩码驱动）。<br>
                    动机：不同模态的扩散模型在潜在去噪步骤上具有互补性，可以在这些步骤上建立双向连接。<br>
                    方法：提出协作扩散模型，该模型将预训练的单模态扩散模型进行协作，实现多模态的面部生成和编辑，无需重新训练。动态扩散器是一种元网络，通过预测每个预训练的单模态模型的空间-时间影响函数，自适应地产生多模态去噪步骤。<br>
                    效果：实验表明，协作扩散模型不仅整合了单模态扩散模型的生成能力，还整合了多种单模态操作进行多模态编辑，在图像质量和条件一致性方面均表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models arise as a powerful generative tool recently. Despite the great progress, existing diffusion models mainly focus on uni-modal control, i.e., the diffusion process is driven by only one modality of condition. To further unleash the users' creativity, it is desirable for the model to be controllable by multiple modalities simultaneously, e.g., generating and editing faces by describing the age (text-driven) while drawing the face shape (mask-driven). In this work, we present Collaborative Diffusion, where pre-trained uni-modal diffusion models collaborate to achieve multi-modal face generation and editing without re-training. Our key insight is that diffusion models driven by different modalities are inherently complementary regarding the latent denoising steps, where bilateral connections can be established upon. Specifically, we propose dynamic diffuser, a meta-network that adaptively hallucinates multi-modal denoising steps by predicting the spatial-temporal influence functions for each pre-trained uni-modal model. Collaborative Diffusion not only collaborates generation capabilities from uni-modal diffusion models, but also integrates multiple uni-modal manipulations to perform multi-modal editing. Extensive qualitative and quantitative experiments demonstrate the superiority of our framework in both image quality and condition consistency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1955.SmartBrush: Text and Shape Guided Object Inpainting With Diffusion Model</span><br>
                <span class="as">Xie, ShaoanandZhang, ZhifeiandLin, ZheandHinz, TobiasandZhang, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22428-22437.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种名为SmartBrush的新的扩散模型，用于使用文本和形状指导完成缺失区域的对象。<br>
                    动机：现有的图像修复方法只能借用周围信息来填补损坏的图像，而多模态修复提供了更灵活和有用的控制，例如可以使用文本提示描述具有更丰富属性的对象，并使用掩码约束修复对象的形状。<br>
                    方法：我们提出了一种新的基于扩散的模型SmartBrush，它结合了文本和形状指导，以精确控制的方式完成缺失区域的对象。为了更好地保护背景，我们通过将对象掩码预测添加到扩散U-net中，提出了一种新的训练和采样策略。最后，我们通过联合训练修复和文本到图像生成任务来进行多任务训练，以利用更多的训练数据。<br>
                    效果：实验结果表明，我们的模型在视觉质量、掩码可控性和背景保护方面均优于所有基线模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generic image inpainting aims to complete a corrupted image by borrowing surrounding information, which barely generates novel content. By contrast, multi-modal inpainting provides more flexible and useful controls on the inpainted content, e.g., a text prompt can be used to describe an object with richer attributes, and a mask can be used to constrain the shape of the inpainted object rather than being only considered as a missing area. We propose a new diffusion-based model named SmartBrush for completing a missing region with an object using both text and shape-guidance. While previous work such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not support shape guidance and tend to modify background texture surrounding the generated object. Our model incorporates both text and shape guidance with precision control. To preserve the background better, we propose a novel training and sampling strategy by augmenting the diffusion U-net with object-mask prediction. Lastly, we introduce a multi-task training strategy by jointly training inpainting with text-to-image generation to leverage more training data. We conduct extensive experiments showing that our model outperforms all baselines in terms of visual quality, mask controllability, and background preservation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1956.StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping</span><br>
                <span class="as">Jiang, DiqiongandSong, DanandTong, RuofengandTang, Min</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_StyleIPSB_Identity-Preserving_Semantic_Basis_of_StyleGAN_for_High_Fidelity_Face_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/352-361.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的人脸交换方法在生成高保真度的人脸图像时，无法保留毛孔级别的细节和身份特征。<br>
                    动机：为了解决上述问题，研究者创新地构建了一种新的StyleGAN模型——StyleIPSB，该模型能够保留毛孔级别的细节并保持身份特征。<br>
                    方法：通过构建一系列与姿势、表情和照明有关的身份保留语义基础（StyleIPSB），并将其应用于StyleGAN模型中，实现了高保真度的人脸交换。<br>
                    效果：实验结果表明，StyleIPSB可以有效地保留毛孔级别的细节和身份特征，并在人脸交换任务上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent researches reveal that StyleGAN can generate highly realistic images, inspiring researchers to use pretrained StyleGAN to generate high-fidelity swapped faces. However, existing methods fail to meet the expectations in two essential aspects of high-fidelity face swapping. Their results are blurry without pore-level details and fail to preserve identity for challenging cases. To overcome the above artifacts, we innovatively construct a series of identity-preserving semantic bases of StyleGAN (called StyleIPSB) in respect of pose, expression, and illumination. Each basis of StyleIPSB controls one specific semantic attribute and disentangles with the others. The StyleIPSB constrains style code in the subspace of W+ space to preserve pore-level details. StyleIPSB gives us a novel tool for high-fidelity face swapping, and we propose a three-stage framework for face swapping with StyleIPSB. Firstly, we transform the target facial images' attributes to the source image. We learn the mapping from 3D Morphable Model (3DMM) parameters, which capture the prominent semantic variance, to the coordinates of StyleIPSB that show higher identity-preserving and fidelity. Secondly, to transform detailed attributes which 3DMM does not capture, we learn the residual attribute between the reenacted face and the target face. Finally, the face is blended into the background of the target image. Extensive results and comparisons demonstrate that StyleIPSB can effectively preserve identity and pore-level details. The results of face swapping can achieve state-of-the-art performance. We will release our code at https://github.com/a686432/StyleIPSB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1957.Discriminator-Cooperated Feature Map Distillation for GAN Compression</span><br>
                <span class="as">Hu, TieandLin, MingbaoandYou, LizhouandChao, FeiandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Discriminator-Cooperated_Feature_Map_Distillation_for_GAN_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20351-20360.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管生成对抗网络（GANs）在图像生成方面表现优秀，但其对大量存储和密集计算的需求是众所周知的。<br>
                    动机：知识蒸馏作为一种有效的方法，被证明可以探索低成本的GANs。<br>
                    方法：本文提出了一种创新的判别器协作蒸馏（DCD）方法，通过将教师判别器用作转换来推动学生生成器的中间结果与教师生成器的对应输出在感知上接近。同时，为了缓解GAN压缩中的模式崩溃问题，我们构建了一个合作对抗训练范例，其中教师判别器从头开始与学生生成器共同训练。<br>
                    效果：实验结果表明，我们的DCD方法优于现有的GAN压缩方法。例如，在减少CycleGAN的40倍MACs和80倍参数后，我们将FID指标从61.53降低到48.24，而目前的最佳方法仅为51.92。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite excellent performance in image generation, Generative Adversarial Networks (GANs) are notorious for its requirements of enormous storage and intensive computation. As an awesome "performance maker", knowledge distillation is demonstrated to be particularly efficacious in exploring low-priced GANs. In this paper, we investigate the irreplaceability of teacher discriminator and present an inventive discriminator-cooperated distillation, abbreviated as DCD, towards refining better feature maps from the generator. In contrast to conventional pixel-to-pixel match methods in feature map distillation, our DCD utilizes teacher discriminator as a transformation to drive intermediate results of the student generator to be perceptually close to corresponding outputs of the teacher generator. Furthermore, in order to mitigate mode collapse in GAN compression, we construct a collaborative adversarial training paradigm where the teacher discriminator is from scratch established to co-train with student generator in company with our DCD. Our DCD shows superior results compared with existing GAN compression methods. For instance, after reducing over 40x MACs and 80x parameters of CycleGAN, we well decrease FID metric from 61.53 to 48.24 while the current SoTA method merely has 51.92. This work's source code has been made accessible at https://github.com/poopit/DCD-official.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1958.Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection</span><br>
                <span class="as">Tan, ChuangchuangandZhao, YaoandWei, ShikuiandGu, GuanghuaandWei, Yunchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12105-12114.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何开发一种能检测出由生成对抗网络（GAN）生成的假图像的通用检测器。<br>
                    动机：现有的图像检测器在未见过的数据领域性能下降明显，而GAN可以轻易生成逼真的假图像，增加了滥用的风险。<br>
                    方法：提出了一种新的检测框架——学习梯度（LGrad）。该框架首先使用预训练的CNN模型将图像转换为梯度，然后将这些梯度作为通用的伪影表示输入分类器以确定图像的真实性。<br>
                    效果：实验表明，该方法能有效且鲁棒地利用梯度作为GAN生成图像的通用伪影表示，其检测器的性能比现有技术提高了11.4%，达到了新的最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, there has been a significant advancement in image generation technology, known as GAN. It can easily generate realistic fake images, leading to an increased risk of abuse. However, most image detectors suffer from sharp performance drops in unseen domains. The key of fake image detection is to develop a generalized representation to describe the artifacts produced by generation models. In this work, we introduce a novel detection framework, named Learning on Gradients (LGrad), designed for identifying GAN-generated images, with the aim of constructing a generalized detector with cross-model and cross-data. Specifically, a pretrained CNN model is employed as a transformation model to convert images into gradients. Subsequently, we leverage these gradients to present the generalized artifacts, which are fed into the classifier to ascertain the authenticity of the images. In our framework, we turn the data-dependent problem into a transformation-model-dependent problem. To the best of our knowledge, this is the first study to utilize gradients as the representation of artifacts in GAN-generated images. Extensive experiments demonstrate the effectiveness and robustness of gradients as generalized artifact representations. Our detector achieves a new state-of-the-art performance with a remarkable gain of 11.4%. The code is released at https://github.com/chuangchuangtan/LGrad.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1959.InstructPix2Pix: Learning To Follow Image Editing Instructions</span><br>
                <span class="as">Brooks, TimandHolynski, AleksanderandEfros, AlexeiA.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18392-18402.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过人类指令编辑图像？<br>
                    动机：现有的图像编辑方法需要手动操作，耗时且效率低下。<br>
                    方法：结合预训练的语言模型（GPT-3）和文本到图像模型（Stable Diffusion），生成大量图像编辑示例作为训练数据，训练条件扩散模型InstructPix2Pix。<br>
                    效果：InstructPix2Pix能够快速地根据用户写的指令在几秒内完成图像编辑，并在多种输入图像和指令上取得了良好的编辑效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models--a language model (GPT-3) and a text-to-image model (Stable Diffusion)--to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1960.Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis</span><br>
                <span class="as">Wang, DuominandDeng, YuandYin, ZixinandShum, Heung-YeungandWang, Baoyuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17979-17989.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新颖的一阶段人头说话合成方法，实现对嘴唇运动、眼神注视和眨眼、头部姿势以及情感表达的解耦和精细化控制。<br>
                    动机：现有的方法无法有效地分离并控制这些不同的运动因素，我们的目标是通过解耦的潜在表示来表示不同的运动，并利用图像生成器从它们中合成说话的头部。<br>
                    方法：我们首先从驱动信号中提取统一的运动特征，然后从统一的特征中分离出每个精细的运动。我们还引入了针对非情感运动的运动特定对比学习和回归，以及针对情感表达的特征级去相关和自我重建，以充分利用无结构视频数据中每个运动因素的内在特性来实现解耦。<br>
                    效果：实验表明，我们的方法在语音和嘴唇运动的同步方面提供了高质量的表现，同时对多种额外的面部运动进行了精确和解耦的控制，这是以前的方法是难以实现的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel one-shot talking head synthesis method that achieves disentangled and fine-grained control over lip motion, eye gaze&blink, head pose, and emotional expression. We represent different motions via disentangled latent representations and leverage an image generator to synthesize talking heads from them. To effectively disentangle each motion factor, we propose a progressive disentangled representation learning strategy by separating the factors in a coarse-to-fine manner, where we first extract unified motion feature from the driving signal, and then isolate each fine-grained motion from the unified feature. We introduce motion-specific contrastive learning and regressing for non-emotional motions, and feature-level decorrelation and self-reconstruction for emotional expression, to fully utilize the inherent properties of each motion factor in unstructured video data to achieve disentanglement. Experiments show that our method provides high quality speech&lip-motion synchronization along with precise and disentangled control over multiple extra facial motions, which can hardly be achieved by previous methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1961.ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection</span><br>
                <span class="as">Jin, ShiweiandWang, ZhenandWang, LeiandBi, NingandNguyen, Truong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_ReDirTrans_Latent-to-Latent_Translation_for_Gaze_and_Head_Redirection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5547-5556.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确改变高分辨率全脸图像的注视方向，同时保留其他属性如身份、表情和发型？<br>
                    动机：现有的图像合成方法主要关注改变低分辨率图像的注视方向，但这种方法在处理高分辨率全脸图像时会限制其应用场景。<br>
                    方法：提出了一种名为ReDirTrans的可移植网络，通过潜在到潜在的翻译来以可解释的方式改变注视方向和头部方向。该方法只对目标属性进行投影，并通过分配的俯仰角和偏航角值来重定向这些嵌入。然后，将初始和编辑的嵌入投射回初始的潜在空间作为残差，通过减法和加法修改输入的潜在向量，表示旧状态的移除和新状态的增加。<br>
                    效果：通过将ReDirTrans与预训练的固定e4e-StyleGAN配对，创建了ReDirTrans-GAN，能够在保持其他属性如身份、表情和发型的同时，准确地改变1024*1024分辨率全脸图像的注视方向。此外，使用重定向样本作为数据集增强，提高了下游基于学习的注视估计任务的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based gaze estimation methods require large amounts of training data with accurate gaze annotations. Facing such demanding requirements of gaze data collection and annotation, several image synthesis methods were proposed, which successfully redirected gaze directions precisely given the assigned conditions. However, these methods focused on changing gaze directions of the images that only include eyes or restricted ranges of faces with low resolution (less than 128*128) to largely reduce interference from other attributes such as hairs, which limits application scenarios. To cope with this limitation, we proposed a portable network, called ReDirTrans, achieving latent-to-latent translation for redirecting gaze directions and head orientations in an interpretable manner. ReDirTrans projects input latent vectors into aimed-attribute embeddings only and redirects these embeddings with assigned pitch and yaw values. Then both the initial and edited embeddings are projected back (deprojected) to the initial latent space as residuals to modify the input latent vectors by subtraction and addition, representing old status removal and new status addition. The projection of aimed attributes only and subtraction-addition operations for status replacement essentially mitigate impacts on other attributes and the distribution of latent vectors. Thus, by combining ReDirTrans with a pretrained fixed e4e-StyleGAN pair, we created ReDirTrans-GAN, which enables accurately redirecting gaze in full-face images with 1024*1024 resolution while preserving other attributes such as identity, expression, and hairstyle. Furthermore, we presented improvements for the downstream learning-based gaze estimation task, using redirected samples as dataset augmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1962.Controllable Light Diffusion for Portraits</span><br>
                <span class="as">Futschik, DavidandRitland, KelvinandVecore, JamesandFanello, SeanandOrts-Escolano, SergioandCurless, BrianandS\&#x27;ykora, DanielandPandey, Rohit</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Futschik_Controllable_Light_Diffusion_for_Portraits_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8412-8421.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法——光扩散，以改善肖像照的照明效果，软化硬阴影和镜面高光，同时保持整体场景的照明。<br>
                    动机：受到专业摄影师的扩散器和纱幕的启发，我们的方法仅使用一张肖像照片就能软化照明。<br>
                    方法：我们提出了一种基于学习的方法，可以控制光扩散的程度并将其应用于自然环境中的肖像。此外，我们还设计了一种方法，可以合成具有次表面散射效应的合理外部阴影，同时符合主体脸部的形状。<br>
                    效果：实验结果表明，我们的方法可以提高高级视觉应用的鲁棒性，如反照率估计、几何估计和语义分割。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce light diffusion, a novel method to improve lighting in portraits, softening harsh shadows and specular highlights while preserving overall scene illumination. Inspired by professional photographers' diffusers and scrims, our method softens lighting given only a single portrait photo. Previous portrait relighting approaches focus on changing the entire lighting environment, removing shadows (ignoring strong specular highlights), or removing shading entirely. In contrast, we propose a learning based method that allows us to control the amount of light diffusion and apply it on in-the-wild portraits. Additionally, we design a method to synthetically generate plausible external shadows with sub-surface scattering effects while conforming to the shape of the subject's face. Finally, we show how our approach can increase the robustness of higher level vision applications, such as albedo estimation, geometry estimation and semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1963.DiffSwap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion</span><br>
                <span class="as">Zhao, WenliangandRao, YongmingandShi, WeikangandLiu, ZuyanandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DiffSwap_High-Fidelity_and_Controllable_Face_Swapping_via_3D-Aware_Masked_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8568-8577.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种基于扩散模型的高保真、可控人脸交换框架。<br>
                    动机：与依赖精心设计的网络架构和损失函数融合源和目标脸部信息的工作不同，本文将人脸交换重新定义为一个条件修复任务，由强大的扩散模型根据所需的脸部属性（如身份和地标）指导执行。<br>
                    方法：提出了DiffSwap，这是一个基于扩散模型的框架，通过仅2步的高效恢复交换后的人脸合理扩散结果，引入身份约束以提高人脸交换质量。<br>
                    效果：实验结果表明，该方法在定性和定量上都能实现最先进的人脸交换效果，具有高度的控制性、高保真度和形状保持性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose DiffSwap, a diffusion model based framework for high-fidelity and controllable face swapping. Unlike previous work that relies on carefully designed network architectures and loss functions to fuse the information from the source and target faces, we reformulate the face swapping as a conditional inpainting task, performed by a powerful diffusion model guided by the desired face attributes (e.g., identity and landmarks). An important issue that makes it nontrivial to apply diffusion models to face swapping is that we cannot perform the time-consuming multi-step sampling to obtain the generated image during training. To overcome this, we propose a midpoint estimation method to efficiently recover a reasonable diffusion result of the swapped face with only 2 steps, which enables us to introduce identity constraints to improve the face swapping quality. Our framework enjoys several favorable properties more appealing than prior arts: 1) Controllable. Our method is based on conditional masked diffusion on the latent space, where the mask and the conditions can be fully controlled and customized. 2) High-fidelity. The formulation of conditional inpainting can fully exploit the generative ability of diffusion models and can preserve the background of target images with minimal artifacts. 3) Shape-preserving. The controllability of our method enables us to use 3D-aware landmarks as the condition during generation to preserve the shape of the source face. Extensive experiments on both FF++ and FFHQ demonstrate that our method can achieve state-of-the-art face swapping results both qualitatively and quantitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1964.Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution</span><br>
                <span class="as">Yao, Jie-EnandTsao, Li-YuanandLo, Yi-ChenandTseng, RoyandChang, Chia-CheandLee, Chun-Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Local_Implicit_Normalizing_Flow_for_Arbitrary-Scale_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1776-1785.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有任意尺度超分辨率（SR）方法忽略的病态问题，以及其只能进行预定义固定尺度SR的限制。<br>
                    动机：虽然流基方法在解决超分辨率的病态问题上表现出了潜力，但它们只能进行预定义的固定尺度SR，限制了其在实际应用中的潜力。同时，任意尺度SR得到了更多的关注并取得了重大进展，但以前的任意尺度SR方法忽视了病态问题，并用逐像素L1损失训练模型，导致模糊的SR输出。<br>
                    方法：我们提出了"局部隐式正则化流"（LINF），该方法通过正则化流对不同缩放因子下的纹理细节分布进行建模。因此，LINF可以在任意尺度因子下生成具有丰富纹理细节的照片级高分辨率图像。<br>
                    效果：通过大量实验评估，我们发现LINF与先前的任意尺度SR方法相比，实现了最先进的感知质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Flow-based methods have demonstrated promising results in addressing the ill-posed nature of super-resolution (SR) by learning the distribution of high-resolution (HR) images with the normalizing flow. However, these methods can only perform a predefined fixed-scale SR, limiting their potential in real-world applications. Meanwhile, arbitrary-scale SR has gained more attention and achieved great progress. Nonetheless, previous arbitrary-scale SR methods ignore the ill-posed problem and train the model with per-pixel L1 loss, leading to blurry SR outputs. In this work, we propose "Local Implicit Normalizing Flow" (LINF) as a unified solution to the above problems. LINF models the distribution of texture details under different scaling factors with normalizing flow. Thus, LINF can generate photo-realistic HR images with rich texture details in arbitrary scale factors. We evaluate LINF with extensive experiments and show that LINF achieves the state-of-the-art perceptual quality compared with prior arbitrary-scale SR methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1965.Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</span><br>
                <span class="as">Xu, JialeandWang, XintaoandCheng, WeihaoandCao, Yan-PeiandShan, YingandQie, XiaohuandGao, Shenghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20908-20918.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高文本到3D的优化方法的准确性和忠实度。<br>
                    动机：目前的文本引导3D优化方法由于缺乏先验知识，经常无法生成与输入文本相符的准确和忠实的3D结构。<br>
                    方法：首次将明确的3D形状先验引入CLIP引导的3D优化过程。首先，在文本到形状阶段从输入文本中生成高质量的3D形状作为3D形状先验。然后将其用作神经辐射场的初始化，并使用完整的提示进行优化。<br>
                    效果：该方法能够生成具有优越视觉质量和形状准确性的富有想象力的3D内容，优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent CLIP-guided 3D optimization methods, such as DreamFields and PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D synthesis. However, due to scratch training and random initialization without prior knowledge, these methods often fail to generate accurate and faithful 3D structures that conform to the input text. In this paper, we make the first attempt to introduce explicit 3D shape priors into the CLIP-guided 3D optimization process. Specifically, we first generate a high-quality 3D shape from the input text in the text-to-shape stage as a 3D shape prior. We then use it as the initialization of a neural radiance field and optimize it with the full prompt. To address the challenging text-to-shape generation task, we present a simple yet effective approach that directly bridges the text and image modalities with a powerful text-to-image diffusion model. To narrow the style domain gap between the images synthesized by the text-to-image diffusion model and shape renderings used to train the image-to-shape generator, we further propose to jointly optimize a learnable text prompt and fine-tune the text-to-image diffusion model for rendering-style image generation. Our method, Dream3D, is capable of generating imaginative 3D content with superior visual quality and shape accuracy compared to state-of-the-art methods. Our project page is at https://bluestyle97.github.io/dream3d/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1966.DINN360: Deformable Invertible Neural Network for Latitude-Aware 360deg Image Rescaling</span><br>
                <span class="as">Guo, YichenandXu, MaiandJiang, LaiandSigal, LeonidandChen, Yunjin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_DINN360_Deformable_Invertible_Neural_Network_for_Latitude-Aware_360deg_Image_Rescaling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21519-21528.png><br>
            
            <span class="tt"><span class="t0">研究问题：随着虚拟现实的快速发展，360度图像越来越受欢迎。然而，其广阔的视野需要高分辨率来保证图像质量，这给获取、存储和处理这些图像带来了困难。<br>
                    动机：为了解决这个问题，我们提出了首次尝试的360度图像缩放方法，即先将360度图像下采样到视觉上有效的低分辨率版本，然后根据低分辨率版本上采样到高分辨率的360度图像。<br>
                    方法：我们首先分析了两个360度图像数据集，并观察到了一些特点，这些特点描述了360度图像通常如何沿着纬度变化。受这些发现的启发，我们提出了一种新的可变形可逆神经网络（INN），名为DINN360，用于纬度感知的360度图像缩放。在DINN360中，设计了一个可变形的INN来下采样低分辨率图像，并通过自适应地处理不同纬度区域发生的各种形变，将高频（HF）分量投影到潜在空间。给定下采样的低分辨率图像，通过从潜在空间恢复与结构相关的HF分量，以条件纬度感知的方式重建高质量的高分辨率图像。<br>
                    效果：我们在四个公共数据集上进行了大量实验，结果表明我们的DINN360方法在2倍、4倍和8倍360度图像缩放方面比其他最先进的方法表现得要好得多。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the rapid development of virtual reality, 360deg images have gained increasing popularity. Their wide field of view necessitates high resolution to ensure image quality. This, however, makes it harder to acquire, store and even process such 360deg images. To alleviate this issue, we propose the first attempt at 360deg image rescaling, which refers to downscaling a 360deg image to a visually valid low-resolution (LR) counterpart and then upscaling to a high-resolution (HR) 360deg image given the LR variant. Specifically, we first analyze two 360deg image datasets and observe several findings that characterize how 360deg images typically change along their latitudes. Inspired by these findings, we propose a novel deformable invertible neural network (INN), named DINN360, for latitude-aware 360deg image rescaling. In DINN360, a deformable INN is designed to downscale the LR image, and project the high-frequency (HF) component to the latent space by adaptively handling various deformations occurring at different latitude regions. Given the downscaled LR image, the high-quality HR image is then reconstructed in a conditional latitude-aware manner by recovering the structure-related HF component from the latent space. Extensive experiments over four public datasets show that our DINN360 method performs considerably better than other state-of-the-art methods for 2x, 4x and 8x 360deg image rescaling.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1967.Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis From Monocular Image</span><br>
                <span class="as">Deng, YuandWang, BaoyuanandShum, Heung-Yeung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Learning_Detailed_Radiance_Manifolds_for_High-Fidelity_and_3D-Consistent_Portrait_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4423-4433.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目肖像图像的新视图合成中的关键挑战是在连续的姿势变化下的3D一致性。<br>
                    动机：大多数现有方法依赖于2D生成模型，这往往导致明显的3D不一致的伪影。<br>
                    方法：提出了一种基于最近提出的3D感知GAN（Generative Radiance Manifolds，GRAM）的3D一致的单目肖像图像新视图合成方法。该方法通过辐射流形表示在多视角虚拟主体图像生成方面表现出强大的3D一致性。<br>
                    效果：通过在野外2D图像上进行训练，该方法实现了高保真度和3D一致的肖像合成，大大超过了先前的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A key challenge for novel view synthesis of monocular portrait images is 3D consistency under continuous pose variations. Most existing methods rely on 2D generative models which often leads to obvious 3D inconsistency artifacts. We present a 3D-consistent novel view synthesis approach for monocular portrait images based on a recent proposed 3D-aware GAN, namely Generative Radiance Manifolds (GRAM), which has shown strong 3D consistency at multiview image generation of virtual subjects via the radiance manifolds representation. However, simply learning an encoder to map a real image into the latent space of GRAM can only reconstruct coarse radiance manifolds without faithful fine details, while improving the reconstruction fidelity via instance-specific optimization is time-consuming. We introduce a novel detail manifolds reconstructor to learn 3D-consistent fine details on the radiance manifolds from monocular images, and combine them with the coarse radiance manifolds for high-fidelity reconstruction. The 3D priors derived from the coarse radiance manifolds are used to regulate the learned details to ensure reasonable synthesized results at novel views. Trained on in-the-wild 2D images, our method achieves high-fidelity and 3D-consistent portrait synthesis largely outperforming the prior art. Project page: https://yudeng.github.io/GRAMInverter/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1968.Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis</span><br>
                <span class="as">Do, HoseokandYoo, EunKyungandKim, TaehyeongandLee, ChulandChoi, JinYoung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Do_Quantitative_Manipulation_of_Custom_Attributes_on_3D-Aware_Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8529-8538.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对3D图像进行细粒度的特定属性控制，而不仅仅局限于某一类别的对象。<br>
                    动机：尽管基于3D的GAN技术已被成功应用于渲染具有各种属性的逼真3D图像并保持视角一致性，但如何精细控制3D图像的属性，而不仅限于特定类别的对象，目前的研究还很少。<br>
                    方法：我们提出了一种新的基于3D-GAN表示的图像操作模型，用于精细控制特定自定义属性。通过扩展最新的基于3D的GAN模型（如EG3D），我们的用户友好的定量操作模型实现了多属性数量的精细且规范化的3D操作控制，同时保持了视角一致性。<br>
                    效果：通过各种实验，我们从定性和定量两个方面验证了我们提出的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While 3D-based GAN techniques have been successfully applied to render photo-realistic 3D images with a variety of attributes while preserving view consistency, there has been little research on how to fine-control 3D images without limiting to a specific category of objects of their properties. To fill such research gap, we propose a novel image manipulation model of 3D-based GAN representations for a fine-grained control of specific custom attributes. By extending the latest 3D-based GAN models (e.g., EG3D), our user-friendly quantitative manipulation model enables a fine yet normalized control of 3D manipulation of multi-attribute quantities while achieving view consistency. We validate the effectiveness of our proposed technique both qualitatively and quantitatively through various experiments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1969.ObjectStitch: Object Compositing With Diffusion Model</span><br>
                <span class="as">Song, YizhiandZhang, ZhifeiandLin, ZheandCohen, ScottandPrice, BrianandZhang, JianmingandKim, SooYeandAliaga, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18310-18319.png><br>
            
            <span class="tt"><span class="t0">研究问题：基于2D图像的对象合成是一个挑战性的问题，因为它通常涉及多个处理阶段，如色彩协调、几何校正和阴影生成，以生成逼真的结果。<br>
                    动机：由于注释合成训练数据对需要专业人员的大量手动努力，且难以扩展，因此我们提出了一种利用条件扩散模型的力量进行对象合成的自监督框架。<br>
                    方法：我们的框架可以在一个统一的模型中全面解决对象合成任务，转换生成对象的视点、几何、颜色和阴影，而无需手动标注。为了保留输入对象的特性，我们引入了一个内容适应器来帮助保持类别语义和对象外观。此外，还采用了一种数据增强方法来提高生成器的保真度。<br>
                    效果：在一项关于各种真实世界图像的用户研究中，我们的方法在合成结果图像的真实性和忠实度方面都优于相关的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object compositing based on 2D images is a challenging problem since it typically involves multiple processing stages such as color harmonization, geometry correction and shadow generation to generate realistic results. Furthermore, annotating training data pairs for compositing requires substantial manual effort from professionals, and is hardly scalable. Thus, with the recent advances in generative models, in this work, we propose a self-supervised framework for object compositing by leveraging the power of conditional diffusion models. Our framework can hollistically address the object compositing task in a unified model, transforming the viewpoint, geometry, color and shadow of the generated object while requiring no manual labeling. To preserve the input object's characteristics, we introduce a content adaptor that helps to maintain categorical semantics and object appearance. A data augmentation method is further adopted to improve the fidelity of the generator. Our method outperforms relevant baselines in both realism and faithfulness of the synthesized result images in a user study on various real-world images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1970.High-Fidelity 3D GAN Inversion by Pseudo-Multi-View Optimization</span><br>
                <span class="as">Xie, JiaxinandOuyang, HaoandPiao, JingtanandLei, ChenyangandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_High-Fidelity_3D_GAN_Inversion_by_Pseudo-Multi-View_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/321-331.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过单张图片生成高保真度的新视图，同时保留输入图像的特定细节。<br>
                    动机：由于几何和纹理之间的权衡，高保真度的3D GAN反转具有固有的挑战性，对单一视图输入图像的过度拟合往往会在潜在优化过程中损害估计的几何结构。<br>
                    方法：我们提出了一种新颖的管道，基于伪多视图估计和可见性分析。我们将原始纹理保留在可见部分，并利用生成的先验知识来处理被遮挡的部分。<br>
                    效果：大量实验表明，我们的方法在重建和新的视图合成质量上优于先前的工作，即使对于具有分布外纹理的图像也是如此。此外，我们的管道还可以使用反转的潜在代码进行图像属性编辑和3D感知的纹理修改。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a high-fidelity 3D generative adversarial network (GAN) inversion framework that can synthesize photo-realistic novel views while preserving specific details of the input image. High-fidelity 3D GAN inversion is inherently challenging due to the geometry-texture trade-off, where overfitting to a single view input image often damages the estimated geometry during the latent optimization. To solve this challenge, we propose a novel pipeline that builds on the pseudo-multi-view estimation with visibility analysis. We keep the original textures for the visible parts and utilize generative priors for the occluded parts. Extensive experiments show that our approach achieves advantageous reconstruction and novel view synthesis quality over prior work, even for images with out-of-distribution textures. The proposed pipeline also enables image attribute editing with the inverted latent code and 3D-aware texture modification. Our approach enables high-fidelity 3D rendering from a single image, which is promising for various applications of AI-generated 3D content. The source code is at https://github.com/jiaxinxie97/HFGI3D/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1971.A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction From In-the-Wild Images</span><br>
                <span class="as">Lei, BiwenandRen, JianqiangandFeng, MengyangandCui, MiaomiaoandXie, Xuansong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_A_Hierarchical_Representation_Network_for_Accurate_and_Detailed_Face_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/394-403.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图片中实现准确且详细的人脸重建。<br>
                    动机：由于3DMM的低维表示能力有限，大多数基于3DMM的人脸重建方法无法恢复高频面部细节，如皱纹、酒窝等。<br>
                    方法：本文提出了一种新的分层表示网络（HRN）来实现准确的面部重建。通过实施几何解耦和引入分层表示来完成详细的面部建模，同时结合面部细节的3D先验来提高重建结果的准确性和真实性。<br>
                    效果：在两个单视图和两个多视图的面部重建基准测试中，该方法在重建准确性和视觉效果上都优于现有方法。最后，引入了一个高质量的3D人脸数据集FaceHD-100，以推动高保真度人脸重建的研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Limited by the nature of the low-dimensional representational capacity of 3DMM, most of the 3DMM-based face reconstruction (FR) methods fail to recover high-frequency facial details, such as wrinkles, dimples, etc. Some attempt to solve the problem by introducing detail maps or non-linear operations, however, the results are still not vivid. To this end, we in this paper present a novel hierarchical representation network (HRN) to achieve accurate and detailed face reconstruction from a single image. Specifically, we implement the geometry disentanglement and introduce the hierarchical representation to fulfill detailed face modeling. Meanwhile, 3D priors of facial details are incorporated to enhance the accuracy and authenticity of the reconstruction results. We also propose a de-retouching module to achieve better decoupling of the geometry and appearance. It is noteworthy that our framework can be extended to a multi-view fashion by considering detail consistency of different views. Extensive experiments on two single-view and two multi-view FR benchmarks demonstrate that our method outperforms the existing methods in both reconstruction accuracy and visual effects. Finally, we introduce a high-quality 3D face dataset FaceHD-100 to boost the research of high-fidelity face reconstruction. The project homepage is at https://younglbw.github.io/HRN-homepage/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1972.NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views</span><br>
                <span class="as">Xu, DejiaandJiang, YifanandWang, PeihaoandFan, ZhiwenandWang, YiandWang, Zhangyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4479-4489.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将单张图片转化为3D物体，并生成与参考图像相符的360度视图。<br>
                    动机：虚拟现实和增强现实对3D内容的需求日益增长，但创建高质量的3D内容需要人类专家的繁琐工作。<br>
                    方法：提出了一种名为NeuralLift-360的新框架，利用深度感知神经辐射表示（NeRF）并通过去噪扩散模型指导场景创作。通过引入排名损失，可以在野外进行粗略的深度估计。还采用了CLIP引导的采样策略以提供连贯的指导。<br>
                    效果：实验表明，NeuralLift-360显著优于现有的最先进的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Virtual reality and augmented reality (XR) bring increasing demand for 3D content generation. However, creating high-quality 3D content requires tedious work from a human expert. In this work, we study the challenging task of lifting a single image to a 3D object and, for the first time, demonstrate the ability to generate a plausible 3D object with 360deg views that corresponds well with the given reference image. By conditioning on the reference image, our model can fulfill the everlasting curiosity for synthesizing novel views of objects from images. Our technique sheds light on a promising direction of easing the workflows for 3D artists and XR designers. We propose a novel framework, dubbed NeuralLift-360, that utilizes a depth-aware neural radiance representation (NeRF) and learns to craft the scene guided by denoising diffusion models. By introducing a ranking loss, our NeuralLift-360 can be guided with rough depth estimation in the wild. We also adopt a CLIP-guided sampling strategy for the diffusion prior to provide coherent guidance. Extensive experiments demonstrate that our NeuralLift-360 significantly outperforms existing state-of-the-art baselines. Project page: https://vita-group.github.io/NeuralLift-360/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1973.Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild</span><br>
                <span class="as">Zhang, ZhenyuandChen, RenwangandCao, WeijianandTai, YingandWang, Chengjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/382-393.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何恢复极端姿态、阴影或外观条件下的3D人脸？<br>
                    动机：目前的生成模型在恢复3D人脸时，由于形状假设有限，容易在极端条件下失败。<br>
                    方法：本文提出了一种新的神经原初人脸场（NPF）进行无监督的鲁棒3D人脸建模。NPF从野外照片集中分离出常见的/特定的面部线索，如身份、表情和场景特定细节，并学习一个脸部原型来聚合3D一致的身份。<br>
                    效果：实验表明，与最先进的方法相比，NPF能够恢复更优或竞争的面部形状和纹理。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative models show good potential for recovering 3D faces beyond limited shape assumptions. While plausible details and resolutions are achieved, these models easily fail under extreme conditions of pose, shadow or appearance, due to the entangled fitting or lack of multi-view priors. To address this problem, this paper presents a novel Neural Proto-face Field (NPF) for unsupervised robust 3D face modeling. Instead of using constrained images as Neural Radiance Field (NeRF), NPF disentangles the common/specific facial cues, i.e., ID, expression and scene-specific details from in-the-wild photo collections. Specifically, NPF learns a face prototype to aggregate 3D-consistent identity via uncertainty modeling, extracting multi-image priors from a photo collection. NPF then learns to deform the prototype with the appropriate facial expressions, constrained by a loss of expression consistency and personal idiosyncrasies. Finally, NPF is optimized to fit a target image in the collection, recovering specific details of appearance and geometry. In this way, the generative model benefits from multi-image priors and meaningful facial structures. Extensive experiments on benchmarks show that NPF recovers superior or competitive facial shapes and textures, compared to state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1974.Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion</span><br>
                <span class="as">Lan, YushiandMeng, XuyiandYang, ShuaiandLoy, ChenChangeandDai, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lan_Self-Supervised_Geometry-Aware_Encoder_for_Style-Based_3D_GAN_Inversion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20940-20949.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D人脸重建和语义编辑中的挑战性问题，即给定一张人脸图像，如何预测其潜码以忠实地恢复其3D形状和详细纹理。<br>
                    动机：尽管2D StyleGAN在二维人脸重建和语义编辑方面取得了重大进展，但将2D StyleGAN扩展到3D人脸的研究仍然缺乏一个通用的3D GAN逆映射框架，这限制了3D人脸重建和语义编辑的应用。<br>
                    方法：我们设计了一种有效的自我训练方案来约束逆映射的学习过程。这种学习是在没有任何真实世界的2D-3D训练对的情况下进行的，而是通过从3D GAN生成的代理样本进行高效学习的。此外，除了捕获粗略形状和纹理信息的全局潜在代码外，我们还增强了生成网络的一个局部分支，其中添加了像素对齐的特征，以忠实地重建人脸细节。我们还考虑了一个新的管道来进行3D视图一致的编辑。<br>
                    效果：大量的实验表明，我们的方法在形状和纹理重建质量上都优于最先进的逆映射方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>StyleGAN has achieved great progress in 2D face reconstruction and semantic editing via image inversion and latent editing. While studies over extending 2D StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion framework is still missing, limiting the applications of 3D face reconstruction and semantic editing. In this paper, we study the challenging problem of 3D GAN inversion where a latent code is predicted given a single face image to faithfully recover its 3D shapes and detailed textures. The problem is ill-posed: innumerable compositions of shape and texture could be rendered to the current image. Furthermore, with the limited capacity of a global latent code, 2D inversion methods cannot preserve faithful shape and texture at the same time when applied to 3D models. To solve this problem, we devise an effective self-training scheme to constrain the learning of inversion. The learning is done efficiently without any real-world 2D-3D training pairs but proxy samples generated from a 3D GAN. In addition, apart from a global latent code that captures the coarse shape and texture information, we augment the generation network with a local branch, where pixel-aligned features are added to faithfully reconstruct face details. We further consider a new pipeline to perform 3D view-consistent editing. Extensive experiments show that our method outperforms state-of-the-art inversion methods in both shape and texture reconstruction quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1975.PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction</span><br>
                <span class="as">Melas-Kyriazi, LukeandRupprecht, ChristianandVedaldi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12923-12932.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中重建物体的3D形状。<br>
                    动机：从单张RGB图像中重建物体的3D形状是计算机视觉中长期存在的问题。<br>
                    方法：提出一种新的方法，通过条件去噪扩散过程生成稀疏点云进行单图像3D重建。该方法以单张RGB图像及其相机位姿为输入，逐步将一组随机采样自三维高斯分布的3D点去噪成物体的形状。其关键在于几何一致的条件处理过程，即投影条件处理：在扩散过程的每一步，我们将局部图像特征从给定的相机位姿投影到部分去噪的点云上。<br>
                    效果：实验结果表明，该方法不仅在合成基准测试上表现良好，而且在复杂的真实世界数据上也取得了显著的质量改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstructing the 3D shape of an object from a single RGB image is a long-standing problem in computer vision. In this paper, we propose a novel method for single-image 3D reconstruction which generates a sparse point cloud via a conditional denoising diffusion process. Our method takes as input a single RGB image along with its camera pose and gradually denoises a set of 3D points, whose positions are initially sampled randomly from a three-dimensional Gaussian distribution, into the shape of an object. The key to our method is a geometrically-consistent conditioning process which we call projection conditioning: at each step in the diffusion process, we project local image features onto the partially-denoised point cloud from the given camera pose. This projection conditioning process enables us to generate high-resolution sparse geometries that are well-aligned with the input image and can additionally be used to predict point colors after shape reconstruction. Moreover, due to the probabilistic nature of the diffusion process, our method is naturally capable of generating multiple different shapes consistent with a single input image. In contrast to prior work, our approach not only performs well on synthetic benchmarks but also gives large qualitative improvements on complex real-world data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1976.Evading Forensic Classifiers With Attribute-Conditioned Adversarial Faces</span><br>
                <span class="as">Shamshad, FahadandSrivatsan, KoushikandNandakumar, Karthik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shamshad_Evading_Forensic_Classifiers_With_Attribute-Conditioned_Adversarial_Faces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16469-16478.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成能成功骗过人脸鉴定分类器的具有特定属性（如发色、眼睛大小、种族、性别等）的对抗性假脸图像。<br>
                    动机：现有的基于深度学习的人脸鉴定分类器虽然可以高准确度地检测出人脸图像是否为合成或真实，但它们容易受到对抗性攻击。这些攻击虽然能成功避开人脸鉴定分类器的检测，但会引入可见的噪声模式，通过仔细的人眼审查就能发现。此外，这些攻击需要访问目标模型，这并不总是可能的。<br>
                    方法：利用最先进的生成对抗网络（GAN）StyleGAN进行操作，该网络具有解耦表示，可以在不离开自然图像流形的情况下进行一系列修改。在StyleGAN的特征空间中搜索对抗性潜在代码，搜索过程可以由文本提示或参考图像引导。还提出了一种基于元学习的优化策略，以实现对未知目标模型的可转移性能。<br>
                    效果：实验表明，该方法可以生成语义上经过操纵的对抗性假脸图像，这些图像符合指定的属性集，并能成功骗过人脸鉴定分类器，同时对人类来说是不可检测的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability of generative models to produce highly realistic synthetic face images has raised security and ethical concerns. As a first line of defense against such fake faces, deep learning based forensic classifiers have been developed. While these forensic models can detect whether a face image is synthetic or real with high accuracy, they are also vulnerable to adversarial attacks. Although such attacks can be highly successful in evading detection by forensic classifiers, they introduce visible noise patterns that are detectable through careful human scrutiny. Additionally, these attacks assume access to the target model(s) which may not always be true. Attempts have been made to directly perturb the latent space of GANs to produce adversarial fake faces that can circumvent forensic classifiers. In this work, we go one step further and show that it is possible to successfully generate adversarial fake faces with a specified set of attributes (e.g., hair color, eye size, race, gender, etc.). To achieve this goal, we leverage the state-of-the-art generative model StyleGAN with disentangled representations, which enables a range of modifications without leaving the manifold of natural images. We propose a framework to search for adversarial latent codes within the feature space of StyleGAN, where the search can be guided either by a text prompt or a reference image. We also propose a meta-learning based optimization strategy to achieve transferable performance on unknown target models. Extensive experiments demonstrate that the proposed approach can produce semantically manipulated adversarial fake faces, which are true to the specified attribute set and can successfully fool forensic face classifiers, while remaining undetectable by humans. Code: https://github.com/koushiksrivats/face_attribute_attack.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1977.Handwritten Text Generation From Visual Archetypes</span><br>
                <span class="as">Pippi, VittorioandCascianelli, SilviaandCucchiara, Rita</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pippi_Handwritten_Text_Generation_From_Visual_Archetypes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22458-22467.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成特定作者风格的手写文本图像，特别是在处理未见过的风格和新词，以及训练中很少遇到的字符时。<br>
                    动机：尽管生成器模型已经可以模仿作者的风格，但对于罕见字符的泛化能力尚未得到解决。<br>
                    方法：设计一种基于Transformer的少样本风格手写文本生成模型，并专注于获取文本和风格的稳健和丰富的表示。特别是，我们提出了一种新的文本内容表示方法，即将符号的图像作为一系列密集向量，这些符号是用标准的GNU Unifont字形书写的，可以被视为其视觉原型。这种方法更适合生成在训练中很少看到的字符，但可能与经常观察到的字符具有视觉细节相似性。对于风格，我们通过在一个大型合成数据集上进行特定的预训练来获取未见过作者书法的稳健表示。<br>
                    效果：定量和定性的结果表明，我们的方法在生成未见过的风格和罕见字符的单词方面比现有的依赖独立字符一热编码的方法更有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating synthetic images of handwritten text in a writer-specific style is a challenging task, especially in the case of unseen styles and new words, and even more when these latter contain characters that are rarely encountered during training. While emulating a writer's style has been recently addressed by generative models, the generalization towards rare characters has been disregarded. In this work, we devise a Transformer-based model for Few-Shot styled handwritten text generation and focus on obtaining a robust and informative representation of both the text and the style. In particular, we propose a novel representation of the textual content as a sequence of dense vectors obtained from images of symbols written as standard GNU Unifont glyphs, which can be considered their visual archetypes. This strategy is more suitable for generating characters that, despite having been seen rarely during training, possibly share visual details with the frequently observed ones. As for the style, we obtain a robust representation of unseen writers' calligraphy by exploiting specific pre-training on a large synthetic dataset. Quantitative and qualitative results demonstrate the effectiveness of our proposal in generating words in unseen styles and with rare characters more faithfully than existing approaches relying on independent one-hot encodings of the characters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1978.Align Your Latents: High-Resolution Video Synthesis With Latent Diffusion Models</span><br>
                <span class="as">Blattmann, AndreasandRombach, RobinandLing, HuanandDockhorn, TimandKim, SeungWookandFidler, SanjaandKreis, Karsten</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22563-22575.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用潜在扩散模型（LDM）进行高质量的高分辨率视频生成，同时避免过度的计算需求。<br>
                    动机：现有的方法在处理资源密集型的视频生成任务时，往往需要大量的计算资源。通过将LDM应用到高分辨率视频生成中，可以在保持高质量输出的同时，降低计算需求。<br>
                    方法：首先，仅在图像上预训练一个LDM；然后，通过在潜在空间扩散模型中引入时间维度，并将编码后的图像序列（即视频）进行微调，将图像生成器转变为视频生成器。同样地，对扩散模型的上采样器进行时间对齐，将其转变为时间一致的视频超分辨率模型。<br>
                    效果：在真实驾驶视频（分辨率为512x1024）上进行验证，实现了最先进的性能。此外，该方法可以轻松利用现成的预训练图像LDMs，只需训练一个时间对齐模型即可。通过这种方式，可以将公开的、最先进的文本到图像LDM Stable Diffusion转化为高效且富有表现力的文本到视频模型，分辨率可达1280x2048。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512x1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280x2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. Project page: https://nv-tlabs.github.io/VideoLDM/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1979.NULL-Text Inversion for Editing Real Images Using Guided Diffusion Models</span><br>
                <span class="as">Mokady, RonandHertz, AmirandAberman, KfirandPritch, YaelandCohen-Or, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6038-6047.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过文本引导的扩散模型进行图像编辑。<br>
                    动机：目前，大型的文本引导扩散模型在图像生成方面具有强大的能力，但需要通过反转图像和有意义的文本提示来修改真实的图像。<br>
                    方法：本文提出了一种精确的反转技术，包括关键的新型组件：（i）对扩散模型的关键性反转，使用单个关键的噪声向量进行优化；（ii）NULL-text优化，只修改用于无分类器指导的无条件文本嵌入，而不是输入文本嵌入。<br>
                    效果：通过在各种图像和各种提示编辑上广泛评估，证明了我们的Null-text反转在真实图像的高保真度编辑方面的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent large-scale text-guided diffusion models provide powerful image generation capabilities. Currently, a massive effort is given to enable the modification of these images using text only as means to offer intuitive and versatile editing tools. To edit a real image using these state-of-the-art tools, one must first invert the image with a meaningful text prompt into the pretrained model's domain. In this paper, we introduce an accurate inversion technique and thus facilitate an intuitive text-based modification of the image. Our proposed inversion consists of two key novel components: (i) Pivotal inversion for diffusion models. While current methods aim at mapping random noise samples to a single input image, we use a single pivotal noise vector for each timestamp and optimize around it. We recognize that a direct DDIM inversion is inadequate on its own, but does provide a rather good anchor for our optimization. (ii) NULL-text optimization, where we only modify the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. Our Null-text inversion, based on the publicly available Stable Diffusion model, is extensively evaluated on a variety of images and various prompt editing, showing high-fidelity editing of real images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1980.Neural Texture Synthesis With Guided Correspondence</span><br>
                <span class="as">Zhou, YangandChen, KaijianandXiao, RongjunandHuang, Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Neural_Texture_Synthesis_With_Guided_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18095-18104.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在重新推广MRFs和神经网络的结合，即CNNMRF模型，用于纹理合成。<br>
                    动机：尽管MRFs是经典基于实例的纹理合成方法的基础，但在深度学习时代并未得到充分重视。<br>
                    方法：首先提出在最近邻搜索中计算引导对应距离，并在此基础上定义引导对应损失以测量输出纹理与示例的相似性。<br>
                    效果：实验表明，该方法在不受控制的和受控制的纹理合成方面超越了现有的神经网络方法。更重要的是，引导对应损失可以作为一般纹理损失，例如在实时受控合成和基于反转的单图像编辑的生成网络训练中使用。相比之下，现有的纹理损失，如切片Wasserstein损失，无法在这些具有挑战性的任务上工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Markov random fields (MRFs) are the cornerstone of classical approaches to example-based texture synthesis. Yet, it is not fully valued in the deep learning era. This paper aims to re-promote the combination of MRFs and neural networks, i.e., the CNNMRF model, for texture synthesis, with two key observations made. We first propose to compute the Guided Correspondence Distance in the nearest neighbor search, based on which a Guided Correspondence loss is defined to measure the similarity of the output texture to the example. Experiments show that our approach surpasses existing neural approaches in uncontrolled and controlled texture synthesis. More importantly, the Guided Correspondence loss can function as a general textural loss in, e.g., training generative networks for real-time controlled synthesis and inversion-based single-image editing. In contrast, existing textural losses, such as the Sliced Wasserstein loss, cannot work on these challenging tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1981.Hierarchical Fine-Grained Image Forgery Detection and Localization</span><br>
                <span class="as">Guo, XiaoandLiu, XiaohongandRen, ZhiyuanandGrosz, StevenandMasi, IacopoandLiu, Xiaoming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Hierarchical_Fine-Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3155-3165.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地检测和定位图像伪造？<br>
                    动机：由于CNN生成的图像和图像编辑领域的图像伪造属性差异大，使得统一的图像伪造检测和定位（IFDL）具有挑战性。<br>
                    方法：提出一种分层精细的IFDL表示学习方法，通过在不同层次上对操纵图像的伪造属性进行多标签表示，并利用这些层次之间的依赖关系进行精细分类。<br>
                    效果：在7个不同的基准测试中，该方法在IFDL任务和伪造属性分类任务上都表现出了有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Differences in forgery attributes of images generated in CNN-synthesized and image-editing domains are large, and such differences make a unified image forgery detection and localization (IFDL) challenging. To this end, we present a hierarchical fine-grained formulation for IFDL representation learning. Specifically, we first represent forgery attributes of a manipulated image with multiple labels at different levels. Then we perform fine-grained classification at these levels using the hierarchical dependency between them. As a result, the algorithm is encouraged to learn both comprehensive features and inherent hierarchical nature of different forgery attributes, thereby improving the IFDL representation. Our proposed IFDL framework contains three components: multi-branch feature extractor, localization and classification modules. Each branch of the feature extractor learns to classify forgery attributes at one level, while localization and classification modules segment the pixel-level forgery region and detect image-level forgery, respectively. Lastly, we construct a hierarchical fine-grained dataset to facilitate our study. We demonstrate the effectiveness of our method on 7 different benchmarks, for both tasks of IFDL and forgery attribute classification. Our source code and dataset can be found at https://github.com/CHELSEA234/HiFi_IFDL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1982.Modernizing Old Photos Using Multiple References via Photorealistic Style Transfer</span><br>
                <span class="as">Gunawan, AgusandKim, SooYeandSim, HyeonjunandLee, Jae-HoandKim, Munchurl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gunawan_Modernizing_Old_Photos_Using_Multiple_References_via_Photorealistic_Style_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12460-12469.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用多种参考进行旧照片的现代化改造。<br>
                    动机：目前的旧照片现代化方法缺乏对多种参考的有效利用，我们提出一种新的基于多参考的旧照片现代化（MROPM）框架来解决这个问题。<br>
                    方法：我们提出了一个新颖的多参考旧照片现代化网络（MROPM-Net）和一种新的合成数据生成方案。MROPM-Net通过逼真的风格转换（PST）使用多种参考对旧照片进行风格化，并进一步优化结果以产生现代感的图片。同时，合成数据生成方案训练网络有效利用多种参考进行现代化改造。<br>
                    效果：实验表明，我们的方法在真实旧照片的现代化改造上优于其他基线方法，即使训练过程中没有使用过旧照片。此外，我们的方法能够为旧照片中的每个语义区域适当选择来自多种参考的风格，进一步提高现代化改造的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper firstly presents old photo modernization using multiple references by performing stylization and enhancement in a unified manner. In order to modernize old photos, we propose a novel multi-reference-based old photo modernization (MROPM) framework consisting of a network MROPM-Net and a novel synthetic data generation scheme. MROPM-Net stylizes old photos using multiple references via photorealistic style transfer (PST) and further enhances the results to produce modern-looking images. Meanwhile, the synthetic data generation scheme trains the network to effectively utilize multiple references to perform modernization. To evaluate the performance, we propose a new old photos benchmark dataset (CHD) consisting of diverse natural indoor and outdoor scenes. Extensive experiments show that the proposed method outperforms other baselines in performing modernization on real old photos, even though no old photos were used during training. Moreover, our method can appropriately select styles from multiple references for each semantic region in the old photo to further improve the modernization performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1983.Interactive Cartoonization With Controllable Perceptual Factors</span><br>
                <span class="as">Ahn, NamhyukandKwon, PatrickandBack, JihyeandHong, KibeomandKim, Seungkwon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Interactive_Cartoonization_With_Controllable_Perceptual_Factors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16827-16835.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将自然照片转化为卡通风格，并允许艺术家对结果进行操作。<br>
                    动机：现有的深度学习方法只能进行端到端的转换，无法让艺术家进行结果操作。<br>
                    方法：提出一种新的解决方案，基于卡通创作过程的纹理和颜色编辑特性，设计了一个模型架构，包括分离的纹理和颜色解码器，以解耦这些属性。在纹理解码器中，提出了一个纹理控制器，使用户能够控制笔触风格和抽象度，生成多样化的卡通纹理。还引入了HSV颜色增强，使网络生成一致的颜色转换。<br>
                    效果：据我们所知，这是第一种在推理阶段控制卡通化的方法，与基线相比，生成了高质量的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cartoonization is a task that renders natural photos into cartoon styles. Previous deep methods only have focused on end-to-end translation, disabling artists from manipulating results. To tackle this, in this work, we propose a novel solution with editing features of texture and color based on the cartoon creation process. To do that, we design a model architecture to have separate decoders, texture and color, to decouple these attributes. In the texture decoder, we propose a texture controller, which enables a user to control stroke style and abstraction to generate diverse cartoon textures. We also introduce an HSV color augmentation to induce the networks to generate consistent color translation. To the best of our knowledge, our work is the first method to control the cartoonization during the inferences step, generating high-quality results compared to baselines.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>