<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic3</h2>
      <b>Topic words : &ensp;</b>domain, &ensp;training, &ensp;distribution, &ensp;propose, &ensp;samples, &ensp;performance, &ensp;target, &ensp;loss</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">541.Deep Frequency Filtering for Domain Generalization</span><br>
                <span class="as">Lin, ShiqiandZhang, ZhizhengandHuang, ZhipengandLu, YanandLan, CuilingandChu, PengandYou, QuanzengandWang, JiangandLiu, ZichengandParulkar, AmeyandNavkal, VirajandChen, Zhibo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Deep_Frequency_Filtering_for_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11797-11807.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络的泛化能力，以应对实际应用中的挑战。<br>
                    动机：深度神经网络在训练过程中对某些频率成分有偏好，这可能影响学习到的特征的鲁棒性。<br>
                    方法：提出深度频率过滤（DFF）方法，通过在训练过程中显式调整不同领域转移困难的频率成分，来学习具有领域通用性的特征。具体做法是对不同层次的特征图进行快速傅里叶变换（FFT），然后采用轻量级模块从FFT后的频率表示中学习注意力掩码，增强可转移成分，同时抑制不利于泛化的成分。<br>
                    效果：实验表明，DFF方法能有效提高深度神经网络的泛化能力，并在不同领域的迁移任务上超越现有最佳方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Improving the generalization ability of Deep Neural Networks (DNNs) is critical for their practical uses, which has been a longstanding challenge. Some theoretical studies have uncovered that DNNs have preferences for some frequency components in the learning process and indicated that this may affect the robustness of learned features. In this paper, we propose Deep Frequency Filtering (DFF) for learning domain-generalizable features, which is the first endeavour to explicitly modulate the frequency components of different transfer difficulties across domains in the latent space during training. To achieve this, we perform Fast Fourier Transform (FFT) for the feature maps at different layers, then adopt a light-weight module to learn attention masks from the frequency representations after FFT to enhance transferable components while suppressing the components not conducive to generalization. Further, we empirically compare the effectiveness of adopting different types of attention designs for implementing DFF. Extensive experiments demonstrate the effectiveness of our proposed DFF and show that applying our DFF on a plain baseline outperforms the state-of-the-art methods on different domain generalization tasks, including close-set classification and open-set retrieval.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">542.Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow</span><br>
                <span class="as">Zhou, HanyuandChang, YiandYan, WendingandYan, Luxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Unsupervised_Cumulative_Domain_Adaptation_for_Foggy_Scene_Optical_Flow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9569-9578.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的光学流方法在清晰场景下表现良好，但在雾天场景下性能受限。<br>
                    动机：为了弥合清晰到雾天的场景差距，现有方法通常采用领域适应将运动知识从清晰转移到合成的雾天领域，但忽视了合成到真实领域的差距，因此在应用到真实世界场景时会出现错误。<br>
                    方法：本文提出了一种新的无监督累积领域适应光学流（UCDA-Flow）框架：深度关联运动适应和相关性对齐运动适应。具体来说，我们发现深度是影响光学流的关键因素：深度越深，光学流越差，这激发我们设计一个深度关联运动适应模块来弥合清晰到雾天的场景差距。此外，我们发现成本体积相关性在合成和真实的雾天图像中具有相似的分布，这启发我们设计一个相关性对齐运动适应模块将合成雾天领域的运动知识提炼到真实雾天领域。<br>
                    效果：在统一的框架下，提出的累积适应逐步将知识从清晰场景转移到真实雾天场景。广泛的实验已经进行以验证所提出方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optical flow has achieved great success under clean scenes, but suffers from restricted performance under foggy scenes. To bridge the clean-to-foggy domain gap, the existing methods typically adopt the domain adaptation to transfer the motion knowledge from clean to synthetic foggy domain. However, these methods unexpectedly neglect the synthetic-to-real domain gap, and thus are erroneous when applied to real-world scenes. To handle the practical optical flow under real foggy scenes, in this work, we propose a novel unsupervised cumulative domain adaptation optical flow (UCDA-Flow) framework: depth-association motion adaptation and correlation-alignment motion adaptation. Specifically, we discover that depth is a key ingredient to influence the optical flow: the deeper depth, the inferior optical flow, which motivates us to design a depth-association motion adaptation module to bridge the clean-to-foggy domain gap. Moreover, we figure out that the cost volume correlation shares similar distribution of the synthetic and real foggy images, which enlightens us to devise a correlation-alignment motion adaptation module to distill motion knowledge of the synthetic foggy domain to the real foggy domain. Note that synthetic fog is designed as the intermediate domain. Under this unified framework, the proposed cumulative adaptation progressively transfers knowledge from clean scenes to real foggy scenes. Extensive experiments have been performed to verify the superiority of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">543.NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs</span><br>
                <span class="as">Rangwani, HarshandBansal, LavishandSharma, KartikandKarmali, TejanandJampani, VarunandBabu, R.Venkatesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rangwani_NoisyTwins_Class-Consistent_and_Diverse_Image_Generation_Through_StyleGANs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5987-5996.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练StyleGANs在大规模长尾数据集上的性能退化问题。<br>
                    动机：StyleGANs在语义解耦的潜在空间中表现优秀，适合图像编辑和操作，但在大规模长尾数据集上进行类别条件训练时性能会严重下降。<br>
                    方法：提出NoisyTwins方法，通过引入有效的低成本类别嵌入增强策略，并在W空间中基于自我监督对潜在变量进行解相关处理。<br>
                    效果：在ImageNet-LT和iNaturalist 2019等大规模真实世界长尾数据集上，该方法比其他方法提高了19%的FID分数，建立了新的最先进的状态。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the W latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the W space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by   19% on FID, establishing a new state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">544.Robust Outlier Rejection for 3D Registration With Variational Bayes</span><br>
                <span class="as">Jiang, HaoboandDang, ZhengandWei, ZhenandXie, JinandYang, JianandSalzmann, Mathieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Robust_Outlier_Rejection_for_3D_Registration_With_Variational_Bayes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1148-1157.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从3D注册中移除异常值（不匹配的对应关系）以实现稳健对齐。<br>
                    动机：现有的基于学习的异常值（不匹配的对应关系）拒绝方法通常将异常值去除问题形式化为内点/外点分类问题，其成功的核心在于学习判别性的内点/外点特征表示。<br>
                    方法：本文提出了一种新的变分非局部网络基异常值拒绝框架用于稳健对齐。通过用变分贝叶斯推理重新表述非局部特征学习，可以建模贝叶斯驱动的长范围依赖性以聚合判别性的几何上下文信息进行内点/外点区分。<br>
                    效果：在3DMatch、3DLoMatch和KITTI数据集上的大量实验验证了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based outlier (mismatched correspondence) rejection for robust 3D registration generally formulates the outlier removal as an inlier/outlier classification problem. The core for this to be successful is to learn the discriminative inlier/outlier feature representations. In this paper, we develop a novel variational non-local network-based outlier rejection framework for robust alignment. By reformulating the non-local feature learning with variational Bayesian inference, the Bayesian-driven long-range dependencies can be modeled to aggregate discriminative geometric context information for inlier/outlier distinction. Specifically, to achieve such Bayesian-driven contextual dependencies, each query/key/value component in our non-local network predicts a prior feature distribution and a posterior one. Embedded with the inlier/outlier label, the posterior feature distribution is label-dependent and discriminative. Thus, pushing the prior to be close to the discriminative posterior in the training step enables the features sampled from this prior at test time to model high-quality long-range dependencies. Notably, to achieve effective posterior feature guidance, a specific probabilistic graphical model is designed over our non-local model, which lets us derive a variational low bound as our optimization objective for model training. Finally, we propose a voting-based inlier searching strategy to cluster the high-quality hypothetical inliers for transformation estimation. Extensive experiments on 3DMatch, 3DLoMatch, and KITTI datasets verify the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">545.Dynamically Instance-Guided Adaptation: A Backward-Free Approach for Test-Time Domain Adaptive Semantic Segmentation</span><br>
                <span class="as">Wang, WeiandZhong, ZhunandWang, WeijieandChen, XiandLing, CharlesandWang, BoyuandSebe, Nicu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamically_Instance-Guided_Adaptation_A_Backward-Free_Approach_for_Test-Time_Domain_Adaptive_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24090-24099.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决语义分割中测试时领域适应的问题，即如何同时提高效率和效果。<br>
                    动机：现有的方法或者效率低（如后向优化），或者忽视了语义适应（如分布对齐）。此外，它们还会受到由不稳定的优化和异常分布引起的累积错误的影响。<br>
                    方法：我们提出了一种名为动态实例引导适应（DIGA）的新型无后向方法来解决这些问题。该方法利用每个实例动态地以非参数方式指导其自身的适应，从而避免了错误累积问题和昂贵的优化成本。具体来说，DIGA由分布适应模块（DAM）和语义适应模块（SAM）组成，使我们能够联合调整模型的两个不可或缺的方面。<br>
                    效果：我们在五个目标领域进行了广泛的实验，结果表明了所提出方法的有效性和效率。我们的DIGA在TTDA-Seg中建立了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study the application of Test-time domain adaptation in semantic segmentation (TTDA-Seg) where both efficiency and effectiveness are crucial. Existing methods either have low efficiency (e.g., backward optimization) or ignore semantic adaptation (e.g., distribution alignment). Besides, they would suffer from the accumulated errors caused by unstable optimization and abnormal distributions. To solve these problems, we propose a novel backward-free approach for TTDA-Seg, called Dynamically Instance-Guided Adaptation (DIGA). Our principle is utilizing each instance to dynamically guide its own adaptation in a non-parametric way, which avoids the error accumulation issue and expensive optimizing cost. Specifically, DIGA is composed of a distribution adaptation module (DAM) and a semantic adaptation module (SAM), enabling us to jointly adapt the model in two indispensable aspects. DAM mixes the instance and source BN statistics to encourage the model to capture robust representation. SAM combines the historical prototypes with instance-level prototypes to adjust semantic predictions, which can be associated with the parametric classifier to mutually benefit the final results. Extensive experiments evaluated on five target domains demonstrate the effectiveness and efficiency of the proposed method. Our DIGA establishes new state-of-the-art performance in TTDA-Seg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">546.LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data</span><br>
                <span class="as">Park, JihyeandKim, SunwooandKim, SoohyunandCho, SeokjuandYoo, JaejunandUh, YoungjungandKim, Seungryong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_LANIT_Language-Driven_Image-to-Image_Translation_for_Unlabeled_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23401-23411.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像到图像翻译技术存在严重依赖每个样本的领域标注和/或无法处理每个图像的多个属性的问题。<br>
                    动机：近期真正无监督的方法采用聚类方法来轻松提供每个样本的独热领域标签，但它们不能解决现实世界的情况：一个样本可能具有多个属性。此外，聚类的语义与人类理解不容易耦合。<br>
                    方法：我们提出了一种语言驱动的图像到图像翻译模型，称为LANIT。我们利用文本中给出的易于获取的候选属性来为数据集生成相似性，该相似性表示每个样本的领域标签。这种形式自然地实现了多热标签，使用户可以用一组语言中的属性指定目标领域。为了解决初始提示不准确的问题，我们还提出了提示学习。我们还提出了域正则化损失，以强制将翻译后的图像映射到相应的域。<br>
                    效果：在几个标准基准上的实验表明，LANIT实现了与现有模型相当或更好的性能。代码可在github.com/KU-CVLAB/LANIT上获得。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability to handle multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to easily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to human understanding. To overcome these, we present LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot labels so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">547.Contrastive Semi-Supervised Learning for Underwater Image Restoration via Reliable Bank</span><br>
                <span class="as">Huang, ShiruiandWang, KeyanandLiu, HuanandChen, JunandLi, Yunsong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Contrastive_Semi-Supervised_Learning_for_Underwater_Image_Restoration_via_Reliable_Bank_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18145-18155.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管近期的水下图像修复技术取得了显著成就，但缺乏标注数据已成为进一步进步的主要障碍。<br>
                    动机：为了解决这一问题，我们提出了一种基于均值教师的半监督水下图像修复（Semi-UIR）框架，以将未标注的数据纳入网络训练中。<br>
                    方法：我们首先引入了一个可靠的银行来存储"有史以来最好的"输出作为伪真实值。然后，我们根据单调性属性进行实证分析，选择最可信的自然图像质量评估（NR-IQA）方法来评估输出的质量。此外，为了防止过度拟合错误标签，我们还引入了对比正则化。<br>
                    效果：我们在全参考和非参考水下基准测试上的实验结果表明，我们的算法在定量和定性上都明显优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable achievement of recent underwater image restoration techniques, the lack of labeled data has become a major hurdle for further progress. In this work, we propose a mean-teacher based Semi-supervised Underwater Image Restoration (Semi-UIR) framework to incorporate the unlabeled data into network training. However, the naive mean-teacher method suffers from two main problems: (1) The consistency loss used in training might become ineffective when the teacher's prediction is wrong. (2) Using L1 distance may cause the network to overfit wrong labels, resulting in confirmation bias. To address the above problems, we first introduce a reliable bank to store the "best-ever" outputs as pseudo ground truth. To assess the quality of outputs, we conduct an empirical analysis based on the monotonicity property to select the most trustworthy NR-IQA method. Besides, in view of the confirmation bias problem, we incorporate contrastive regularization to prevent the overfitting on wrong labels. Experimental results on both full-reference and non-reference underwater benchmarks demonstrate that our algorithm has obvious improvement over SOTA methods quantitatively and qualitatively. Code has been released at https://github.com/Huang-ShiRui/Semi-UIR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">548.EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization</span><br>
                <span class="as">Song, JunhaandLee, JungsooandKweon, InSoandChoi, Sungha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11920-11929.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在有限的内存下提高持续测试时适应（TTA）的效率。<br>
                    动机：TTA主要在边缘设备上进行，这些设备的内存有限，因此减少内存消耗至关重要，但以前的TTA研究忽视了这一点。此外，长期适应往往会导致灾难性遗忘和错误积累，阻碍了TTA在实际部署中的应用。<br>
                    方法：提出了两个组件来解决这个问题。首先，我们提出了轻量级的元网络，可以适应冻结的原始网络到目标领域。这种新的架构通过减少反向传播所需的中间激活的大小来最小化内存消耗。其次，我们的自我蒸馏正则化控制元网络的输出不会显著偏离冻结的原始网络的输出，从而保留源领域的训练良好的知识。没有额外的内存，这种正则化防止了错误积累和灾难性遗忘，即使在长期测试时也能保持稳定的性能。<br>
                    效果：我们在各种基准测试中展示了这种方法在图像分类和语义分割任务上优于其他最先进的方法。值得注意的是，我们使用ResNet-50和WideResNet-40的方法比最近最先进的CoTTA方法减少了86%和80%的内存消耗。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a simple yet effective approach that improves continual test-time adaptation (TTA) in a memory-efficient manner. TTA may primarily be conducted on edge devices with limited memory, so reducing memory is crucial but has been overlooked in previous TTA studies. In addition, long-term adaptation often leads to catastrophic forgetting and error accumulation, which hinders applying TTA in real-world deployments. Our approach consists of two components to address these issues. First, we present lightweight meta networks that can adapt the frozen original networks to the target domain. This novel architecture minimizes memory consumption by decreasing the size of intermediate activations required for backpropagation. Second, our novel self-distilled regularization controls the output of the meta networks not to deviate significantly from the output of the frozen original networks, thereby preserving well-trained knowledge from the source domain. Without additional memory, this regularization prevents error accumulation and catastrophic forgetting, resulting in stable performance even in long-term test-time adaptation. We demonstrate that our simple yet effective strategy outperforms other state-of-the-art methods on various benchmarks for image classification and semantic segmentation tasks. Notably, our proposed method with ResNet-50 and WideResNet-40 takes 86% and 80% less memory than the recent state-of-the-art method, CoTTA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">549.Unlearnable Clusters: Towards Label-Agnostic Unlearnable Examples</span><br>
                <span class="as">Zhang, JiamingandMa, XingjunandYi, QiandSang, JitaoandJiang, Yu-GangandWang, YaoweiandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Unlearnable_Clusters_Towards_Label-Agnostic_Unlearnable_Examples_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3984-3993.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地防止网络视觉隐私泄露。<br>
                    动机：现有的不可学习示例（UEs）生成方法都依赖于标签一致性的假设，但在实际中，攻击者可能会以与保护者完全不同的方式利用受保护的数据。<br>
                    方法：提出并推广一种更实用的标签无关设置，以及一种新的技术——不可学习簇（UCs），通过簇级扰动生成标签无关的不可学习示例。同时，利用像CLIP这样的视觉和语言预训练模型作为替代模型，提高所构建的UCs在不同领域的可转移性。<br>
                    效果：在各种设置下，包括不同的数据集、目标模型，甚至商业平台如微软Azure和百度PaddlePaddle等，实证验证了所提出方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called labelconsistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in this challenging setting. To tackle this challenge, we present a novel technique called Unlearnable Clusters (UCs) to generate label-agnostic unlearnable examples with cluster-wise perturbations. Furthermore, we propose to leverage Vision-and-Language Pretrained Models (VLPMs) like CLIP as the surrogate model to improve the transferability of the crafted UCs to diverse domains. We empirically verify the effectiveness of our proposed approach under a variety of settings with different datasets, target models, and even commercial platforms Microsoft Azure and Baidu PaddlePaddle. Code is available at https://github.com/jiamingzhang94/ Unlearnable-Clusters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">550.Rethinking Federated Learning With Domain Shift: A Prototype View</span><br>
                <span class="as">Huang, WenkeandYe, MangandShi, ZekunandLi, HeandDu, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16312-16322.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在数据来自不同领域时，通过联邦学习提高模型的泛化性能。<br>
                    动机：现有的联邦学习方法主要关注同一领域的私有数据，当分布式数据来自不同领域时，本地模型在其他领域会出现退化性能（即领域偏移）。<br>
                    方法：提出联邦原型学习（FPL）方法，构建集群原型和无偏原型，提供有价值的领域知识和公平的收敛目标。一方面，将样本嵌入拉向属于相同语义的集群原型，而不是不同类别的集群原型；另一方面，引入一致性正则化，使本地实例与各自的无偏原型对齐。<br>
                    效果：在Digits和Office Caltech任务上进行的实证结果表明，该方法有效，关键模块效率高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning shows a bright promise as a privacy-preserving collaborative learning technique. However, prevalent solutions mainly focus on all private data sampled from the same domain. An important challenge is that when distributed data are derived from diverse domains. The private model presents degenerative performance on other domains (with domain shift). Therefore, we expect that the global model optimized after the federated learning process stably provides generalizability performance on multiple domains. In this paper, we propose Federated Prototypes Learning (FPL) for federated learning under domain shift. The core idea is to construct cluster prototypes and unbiased prototypes, providing fruitful domain knowledge and a fair convergent target. On the one hand, we pull the sample embedding closer to cluster prototypes belonging to the same semantics than cluster prototypes from distinct classes. On the other hand, we introduce consistency regularization to align the local instance with the respective unbiased prototype. Empirical results on Digits and Office Caltech tasks demonstrate the effectiveness of the proposed solution and the efficiency of crucial modules.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">551.Augmentation Matters: A Simple-Yet-Effective Approach to Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Zhao, ZhenandYang, LiheandLong, SifanandPi, JiminandZhou, LupingandWang, Jingdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Augmentation_Matters_A_Simple-Yet-Effective_Approach_to_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11350-11359.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过数据扰动提高半监督语义分割（SSS）的性能。<br>
                    动机：尽管现有的最先进的方法在性能上表现出色，但它们往往设计复杂，引入更多的网络组件和额外的训练过程。<br>
                    方法：本文遵循标准的教师-学生框架，提出了AugSeg，一种简单而干净的方法，主要关注数据扰动以提升SSS性能。我们调整了各种数据增强方法，使其更好地适应半监督场景，而不是直接从监督学习中应用这些技术。<br>
                    效果：实验结果表明，我们的简单AugSeg在不使用任何花哨技巧的情况下，就能轻易地在不同的划分协议下实现SSS基准测试的新的最佳性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies on semi-supervised semantic segmentation (SSS) have seen fast progress. Despite their promising performance, current state-of-the-art methods tend to increasingly complex designs at the cost of introducing more network components and additional training procedures. Differently, in this work, we follow a standard teacher-student framework and propose AugSeg, a simple and clean approach that focuses mainly on data perturbations to boost the SSS performance. We argue that various data augmentations should be adjusted to better adapt to the semi-supervised scenarios instead of directly applying these techniques from supervised learning. Specifically, we adopt a simplified intensity-based augmentation that selects a random number of data transformations with uniformly sampling distortion strengths from a continuous space. Based on the estimated confidence of the model on different unlabeled samples, we also randomly inject labelled information to augment the unlabeled samples in an adaptive manner. Without bells and whistles, our simple AugSeg can readily achieve new state-of-the-art performance on SSS benchmarks under different partition protocols.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">552.Multiclass Confidence and Localization Calibration for Object Detection</span><br>
                <span class="as">Pathiraja, BimsaraandGunawardhana, MalithaandKhan, MuhammadHaris</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pathiraja_Multiclass_Confidence_and_Localization_Calibration_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19734-19743.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管深度神经网络在许多具有挑战性的计算机视觉问题上取得了高预测准确性，但它们往往过于自信，导致预测结果的校准效果不佳。<br>
                    动机：目前大多数改进深度神经网络校准的研究都仅限于分类任务，并且只针对同领域的预测进行校准。然而，对于占据视觉安全敏感和安全关键应用核心地位的目标检测方法的校准研究却鲜有涉及。<br>
                    方法：本文提出了一种新的训练时目标检测方法的校准技术，该技术能够通过利用预测不确定性来联合校准多类别置信度和框定位。<br>
                    效果：我们在多个同领域和异领域的检测基准上进行了广泛的实验。结果显示，我们提出的新型训练时校准方法在减少同领域和异领域预测的校准误差方面始终优于几个基线。我们的代码和模型可以在https://github.com/bimsarapathiraja/MCCL获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Albeit achieving high predictive accuracy across many challenging computer vision problems, recent studies suggest that deep neural networks (DNNs) tend to make overconfident predictions, rendering them poorly calibrated. Most of the existing attempts for improving DNN calibration are limited to classification tasks and restricted to calibrating in-domain predictions. Surprisingly, very little to no attempts have been made in studying the calibration of object detection methods, which occupy a pivotal space in vision-based security-sensitive, and safety-critical applications. In this paper, we propose a new train-time technique for calibrating modern object detection methods. It is capable of jointly calibrating multiclass confidence and box localization by leveraging their predictive uncertainties. We perform extensive experiments on several in-domain and out-of-domain detection benchmarks. Results demonstrate that our proposed train-time calibration method consistently outperforms several baselines in reducing calibration error for both in-domain and out-of-domain predictions. Our code and models are available at https://github.com/bimsarapathiraja/MCCL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">553.Leveraging Inter-Rater Agreement for Classification in the Presence of Noisy Labels</span><br>
                <span class="as">Bucarelli, MariaSofiaandCassano, LucasandSiciliano, FedericoandMantrach, AminandSilvestri, Fabrizio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bucarelli_Leveraging_Inter-Rater_Agreement_for_Classification_in_the_Presence_of_Noisy_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3439-3448.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用标注者之间的统计信息来估计标签所受的噪声分布，以及如何使用该噪声分布来从有噪声的数据集中学习。<br>
                    动机：在实际应用中，分类数据集是通过人工标注过程获得的，由于多个可能意见不合的标注者对同一样本的标注可能不同，因此标签可能存在噪声。<br>
                    方法：本文提出了利用标注者之间的统计信息来估计标签所受的噪声分布的方法，并介绍了使用该噪声分布来从有噪声的数据集中学习的方法。<br>
                    效果：通过实验验证了本文提出的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In practical settings, classification datasets are obtained through a labelling process that is usually done by humans. Labels can be noisy as they are obtained by aggregating the different individual labels assigned to the same sample by multiple, and possibly disagreeing, annotators. The inter-rater agreement on these datasets can be measured while the underlying noise distribution to which the labels are subject is assumed to be unknown. In this work, we: (i) show how to leverage the inter-annotator statistics to estimate the noise distribution to which labels are subject; (ii) introduce methods that use the estimate of the noise distribution to learn from the noisy dataset; and (iii) establish generalization bounds in the empirical risk minimization framework that depend on the estimated quantities. We conclude the paper by providing experiments that illustrate our findings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">554.Meta Compositional Referring Expression Segmentation</span><br>
                <span class="as">Xu, LiandHuang, MarkHeandShang, XindiandYuan, ZehuanandSun, YingandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Meta_Compositional_Referring_Expression_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19478-19487.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决参照表达式分割任务中，现有模型可能无法充分捕捉语义和视觉概念表示的问题。<br>
                    动机：尽管参照表达式分割任务取得了进展，但现有的模型在处理新的概念组合时，其泛化能力受到限制。<br>
                    方法：通过元学习的视角，提出了一种元成分参照表达式分割（MCRES）框架，以增强模型的组合泛化性能。该框架首先使用训练数据构建虚拟训练集和多个虚拟测试集，然后通过一种新的元优化方案，使模型在虚拟训练集上训练后能在虚拟测试集上获得良好的测试性能。<br>
                    效果：实验证明，该框架能有效提升模型对单个概念的语义和视觉表示的捕捉能力，从而在处理新的概念组合时获得稳健的泛化性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring expression segmentation aims to segment an object described by a language expression from an image. Despite the recent progress on this task, existing models tackling this task may not be able to fully capture semantics and visual representations of individual concepts, which limits their generalization capability, especially when handling novel compositions of learned concepts. In this work, through the lens of meta learning, we propose a Meta Compositional Referring Expression Segmentation (MCRES) framework to enhance model compositional generalization performance. Specifically, to handle various levels of novel compositions, our framework first uses training data to construct a virtual training set and multiple virtual testing sets, where data samples in each virtual testing set contain a level of novel compositions w.r.t. the support set. Then, following a novel meta optimization scheme to optimize the model to obtain good testing performance on the virtual testing sets after training on the virtual training set, our framework can effectively drive the model to better capture semantics and visual representations of individual concepts, and thus obtain robust generalization performance even when handling novel compositions. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">555.Continual Semantic Segmentation With Automatic Memory Sample Selection</span><br>
                <span class="as">Zhu, LanyunandChen, TianrunandYin, JianxiongandSee, SimonandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Continual_Semantic_Segmentation_With_Automatic_Memory_Sample_Selection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3082-3092.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决连续语义分割（CSS）中新类别引入导致的灾难性遗忘问题。<br>
                    动机：现有的方法在处理CSS中的新类别时，会随机或基于单一因素选择记忆样本进行重放，这无法保证最优效果。<br>
                    方法：本文提出了一种新的记忆样本选择机制，通过考虑样本多样性和类别性能等全面因素，自动选择有信息量的记忆样本进行有效重放。<br>
                    效果：在Pascal-VOC 2012和ADE 20K数据集上的大量实验表明，该方法的有效性，并在6阶段设置上以比第二名高出12.54%的性能达到了最先进的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual Semantic Segmentation (CSS) extends static semantic segmentation by incrementally introducing new classes for training. To alleviate the catastrophic forgetting issue in CSS, a memory buffer that stores a small number of samples from the previous classes is constructed for replay. However, existing methods select the memory samples either randomly or based on a single-factor-driven hand-crafted strategy, which has no guarantee to be optimal. In this work, we propose a novel memory sample selection mechanism that selects informative samples for effective replay in a fully automatic way by considering comprehensive factors including sample diversity and class performance. Our mechanism regards the selection operation as a decision-making process and learns an optimal selection policy that directly maximizes the validation performance on a reward set. To facilitate the selection decision, we design a novel state representation and a dual-stage action space. Our extensive experiments on Pascal-VOC 2012 and ADE 20K datasets demonstrate the effectiveness of our approach with state-of-the-art (SOTA) performance achieved, outperforming the second-place one by 12.54% for the 6-stage setting on Pascal-VOC 2012.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">556.Meta-Tuning Loss Functions and Data Augmentation for Few-Shot Object Detection</span><br>
                <span class="as">Demirel, BerkanandBaran, OrhunBu\u{g</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Demirel_Meta-Tuning_Loss_Functions_and_Data_Augmentation_for_Few-Shot_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7339-7349.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在少量训练实例下进行新物体检测类别建模的问题。<br>
                    动机：目前的少量学习技术和物体检测技术中，针对新物体类别的检测效果仍有待提高。<br>
                    方法：提出一种基于元学习的优化损失函数和增强策略的训练方案，通过这种方式来改进现有的少量学习技术。<br>
                    效果：实验结果表明，该方法在标准和广义的少量性能指标上均优于现有的基于微调的少量物体检测基线，并在Pascal VOC和MS-COCO数据集上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot object detection, the problem of modelling novel object detection categories with few training instances, is an emerging topic in the area of few-shot learning and object detection. Contemporary techniques can be divided into two groups: fine-tuning based and meta-learning based approaches. While meta-learning approaches aim to learn dedicated meta-models for mapping samples to novel class models, fine-tuning approaches tackle few-shot detection in a simpler manner, by adapting the detection model to novel classes through gradient based optimization. Despite their simplicity, fine-tuning based approaches typically yield competitive detection results. Based on this observation, we focus on the role of loss functions and augmentations as the force driving the fine-tuning process, and propose to tune their dynamics through meta-learning principles. The proposed training scheme, therefore, allows learning inductive biases that can boost few-shot detection, while keeping the advantages of fine-tuning based approaches. In addition, the proposed approach yields interpretable loss functions, as opposed to highly parametric and complex few-shot meta-models. The experimental results highlight the merits of the proposed scheme, with significant improvements over the strong fine-tuning based few-shot detection baselines on benchmark Pascal VOC and MS-COCO datasets, in terms of both standard and generalized few-shot performance metrics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">557.GCFAgg: Global and Cross-View Feature Aggregation for Multi-View Clustering</span><br>
                <span class="as">Yan, WeiqingandZhang, YuanyangandLv, ChenleiandTang, ChangandYue, GuanghuiandLiao, LiangandLin, Weisi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_GCFAgg_Global_and_Cross-View_Feature_Aggregation_for_Multi-View_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19863-19872.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-view clustering can partition data samples into their categories by learning a consensus representation in unsupervised way and has received more and more attention in recent years. However, most existing deep clustering methods learn consensus representation or view-specific representations from multiple views via view-wise aggregation way, where they ignore structure relationship of all samples. In this paper, we propose a novel multi-view clustering network to address these problems, called Global and Cross-view Feature Aggregation for Multi-View Clustering (GCFAggMVC). Specifically, the consensus data presentation from multiple views is obtained via cross-sample and cross-view feature aggregation, which fully explores the complementary of similar samples. Moreover, we align the consensus representation and the view-specific representation by the structure-guided contrastive learning module, which makes the view-specific representations from different samples with high structure relationship similar. The proposed module is a flexible multi-view data representation module, which can be also embedded to the incomplete multi-view data clustering task via plugging our module into other frameworks. Extensive experiments show that the proposed method achieves excellent performance in both complete multi-view data clustering tasks and incomplete multi-view data clustering tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">558.Class Balanced Adaptive Pseudo Labeling for Federated Semi-Supervised Learning</span><br>
                <span class="as">Li, MingandLi, QingliandWang, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Class_Balanced_Adaptive_Pseudo_Labeling_for_Federated_Semi-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16292-16301.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决联邦半监督学习中的问题，即少数客户端有完全标记的数据（标记客户端），而其他客户端的训练数据集是完全未标记的（未标记客户端）。<br>
                    动机：现有的方法试图处理非独立同分布数据设置带来的挑战。尽管已经提出了如子共识模型等方法，但它们通常在未标记的客户端上采用标准的伪标签或一致性正则化，这很容易受到不平衡的类别分布的影响。因此，联邦半监督学习的问题仍然有待解决。<br>
                    方法：我们提出了一种名为“Class Balanced Adaptive Pseudo Labeling”的方法，从伪标签的角度来研究联邦半监督学习。在CBAFed中，第一个关键元素是固定伪标签策略，用于处理灾难性遗忘问题；第二个关键元素是通过考虑本地客户端所有训练数据的实证分布来设计类平衡自适应阈值，以鼓励平衡的训练过程。为了使模型达到更好的优化状态，我们还提出了局部有监督训练和全局模型聚合之间的残差权重连接。<br>
                    效果：我们在五个数据集上进行了广泛的实验，证明了CBAFed的优越性。代码将发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper focuses on federated semi-supervised learning (FSSL), assuming that few clients have fully labeled data (labeled clients) and the training datasets in other clients are fully unlabeled (unlabeled clients). Existing methods attempt to deal with the challenges caused by not independent and identically distributed data (Non-IID) setting. Though methods such as sub-consensus models have been proposed, they usually adopt standard pseudo labeling or consistency regularization on unlabeled clients which can be easily influenced by imbalanced class distribution. Thus, problems in FSSL are still yet to be solved. To seek for a fundamental solution to this problem, we present Class Balanced Adaptive Pseudo Labeling (CBAFed), to study FSSL from the perspective of pseudo labeling. In CBAFed, the first key element is a fixed pseudo labeling strategy to handle the catastrophic forgetting problem, where we keep a fixed set by letting pass information of unlabeled data at the beginning of the unlabeled client training in each communication round. The second key element is that we design class balanced adaptive thresholds via considering the empirical distribution of all training data in local clients, to encourage a balanced training process. To make the model reach a better optimum, we further propose a residual weight connection in local supervised training and global model aggregation. Extensive experiments on five datasets demonstrate the superiority of CBAFed. Code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">559.Rethinking Out-of-Distribution (OOD) Detection: Masked Image Modeling Is All You Need</span><br>
                <span class="as">Li, JingyaoandChen, PengguangandHe, ZexinandYu, ShaozuoandLiu, ShuandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Rethinking_Out-of-Distribution_OOD_Detection_Masked_Image_Modeling_Is_All_You_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11578-11589.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高模型在分布外（OOD）检测中的性能。<br>
                    动机：现有的OOD检测方法主要基于识别，但这种方法往往学习的是捷径而非全面表示。<br>
                    方法：本文提出了一种基于重建的预训练任务，即掩蔽图像建模，用于OOD检测框架（MOOD）。<br>
                    效果：实验结果显示，MOOD在各种OOD检测任务上均取得了显著提升，包括单类OOD检测、多类OOD检测和近分布OOD检测，甚至击败了不包括任何OOD样本的10类异常暴露OOD检测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The core of out-of-distribution (OOD) detection is to learn the in-distribution (ID) representation, which is distinguishable from OOD samples. Previous work applied recognition-based methods to learn the ID features, which tend to learn shortcuts instead of comprehensive representations. In this work, we find surprisingly that simply using reconstruction-based methods could boost the performance of OOD detection significantly. We deeply explore the main contributors of OOD detection and find that reconstruction-based pretext tasks have the potential to provide a generally applicable and efficacious prior, which benefits the model in learning intrinsic data distributions of the ID dataset. Specifically, we take Masked Image Modeling as a pretext task for our OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by 3.0%, and near-distribution OOD detection by 2.1%. It even defeats the 10-shot-per-class outlier exposure OOD detection, although we do not include any OOD samples for our detection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">560.Multi Domain Learning for Motion Magnification</span><br>
                <span class="as">Singh, JasdeepandMurala, SubrahmanyamandKosuru, G.SankaraRaju</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Multi_Domain_Learning_for_Motion_Magnification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13914-13923.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地放大视频中的微小运动，如呼吸时的胸部微动和移动物体的细微振动，同时减少噪声、光照变化和大运动的影响。<br>
                    动机：现有的最先进方法主要依赖于手工设计的概念，虽然可以实现一定的放大效果，但会产生环状伪影等问题。基于深度学习的方法虽然可以提供更高的放大效果，但在一些场景中会出现严重的伪影。<br>
                    方法：我们提出了一种新的基于相位的深度网络进行视频运动放大，该网络在频率和空间两个域内操作，首先从频率域的相位波动生成运动放大，然后在空间域内改善其质量。所提出的模型是轻量级网络，参数较少（分别为0.11M和0.05M）。<br>
                    效果：我们将提出的方法与最先进的方法进行了比较，并在真实世界和合成视频上进行了评估。最后，我们还进行了消融研究，以显示网络不同部分的影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video motion magnification makes subtle invisible motions visible, such as small chest movements while breathing, subtle vibrations in the moving objects etc. But small motions are prone to noise, illumination changes, large motions, etc. making the task difficult. Most state-of-the-art methods use hand-crafted concepts which result in small magnification, ringing artifacts etc. The deep learning based approach has higher magnification but is prone to severe artifacts in some scenarios. We propose a new phase based deep network for video motion magnification that operates in both domains (frequency and spatial) to address this issue. It generates motion magnification from frequency domain phase fluctuations and then improves its quality in the spatial domain. The proposed models are lightweight networks with fewer parameters (  0.11M and   0.05M). Further, the proposed networks performance is compared to the SOTA approaches and evaluated on real-world and synthetic videos. Finally, an ablation study is also conducted to show the impact of different parts of the network.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">561.You Do Not Need Additional Priors or Regularizers in Retinex-Based Low-Light Image Enhancement</span><br>
                <span class="as">Fu, HuiyuanandZheng, WenkaiandMeng, XiangyuandWang, XinandWang, ChuanmingandMa, Huadong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_You_Do_Not_Need_Additional_Priors_or_Regularizers_in_Retinex-Based_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18125-18134.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善在低光条件下拍摄的图像质量。<br>
                    动机：现有的基于深度视网膜的方法需要将图像分解为反射和照明成分，这是一个病态难题，且没有可用的地面真值。<br>
                    方法：提出一种对比学习和自我知识蒸馏方法，训练模型进行视网膜分解，无需复杂的手工制作的正则化函数。<br>
                    效果：实验结果表明，该方法优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Images captured in low-light conditions often suffer from significant quality degradation. Recent works have built a large variety of deep Retinex-based networks to enhance low-light images. The Retinex-based methods require decomposing the image into reflectance and illumination components, which is a highly ill-posed problem and there is no available ground truth. Previous works addressed this problem by imposing some additional priors or regularizers. However, finding an effective prior or regularizer that can be applied in various scenes is challenging, and the performance of the model suffers from too many additional constraints. We propose a contrastive learning method and a self-knowledge distillation method that allow training our Retinex-based model for Retinex decomposition without elaborate hand-crafted regularization functions. Rather than estimating reflectance and illuminance images and representing the final images as their element-wise products as in previous works, our regularizer-free Retinex decomposition and synthesis network (RFR) extracts reflectance and illuminance features and synthesizes them end-to-end. In addition, we propose a loss function for contrastive learning and a progressive learning strategy for self-knowledge distillation. Extensive experimental results demonstrate that our proposed methods can achieve superior performance compared with state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">562.Re-Thinking Model Inversion Attacks Against Deep Neural Networks</span><br>
                <span class="as">Nguyen, Ngoc-BaoandChandrasegaran, KeshigeyanandAbdollahzadeh, MiladandCheung, Ngai-Man</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_Re-Thinking_Model_Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16384-16393.png><br>
            
            <span class="tt"><span class="t0">研究问题：模型倒置攻击（MI）旨在通过滥用对模型的访问来推断和重建私有训练数据，引发了对敏感信息泄露（如用于训练人脸识别系统的私人面部图像）的关注。<br>
                    动机：现有的所有最先进的MI算法存在两个基本问题，我们提出了解决方案，显著提高了所有最先进的MI的攻击性能。<br>
                    方法：我们重新审视了MI，研究了所有最先进的MI算法的两个基本问题，并提出了解决方案，显著提高了所有最先进的MI的攻击性能。特别是，我们的贡献有两个方面：1) 我们分析了最先进的MI算法的优化目标，认为该目标对于实现MI是次优的，并提出了改进的优化目标，显著提高了攻击性能。2) 我们分析了"MI过拟合"，表明它可能会阻止重构的图像学习训练数据的语义，并提出了一种新颖的"模型增强"思想来克服这个问题。<br>
                    效果：例如，在标准的CelebA基准测试中，我们的解决方案将准确率提高了11.8%，并首次实现了超过90%的攻击准确率。我们的发现表明，深度学习模型存在明显的敏感信息泄露风险。我们敦促人们认真考虑隐私影响。我们的代码、演示和模型可在https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze "MI overfitting", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel "model augmentation" idea to overcome this issue. Our proposed solutions are simple and improve all SOTA MI attack accuracy significantly. E.g., in the standard CelebA benchmark, our solutions improve accuracy by 11.8% and achieve for the first time over 90% attack accuracy. Our findings demonstrate that there is a clear risk of leaking sensitive information from deep learning models. We urge serious consideration to be given to the privacy implications. Our code, demo, and models are available at https://ngoc-nguyen-0.github.io/re-thinking_model_inversion_attacks/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">563.MetaMix: Towards Corruption-Robust Continual Learning With Temporally Self-Adaptive Data Transformation</span><br>
                <span class="as">Wang, ZhenyiandShen, LiandZhan, DonglinandSuo, QiulingandZhu, YanjunandDuan, TiehangandGao, Mingchen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MetaMix_Towards_Corruption-Robust_Continual_Learning_With_Temporally_Self-Adaptive_Data_Transformation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24521-24531.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何评估和提高持续学习模型的抗腐败能力，使其在安全关键场景中具有可信赖性和鲁棒性。<br>
                    动机：现有的持续学习模型对各种数据腐败非常脆弱，特别是在测试阶段。<br>
                    方法：提出一种元学习框架——自我适应的数据增强（MetaMix），通过自动转换新任务数据或记忆数据来处理持续学习中的腐败鲁棒性问题。<br>
                    效果：通过构建不同严重程度的持续学习腐败数据集，并在任务连续学习和类别连续学习上进行全面实验，证明该方法比现有的最佳基线更有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual Learning (CL) has achieved rapid progress in recent years. However, it is still largely unknown how to determine whether a CL model is trustworthy and how to foster its trustworthiness. This work focuses on evaluating and improving the robustness to corruptions of existing CL models. Our empirical evaluation results show that existing state-of-the-art (SOTA) CL models are particularly vulnerable to various data corruptions during testing. To make them trustworthy and robust to corruptions deployed in safety-critical scenarios, we propose a meta-learning framework of self-adaptive data augmentation to tackle the corruption robustness in CL. The proposed framework, MetaMix, learns to augment and mix data, automatically transforming the new task data or memory data. It directly optimizes the generalization performance against data corruptions during training. To evaluate the corruption robustness of our proposed approach, we construct several CL corruption datasets with different levels of severity. We perform comprehensive experiments on both task- and class-continual learning. Extensive experiments demonstrate the effectiveness of our proposed method compared to SOTA baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">564.DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks</span><br>
                <span class="as">Jain, SamyakandAddepalli, SravantiandSahu, PawanKumarandDey, PriyamandBabu, R.Venkatesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_DART_Diversify-Aggregate-Repeat_Training_Improves_Generalization_of_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16048-16059.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高神经网络在真实世界中的安全性部署？<br>
                    动机：常见的训练策略，如数据增强、集成和模型平均，有助于改善神经网络的泛化能力。<br>
                    方法：我们首先建立了一个简单但强大的泛化基准，利用训练小批量内的多样化增强，并发现这可以学习到更平衡的特征分布。然后，我们提出了“多样化-聚合-重复训练”（DART）策略，该策略首先使用不同的增强（或领域）训练多样化的模型以探索损失盆地，然后聚合它们的权重以结合它们的专业知识并获得改进的泛化能力。我们发现在整个训练过程中重复聚合步骤可以改善整体优化轨迹，并确保个体模型具有足够低的损失障碍以获得更好的组合泛化效果。<br>
                    效果：除了在域内泛化上的改进外，我们在流行的DomainBed框架中的领域泛化基准上展示了最先进的性能。我们的方法具有通用性，可以很容易地与几种基本训练算法集成以实现性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalization of Neural Networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have sufficiently low loss barrier to obtain improved generalization on combining them. We theoretically justify the proposed approach and show that it indeed generalizes better. In addition to improvements in In-Domain generalization, we demonstrate SOTA performance on the Domain Generalization benchmarks in the popular DomainBed framework as well. Our method is generic and can easily be integrated with several base training algorithms to achieve performance gains. Our code is available here: https://github.com/val-iisc/DART.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">565.Finding Geometric Models by Clustering in the Consensus Space</span><br>
                <span class="as">Barath, DanielandRozumnyi, DenysandEichhardt, IvanandHajder, LeventeandMatas, Jiri</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_Finding_Geometric_Models_by_Clustering_in_the_Consensus_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5414-5424.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的算法，用于寻找未知数量的几何模型，如单应性。<br>
                    动机：目前的问题是形式化为在不形成明确的点到模型分配的情况下逐步找到主导模型实例。<br>
                    方法：通过类似RANSAC的采样和由考虑先前提出的实例的模型质量函数驱动的整合过程来找到主导实例。新的实例是通过在共识空间中进行聚类找到的。这种新的方法导致了一个简单而高效的迭代算法，同时在实时运行多个视觉问题上具有最先进的准确性。<br>
                    效果：该算法比竞争对手在两个视图的运动估计上快至少两个数量级，且在多个应用中，使用多个几何模型可以提高准确性，包括从多个广义单应性进行姿态估计、快速移动物体的轨迹估计等。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a new algorithm for finding an unknown number of geometric models, e.g., homographies. The problem is formalized as finding dominant model instances progressively without forming crisp point-to-model assignments. Dominant instances are found via a RANSAC-like sampling and a consolidation process driven by a model quality function considering previously proposed instances. New ones are found by clustering in the consensus space. This new formulation leads to a simple iterative algorithm with state-of-the-art accuracy while running in real-time on a number of vision problems -- at least two orders of magnitude faster than the competitors on two-view motion estimation. Also, we propose a deterministic sampler reflecting the fact that real-world data tend to form spatially coherent structures. The sampler returns connected components in a progressively densified neighborhood-graph. We present a number of applications where the use of multiple geometric models improves accuracy. These include pose estimation from multiple generalized homographies; trajectory estimation of fast-moving objects; and we also propose a way of using multiple homographies in global SfM algorithms. Source code: https://github.com/danini/clustering-in-consensus-space.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">566.DaFKD: Domain-Aware Federated Knowledge Distillation</span><br>
                <span class="as">Wang, HaozhaoandLi, YichenandXu, WenchaoandLi, RuixuanandZhan, YufengandZeng, Zhigang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DaFKD_Domain-Aware_Federated_Knowledge_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20412-20421.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的联邦蒸馏方法在处理来自分布式客户端的统计异构数据时，通常对所有本地模型一视同仁，这导致聚合模型的性能下降。<br>
                    动机：为了解决现有联邦蒸馏方法中，由于忽视所有本地模型之间的多样性而导致的聚合模型性能下降的问题。<br>
                    方法：提出了一种新的领域知识感知的联邦蒸馏方法（DaFKD），该方法将每个客户端的本地数据视为特定的领域，并设计了一个领域鉴别器来识别每个模型对蒸馏样本的重要性，从而优化来自不同模型的软预测集合。<br>
                    效果：通过在各种数据集和设置上进行大量实验，发现与最先进的基线相比，该方法可以将模型精度提高多达6.02%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Distillation (FD) has recently attracted increasing attention for its efficiency in aggregating multiple diverse local models trained from statistically heterogeneous data of distributed clients. Existing FD methods generally treat these models equally by merely computing the average of their output soft predictions for some given input distillation sample, which does not take the diversity across all local models into account, thus leading to degraded performance of the aggregated model, especially when some local models learn little knowledge about the sample. In this paper, we propose a new perspective that treats the local data in each client as a specific domain and design a novel domain knowledge aware federated distillation method, dubbed DaFKD, that can discern the importance of each model to the distillation sample, and thus is able to optimize the ensemble of soft predictions from diverse models. Specifically, we employ a domain discriminator for each client, which is trained to identify the correlation factor between the sample and the corresponding domain. Then, to facilitate the training of the domain discriminator while saving communication costs, we propose sharing its partial parameters with the classification model. Extensive experiments on various datasets and settings show that the proposed method can improve the model accuracy by up to 6.02% compared to state-of-the-art baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">567.Spectral Bayesian Uncertainty for Image Super-Resolution</span><br>
                <span class="as">Liu, TaoandCheng, JunandTan, Shan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Spectral_Bayesian_Uncertainty_for_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18166-18175.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何量化图像超分辨率（SR）的重建不确定性？<br>
                    动机：现有的SR不确定性估计方法主要关注空间域的像素级不确定性，而与图像SR高度相关的频域SR不确定性却很少被探索。<br>
                    方法：提出了一种双域学习（DDL）框架，结合贝叶斯方法，能够准确估计频谱不确定性，从频域角度评估高频推理的可靠性。<br>
                    效果：在非理想条件下进行的大量实验表明，所提出的频谱不确定性有效。此外，还提出了一种新的基于频谱不确定性的解耦频率（SUDF）训练方案，用于感知SR。实验结果表明，所提出的SUDF可以显著提高SR结果的感知质量，而不牺牲太多的像素精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently deep learning techniques have significantly advanced image super-resolution (SR). Due to the black-box nature, quantifying reconstruction uncertainty is crucial when employing these deep SR networks. Previous approaches for SR uncertainty estimation mostly focus on capturing pixel-wise uncertainty in the spatial domain. SR uncertainty in the frequency domain which is highly related to image SR is seldom explored. In this paper, we propose to quantify spectral Bayesian uncertainty in image SR. To achieve this, a Dual-Domain Learning (DDL) framework is first proposed. Combined with Bayesian approaches, the DDL model is able to estimate spectral uncertainty accurately, enabling a reliability assessment for high frequencies reasoning from the frequency domain perspective. Extensive experiments under non-ideal premises are conducted and demonstrate the effectiveness of the proposed spectral uncertainty. Furthermore, we propose a novel Spectral Uncertainty based Decoupled Frequency (SUDF) training scheme for perceptual SR. Experimental results show the proposed SUDF can evidently boost perceptual quality of SR results without sacrificing much pixel accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">568.BiasBed - Rigorous Texture Bias Evaluation</span><br>
                <span class="as">Kalischek, NikolaiandDaudt, RodrigoCayeandPeters, TorbenandFurrer, ReinhardandWegner, JanD.andSchindler, Konrad</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kalischek_BiasBed_-_Rigorous_Texture_Bias_Evaluation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22221-22230.png><br>
            
            <span class="tt"><span class="t0">研究问题：现代卷积神经网络中普遍存在的纹理偏差导致了许多强调形状线索的算法，以支持对新领域的泛化。然而，常见的数据集、基准和通用模型选择策略缺失，也没有公认的严格评估协议。<br>
                    动机：本研究调查了在减少纹理偏差训练网络时遇到的困难和限制，并指出适当的评估和有意义的方法比较并非易事。<br>
                    方法：我们引入了BiasBed，这是一个用于纹理和风格偏差训练的测试平台，包括多个数据集和一系列现有算法。它附带了一个广泛的评估协议，包括严格的假设检验来衡量结果的重要性，尽管一些风格偏差方法的训练稳定性较差。<br>
                    效果：我们的大量实验揭示了对风格偏差（及更广泛领域）进行仔细、基于统计的评估协议的必要性。例如，我们发现文献中提出的一些算法并未显著减轻风格偏差的影响。通过发布BiasBed，我们希望促进对一致且有意义的比较的共同理解，从而加快学习无纹理偏差的方法的进展。代码可在https://github.com/D1noFuzi/BiasBed获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The well-documented presence of texture bias in modern convolutional neural networks has led to a plethora of algorithms that promote an emphasis on shape cues, often to support generalization to new domains. Yet, common datasets, benchmarks and general model selection strategies are missing, and there is no agreed, rigorous evaluation protocol. In this paper, we investigate difficulties and limitations when training networks with reduced texture bias. In particular, we also show that proper evaluation and meaningful comparisons between methods are not trivial. We introduce BiasBed, a testbed for texture- and style-biased training, including multiple datasets and a range of existing algorithms. It comes with an extensive evaluation protocol that includes rigorous hypothesis testing to gauge the significance of the results, despite the considerable training instability of some style bias methods. Our extensive experiments, shed new light on the need for careful, statistically founded evaluation protocols for style bias (and beyond). E.g., we find that some algorithms proposed in the literature do not significantly mitigate the impact of style bias at all. With the release of BiasBed, we hope to foster a common understanding of consistent and meaningful comparisons, and consequently faster progress towards learning methods free of texture bias. Code is available at https://github.com/D1noFuzi/BiasBed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">569.Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly Detection</span><br>
                <span class="as">Yao, XinchengandLi, RuoqiandZhang, JingandSun, JunandZhang, Chongyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Explicit_Boundary_Guided_Semi-Push-Pull_Contrastive_Learning_for_Supervised_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24490-24499.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数异常检测模型仅使用正常样本进行无监督学习，可能导致模糊的决策边界和不足的可分辨性。<br>
                    动机：在现实世界的应用中，异常样本通常很少，因此需要有效利用已知异常的有价值知识。然而，训练过程中只使用少量已知异常可能会使模型偏向这些已知异常，无法泛化到未见过的新异常。<br>
                    方法：提出一种新颖的明确边界引导半推拉对比学习机制，旨在检测已见和未见的异常。该机制包括两个核心设计：首先，找到一个明确且紧凑的分离边界作为进一步特征学习的指导；其次，开发一个边界引导的半推拉损失函数，以在特定区域将正常特征拉近、将异常特征推离分离边界。<br>
                    效果：实验结果表明，该方法能够形成更明确和可分辨的决策边界，从而更有效地区分已知和新未见的异常与正常样本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most anomaly detection (AD) models are learned using only normal samples in an unsupervised way, which may result in ambiguous decision boundary and insufficient discriminability. In fact, a few anomaly samples are often available in real-world applications, the valuable knowledge of known anomalies should also be effectively exploited. However, utilizing a few known anomalies during training may cause another issue that the model may be biased by those known anomalies and fail to generalize to unseen anomalies. In this paper, we tackle supervised anomaly detection, i.e., we learn AD models using a few available anomalies with the objective to detect both the seen and unseen anomalies. We propose a novel explicit boundary guided semi-push-pull contrastive learning mechanism, which can enhance model's discriminability while mitigating the bias issue. Our approach is based on two core designs: First, we find an explicit and compact separating boundary as the guidance for further feature learning. As the boundary only relies on the normal feature distribution, the bias problem caused by a few known anomalies can be alleviated. Second, a boundary guided semi-push-pull loss is developed to only pull the normal features together while pushing the abnormal features apart from the separating boundary beyond a certain margin region. In this way, our model can form a more explicit and discriminative decision boundary to distinguish known and also unseen anomalies from normal samples more effectively. Code will be available at https://github.com/xcyao00/BGAD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">570.Style Projected Clustering for Domain Generalized Semantic Segmentation</span><br>
                <span class="as">Huang, WeiandChen, ChangandLi, YongandLi, JiachengandLi, ChengandSong, FenglongandYan, YouliangandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Style_Projected_Clustering_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3061-3071.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的语义分割方法通过将各种图像规范化到一个标准的特征空间来提高泛化能力，但这不可避免地削弱了表示。<br>
                    动机：与现有方法不同，我们利用图像之间的差异来构建一个更好的表示空间，其中提取并存储独特的风格特征作为表示的基础。<br>
                    方法：我们将风格投影实现为存储基础的加权组合，其中相似性距离被用作权重因子。基于相同的概念，我们将此过程扩展到模型的决策部分，并促进语义预测的泛化。<br>
                    效果：综合实验表明，所提出的方法在未见过的情境上比现有技术有优势，平均而言mIoU提高了3.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing semantic segmentation methods improve generalization capability, by regularizing various images to a canonical feature space. While this process contributes to generalization, it weakens the representation inevitably. In contrast to existing methods, we instead utilize the difference between images to build a better representation space, where the distinct style features are extracted and stored as the bases of representation. Then, the generalization to unseen image styles is achieved by projecting features to this known space. Specifically, we realize the style projection as a weighted combination of stored bases, where the similarity distances are adopted as the weighting factors. Based on the same concept, we extend this process to the decision part of model and promote the generalization of semantic prediction. By measuring the similarity distances to semantic bases (i.e., prototypes), we replace the common deterministic prediction with semantic clustering. Comprehensive experiments demonstrate the advantage of proposed method to the state of the art, up to 3.6% mIoU improvement in average on unseen scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">571.DIP: Dual Incongruity Perceiving Network for Sarcasm Detection</span><br>
                <span class="as">Wen, ChangsongandJia, GuoliandYang, Jufeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_DIP_Dual_Incongruity_Perceiving_Network_for_Sarcasm_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2540-2550.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究多模态讽刺检测任务，考虑到图像和文本数据的普及性和互补性。<br>
                    动机：与其他多模态任务不同，讽刺数据在图像和文本之间存在内在不协调性，这在心理学理论中得到了证明。<br>
                    方法：我们提出了一个双不协调感知（DIP）网络，由两个分支挖掘讽刺信息，从事实和情感层面出发。对于事实方面，我们引入了通道重权策略以获取语义区分的嵌入，并利用高斯分布来模拟不协调性引起的不确定关联。对于情感方面，我们使用共享参数的孪生层来学习跨模态的情感信息。<br>
                    效果：大量实验表明，我们提出的方法优于最先进的方法。我们的代码已在GitHub上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sarcasm indicates the literal meaning is contrary to the real attitude. Considering the popularity and complementarity of image-text data, we investigate the task of multi-modal sarcasm detection. Different from other multi-modal tasks, for the sarcastic data, there exists intrinsic incongruity between a pair of image and text as demonstrated in psychological theories. To tackle this issue, we propose a Dual Incongruity Perceiving (DIP) network consisting of two branches to mine the sarcastic information from factual and affective levels. For the factual aspect, we introduce a channel-wise reweighting strategy to obtain semantically discriminative embeddings, and leverage gaussian distribution to model the uncertain correlation caused by the incongruity. The distribution is generated from the latest data stored in the memory bank, which can adaptively model the difference of semantic similarity between sarcastic and non-sarcastic data. For the affective aspect, we utilize siamese layers with shared parameters to learn cross-modal sentiment information. Furthermore, we use the polarity value to construct a relation graph for the mini-batch, which forms the continuous contrastive loss to acquire affective embeddings. Extensive experiments demonstrate that our proposed method performs favorably against state-of-the-art approaches. Our code is released on https://github.com/downdric/MSD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">572.PA\&amp;DA: Jointly Sampling Path and Data for Consistent NAS</span><br>
                <span class="as">Lu, ShunandHu, YuandYang, LongxingandSun, ZihaoandMei, JilinandTan, JianchaoandSong, Chengru</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_PADA_Jointly_Sampling_Path_and_Data_for_Consistent_NAS_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11940-11949.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的一阶段NAS方法在训练超网络时，由于共享权重导致梯度下降方向研究问题：现有的一阶段NAS方法在训练超网络时，由于共享权重导致梯度下降方向不同以及训练过程中大的梯度方差，使得超网络的排名一致性降低。<br>
                    动机：为了解决上述问题，提出了一种通过优化PAth和DAta（PA&DA）的采样分布来显式减小超网络训练中梯度方差的方法。<br>
                    方法：理论推导出梯度方差与采样分布的关系，并发现最优采样概率与路径和训练数据的归一化梯度范数成正比。因此，使用归一化梯度范数作为路径和数据的重要性指标，并在超网络训练中采用重要性采样策略。<br>
                    效果：与其他改进方法相比，该方法在各种搜索空间中都表现出更可靠的排名性能和更高的搜索架构准确性，证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Based on the weight-sharing mechanism, one-shot NAS methods train a supernet and then inherit the pre-trained weights to evaluate sub-models, largely reducing the search cost. However, several works have pointed out that the shared weights suffer from different gradient descent directions during training. And we further find that large gradient variance occurs during supernet training, which degrades the supernet ranking consistency. To mitigate this issue, we propose to explicitly minimize the gradient variance of the supernet training by jointly optimizing the sampling distributions of PAth and DAta (PA&DA). We theoretically derive the relationship between the gradient variance and the sampling distributions, and reveal that the optimal sampling probability is proportional to the normalized gradient norm of path and training data. Hence, we use the normalized gradient norm as the importance indicator for path and training data, and adopt an importance sampling strategy for the supernet training. Our method only requires negligible computation cost for optimizing the sampling distributions of path and data, but achieves lower gradient variance during supernet training and better generalization performance for the supernet, resulting in a more consistent NAS. We conduct comprehensive comparisons with other improved approaches in various search spaces. Results show that our method surpasses others with more reliable ranking performance and higher accuracy of searched architectures, showing the effectiveness of our method. Code is available at https://github.com/ShunLu91/PA-DA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">573.Bias Mimicking: A Simple Sampling Approach for Bias Mitigation</span><br>
                <span class="as">Qraitem, MaanandSaenko, KateandPlummer, BryanA.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qraitem_Bias_Mimicking_A_Simple_Sampling_Approach_for_Bias_Mitigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20311-20320.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉识别数据集在类别标签中经常对偏见群体（如女性）进行少报，这可能导致模型学习到年龄、性别或种族等类别标签和偏见群体之间的虚假关联。<br>
                    动机：目前的解决此问题的方法需要重大的架构改变或额外的损失函数，这需要更多的超参数调整。而数据采样基线方法虽然简单且无需超参数，但存在明显的缺点。<br>
                    方法：提出一种新的类别条件采样方法——偏见模仿（Bias Mimicking）。该方法基于一个观察结果，即如果模仿每个类别c' != c的类别c的偏见分布，那么Y和B在统计上是独立的。通过这种新训练过程，BM确保模型在每个时期都接触到完整的分布，而不重复样本。<br>
                    效果：实验结果表明，BM在四个基准测试中将少报群体的采样方法的准确性提高了3%，同时保持甚至在某些情况下超过了非采样方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prior work has shown that Visual Recognition datasets frequently underrepresent bias groups B (e.g. Female) within class labels Y (e.g. Programmers). This dataset bias can lead to models that learn spurious correlations between class labels and bias groups such as age, gender, or race. Most recent methods that address this problem require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Alternatively, data sampling baselines from the class imbalance literature (eg Undersampling, Upweighting), which can often be implemented in a single line of code and often have no hyperparameters, offer a cheaper and more efficient solution. However, these methods suffer from significant shortcomings. For example, Undersampling drops a significant part of the input distribution per epoch while Oversampling repeats samples, causing overfitting. To address these shortcomings, we introduce a new class-conditioned sampling method: Bias Mimicking. The method is based on the observation that if a class c bias distribution, i.e., P_D(B|Y=c) is mimicked across every c' != c, then Y and B are statistically independent. Using this notion, BM, through a novel training procedure, ensures that the model is exposed to the entire distribution per epoch without repeating samples. Consequently, Bias Mimicking improves underrepresented groups' accuracy of sampling methods by 3% over four benchmarks while maintaining and sometimes improving performance over nonsampling methods. Code: https://github.com/mqraitem/Bias-Mimicking</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">574.Efficient Loss Function by Minimizing the Detrimental Effect of Floating-Point Errors on Gradient-Based Attacks</span><br>
                <span class="as">Yu, YunruiandXu, Cheng-Zhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Efficient_Loss_Function_by_Minimizing_the_Detrimental_Effect_of_Floating-Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4056-4066.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习网络易受攻击者通过在输入数据中添加人类无法察觉的扰动来欺骗，这揭示了当前深度神经网络的脆弱性和弱鲁棒性。<br>
                    动机：许多攻击技术已被提出以评估模型的鲁棒性，但基于梯度的攻击由于严重高估了鲁棒性而失败。本文发现，浮点误差（包括浮点下溢和舍入错误）导致的计算梯度中的相对误差是导致基于梯度的攻击无法准确评估模型鲁棒性的根本原因。<br>
                    方法：虽然很难消除梯度中的相对误差，但我们可以通过控制其对基于梯度的攻击的影响来应对这个问题。因此，我们提出了一种有效的损失函数，通过最小化浮点误差对攻击的不利影响。<br>
                    效果：实验结果表明，当检查广泛的防御机制时，它比其他损失函数更有效、更可靠。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Attackers can deceive neural networks by adding human imperceptive perturbations to their input data; this reveals the vulnerability and weak robustness of current deep-learning networks. Many attack techniques have been proposed to evaluate the model's robustness. Gradient-based attacks suffer from severely overestimating the robustness. This paper identifies that the relative error in calculated gradients caused by floating-point errors, including floating-point underflow and rounding errors, is a fundamental reason why gradient-based attacks fail to accurately assess the model's robustness. Although it is hard to eliminate the relative error in the gradients, we can control its effect on the gradient-based attacks. Correspondingly, we propose an efficient loss function by minimizing the detrimental impact of the floating-point errors on the attacks. Experimental results show that it is more efficient and reliable than other loss functions when examined across a wide range of defence mechanisms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">575.Revisiting Prototypical Network for Cross Domain Few-Shot Learning</span><br>
                <span class="as">Zhou, FeiandWang, PengandZhang, LeiandWei, WeiandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Revisiting_Prototypical_Network_for_Cross_Domain_Few-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20061-20070.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决神经网络在面对新的领域时，性能显著下降的问题。<br>
                    动机：这个问题源于神经网络的简单性偏见陷阱，网络倾向于关注一些特定的快捷特征，如颜色、形状等，这些特征只能区分少数类别，无法跨领域进行泛化。<br>
                    方法：提出局部-全局蒸馏原型网络（LDP-net），通过建立两个分支对查询图像及其随机局部裁剪进行分类，然后在这两个分支之间进行知识蒸馏，强制他们的类别归属一致性。<br>
                    效果：实验结果证明，这种方法能有效提高模型的泛化性能，并在八个跨领域的少样本分类基准测试中取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prototypical Network is a popular few-shot solver that aims at establishing a feature metric generalizable to novel few-shot classification (FSC) tasks using deep neural networks. However, its performance drops dramatically when generalizing to the FSC tasks in new domains. In this study, we revisit this problem and argue that the devil lies in the simplicity bias pitfall in neural networks. In specific, the network tends to focus on some biased shortcut features (e.g., color, shape, etc.) that are exclusively sufficient to distinguish very few classes in the meta-training tasks within a pre-defined domain, but fail to generalize across domains as some desirable semantic features. To mitigate this problem, we propose a Local-global Distillation Prototypical Network (LDP-net). Different from the standard Prototypical Network, we establish a two-branch network to classify the query image and its random local crops, respectively. Then, knowledge distillation is conducted among these two branches to enforce their class affiliation consistency. The rationale behind is that since such global-local semantic relationship is expected to hold regardless of data domains, the local-global distillation is beneficial to exploit some cross-domain transferable semantic features for feature metric establishment. Moreover, such local-global semantic consistency is further enforced among different images of the same class to reduce the intra-class semantic variation of the resultant feature. In addition, we propose to update the local branch as Exponential Moving Average (EMA) over training episodes, which makes it possible to better distill cross-episode knowledge and further enhance the generalization performance. Experiments on eight cross-domain FSC benchmarks empirically clarify our argument and show the state-of-the-art results of LDP-net. Code is available in https://github.com/NWPUZhoufei/LDP-Net</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">576.Perception and Semantic Aware Regularization for Sequential Confidence Calibration</span><br>
                <span class="as">Peng, ZhenghuaandLuo, YuandChen, TianshuiandXu, KekeandHuang, Shuangping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Perception_and_Semantic_Aware_Regularization_for_Sequential_Confidence_Calibration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10658-10668.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度序列识别（DSR）模型在各种应用中受到越来越多的关注，但大多数研究问题：深度序列识别（DSR）模型在各种应用中受到越来越多的关注，但大多数模型只使用目标序列作为监督，没有考虑其他相关序列，导致预测过于自信。<br>
                    动机：目前的DSR模型通过等同且独立地平滑每个标记来规范标签，从而减轻过度自信的问题。然而，它们并没有考虑到标记/序列的相关性，这可能提供更有效的信息来进行训练规范化，从而导致次优的性能。<br>
                    方法：我们提出了一个感知和语义感知的序列规范化框架，该框架探索与目标序列具有高度感知和语义相关性的标记/序列进行规范化。具体来说，我们引入了一个语义上下文自由识别和一个语言模型来获取具有高度感知相似性和语义相关性的相似序列。此外，由于不同样本的难度不同，过度自信的程度也会有所不同。因此，我们进一步设计了一个自适应校准强度模块来计算每个样本的难度分数，以获得更精细的规范化。<br>
                    效果：我们在场景文本和语音识别等典型的序列识别任务上进行了广泛的实验，结果表明我们的方法取得了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep sequence recognition (DSR) models receive increasing attention due to their superior application to various applications. Most DSR models use merely the target sequences as supervision without considering other related sequences, leading to over-confidence in their predictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more effective information to regularize training and thus lead to sub-optimal performance. In this work, we find tokens/sequences with high perception and semantic correlations with the target ones contain more correlated and effective information and thus facilitate more effective regularization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens/sequences as regularization. Specifically, we introduce a semantic context-free recognition and a language model to acquire similar sequences with high perceptive similarities and semantic correlation, respectively. Moreover, over-confidence degree varies across samples according to their difficulties. Thus, we further design an adaptive calibration intensity module to compute a difficulty score for each samples to obtain finer-grained regularization. Extensive experiments on canonical sequence recognition tasks, including scene text and speech recognition, demonstrate that our method sets novel state-of-the-art results. Code is available at https://github.com/husterpzh/PSSR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">577.A Practical Upper Bound for the Worst-Case Attribution Deviations</span><br>
                <span class="as">Wang, FanandKong, AdamsWai-Kin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_A_Practical_Upper_Bound_for_the_Worst-Case_Attribution_Deviations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24616-24625.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习模型解释性方法易受攻击，生成具有显著不同解释但分类结果相同的图像。<br>
                    动机：为了提高模型对此类攻击的鲁棒性，需要量化解释的最大差异。<br>
                    方法：通过约束优化问题，提出一种基于欧几里得距离和余弦相似性的上限计算方法，以衡量在特定区域内添加任何噪声时，分类结果保持不变的情况下，解释的最大差异。<br>
                    效果：实验验证了所提出的上限在各种数据集和两种不同类型的攻击（PGD攻击和IFIA属性攻击）上的效果。超过1000万次的攻击表明，所提出的最大上限能有效量化基于最坏情况解释差异的模型的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model attribution is a critical component of deep neural networks (DNNs) for its interpretability to complex models. Recent studies bring up attention to the security of attribution methods as they are vulnerable to attribution attacks that generate similar images with dramatically different attributions. Existing works have been investigating empirically improving the robustness of DNNs against those attacks; however, none of them explicitly quantifies the actual deviations of attributions. In this work, for the first time, a constrained optimization problem is formulated to derive an upper bound that measures the largest dissimilarity of attributions after the samples are perturbed by any noises within a certain region while the classification results remain the same. Based on the formulation, different practical approaches are introduced to bound the attributions above using Euclidean distance and cosine similarity under both L2 and Linf-norm perturbations constraints. The bounds developed by our theoretical study are validated on various datasets and two different types of attacks (PGD attack and IFIA attribution attack). Over 10 million attacks in the experiments indicate that the proposed upper bounds effectively quantify the robustness of models based on the worst-case attribution dissimilarities.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">578.Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification</span><br>
                <span class="as">Xie, MengyaoandHan, ZongboandZhang, ChangqingandBai, YichenandHu, Qinghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Exploring_and_Exploiting_Uncertainty_for_Incomplete_Multi-View_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19873-19882.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对不完整的多视图数据进行分类。<br>
                    动机：在现实应用中，任意视图的缺失是普遍存在的，而现有的不完整多视图方法由于缺失视图的高不确定性特性，难以获得可信的预测结果。<br>
                    方法：提出了一种不确定性诱导的不完整多视图数据分类（UIMC）模型，通过构建分布并多次采样来描述缺失视图的不确定性，并根据采样质量自适应地利用这些不确定性。具体来说，我们将每个缺失的数据建模为一个条件分布，以引入不确定性，然后采用基于证据的融合策略来保证融合后的视图的可信性。<br>
                    效果：在多个基准数据集上进行的大量实验表明，该方法在性能和可信度方面均达到了最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Classifying incomplete multi-view data is inevitable since arbitrary view missing widely exists in real-world applications. Although great progress has been achieved, existing incomplete multi-view methods are still difficult to obtain a trustworthy prediction due to the relatively high uncertainty nature of missing views. First, the missing view is of high uncertainty, and thus it is not reasonable to provide a single deterministic imputation. Second, the quality of the imputed data itself is of high uncertainty. To explore and exploit the uncertainty, we propose an Uncertainty-induced Incomplete Multi-View Data Classification (UIMC) model to classify the incomplete multi-view data under a stable and reliable framework. We construct a distribution and sample multiple times to characterize the uncertainty of missing views, and adaptively utilize them according to the sampling quality. Accordingly, the proposed method realizes more perceivable imputation and controllable fusion. Specifically, we model each missing data with a distribution conditioning on the available views and thus introducing uncertainty. Then an evidence-based fusion strategy is employed to guarantee the trustworthy integration of the imputed views. Extensive experiments are conducted on multiple benchmark data sets and our method establishes a state-of-the-art performance in terms of both performance and trustworthiness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">579.Learning Transformations To Reduce the Geometric Shift in Object Detection</span><br>
                <span class="as">Vidit, ViditandEngilberge, MartinandSalzmann, Mathieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_Learning_Transformations_To_Reduce_the_Geometric_Shift_in_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17441-17450.png><br>
            
            <span class="tt"><span class="t0">研究问题：现代物体检测器在测试分布与训练分布不同的情况下性能下降。<br>
                    动机：大多数解决此问题的方法都集中在由不同光照条件或合成图像和真实图像之间的间隙引起的对象外观变化上，而本文则通过对比处理由于图像捕获过程中的变化或环境限制导致的内容本身的外观几何差异而产生的几何移位。<br>
                    方法：引入了一种自我训练方法，该方法学习一组几何变换以最小化这些移位，而不利用新领域中的任何标记数据，也不涉及任何有关相机的信息。<br>
                    效果：我们在两个不同的移位（即相机的视场（FoV）变化和视角变化）上评估了我们的方法，结果表明，学习几何变换有助于检测器在目标领域中表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The performance of modern object detectors drops when the test distribution differs from the training one. Most of the methods that address this focus on object appearance changes caused by, e.g., different illumination conditions, or gaps between synthetic and real images. Here, by contrast, we tackle geometric shifts emerging from variations in the image capture process, or due to the constraints of the environment causing differences in the apparent geometry of the content itself. We introduce a self-training approach that learns a set of geometric transformations to minimize these shifts without leveraging any labeled data in the new domain, nor any information about the cameras. We evaluate our method on two different shifts, i.e., a camera's field of view (FoV) change and a viewpoint change. Our results evidence that learning geometric transformations helps detectors to perform better in the target domains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">580.Revisiting Rotation Averaging: Uncertainties and Robust Losses</span><br>
                <span class="as">Zhang, GanlinandLarsson, ViktorandBarath, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_Rotation_Averaging_Uncertainties_and_Robust_Losses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17215-17224.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文重新审视了全局结构从运动（SfM）管道中应用的旋转平均问题。<br>
                    动机：当前方法的主要问题是，其最小化的成本函数仅通过估计的极几何与输入数据弱相关联。<br>
                    方法：我们提出通过直接将点对应关系的不确定性传播到旋转平均中来更好地模拟底层噪声分布。这种不确定性通过考虑两视图细化的雅可比矩阵而获得。此外，我们还探索将MAGSAC损失的一个变体整合到旋转平均问题中，而不是使用当前框架中使用的经典鲁棒损失。<br>
                    效果：所提出的方法在大型公共基准测试上的结果优于基线，无论是在准确性方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we revisit the rotation averaging problem applied in global Structure-from-Motion pipelines. We argue that the main problem of current methods is the minimized cost function that is only weakly connected with the input data via the estimated epipolar geometries. We propose to better model the underlying noise distributions by directly propagating the uncertainty from the point correspondences into the rotation averaging. Such uncertainties are obtained for free by considering the Jacobians of two-view refinements. Moreover, we explore integrating a variant of the MAGSAC loss into the rotation averaging problem, instead of using classical robust losses employed in current frameworks. The proposed method leads to results superior to baselines, in terms of accuracy, on large-scale public benchmarks. The code is public. https://github.com/zhangganlin/GlobalSfMpy</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">581.Model Barrier: A Compact Un-Transferable Isolation Domain for Model Intellectual Property Protection</span><br>
                <span class="as">Wang, LianyuandWang, MengandZhang, DaoqiangandFu, Huazhu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Model_Barrier_A_Compact_Un-Transferable_Isolation_Domain_for_Model_Intellectual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20475-20484.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效保护预训练模型的知识产权，防止其在未经授权的领域被使用。<br>
                    动机：为了激发模型所有者和创建者的积极性，需要对由人类智力劳动和计算成本产生的科技成果进行模型知识产权保护。<br>
                    方法：提出了一种新的紧凑型不可转移隔离领域（CUTI-domain），作为阻止模型从授权领域非法转移到未授权领域的障碍。<br>
                    效果：在四个数字数据集、CIFAR10 & STL10和VisDA-2017数据集上的全面实验结果表明，我们的CUTI-domain可以很容易地与不同的骨干网络一起实施为即插即用模块，并为模型IP保护提供了一个有效的解决方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As the scientific and technological achievements produced by human intellectual labor and computation cost, model intellectual property (IP) protection, which refers to preventing the usage of the well-trained model on an unauthorized domain, deserves further attention, so as to effectively mobilize the enthusiasm of model owners and creators. To this end, we propose a novel compact un-transferable isolation domain (CUTI-domain), which acts as a model barrier to block illegal transferring from the authorized domain to the unauthorized domain. Specifically, CUTI-domain is investigated to block cross-domain transferring by highlighting private style features of the authorized domain and lead to the failure of recognition on unauthorized domains that contain irrelative private style features. Furthermore, depending on whether the unauthorized domain is known or not, two solutions of using CUTI-domain are provided: target-specified CUTI-domain and target-free CUTI-domain. Comprehensive experimental results on four digit datasets, CIFAR10 & STL10, and VisDA-2017 dataset, demonstrate that our CUTI-domain can be easily implemented with different backbones as a plug-and-play module and provides an efficient solution for model IP protection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">582.Bootstrap Your Own Prior: Towards Distribution-Agnostic Novel Class Discovery</span><br>
                <span class="as">Yang, MuliandWang, LianchengandDeng, ChengandZhang, Hanwang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Bootstrap_Your_Own_Prior_Towards_Distribution-Agnostic_Novel_Class_Discovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3459-3468.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无标注情况下未知类别的发现，通过利用已知类别的转移知识。<br>
                    动机：现有的方法假设未知类别分布是均匀的，忽视了真实世界数据的不平衡性。<br>
                    方法：提出一种新的挑战性任务——分布不可知的NCD，允许数据来自任意未知类别分布，并提出了一种新的方法“Bootstrapping Your Own Prior（BYOP）”，通过迭代估计类先验。<br>
                    效果：实验表明，现有方法在不平衡的类别分布下表现不佳，而BYOP则通过鼓励对信心不足的样本进行更尖锐的预测，从而获得更准确的伪标签，并在各种分布场景下表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel Class Discovery (NCD) aims to discover unknown classes without any annotation, by exploiting the transferable knowledge already learned from a base set of known classes. Existing works hold an impractical assumption that the novel class distribution prior is uniform, yet neglect the imbalanced nature of real-world data. In this paper, we relax this assumption by proposing a new challenging task: distribution-agnostic NCD, which allows data drawn from arbitrary unknown class distributions and thus renders existing methods useless or even harmful. We tackle this challenge by proposing a new method, dubbed "Bootstrapping Your Own Prior (BYOP)", which iteratively estimates the class prior based on the model prediction itself. At each iteration, we devise a dynamic temperature technique that better estimates the class prior by encouraging sharper predictions for less-confident samples. Thus, BYOP obtains more accurate pseudo-labels for the novel samples, which are beneficial for the next training iteration. Extensive experiments show that existing methods suffer from imbalanced class distributions, while BYOP outperforms them by clear margins, demonstrating its effectiveness across various distribution scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">583.MOT: Masked Optimal Transport for Partial Domain Adaptation</span><br>
                <span class="as">Luo, You-WeiandRen, Chuan-Xian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_MOT_Masked_Optimal_Transport_for_Partial_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3531-3540.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更有效地在现实世界的场景中应用最优传输（OT）模型，特别是在部分领域适应等具有挑战性的情况下。<br>
                    动机：现有的OT模型在实际应用中存在严格的先验假设和隐含对齐的问题，可能导致学习到的传输计划有偏差，并可能产生负面转移。<br>
                    方法：提出了一种新的条件分布匹配和标签移位校正的严格OT建模方法，即掩蔽OT（MOT）方法。通过定义带有标签信息的掩蔽操作来改进。<br>
                    效果：理论证明条件OT与MOT等价，表明定义明确的MOT可以作为计算友好的代理。大量实验验证了理论结果和提出的模型的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As an important methodology to measure distribution discrepancy, optimal transport (OT) has been successfully applied to learn generalizable visual models under changing environments. However, there are still limitations, including strict prior assumption and implicit alignment, for current OT modeling in challenging real-world scenarios like partial domain adaptation, where the learned transport plan may be biased and negative transfer is inevitable. Thus, it is necessary to explore a more feasible OT methodology for real-world applications. In this work, we focus on the rigorous OT modeling for conditional distribution matching and label shift correction. A novel masked OT (MOT) methodology on conditional distributions is proposed by defining a mask operation with label information. Further, a relaxed and reweighting formulation is proposed to improve the robustness of OT in extreme scenarios. We prove the theoretical equivalence between conditional OT and MOT, which implies the well-defined MOT serves as a computation-friendly proxy. Extensive experiments validate the effectiveness of theoretical results and proposed model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">584.Adaptive Sparse Pairwise Loss for Object Re-Identification</span><br>
                <span class="as">Zhou, XiaoandZhong, YujieandCheng, ZhenandLiang, FanandMa, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Adaptive_Sparse_Pairwise_Loss_for_Object_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19691-19701.png><br>
            
            <span class="tt"><span class="t0">研究问题：对象重识别（ReID）旨在从大量图库中找到与给定探针相同身份的实例。<br>
                    动机：现有的对重识别网络的训练中，成对损失起着重要的作用。现有的成对损失密集地将每个实例作为锚点，并在一个小批次中采样其三元组。这种密集的采样机制不可避免地引入了共享少量视觉相似性的正样本对，这可能对训练有害。<br>
                    方法：我们提出了一种名为稀疏成对（SP）损失的新的损失范式，该范式仅在一个小批次中为每个类别利用少量的适当对，并经验性地证明这对ReID任务是足够的。基于提出的损失框架，我们提出了一种自适应的正样本挖掘策略，该策略可以动态适应不同的类内变化。<br>
                    效果：实验表明，SP损失及其自适应变体AdaSP损失优于其他成对损失，并在几个ReID基准测试中实现了最先进的性能。代码可在https://github.com/Astaxanthin/AdaSP获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object re-identification (ReID) aims to find instances with the same identity as the given probe from a large gallery. Pairwise losses play an important role in training a strong ReID network. Existing pairwise losses densely exploit each instance as an anchor and sample its triplets in a mini-batch. This dense sampling mechanism inevitably introduces positive pairs that share few visual similarities, which can be harmful to the training. To address this problem, we propose a novel loss paradigm termed Sparse Pairwise (SP) loss that only leverages few appropriate pairs for each class in a mini-batch, and empirically demonstrate that it is sufficient for the ReID tasks. Based on the proposed loss framework, we propose an adaptive positive mining strategy that can dynamically adapt to diverse intra-class variations. Extensive experiments show that SP loss and its adaptive variant AdaSP loss outperform other pairwise losses, and achieve state-of-the-art performance across several ReID benchmarks. Code is available at https://github.com/Astaxanthin/AdaSP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">585.Progressive Open Space Expansion for Open-Set Model Attribution</span><br>
                <span class="as">Yang, TianyunandWang, DandingandTang, FanandZhao, XinyingandCao, JuanandTang, Sheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Progressive_Open_Space_Expansion_for_Open-Set_Model_Attribution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15856-15865.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管在生成技术上取得了显著进步，但知识产权保护和恶意内容监管的双重问题已经出现。<br>
                    动机：目前的研究主要通过将合成图像归因于一组可能的源模型来管理合成图像，但这种封闭的分类设置限制了在处理由任意模型生成的内容方面的实际应用。<br>
                    方法：本研究专注于一项具有挑战性的任务，即开放集模型归属（OSMA），以同时将图像归属于已知模型并识别出那些来自未知模型的图像。<br>
                    效果：与现有的关注语义新颖性的开放集识别（OSR）任务相比，OSMA更具挑战性，因为来自已知和未知模型的图像之间的差异可能仅在于视觉上难以察觉的痕迹。为此，我们提出了一种渐进式开放空间扩展（POSE）解决方案，该方案模拟了保持与封闭集样本相同语义但嵌入有不同难以察觉痕迹的开放集样本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable progress in generative technology, the Janus-faced issues of intellectual property protection and malicious content supervision have arisen. Efforts have been paid to manage synthetic images by attributing them to a set of potential source models. However, the closed-set classification setting limits the application in real-world scenarios for handling contents generated by arbitrary models. In this study, we focus on a challenging task, namely Open-Set Model Attribution (OSMA), to simultaneously attribute images to known models and identify those from unknown ones. Compared to existing open-set recognition (OSR) tasks focusing on semantic novelty, OSMA is more challenging as the distinction between images from known and unknown models may only lie in visually imperceptible traces. To this end, we propose a Progressive Open Space Expansion (POSE) solution, which simulates open-set samples that maintain the same semantics as closed-set samples but embedded with different imperceptible traces. Guided by a diversity constraint, the open space is simulated progressively by a set of lightweight augmentation models. We consider three real-world scenarios and construct an OSMA benchmark dataset, including unknown models trained with different random seeds, architectures, and datasets from known ones. Extensive experiments on the dataset demonstrate POSE is superior to both existing model attribution methods and off-the-shelf OSR methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">586.Improving Generalization With Domain Convex Game</span><br>
                <span class="as">Lv, FangruiandLiang, JianandLi, ShuangandZhang, JinmingandLiu, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lv_Improving_Generalization_With_Domain_Convex_Game_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24315-24324.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习网络在面对不同源领域时，泛化能力差的问题。<br>
                    动机：虽然人们普遍认为通过增加源领域的多样性可以提高模型的泛化能力，但这种观点缺乏数学理论的支持。<br>
                    方法：作者提出了一种新的视角，将领域泛化视为领域之间的凸博弈。设计了一个基于超模态的正则化项来鼓励每个多样化的领域提高模型的泛化能力，同时构建了一个样本过滤器来消除低质量的样本。<br>
                    效果：通过形式分析、启发式分析和大量实验，证明了该框架的合理性和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-quality samples, thereby avoiding the impact of potentially harmful information. Our framework presents a new avenue for the formal analysis of DG, heuristic analysis and extensive experiments demonstrate the rationality and effectiveness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">587.Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration</span><br>
                <span class="as">Mei, GuofengandTang, HaoandHuang, XiaoshuiandWang, WeijieandLiu, JuanandZhang, JianandVanGool, LucandWu, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Unsupervised_Deep_Probabilistic_Approach_for_Partial_Point_Cloud_Registration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13611-13620.png><br>
            
            <span class="tt"><span class="t0">研究问题：点云配准方法面临部分重叠和依赖标记数据的挑战。<br>
                    动机：为了解决这些问题，我们提出了UDPreg，一种用于部分重叠点云的无监督深度概率配准框架。<br>
                    方法：首先，我们采用网络从点云中学习高斯混合模型（GMMs）的后验概率分布。然后，为了处理部分点云配准，我们在GMMs的混合权重约束下应用了Sinkhorn算法来预测分布级别的对应关系。最后，为了实现无监督学习，我们设计了三种基于分布一致性的损失函数：自一致性、交叉一致性和局部对比损失。<br>
                    效果：我们的UDPreg在3DMatch/3DLoMatch和ModelNet/ModelLoNet基准测试上取得了有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep point cloud registration methods face challenges to partial overlaps and rely on labeled data. To address these issues, we propose UDPReg, an unsupervised deep probabilistic registration framework for point clouds with partial overlaps. Specifically, we first adopt a network to learn posterior probability distributions of Gaussian mixture models (GMMs) from point clouds. To handle partial point cloud registration, we apply the Sinkhorn algorithm to predict the distribution-level correspondences under the constraint of the mixing weights of GMMs. To enable unsupervised learning, we design three distribution consistency-based losses: self-consistency, cross-consistency, and local contrastive. The self-consistency loss is formulated by encouraging GMMs in Euclidean and feature spaces to share identical posterior distributions. The cross-consistency loss derives from the fact that the points of two partially overlapping point clouds belonging to the same clusters share the cluster centroids. The cross-consistency loss allows the network to flexibly learn a transformation-invariant posterior distribution of two aligned point clouds. The local contrastive loss facilitates the network to extract discriminative local features. Our UDPReg achieves competitive performance on the 3DMatch/3DLoMatch and ModelNet/ModelLoNet benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">588.Learning Adaptive Dense Event Stereo From the Image Domain</span><br>
                <span class="as">Cho, HoonheeandCho, JegyeongandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Learning_Adaptive_Dense_Event_Stereo_From_the_Image_Domain_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17797-17807.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的事件驱动立体匹配在领域转移时性能严重下降。<br>
                    动机：传统的无监督领域适应需要源领域的输入事件数据和目标领域的地面真值，这比图像数据更具挑战性和成本。<br>
                    方法：提出一种新的无监督领域适应密集事件立体匹配（ADES）框架，通过图像重建训练网络，同时利用源领域的辅助网络消除重构图像中的间歇性伪影。<br>
                    效果：实验表明，该方法在事件驱动立体匹配的领域适应能力上取得了显著的成果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, event-based stereo matching has been studied due to its robustness in poor light conditions. However, existing event-based stereo networks suffer severe performance degradation when domains shift. Unsupervised domain adaptation (UDA) aims at resolving this problem without using the target domain ground-truth. However, traditional UDA still needs the input event data with ground-truth in the source domain, which is more challenging and costly to obtain than image data. To tackle this issue, we propose a novel unsupervised domain Adaptive Dense Event Stereo (ADES), which resolves gaps between the different domains and input modalities. The proposed ADES framework adapts event-based stereo networks from abundant image datasets with ground-truth on the source domain to event datasets without ground-truth on the target domain, which is a more practical setup. First, we propose a self-supervision module that trains the network on the target domain through image reconstruction, while an artifact prediction network trained on the source domain assists in removing intermittent artifacts in the reconstructed image. Secondly, we utilize the feature-level normalization scheme to align the extracted features along the epipolar line. Finally, we present the motion-invariant consistency module to impose the consistent output between the perturbed motion. Our experiments demonstrate that our approach achieves remarkable results in the adaptation ability of event-based stereo matching from the image domain.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">589.Conjugate Product Graphs for Globally Optimal 2D-3D Shape Matching</span><br>
                <span class="as">Roetzer, PaulandL\&quot;ahner, ZorahandBernard, Florian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Roetzer_Conjugate_Product_Graphs_for_Globally_Optimal_2D-3D_Shape_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21866-21875.png><br>
            
            <span class="tt"><span class="t0">研究问题：寻找二维轮廓和三维网格之间的连续和非刚性匹配。<br>
                    动机：现有的解决方案严重依赖于不切实际的先验假设来避免退化的解决方案，如知道2D轮廓的每个点与3D形状的哪个区域匹配的知识。<br>
                    方法：提出一种新的基于二维轮廓和三维形状共轭积图的2D-3D形状匹配形式化方法，首次考虑了定义在边链上的高阶成本，而非单个边的成本。<br>
                    效果：该方法能够找到全局最优和连续的2D-3D匹配，具有与以往解决方案相同的渐进复杂性，产生形状匹配的最新成果，甚至能够匹配部分形状。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the problem of finding a continuous and non-rigid matching between a 2D contour and a 3D mesh. While such problems can be solved to global optimality by finding a shortest path in the product graph between both shapes, existing solutions heavily rely on unrealistic prior assumptions to avoid degenerate solutions (e.g. knowledge to which region of the 3D shape each point of the 2D contour is matched). To address this, we propose a novel 2D-3D shape matching formalism based on the conjugate product graph between the 2D contour and the 3D shape. Doing so allows us for the first time to consider higher-order costs, i.e. defined for edge chains, as opposed to costs defined for single edges. This offers substantially more flexibility, which we utilise to incorporate a local rigidity prior. By doing so, we effectively circumvent degenerate solutions and thereby obtain smoother and more realistic matchings, even when using only a one-dimensional feature descriptor. Overall, our method finds globally optimal and continuous 2D-3D matchings, has the same asymptotic complexity as previous solutions, produces state-of-the-art results for shape matching and is even capable of matching partial shapes. Our code is publicly available (https://github.com/paul0noah/sm-2D3D).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">590.Train/Test-Time Adaptation With Retrieval</span><br>
                <span class="as">Zancato, LucaandAchille, AlessandroandLiu, TianYuandTrager, MatthewandPerera, PramudithaandSoatto, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zancato_TrainTest-Time_Adaptation_With_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15911-15921.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过检索模块和可搜索的外部样本库在训练和测试时调整模型？<br>
                    动机：现有的模型调整方法主要依赖合成数据增强来弥补缺乏适应数据的问题，而T3AR通过检索真实图像进行模型调整，提高了特征适应性。<br>
                    方法：T3AR采用检索模块和可搜索的外部样本库，利用检索到的真实样本改进目标数据流形上的特征适应，并在推理前使用精炼的伪标签和自监督对比目标函数对给定模型进行下游任务调整。<br>
                    效果：实验表明，T3AR可以在训练时提高下游细粒度分类性能，尤其是在适应数据较少的情况下（最高达13%）；在测试时，利用外部图像池进行模型调整，使模型在DomainNet-126和VISDA-C上的表现优于现有方法，特别是在适应数据较少的情况下（最高达8%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Train/Test-Time Adaptation with Retrieval (T3AR), a method to adapt models both at train and test time by means of a retrieval module and a searchable pool of external samples. Before inference, T3AR adapts a given model to the downstream task using refined pseudo-labels and a self-supervised contrastive objective function whose noise distribution leverages retrieved real samples to improve feature adaptation on the target data manifold. The retrieval of real images is key to T3AR since it does not rely solely on synthetic data augmentations to compensate for the lack of adaptation data, as typically done by other adaptation algorithms. Furthermore, thanks to the retrieval module, our method gives the user or service provider the possibility to improve model adaptation on the downstream task by incorporating further relevant data or to fully remove samples that may no longer be available due to changes in user preference after deployment. First, we show that T3AR can be used at training time to improve downstream fine-grained classification over standard fine-tuning baselines, and the fewer the adaptation data the higher the relative improvement (up to 13%). Second, we apply T3AR for test-time adaptation and show that exploiting a pool of external images at test-time leads to more robust representations over existing methods on DomainNet-126 and VISDA-C, especially when few adaptation data are available (up to 8%).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">591.Best of Both Worlds: Multimodal Contrastive Learning With Tabular and Imaging Data</span><br>
                <span class="as">Hager, PaulandMenten, MartinJ.andRueckert, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hager_Best_of_Both_Worlds_Multimodal_Contrastive_Learning_With_Tabular_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23924-23935.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用图像和表格数据进行自我监督的对比学习，以训练单模态编码器。<br>
                    动机：医疗数据集和生物库中含有大量丰富的临床信息，但医生的数据量和多样性有限，且标注成本高昂。因此，需要一种能从多模态预训练并单模态预测的自我监督方法。<br>
                    方法：提出了第一个利用图像和表格数据进行自我监督对比学习框架，该框架结合了SimCLR和SCARF两种领先的对比学习策略。<br>
                    效果：通过心脏MR图像和40,000个UK Biobank主题的120个临床特征预测心肌梗死和冠状动脉疾病的风险，实验表明该方法的有效性。同时，通过对DVM汽车广告数据集的应用，证明了该方法对自然图像的泛化能力。此外，通过实验发现形态学表格特征在对比学习过程中的重要性，并提高了学习嵌入的质量。最后，通过附加真实标签作为表格特征的形式引入了一种新的监督对比学习方法，该方法在所有监督对比基线上表现优越。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Medical datasets and especially biobanks, often contain extensive tabular data with rich clinical information in addition to images. In practice, clinicians typically have less data, both in terms of diversity and scale, but still wish to deploy deep learning solutions. Combined with increasing medical dataset sizes and expensive annotation costs, the necessity for unsupervised methods that can pretrain multimodally and predict unimodally has risen. To address these needs, we propose the first self-supervised contrastive learning framework that takes advantage of images and tabular data to train unimodal encoders. Our solution combines SimCLR and SCARF, two leading contrastive learning strategies, and is simple and effective. In our experiments, we demonstrate the strength of our framework by predicting risks of myocardial infarction and coronary artery disease (CAD) using cardiac MR images and 120 clinical features from 40,000 UK Biobank subjects. Furthermore, we show the generalizability of our approach to natural images using the DVM car advertisement dataset. We take advantage of the high interpretability of tabular data and through attribution and ablation experiments find that morphometric tabular features, describing size and shape, have outsized importance during the contrastive learning process and improve the quality of the learned embeddings. Finally, we introduce a novel form of supervised contrastive learning, label as a feature (LaaF), by appending the ground truth label as a tabular feature during multimodal pretraining, outperforming all supervised contrastive baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">592.Masked Images Are Counterfactual Samples for Robust Fine-Tuning</span><br>
                <span class="as">Xiao, YaoandTang, ZiyiandWei, PengxuandLiu, CongandLin, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Masked_Images_Are_Counterfactual_Samples_for_Robust_Fine-Tuning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20301-20310.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习模型在训练数据和测试数据分布转移上面临挑战，尤其是在微调过程中的内分布（ID）性能与外分布（OOD）鲁棒性之间的权衡。<br>
                    动机：现有的方法并未明确解决OOD鲁棒性问题，因此需要一种新的微调方法来改善这个问题。<br>
                    方法：本文提出了一种新颖的微调方法，使用被遮蔽的图像作为反事实样本，以增强微调模型的鲁棒性。具体来说，根据类激活图，遮蔽图像中语义相关或无关的补丁，打破虚假相关性，并用其他图像中的补丁填充被遮蔽的补丁。生成的反事实样本用于特征基的蒸馏与预训练模型。<br>
                    效果：实验证明，用提出的遮蔽图像进行正则化的微调可以在ID和OOD性能之间实现更好的权衡，超越以往的方法在OOD性能上的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data have demonstrated unprecedented robustness to various distribution shifts. However, fine-tuning these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis of the aforementioned problems, we propose a novel fine-tuning method, which uses masked images as counterfactual samples that help improve the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the pre-trained model. Extensive experiments verify that regularizing the fine-tuning with the proposed masked images can achieve a better trade-off between ID and OOD performance, surpassing previous methods on the OOD performance. Our code is available at https://github.com/Coxy7/robust-finetuning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">593.CLIP the Gap: A Single Domain Generalization Approach for Object Detection</span><br>
                <span class="as">Vidit, ViditandEngilberge, MartinandSalzmann, Mathieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3219-3229.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个模型使其能从一个源领域泛化到任何未见过的目标领域。<br>
                    动机：尽管在图像分类中已经对单领域泛化（SDG）进行了深入研究，但在目标检测领域的相关文献却几乎不存在。为了解决同时学习稳健的目标定位和表示的挑战，我们提出利用预训练的视觉-语言模型通过文本提示引入语义领域概念。<br>
                    方法：我们通过作用于检测器主干提取的特征的语义增强策略以及基于文本的分类损失来实现这一目标。<br>
                    效果：我们的实验证明了该方法的益处，在他们自己多样化的天气驱动基准上，比现有的唯一SDG目标检测方法Single-DGOD[49]提高了10%的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD[49], on their own diverse weather-driving benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">594.Unbalanced Optimal Transport: A Unified Framework for Object Detection</span><br>
                <span class="as">DePlaen, HenriandDePlaen, Pierre-Fran\c{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/De_Plaen_Unbalanced_Optimal_Transport_A_Unified_Framework_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3198-3207.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地匹配预测的边界框和相关分类分数与地面实况，以优化对象检测模型的训练。<br>
                    动机：目前流行的匹配策略包括匹配最近的地面实况框和通过匈牙利算法进行匹配，但这些方法各有优缺点。<br>
                    方法：提出使用不平衡最优传输（Unbalanced Optimal Transport）来统一这些不同的方法，并在其间形成一系列的新方法。<br>
                    效果：实验证明，使用不平衡最优传输训练的对象检测模型在平均精度、平均召回率上均达到最先进的水平，同时能够更快地收敛，且适合大规模模型的GPU实现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>During training, supervised object detection tries to correctly match the predicted bounding boxes and associated classification scores to the ground truth. This is essential to determine which predictions are to be pushed towards which solutions, or to be discarded. Popular matching strategies include matching to the closest ground truth box (mostly used in combination with anchors), or matching via the Hungarian algorithm (mostly used in anchor-free methods). Each of these strategies comes with its own properties, underlying losses, and heuristics. We show how Unbalanced Optimal Transport unifies these different approaches and opens a whole continuum of methods in between. This allows for a finer selection of the desired properties. Experimentally, we show that training an object detection model with Unbalanced Optimal Transport is able to reach the state-of-the-art both in terms of Average Precision and Average Recall as well as to provide a faster initial convergence. The approach is well suited for GPU implementation, which proves to be an advantage for large-scale models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">595.MMANet: Margin-Aware Distillation and Modality-Aware Regularization for Incomplete Multimodal Learning</span><br>
                <span class="as">Wei, ShicaiandLuo, ChunboandLuo, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_MMANet_Margin-Aware_Distillation_and_Modality-Aware_Regularization_for_Incomplete_Multimodal_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20039-20049.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态学习在许多场景中具有巨大潜力，但在实践中经常遇到模态数据缺失的问题，导致性能严重下降。<br>
                    动机：为了解决这个问题，我们提出了一个名为MMANet的通用框架来辅助不完整的多模态学习。<br>
                    方法：MMANet由三个组件组成：用于推理的部署网络、将全面多模态信息传递给部署网络的教师网络，以及引导部署网络平衡弱模态组合的正则化网络。我们还提出了一种新的边际感知蒸馏（MAD）方法，通过权衡样本贡献和分类不确定性来协助信息传递。此外，我们还设计了一种模态感知正则化（MAR）算法，以挖掘弱模态组合并指导正则化网络为它们计算预测损失。<br>
                    效果：我们在多模态分类和分割任务上进行了大量实验，结果表明我们的MMANet显著优于最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal learning has shown great potentials in numerous scenes and attracts increasing interest recently. However, it often encounters the problem of missing modality data and thus suffers severe performance degradation in practice. To this end, we propose a general framework called MMANet to assist incomplete multimodal learning. It consists of three components: the deployment network used for inference, the teacher network transferring comprehensive multimodal information to the deployment network, and the regularization network guiding the deployment network to balance weak modality combinations. Specifically, we propose a novel margin-aware distillation (MAD) to assist the information transfer by weighing the sample contribution with the classification uncertainty. This encourages the deployment network to focus on the samples near decision boundaries and acquire the refined inter-class margin. Besides, we design a modality-aware regularization (MAR) algorithm to mine the weak modality combinations and guide the regularization network to calculate prediction loss for them. This forces the deployment network to improve its representation ability for the weak modality combinations adaptively. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate that our MMANet outperforms the state-of-the-art significantly.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">596.Regularized Vector Quantization for Tokenized Image Synthesis</span><br>
                <span class="as">Zhang, JiahuiandZhan, FangnengandTheobalt, ChristianandLu, Shijian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Regularized_Vector_Quantization_for_Tokenized_Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18467-18476.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将图像量化为离散表示，以解决统一生成模型中的基本问题。<br>
                    动机：现有的主要方法要么通过选择最佳匹配的标记进行确定性量化，要么通过从预测分布中采样进行随机量化，但都存在问题。<br>
                    方法：本文提出了一种正则化向量量化框架，通过两个方面的正则化来有效缓解上述问题。一是先验分布正则化，测量先验标记分布与预测标记分布之间的差异，以避免码本崩溃和低码本利用率；二是随机掩码正则化，在量化过程中引入随机性，以在推理阶段失配和未受干扰的重建目标之间取得良好平衡。此外，设计了一种概率对比损失作为校准度量，进一步减轻了受干扰的重建目标。<br>
                    效果：大量实验表明，所提出的量化框架在不同生成模型上始终优于现有的向量量化器，包括自回归模型和扩散模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misaligned inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantizers consistently across different generative models including auto-regressive models and diffusion models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">597.Deep Factorized Metric Learning</span><br>
                <span class="as">Wang, ChengkunandZheng, WenzhaoandLi, JunlongandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Factorized_Metric_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7672-7682.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何学习一个具有泛化性和全面性的相似度度量，以描绘图像之间的语义差异。<br>
                    动机：现有的方法通过学习具有不同目标的嵌入集合来解决这个问题，但主干网络仍然接收所有训练信号的混合。<br>
                    方法：提出一种深度分解的度量学习方法（DFML），将训练信号进行分解，并使用不同的样本来训练主干网络的不同部分。我们还将网络分解为不同的子块，并设计一个可学习的路由器，以自适应地将训练样本分配给每个子块，目标是捕获最多的信息。<br>
                    效果：DFML在CUB-200-2011、Cars196和Stanford Online Products等三个深度度量学习基准测试中实现了最先进的性能。我们还将DFML推广到ImageNet-1K的图像分类任务，并在准确率/计算负载权衡方面观察到了一致的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning a generalizable and comprehensive similarity metric to depict the semantic discrepancies between images is the foundation of many computer vision tasks. While existing methods approach this goal by learning an ensemble of embeddings with diverse objectives, the backbone network still receives a mix of all the training signals. Differently, we propose a deep factorized metric learning method (DFML) to factorize the training signal and employ different samples to train various components of the backbone network. We factorize the network to different sub-blocks and devise a learnable router to adaptively allocate the training samples to each sub-block with the objective to capture the most information. The metric model trained by DFML captures different characteristics with different sub-blocks and constitutes a generalizable metric when using all the sub-blocks. The proposed DFML achieves state-of-the-art performance on all three benchmarks for deep metric learning including CUB-200-2011, Cars196, and Stanford Online Products. We also generalize DFML to the image classification task on ImageNet-1K and observe consistent improvement in accuracy/computation trade-off. Specifically, we improve the performance of ViT-B on ImageNet (+0.2% accuracy) with less computation load (-24% FLOPs).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">598.Multi-Level Logit Distillation</span><br>
                <span class="as">Jin, YingandWang, JiaqiandLin, Dahua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24276-24285.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练语言模型对结构化知识的利用不足，以及知识蒸馏方法在性能和实际应用中的局限性。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，而主流的知识蒸馏方法存在性能和隐私安全问题。<br>
                    方法：本文提出了一种增强的logit蒸馏方法，通过多级别的预测对齐，使学生模型同时学习实例预测、输入关联和类别关联。并通过基于模型校准的预测增强机制进一步提升性能。<br>
                    效果：实验结果表明，该方法在各种任务上的性能均优于现有的logit蒸馏方法，甚至达到与主流特征蒸馏方法相当的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Knowledge Distillation (KD) aims at distilling the knowledge from the large teacher model to a lightweight student model. Mainstream KD methods can be divided into two categories, logit distillation, and feature distillation. The former is easy to implement, but inferior in performance, while the latter is not applicable to some practical circumstances due to concerns such as privacy and safety. Towards this dilemma, in this paper, we explore a stronger logit distillation method via making better utilization of logit outputs. Concretely, we propose a simple yet effective approach to logit distillation via multi-level prediction alignment. Through this framework, the prediction alignment is not only conducted at the instance level, but also at the batch and class level, through which the student model learns instance prediction, input correlation, and category correlation simultaneously. In addition, a prediction augmentation mechanism based on model calibration further boosts the performance. Extensive experiment results validate that our method enjoys consistently higher performance than previous logit distillation methods, and even reaches competitive performance with mainstream feature distillation methods. We promise to release our code and models to ensure reproducibility.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">599.Dual-Path Adaptation From Image to Video Transformers</span><br>
                <span class="as">Park, JunginandLee, JiyoungandSohn, Kwanghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Dual-Path_Adaptation_From_Image_to_Video_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2203-2213.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将视觉基础模型（如ViT和Swin）的强大表示能力转移到视频理解中，同时只增加少量可训练参数。<br>
                    动机：现有的适应方法虽然同时考虑了空间和时间建模，但并未充分利用图像转换器的代表性能力。<br>
                    方法：提出一种新的DUALPATH适应方法，分为空间和时间适应路径，在每个转换器模块中使用轻量级的瓶颈适配器。对于时间动态建模，将连续的帧合并到一个类似网格的帧集中，以精确模仿图像转换器在标记之间推断关系的能力。<br>
                    效果：在四个动作识别基准测试上的实验结果表明，使用DUALPATH预训练的图像转换器可以在数据领域之外进行有效泛化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we efficiently transfer the surpassing representation power of the vision foundation models, such as ViT and Swin, for video understanding with only a few trainable parameters. Previous adaptation methods have simultaneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully leveraging the representative capabilities of image transformers. We argue that the popular dual-path (two-stream) architecture in video models can mitigate this problem. We propose a novel DUALPATH adaptation separated into spatial and temporal adaptation paths, where a lightweight bottleneck adapter is employed in each transformer block. Especially for temporal dynamic modeling, we incorporate consecutive frames into a grid-like frameset to precisely imitate vision transformers' capability that extrapolates relationships between tokens. In addition, we extensively investigate the multiple baselines from a unified perspective in video understanding and compare them with DUALPATH. Experimental results on four action recognition benchmarks prove that pretrained image transformers with DUALPATH can be effectively generalized beyond the data domain.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">600.Transfer Knowledge From Head to Tail: Uncertainty Calibration Under Long-Tailed Distribution</span><br>
                <span class="as">Chen, JiahaoandSu, Bing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Transfer_Knowledge_From_Head_to_Tail_Uncertainty_Calibration_Under_Long-Tailed_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19978-19987.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何估计给定模型的不确定性是一个关键问题。<br>
                    动机：现有的校准技术假设训练数据的分布是平衡的，忽视了真实世界数据往往遵循长尾分布的事实。<br>
                    方法：提出了一种基于知识转移的校准方法，通过为尾部类别的样本估计重要性权重来实现长尾校准。<br>
                    效果：在CIFAR-10-LT、MNIST-LT、CIFAR-100-LT和ImageNet-LT数据集上的大量实验证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How to estimate the uncertainty of a given model is a crucial problem. Current calibration techniques treat different classes equally and thus implicitly assume that the distribution of training data is balanced, but ignore the fact that real-world data often follows a long-tailed distribution. In this paper, we explore the problem of calibrating the model trained from a long-tailed distribution. Due to the difference between the imbalanced training distribution and balanced test distribution, existing calibration methods such as temperature scaling can not generalize well to this problem. Specific calibration methods for domain adaptation are also not applicable because they rely on unlabeled target domain instances which are not available. Models trained from a long-tailed distribution tend to be more overconfident to head classes. To this end, we propose a novel knowledge-transferring-based calibration method by estimating the importance weights for samples of tail classes to realize long-tailed calibration. Our method models the distribution of each class as a Gaussian distribution and views the source statistics of head classes as a prior to calibrate the target distributions of tail classes. We adaptively transfer knowledge from head classes to get the target probability density of tail classes. The importance weight is estimated by the ratio of the target probability density over the source probability density. Extensive experiments on CIFAR-10-LT, MNIST-LT, CIFAR-100-LT, and ImageNet-LT datasets demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">601.Class-Conditional Sharpness-Aware Minimization for Deep Long-Tailed Recognition</span><br>
                <span class="as">Zhou, ZhipengandLi, LanqingandZhao, PeilinandHeng, Pheng-AnnandGong, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Class-Conditional_Sharpness-Aware_Minimization_for_Deep_Long-Tailed_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3499-3509.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度长尾识别（DLTR）中，模型需要在高度不平衡的标签分布下对所有类别进行同等的泛化，但现有的平缓损失景观属性在这一领域尚未得到充分探索。<br>
                    动机：尽管深度学习模型在损失景观中存在更平缓的极小值时具有更好的泛化能力，但在长尾识别任务中，尖峰极小值更为普遍。然而，简单地将现有的平缓操作整合到长尾学习算法中并未带来显著改进。<br>
                    方法：提出了一种基于解耦范式的两级锐度感知优化方法。在第一阶段，特征提取器和分类器都在类条件尺度的参数扰动下进行训练；在第二阶段，通过类平衡采样生成对抗性特征，进一步固定骨干网络并强化分类器。<br>
                    效果：在多个长尾视觉识别基准测试上进行的大量实验表明，所提出的类条件锐度感知最小化（CC-SAM）方法在性能上与最先进的方法相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It's widely acknowledged that deep learning models with flatter minima in its loss landscape tend to generalize better. However, such property is under-explored in deep long-tailed recognition (DLTR), a practical problem where the model is required to generalize equally well across all classes when trained on highly imbalanced label distribution. In this paper, through empirical observations, we argue that sharp minima are in fact prevalent in deep longtailed models, whereas naive integration of existing flattening operations into long-tailed learning algorithms brings little improvement. Instead, we propose an effective twostage sharpness-aware optimization approach based on the decoupling paradigm in DLTR. In the first stage, both the feature extractor and classifier are trained under parameter perturbations at a class-conditioned scale, which is theoretically motivated by the characteristic radius of flat minima under the PAC-Bayesian framework. In the second stage, we generate adversarial features with classbalanced sampling to further robustify the classifier with the backbone frozen. Extensive experiments on multiple longtailed visual recognition benchmarks show that, our proposed Class-Conditional Sharpness-Aware Minimization (CC-SAM), achieves competitive performance compared to the state-of-the-arts. Code is available at https:// github.com/zzpustc/CC-SAM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">602.CUDA: Convolution-Based Unlearnable Datasets</span><br>
                <span class="as">Sadasivan, VinuSankarandSoltanolkotabi, MahdiandFeizi, Soheil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sadasivan_CUDA_Convolution-Based_Unlearnable_Datasets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3862-3871.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过添加特殊设计的噪声，使深度学习模型无法学习到网络数据，以解决在线数据的潜在未授权使用和数据隐私问题。<br>
                    动机：现有的方法在处理这个问题时容易受到对抗性训练的影响，或者计算量过大。因此，本文提出了一种新的、无需模型的卷积无可学习数据集（CUDA）生成技术。<br>
                    方法：CUDA是通过受控的类别卷积生成的，其过滤器是通过私钥随机生成的。这种方法鼓励网络学习过滤器和标签之间的关系，而不是学习用于分类干净数据的有信息的特征。<br>
                    效果：实验结果表明，CUDA对各种数据集（如CIFAR-10、CIFAR-100、ImageNet-100和Tiny-ImageNet）和架构（如ResNet-18、VGG-16、Wide ResNet-34-10、DenseNet-121、DeIT、EfficientNetV2-S和MobileNetV2）具有强大的鲁棒性。例如，在ImageNet-100 CUDA上训练ResNet-18，其在经验风险最小化（ERM）、L_infinity对抗性训练和L_2对抗性训练下的清洁测试精度分别仅为8.96%、40.08%和20.58%。此外，即使只有一小部分训练数据集被干扰，CUDA也显示出了与ERM的经验风险最小化相同的不可学习效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale training of modern deep learning models heavily relies on publicly available data on the web. This potentially unauthorized usage of online data leads to concerns regarding data privacy. Recent works aim to make unlearnable data for deep learning models by adding small, specially designed noises to tackle this issue. However, these methods are vulnerable to adversarial training (AT) and/or are computationally heavy. In this work, we propose a novel, model-free, Convolution-based Unlearnable DAtaset (CUDA) generation technique. CUDA is generated using controlled class-wise convolutions with filters that are randomly generated via a private key. CUDA encourages the network to learn the relation between filters and labels rather than informative features for classifying the clean data. We develop some theoretical analysis demonstrating that CUDA can successfully poison Gaussian mixture data by reducing the clean data performance of the optimal Bayes classifier. We also empirically demonstrate the effectiveness of CUDA with various datasets (CIFAR-10, CIFAR-100, ImageNet-100, and Tiny-ImageNet), and architectures (ResNet-18, VGG-16, Wide ResNet-34-10, DenseNet-121, DeIT, EfficientNetV2-S, and MobileNetV2). Our experiments show that CUDA is robust to various data augmentations and training approaches such as smoothing, AT with different budgets, transfer learning, and fine-tuning. For instance, training a ResNet-18 on ImageNet-100 CUDA achieves only 8.96%, 40.08%, and 20.58% clean test accuracies with empirical risk minimization (ERM), L_infinity AT, and L_2 AT, respectively. Here, ERM on the clean training data achieves a clean test accuracy of 80.66%. CUDA exhibits unlearnability effect with ERM even when only a fraction of the training dataset is perturbed. Furthermore, we also show that CUDA is robust to adaptive defenses designed specifically to break it.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">603.No One Left Behind: Improving the Worst Categories in Long-Tailed Learning</span><br>
                <span class="as">Du, YingxiaoandWu, Jianxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_No_One_Left_Behind_Improving_the_Worst_Categories_in_Long-Tailed_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15804-15813.png><br>
            
            <span class="tt"><span class="t0">研究问题：使用不平衡数据集训练的神经网络在类别间的准确率变化大，如何改善这种情况？<br>
                    动机：传统的长尾巴识别方法会将所有类别手动分为三个子集，然后报告每个子集的平均准确率，但这种方法会导致一些类别被牺牲。<br>
                    方法：提出一种简单的插值方法，通过重新训练预训练模型的分类器并使用我们的损失函数，以及可选的将两个分类器的预测结果进行组合的集成技巧，使得所有类别的召回率分布更均匀，从而提高谐平均准确率。<br>
                    效果：该方法在广泛使用的基准数据集上证明了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unlike the case when using a balanced training dataset, the per-class recall (i.e., accuracy) of neural networks trained with an imbalanced dataset are known to vary a lot from category to category. The convention in long-tailed recognition is to manually split all categories into three subsets and report the average accuracy within each subset. We argue that under such an evaluation setting, some categories are inevitably sacrificed. On one hand, focusing on the average accuracy on a balanced test set incurs little penalty even if some worst performing categories have zero accuracy. On the other hand, classes in the "Few" subset do not necessarily perform worse than those in the "Many" or "Medium" subsets. We therefore advocate to focus more on improving the lowest recall among all categories and the harmonic mean of all recall values. Specifically, we propose a simple plug-in method that is applicable to a wide range of methods. By simply re-training the classifier of an existing pre-trained model with our proposed loss function and using an optional ensemble trick that combines the predictions of the two classifiers, we achieve a more uniform distribution of recall values across categories, which leads to a higher harmonic mean accuracy while the (arithmetic) average accuracy is still high. The effectiveness of our method is justified on widely used benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">604.Deep Fair Clustering via Maximizing and Minimizing Mutual Information: Theory, Algorithm and Metric</span><br>
                <span class="as">Zeng, PengxinandLi, YunfanandHu, PengandPeng, DezhongandLv, JianchengandPeng, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Deep_Fair_Clustering_via_Maximizing_and_Minimizing_Mutual_Information_Theory_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23986-23995.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何防止敏感属性主导聚类，同时将数据划分为不同的集群。<br>
                    动机：尽管最近已经进行了许多工作并取得了巨大的成功，但大多数都是基于启发式的，并且缺乏统一的算法设计理论。<br>
                    方法：通过最大化和最小化互信息，开发了一种深度公平聚类的互信息理论，并相应地设计了一种新的算法，称为FCMI。<br>
                    效果：在六个基准测试中，包括与11种最先进的方法相比的单细胞RNA测序图谱，通过五个指标验证了所提出的FCMI的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fair clustering aims to divide data into distinct clusters while preventing sensitive attributes (e.g., gender, race, RNA sequencing technique) from dominating the clustering. Although a number of works have been conducted and achieved huge success recently, most of them are heuristical, and there lacks a unified theory for algorithm design. In this work, we fill this blank by developing a mutual information theory for deep fair clustering and accordingly designing a novel algorithm, dubbed FCMI. In brief, through maximizing and minimizing mutual information, FCMI is designed to achieve four characteristics highly expected by deep fair clustering, i.e., compact, balanced, and fair clusters, as well as informative features. Besides the contributions to theory and algorithm, another contribution of this work is proposing a novel fair clustering metric built upon information theory as well. Unlike existing evaluation metrics, our metric measures the clustering quality and fairness as a whole instead of separate manner. To verify the effectiveness of the proposed FCMI, we conduct experiments on six benchmarks including a single-cell RNA-seq atlas compared with 11 state-of-the-art methods in terms of five metrics. The code could be accessed from https://pengxi.me.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">605.COT: Unsupervised Domain Adaptation With Clustering and Optimal Transport</span><br>
                <span class="as">Liu, YangandZhou, ZhipengandSun, Baigui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_COT_Unsupervised_Domain_Adaptation_With_Clustering_and_Optimal_Transport_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19998-20007.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将有标签源领域知识迁移到无标签目标领域，特别是在处理类不平衡和大规模训练情况下的计算开销问题。<br>
                    动机：现有的无监督领域适应方法主要关注全局级别的分布对齐，而忽视了局部级别的实例对齐。同时，现有的基于最优传输的方法在处理类不平衡和大规模训练时存在计算开销大的问题。<br>
                    方法：提出一种基于聚类的最优传输（COT）算法，该方法将对齐过程形式化为最优传输问题，并通过端到端的方式在源领域和目标领域的聚类中心之间构建映射。<br>
                    效果：实验结果表明，COT在几个权威基准数据集上取得了最先进的性能，有效地解决了类不平衡和大规模训练下的计算开销问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised domain adaptation (UDA) aims to transfer the knowledge from a labeled source domain to an unlabeled target domain. Typically, to guarantee desirable knowledge transfer, aligning the distribution between source and target domain from a global perspective is widely adopted in UDA. Recent researchers further point out the importance of local-level alignment and propose to construct instance-pair alignment by leveraging on Optimal Transport (OT) theory. However, existing OT-based UDA approaches are limited to handling class imbalance challenges and introduce a heavy computation overhead when considering a large-scale training situation. To cope with two aforementioned issues, we propose a Clustering-based Optimal Transport (COT) algorithm, which formulates the alignment procedure as an Optimal Transport problem and constructs a mapping between clustering centers in the source and target domain via an end-to-end manner. With this alignment on clustering centers, our COT eliminates the negative effect caused by class imbalance and reduces the computation cost simultaneously. Empirically, our COT achieves state-of-the-art performance on several authoritative benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">606.TIPI: Test Time Adaptation With Transformation Invariance</span><br>
                <span class="as">Nguyen, A.TuanandNguyen-Tang, ThanhandLim, Ser-NamandTorr, PhilipH.S.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_TIPI_Test_Time_Adaptation_With_Transformation_Invariance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24162-24171.png><br>
            
            <span class="tt"><span class="t0">研究问题：在将机器学习模型部署到新环境时，经常会遇到分布偏移问题，即目标数据分布与模型的训练分布不同。<br>
                    动机：当标签在新领域不可用且源数据无法存储（如出于隐私原因）时，如何使模型适应新的数据分布。<br>
                    方法：提出一种在测试时间使用变换不变性正则化器作为辅助损失进行测试时间适应的方法（TIPI）。<br>
                    效果：通过大量实验验证，TIPI对小批量大小具有鲁棒性，并在所有设置中始终优于TENT。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When deploying a machine learning model to a new environment, we often encounter the distribution shift problem -- meaning the target data distribution is different from the model's training distribution. In this paper, we assume that labels are not provided for this new domain, and that we do not store the source data (e.g., for privacy reasons). It has been shown that even small shifts in the data distribution can affect the model's performance severely. Test Time Adaptation offers a means to combat this problem, as it allows the model to adapt during test time to the new data distribution, using only unlabeled test data batches. To achieve this, the predominant approach is to optimize a surrogate loss on the test-time unlabeled target data. In particular, minimizing the prediction's entropy on target samples has received much interest as it is task-agnostic and does not require altering the model's training phase (e.g., does not require adding a self-supervised task during training on the source domain). However, as the target data's batch size is often small in real-world scenarios (e.g., autonomous driving models process each few frames in real-time), we argue that this surrogate loss is not optimal since it often collapses with small batch sizes. To tackle this problem, in this paper, we propose to use an invariance regularizer as the surrogate loss during test-time adaptation, motivated by our theoretical results regarding the model's performance under input transformations. The resulting method (TIPI -- Test tIme adaPtation with transformation Invariance) is validated with extensive experiments in various benchmarks (Cifar10-C, Cifar100-C, ImageNet-C, DIGITS, and VisDA17). Remarkably, TIPI is robust against small batch sizes (as small as 2 in our experiments), and consistently outperforms TENT in all settings. Our code is released at https://github.com/atuannguyen/TIPI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">607.CFA: Class-Wise Calibrated Fair Adversarial Training</span><br>
                <span class="as">Wei, ZemingandWang, YifeiandGuo, YiwenandWang, Yisen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_CFA_Class-Wise_Calibrated_Fair_Adversarial_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8193-8201.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络对对抗性例子的对抗鲁棒性，并实现类别间的公平性。<br>
                    动机：大多数现有工作关注提高整体模型的鲁棒性，但在训练和测试阶段等同对待每个类别，忽视了类别间在对抗配置（包括扰动边界、正则化和权重平均）上的偏好差异。<br>
                    方法：首次从理论和实证两方面研究了不同类别对对抗配置的偏好，并据此提出了一种类别级校准的公平对抗训练框架（CFA），该框架能自动为每个类别定制特定的训练配置。<br>
                    效果：实验证明，提出的CFA在提高总体鲁棒性和公平性方面优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adversarial training has been widely acknowledged as the most effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). So far, most existing works focus on enhancing the overall model robustness, treating each class equally in both the training and testing phases. Although revealing the disparity in robustness among classes, few works try to make adversarial training fair at the class level without sacrificing overall robustness. In this paper, we are the first to theoretically and empirically investigate the preference of different classes for adversarial configurations, including perturbation margin, regularization, and weight averaging. Motivated by this, we further propose a Class-wise calibrated Fair Adversarial training framework, named CFA, which customizes specific training configurations for each class automatically. Experiments on benchmark datasets demonstrate that our proposed CFA can improve both overall robustness and fairness notably over other state-of-the-art methods. Code is available at https://github.com/PKU-ML/CFA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">608.Glocal Energy-Based Learning for Few-Shot Open-Set Recognition</span><br>
                <span class="as">Wang, HaoyuandPang, GuansongandWang, PengandZhang, LeiandWei, WeiandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Glocal_Energy-Based_Learning_for_Few-Shot_Open-Set_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7507-7516.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决少样本开放集识别（FSOR）任务，即在只有少量示例的情况下，将样本分类到预定义的闭集中，同时拒绝未知类别的样本。<br>
                    动机：FSOR是一项具有重要实用价值的挑战性任务，但现有的方法往往难以实现对开放集样本的全面检测。<br>
                    方法：本文提出了一种新颖的能量基混合模型，该模型由两个分支组成，一个分支用于学习将样本分类到闭集中的度量，另一个分支用于显式估计开放集概率。为了实现对开放集样本的全局检测，模型利用类特征和像素特征分别学习全局能量得分和局部能量得分。<br>
                    效果：实验结果表明，本文提出的能量基混合模型在三个标准的FSOR数据集上表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot open-set recognition (FSOR) is a challenging task of great practical value. It aims to categorize a sample to one of the pre-defined, closed-set classes illustrated by few examples while being able to reject the sample from unknown classes. In this work, we approach the FSOR task by proposing a novel energy-based hybrid model. The model is composed of two branches, where a classification branch learns a metric to classify a sample to one of closed-set classes and the energy branch explicitly estimates the open-set probability. To achieve holistic detection of open-set samples, our model leverages both class-wise and pixel-wise features to learn a glocal energy-based score, in which a global energy score is learned using the class-wise features, while a local energy score is learned using the pixel-wise features. The model is enforced to assign large energy scores to samples that are deviated from the few-shot examples in either the class-wise features or the pixel-wise features, and to assign small energy scores otherwise. Experiments on three standard FSOR datasets show the superior performance of our model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">609.AutoLabel: CLIP-Based Framework for Open-Set Video Domain Adaptation</span><br>
                <span class="as">Zara, GiacomoandRoy, SubhankarandRota, PaoloandRicci, Elisa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zara_AutoLabel_CLIP-Based_Framework_for_Open-Set_Video_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11504-11513.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将一个有标签的源领域的动作识别模型适应到一个包含“目标专用”类别的无标签的目标领域。<br>
                    动机：现有的开放集无监督视频领域适应方法需要专门的开放集分类器或加权对抗学习，我们提出使用预训练的语言和视觉模型CLIP来解决这个问题。<br>
                    方法：我们提出了AutoLabel，它自动发现并生成以对象为中心的组合候选目标专用类名，使CLIP能够拒绝目标专用实例，从而更好地对齐两个领域的共享类别。<br>
                    效果：实验结果表明，配备AutoLabel的CLIP可以满意地拒绝目标专用实例，从而实现更好的领域适应效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-set Unsupervised Video Domain Adaptation (OUVDA) deals with the task of adapting an action recognition model from a labelled source domain to an unlabelled target domain that contains "target-private" categories, which are present in the target but absent in the source. In this work we deviate from the prior work of training a specialized open-set classifier or weighted adversarial learning by proposing to use pre-trained Language and Vision Models (CLIP). The CLIP is well suited for OUVDA due to its rich representation and the zero-shot recognition capabilities. However, rejecting target-private instances with the CLIP's zero-shot protocol requires oracle knowledge about the target-private label names. To circumvent the impossibility of the knowledge of label names, we propose AutoLabel that automatically discovers and generates object-centric compositional candidate target-private class names. Despite its simplicity, we show that CLIP when equipped with AutoLabel can satisfactorily reject the target-private instances, thereby facilitating better alignment between the shared classes of the two domains. The code is available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">610.Instant Domain Augmentation for LiDAR Semantic Segmentation</span><br>
                <span class="as">Ryu, KwonyoungandHwang, SoonminandPark, Jaesik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ryu_Instant_Domain_Augmentation_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9350-9360.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的使用3D激光雷达数据的感知算法在面对'传感器偏差问题'时表现不佳，即当在测试时间应用未见过的具体激光雷达传感器规格时，由于领域差异，算法性能会显著下降。<br>
                    动机：为了解决这个问题，本文提出了一种快速灵活的激光雷达增强方法，称为'LiDomAug'。该方法通过聚合原始激光雷达扫描并考虑到动态畸变和遮挡来创建任何配置的激光雷达扫描，从而实现即时领域增强。<br>
                    方法：LiDomAug模块可以无缝集成到学习框架的数据加载器中，运行速度为330FPS。在实验中，我们使用LiDomAug对基于学习的方案进行辅助，发现这些方案受到传感器偏差问题的影响较小，并在SemanticKITTI和nuScenes数据集上实现了新的最先进的领域适应性能，而无需使用目标领域数据。<br>
                    效果：我们还展示了一个传感器无关模型，该模型能够在不同的激光雷达配置上忠实地工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the increasing popularity of LiDAR sensors, perception algorithms using 3D LiDAR data struggle with the 'sensor-bias problem'. Specifically, the performance of perception algorithms significantly drops when an unseen specification of LiDAR sensor is applied at test time due to the domain discrepancy. This paper presents a fast and flexible LiDAR augmentation method for the semantic segmentation task, called 'LiDomAug'. It aggregates raw LiDAR scans and creates a LiDAR scan of any configurations with the consideration of dynamic distortion and occlusion, resulting in instant domain augmentation. Our on-demand augmentation module runs at 330 FPS, so it can be seamlessly integrated into the data loader in the learning framework. In our experiments, learning-based approaches aided with the proposed LiDomAug are less affected by the sensor-bias issue and achieve new state-of-the-art domain adaptation performances on SemanticKITTI and nuScenes dataset without the use of the target domain data. We also present a sensor-agnostic model that faithfully works on the various LiDAR configurations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">611.Robust Test-Time Adaptation in Dynamic Scenarios</span><br>
                <span class="as">Yuan, LonghuiandXie, BinhuiandLi, Shuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Robust_Test-Time_Adaptation_in_Dynamic_Scenarios_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15922-15932.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练模型适应到测试分布，特别是在现实世界应用的动态场景中。<br>
                    动机：现有的TTA方法在简单的测试数据流上取得了成功，但在如自动驾驶等环境逐渐变化、测试数据随时间相关采样的现实应用中可能失败。<br>
                    方法：提出了一种针对复杂数据流的实际测试时适应（PTTA）的鲁棒测试时适应（RoTTA）方法。具体包括：设计了一种鲁棒的批量归一化方案来估计归一化统计数据；利用记忆库采样考虑时效性和不确定性的类别平衡数据；开发了一种带有教师-学生模型的时间感知重加权策略以稳定训练过程。<br>
                    效果：实验证明，RoTTA能够在相关采样的数据流上进行持续的测试时适应，且该方法易于实施，非常适合快速部署。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this work, we explore such practical test data streams to deploy the model on the fly, namely practical test-time adaptation (PTTA). To do so, we elaborate a Robust Test-Time Adaptation (RoTTA) method against the complex data stream in PTTA. More specifically, we present a robust batch normalization scheme to estimate the normalization statistics. Meanwhile, a memory bank is utilized to sample category-balanced data with consideration of timeliness and uncertainty. Further, to stabilize the training procedure, we develop a time-aware reweighting strategy with a teacher-student model. Extensive experiments prove that RoTTA enables continual testtime adaptation on the correlatively sampled data streams. Our method is easy to implement, making it a good choice for rapid deployment. The code is publicly available at https://github.com/BIT-DA/RoTTA</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">612.Global and Local Mixture Consistency Cumulative Learning for Long-Tailed Visual Recognitions</span><br>
                <span class="as">Du, FeiandYang, PengandJia, QiandNan, FengtaoandChen, XiaotingandYang, Yun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Global_and_Local_Mixture_Consistency_Cumulative_Learning_for_Long-Tailed_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15814-15823.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种简单学习模式，提高长尾视觉识别的鲁棒性，减少训练技巧和开销，同时减轻分类器对头部类别的偏见。<br>
                    动机：现有的方法在处理长尾视觉识别问题上存在一些挑战，如特征提取器的鲁棒性不足，分类器对头部类别的偏见等。<br>
                    方法：提出一种名为全局与局部混合一致性累积学习的高效单阶段训练策略（GLMC）。主要包括两部分：(1) 通过全局和局部混合一致性损失来提高特征提取器的鲁棒性；(2) 通过累积的头尾软标签重加权损失来减轻头部类别偏见问题。<br>
                    效果：在CIFAR10-LT、CIFAR100-LT和ImageNet-LT数据集上，该方法取得了最先进的精度。在平衡的ImageNet和CIFAR上的额外实验表明，GLMC可以显著提高主干网络的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, our goal is to design a simple learning paradigm for long-tail visual recognition, which not only improves the robustness of the feature extractor but also alleviates the bias of the classifier towards head classes while reducing the training skills and overhead. We propose an efficient one-stage training strategy for long-tailed visual recognition called Global and Local Mixture Consistency cumulative learning (GLMC). Our core ideas are twofold: (1) a global and local mixture consistency loss improves the robustness of the feature extractor. Specifically, we generate two augmented batches by the global MixUp and local CutMix from the same batch data, respectively, and then use cosine similarity to minimize the difference. (2) A cumulative head-tail soft label reweighted loss mitigates the head class bias problem. We use empirical class frequencies to reweight the mixed label of the head-tail class for long-tailed data and then balance the conventional loss and the rebalanced loss with a coefficient accumulated by epochs. Our approach achieves state-of-the-art accuracy on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets. Additional experiments on balanced ImageNet and CIFAR demonstrate that GLMC can significantly improve the generalization of backbones. Code is made publicly available at https://github.com/ynu-yangpeng/GLMC</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">613.MHPL: Minimum Happy Points Learning for Active Source Free Domain Adaptation</span><br>
                <span class="as">Wang, FanandHan, ZhongyiandZhang, ZhiyanandHe, RundongandYin, Yilong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MHPL_Minimum_Happy_Points_Learning_for_Active_Source_Free_Domain_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20008-20018.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练的源模型迁移到无标签的目标领域，而不访问源数据。<br>
                    动机：源自由领域适应（SFDA）设置面临性能瓶颈，因为缺乏源数据和目标监督信息。<br>
                    方法：通过主动学习探索和利用少量有信息量的样本进行活动源自由领域适应（ASFDA）。提出最小快乐点学习（MHPL）来主动探索和利用最小快乐点。设计了三种独特的策略：邻居环境不确定性、邻居多样性放松和一次查询，以探索最小快乐点。为了在学习过程中充分利用最小快乐点，设计了一个邻居焦点损失，将加权的邻居纯度分配给最小快乐点的交叉熵损失，使模型更关注它们。<br>
                    效果：实验证明，MHPL显著超过了各种类型的基线，并在小的标签成本下实现了显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Source free domain adaptation (SFDA) aims to transfer a trained source model to the unlabeled target domain without accessing the source data. However, the SFDA setting faces a performance bottleneck due to the absence of source data and target supervised information, as evidenced by the limited performance gains of the newest SFDA methods. Active source free domain adaptation (ASFDA) can break through the problem by exploring and exploiting a small set of informative samples via active learning. In this paper, we first find that those satisfying the properties of neighbor-chaotic, individual-different, and source-dissimilar are the best points to select. We define them as the minimum happy (MH) points challenging to explore with existing methods. We propose minimum happy points learning (MHPL) to explore and exploit MH points actively. We design three unique strategies: neighbor environment uncertainty, neighbor diversity relaxation, and one-shot querying, to explore the MH points. Further, to fully exploit MH points in the learning process, we design a neighbor focal loss that assigns the weighted neighbor purity to the cross entropy loss of MH points to make the model focus more on them. Extensive experiments verify that MHPL remarkably exceeds the various types of baselines and achieves significant performance gains at a small cost of labeling.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">614.Diversity-Aware Meta Visual Prompting</span><br>
                <span class="as">Huang, QidongandDong, XiaoyiandChen, DongdongandZhang, WeimingandWang, FeifeiandHua, GangandYu, Nenghai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10878-10887.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将预训练模型转移到下游任务，同时保持模型的固定主干。<br>
                    动机：视觉提示的一个挑战是，图像数据集的数据多样性有时很大，而每个数据集的通用提示很难正确处理向原始预训练数据分布的复杂分布转移。<br>
                    方法：提出了一种具有多样性感知能力的元视觉提示（DAM-VP）方法，通过元提示初始化，将下游数据集划分为小的同质子集，每个子集都有自己的优化提示。<br>
                    效果：实验表明，DAM-VP在一系列不同的预训练模型下游数据集上，明显优于以往的提示方法，表现出更高的效率和效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Diversity-Aware Meta Visual Prompting (DAM-VP), an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone. A challenging issue in visual prompting is that image datasets sometimes have a large data diversity whereas a per-dataset generic prompt can hardly handle the complex distribution shift toward the original pretraining data distribution properly. To address this issue, we propose a dataset Diversity-Aware prompting strategy whose initialization is realized by a Meta-prompt. Specifically, we cluster the downstream dataset into small homogeneity subsets in a diversity-adaptive way, with each subset has its own prompt optimized separately. Such a divide-and-conquer design reduces the optimization difficulty greatly and significantly boosts the prompting performance. Furthermore, all the prompts are initialized with a meta-prompt, which is learned across several datasets. It is a bootstrapped paradigm, with the key observation that the prompting knowledge learned from previous datasets could help the prompt to converge faster and perform better on a new dataset. During inference, we dynamically select a proper prompt for each input, based on the feature distance between the input and each subset. Through extensive experiments, our DAM-VP demonstrates superior efficiency and effectiveness, clearly surpassing previous prompting methods in a series of downstream datasets for different pretraining models. Our code is available at: https://github.com/shikiw/DAM-VP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">615.Real-Time Evaluation in Online Continual Learning: A New Hope</span><br>
                <span class="as">Ghunaim, YasirandBibi, AdelandAlhamoud, KumailandAlfarra, MotasemandAlKaderHammoud, HasanAbedandPrabhu, AmeyaandTorr, PhilipH.S.andGhanem, Bernard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ghunaim_Real-Time_Evaluation_in_Online_Continual_Learning_A_New_Hope_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11888-11897.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前连续学习（CL）方法的评估通常假设训练时间和计算没有限制，这在真实世界环境中是不现实的。<br>
                    动机：我们提出了一种实用的实时评估连续学习方法，其中数据流不会等待模型完成训练再显示下一个预测数据。<br>
                    方法：我们在包含3900万带有地理位置标签的时间戳图像的大型数据集CLOC上进行广泛的实验，对现有的CL方法进行计算成本评估。<br>
                    效果：结果显示，一个简单的基线在所有考虑的方法中表现最好，这表明大多数现有的CL文献都是针对一类特定的、不现实的流进行优化的。我们希望这个评估能推动在线连续学习方法的发展，使其更加考虑计算成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against our simple baseline. This surprisingly suggests that the majority of existing CL literature is tailored to a specific class of streams that is not practical. We hope that the evaluation we provide will be the first step towards a paradigm shift to consider the computational cost in the development of online continual learning methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">616.Equiangular Basis Vectors</span><br>
                <span class="as">Shen, YangandSun, XuhaoandWei, Xiu-Shen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Equiangular_Basis_Vectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11755-11765.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种用于分类任务的等角基向量（EBVs）。<br>
                    动机：目前的深度神经网络模型在处理不同分类任务时，通常采用k-way全连接层和softmax进行处理，而度量学习方法的主要目标是学习一个将训练数据点从原始空间映射到新空间的转换函数，使得相似点更近，不相似点更远。<br>
                    方法：与以往的方法不同，我们的EBVs生成标准化的向量嵌入作为“预定义的分类器”，不仅要求它们之间地位相等，而且尽可能正交。通过最小化输入在其训练中的类别EBV之间的球面距离，可以在推理过程中通过识别具有最小距离的类别EBV来获得预测结果。<br>
                    效果：在ImageNet-1K数据集和其他下游任务上进行的各种实验表明，我们的方法优于一般的全连接分类器，同时与经典的度量学习方法相比，没有引入巨大的额外计算。我们的EBVs在2022年DIGIX全球AI挑战赛中获得第一名，我们的代码是开源的，可以在https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors上找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Equiangular Basis Vectors (EBVs) for classification tasks. In deep neural networks, models usually end with a k-way fully connected layer with softmax to handle different classification tasks. The learning objective of these methods can be summarized as mapping the learned feature representations to the samples' label space. While in metric learning approaches, the main objective is to learn a transformation function that maps training data points from the original space to a new space where similar points are closer while dissimilar points become farther apart. Different from previous methods, our EBVs generate normalized vector embeddings as "predefined classifiers" which are required to not only be with the equal status between each other, but also be as orthogonal as possible. By minimizing the spherical distance of the embedding of an input between its categorical EBV in training, the predictions can be obtained by identifying the categorical EBV with the smallest distance during inference. Various experiments on the ImageNet-1K dataset and other downstream tasks demonstrate that our method outperforms the general fully connected classifier while it does not introduce huge additional computation compared with classical metric learning methods. Our EBVs won the first place in the 2022 DIGIX Global AI Challenge, and our code is open-source and available at https://github.com/NJUST-VIPGroup/Equiangular-Basis-Vectors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">617.Rethinking Domain Generalization for Face Anti-Spoofing: Separability and Alignment</span><br>
                <span class="as">Sun, YiyouandLiu, YaojieandLiu, XiaomingandLi, YixuanandChu, Wen-Sheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Rethinking_Domain_Generalization_for_Face_Anti-Spoofing_Separability_and_Alignment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24563-24574.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了面部反欺诈（FAS）模型在图像分辨率、模糊度和传感器变化等领域差距上的泛化问题。<br>
                    动机：大多数先前的工作都将领域特定信号视为负面影响，并应用度量学习或对抗性损失来从特征表示中移除它。尽管学习一个领域不变的特征空间对于训练数据是可行的，但我们发现在未见过测试领域中，特征偏移仍然存在，这会对分类器的泛化能力产生反效果。<br>
                    方法：我们没有构建一个领域不变的特征空间，而是鼓励领域分离性，同时将实况到欺诈的转变（即从实况到欺诈的轨迹）对所有领域进行对齐。我们将这种反欺诈策略的分离性和对齐（SA-FAS）问题形式化为不变风险最小化（IRM），并学习领域可变的但领域不变的分类器的特征表示。<br>
                    效果：我们在具有挑战性的跨领域反欺诈数据集上展示了SA-FAS的有效性，并建立了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work studies the generalization issue of face anti-spoofing (FAS) models on domain gaps, such as image resolution, blurriness and sensor variations. Most prior works regard domain-specific signals as a negative impact, and apply metric learning or adversarial losses to remove it from feature representation. Though learning a domain-invariant feature space is viable for the training data, we show that the feature shift still exists in an unseen test domain, which backfires on the generalizability of the classifier. In this work, instead of constructing a domain-invariant feature space, we encourage domain separability while aligning the live-to-spoof transition (i.e., the trajectory from live to spoof) to be the same for all domains. We formulate this FAS strategy of separability and alignment (SA-FAS) as a problem of invariant risk minimization (IRM), and learn domain-variant feature representation but domain-invariant classifier. We demonstrate the effectiveness of SA-FAS on challenging cross-domain FAS datasets and establish state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">618.Learning Imbalanced Data With Vision Transformers</span><br>
                <span class="as">Xu, ZhengzhuoandLiu, RuikangandYang, ShuoandChai, ZenghaoandYuan, Chun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Imbalanced_Data_With_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15793-15803.png><br>
            
            <span class="tt"><span class="t0">研究问题：现实世界的数据往往严重不平衡，这使长尾巴识别（LTR）成为一个巨大挑战。<br>
                    动机：现有的LTR方法很少使用视觉转换器（ViTs）和长尾巴（LT）数据进行训练，而预训练的ViTs权重往往会带来不公平的比较。<br>
                    方法：我们系统地研究了ViTs在LTR中的表现，并提出了LiVT，该方法只使用LT数据从头开始训练ViTs。通过观察发现ViTs在LTR问题上面临更严重的问题，我们进行了掩蔽生成预训练（MGP），以学习通用特征。<br>
                    效果：大量实验证明，通过MGP和平衡BCE，LiVT成功地训练了ViTs，无需任何额外数据，并在没有任何附加功能的情况下显著超越了最先进的方法。例如，我们的ViT-B在iNaturalist 2018上达到了81.0%的Top-1准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The real-world data tends to be heavily imbalanced and severely skew the data-driven deep neural networks, which makes Long-Tailed Recognition (LTR) a massive challenging task. Existing LTR methods seldom train Vision Transformers (ViTs) with Long-Tailed (LT) data, while the off-the-shelf pretrain weight of ViTs always leads to unfair comparisons. In this paper, we systematically investigate the ViTs' performance in LTR and propose LiVT to train ViTs from scratch only with LT data. With the observation that ViTs suffer more severe LTR problems, we conduct Masked Generative Pretraining (MGP) to learn generalized features. With ample and solid evidence, we show that MGP is more robust than supervised manners. Although Binary Cross Entropy (BCE) loss performs well with ViTs, it struggles on the LTR tasks. We further propose the balanced BCE to ameliorate it with strong theoretical groundings. Specially, we derive the unbiased extension of Sigmoid and compensate extra logit margins for deploying it. Our Bal-BCE contributes to the quick convergence of ViTs in just a few epochs. Extensive experiments demonstrate that with MGP and Bal-BCE, LiVT successfully trains ViTs well without any additional data and outperforms comparable state-of-the-art methods significantly, e.g., our ViT-B achieves 81.0% Top-1 accuracy in iNaturalist 2018 without bells and whistles. Code is available at https://github.com/XuZhengzhuo/LiVT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">619.LINe: Out-of-Distribution Detection by Leveraging Important Neurons</span><br>
                <span class="as">Ahn, YongHyunandPark, Gyeong-MoonandKim, SeongTae</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_LINe_Out-of-Distribution_Detection_by_Leveraging_Important_Neurons_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19852-19862.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何量化输入样本的不确定性，特别是在自动驾驶和医疗等关键领域，对未知数据的预测失败可能会带来大问题。<br>
                    动机：模型无法表达其不知道的内容，这是OOD检测问题的根源。由于不需要额外的再训练过程，后验OOD检测方法受到了广泛的探索。<br>
                    方法：从模型深层神经元代表高级特征的角度出发，提出了一种新的分析模型在分布内数据和OOD数据输出差异的方法。提出了一种新颖的方法，即利用重要神经元（LINe）进行后验OOD检测。通过基于Shapley值的剪枝，只选择高贡献的神经元来预测特定输入数据类别，屏蔽其余部分，从而降低噪声输出的影响。激活剪辑将所有高于一定阈值的值固定为相同值，使LINe平等对待所有类特定特征，只考虑分布内和OOD数据之间激活特征数量的差异。<br>
                    效果：通过在CIFAR-10、CIFAR-100和ImageNet数据集上超越最先进的后验OOD检测方法，全面的实验验证了所提出方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It is important to quantify the uncertainty of input samples, especially in mission-critical domains such as autonomous driving and healthcare, where failure predictions on out-of-distribution (OOD) data are likely to cause big problems. OOD detection problem fundamentally begins in that the model cannot express what it is not aware of. Post-hoc OOD detection approaches are widely explored because they do not require an additional re-training process which might degrade the model's performance and increase the training cost. In this study, from the perspective of neurons in the deep layer of the model representing high-level features, we introduce a new aspect for analyzing the difference in model outputs between in-distribution data and OOD data. We propose a novel method, Leveraging Important Neurons (LINe), for post-hoc Out of distribution detection. Shapley value-based pruning reduces the effects of noisy outputs by selecting only high-contribution neurons for predicting specific classes of input data and masking the rest. Activation clipping fixes all values above a certain threshold into the same value, allowing LINe to treat all the class-specific features equally and just consider the difference between the number of activated feature differences between in-distribution and OOD data. Comprehensive experiments verify the effectiveness of the proposed method by outperforming state-of-the-art post-hoc OOD detection methods on CIFAR-10, CIFAR-100, and ImageNet datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">620.Exploring Data Geometry for Continual Learning</span><br>
                <span class="as">Gao, ZhiandXu, ChenandLi, FengandJia, YundeandHarandi, MehrtashandWu, Yuwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Exploring_Data_Geometry_for_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24325-24334.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过探索数据几何来解决非平稳数据流的持续学习问题，并防止对旧数据的遗忘。<br>
                    动机：在许多实际应用中，数据符合非欧几里得几何。因此，常用的欧几里得空间无法优雅地捕获数据非欧几里得几何结构，导致结果较差。<br>
                    方法：我们的方法通过动态扩展基础空间的几何结构来匹配新数据引发的增长几何结构，并通过考虑旧数据的几何结构来防止遗忘。为此，我们利用混合曲率空间并提出一种增量搜索方案，以编码不断增长的几何结构。然后，我们引入角正则化损失和邻居鲁棒性损失来训练模型，使其能够惩罚全局几何结构和局部几何结构的变化。<br>
                    效果：实验表明，我们的方法在性能上优于在欧几里得空间设计的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning aims to efficiently learn from a non-stationary stream of data while avoiding forgetting the knowledge of old data. In many practical applications, data complies with non-Euclidean geometry. As such, the commonly used Euclidean space cannot gracefully capture non-Euclidean geometric structures of data, leading to inferior results. In this paper, we study continual learning from a novel perspective by exploring data geometry for the non-stationary stream of data. Our method dynamically expands the geometry of the underlying space to match growing geometric structures induced by new data, and prevents forgetting by keeping geometric structures of old data into account. In doing so, we make use of the mixed-curvature space and propose an incremental search scheme, through which the growing geometric structures are encoded. Then, we introduce an angular-regularization loss and a neighbor-robustness loss to train the model, capable of penalizing the change of global geometric structures and local geometric structures. Experiments show that our method achieves better performance than baseline methods designed in Euclidean space.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">621.Visual DNA: Representing and Comparing Images Using Distributions of Neuron Activations</span><br>
                <span class="as">Ramtoula, BenjaminandGadd, MatthewandNewman, PaulandDeMartini, Daniele</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramtoula_Visual_DNA_Representing_and_Comparing_Images_Using_Distributions_of_Neuron_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11113-11123.png><br>
            
            <span class="tt"><span class="t0">研究问题：在现代计算机视觉中，选择合适的数据集至关重要，但目前没有通用的工具来评估两个数据集的差异程度。<br>
                    动机：为了解决这个问题，我们提出了使用神经元激活分布（DNAs）来表示图像和扩展数据集的方法。<br>
                    方法：通过预训练的特征提取器将图像传递给DNAs进行表示，该提取器在所有数据集中都是固定的。通过比较两个DNAs，我们可以评估两个数据集的差异程度，并具有对感兴趣的比较属性的精细控制能力。<br>
                    效果：我们证明了DNAs在不同任务和多样化数据集上的适用性，包括条件数据集比较、合成图像评估和迁移学习等。DNAs具有紧凑性，可以表示任何规模的数据集，且大小不超过15兆字节。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Selecting appropriate datasets is critical in modern computer vision. However, no general-purpose tools exist to evaluate the extent to which two datasets differ. For this, we propose representing images -- and by extension datasets -- using Distributions of Neuron Activations (DNAs). DNAs fit distributions, such as histograms or Gaussians, to activations of neurons in a pre-trained feature extractor through which we pass the image(s) to represent. This extractor is frozen for all datasets, and we rely on its generally expressive power in feature space. By comparing two DNAs, we can evaluate the extent to which two datasets differ with granular control over the comparison attributes of interest, providing the ability to customise the way distances are measured to suit the requirements of the task at hand. Furthermore, DNAs are compact, representing datasets of any size with less than 15 megabytes. We demonstrate the value of DNAs by evaluating their applicability on several tasks, including conditional dataset comparison, synthetic image evaluation, and transfer learning, and across diverse datasets, ranging from synthetic cat images to celebrity faces and urban driving scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">622.Neuron Structure Modeling for Generalizable Remote Physiological Measurement</span><br>
                <span class="as">Lu, HaoandYu, ZitongandNiu, XuesongandChen, Ying-Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Neuron_Structure_Modeling_for_Generalizable_Remote_Physiological_Measurement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18589-18599.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，远程光体积描记术（rPPG）技术引起了广泛关注。然而，由于血液容积脉搏信号容易受到环境变化的影响，现有的方法在未见过的环境中泛化效果不佳。<br>
                    动机：针对这一问题，本文提出了一种无需领域标签的神经结构建模（NEST）方法，通过最大化训练过程中特征空间的覆盖范围来提高泛化能力。<br>
                    方法：NEST方法通过减少推理过程中特征激活不足的机会，增强跨多领域的领域不变特征。<br>
                    效果：实验表明，NEST方法在跨数据集和 intra-dataset 设置上都优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Remote photoplethysmography (rPPG) technology has drawn increasing attention in recent years. It can extract Blood Volume Pulse (BVP) from facial videos, making many applications like health monitoring and emotional analysis more accessible. However, as the BVP signal is easily affected by environmental changes, existing methods struggle to generalize well for unseen domains. In this paper, we systematically address the domain shift problem in the rPPG measurement task. We show that most domain generalization methods do not work well in this problem, as domain labels are ambiguous in complicated environmental changes. In light of this, we propose a domain-label-free approach called NEuron STructure modeling (NEST). NEST improves the generalization capacity by maximizing the coverage of feature space during training, which reduces the chance for under-optimized feature activation during inference. Besides, NEST can also enrich and enhance domain invariant features across multi-domain. We create and benchmark a large-scale domain generalization protocol for the rPPG measurement task. Extensive experiments show that our approach outperforms the state-of-the-art methods on both cross-dataset and intra-dataset settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">623.Enhancing Multiple Reliability Measures via Nuisance-Extended Information Bottleneck</span><br>
                <span class="as">Jeong, JongheonandYu, SihyunandLee, HankookandShin, Jinwoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_Enhancing_Multiple_Reliability_Measures_via_Nuisance-Extended_Information_Bottleneck_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16206-16218.png><br>
            
            <span class="tt"><span class="t0">研究问题：在训练数据有限的情况下，如何提高模型的鲁棒性，防止模型过度依赖数据获取中的偏差信号。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。同时，通过扩展标准信息瓶颈以额外模拟干扰信息，并使用基于自编码器的培训来实现目标。<br>
                    效果：实验结果表明，该方法提高了学习到的表示的鲁棒性（显著地无需使用任何特定领域的知识），并在多个具有挑战性的可靠性测量方面取得了良好的效果。例如，该方法可以在最新的具有挑战性的OBJECTS基准测试中，将新颖性检测的性能从78.4%提高到87.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In practical scenarios where training data is limited, many predictive signals in the data can be rather from some biases in data acquisition (i.e., less generalizable), so that one cannot prevent a model from co-adapting on such (so-called) "shortcut" signals: this makes the model fragile in various distribution shifts. To bypass such failure modes, we consider an adversarial threat model under a mutual information constraint to cover a wider class of perturbations in training. This motivates us to extend the standard information bottleneck to additionally model the nuisance information. We propose an autoencoder-based training to implement the objective, as well as practical encoder designs to facilitate the proposed hybrid discriminative-generative training concerning both convolutional- and Transformer-based architectures. Our experimental results show that the proposed scheme improves robustness of learned representations (remarkably without using any domain-specific knowledge), with respect to multiple challenging reliability measures. For example, our model could advance the state-of-the-art on a recent challenging OBJECTS benchmark in novelty detection by 78.4% -> 87.2% in AUROC, while simultaneously enjoying improved corruption, background and (certified) adversarial robustness. Code is available at https://github.com/jh-jeong/nuisance_ib.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">624.Image Quality-Aware Diagnosis via Meta-Knowledge Co-Embedding</span><br>
                <span class="as">Che, HaoxuanandChen, SiyuandChen, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Che_Image_Quality-Aware_Diagnosis_via_Meta-Knowledge_Co-Embedding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19819-19829.png><br>
            
            <span class="tt"><span class="t0">研究问题：医疗图像在临床实践中通常会出现图像退化，导致基于深度学习的模型性能下降。<br>
                    动机：大多数先前的研究都集中在过滤掉引起退化的低质量图像上，而忽视了它们对模型的潜在价值。<br>
                    方法：通过有效地学习和利用退化的知识，模型可以更好地抵抗其负面影响，避免误诊。本文提出了一个图像质量感知诊断的问题，旨在利用低质量图像和图像质量标签来实现更准确和鲁棒的诊断。为此，我们提出了一个新的元知识共嵌入网络，包括任务网络和元学习器两个子网。<br>
                    效果：在五个数据集上进行的实验表明，我们的方法在四种常用的医学成像模态上都表现出了优越的性能和泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Medical images usually suffer from image degradation in clinical practice, leading to decreased performance of deep learning-based models. To resolve this problem, most previous works have focused on filtering out degradation-causing low-quality images while ignoring their potential value for models. Through effectively learning and leveraging the knowledge of degradations, models can better resist their adverse effects and avoid misdiagnosis. In this paper, we raise the problem of image quality-aware diagnosis, which aims to take advantage of low-quality images and image quality labels to achieve a more accurate and robust diagnosis. However, the diversity of degradations and superficially unrelated targets between image quality assessment and disease diagnosis makes it still quite challenging to effectively leverage quality labels to assist diagnosis. Thus, to tackle these issues, we propose a novel meta-knowledge co-embedding network, consisting of two subnets: Task Net and Meta Learner. Task Net constructs an explicit quality information utilization mechanism to enhance diagnosis via knowledge co-embedding features, while Meta Learner ensures the effectiveness and constrains the semantics of these features via meta-learning and joint-encoding masking. Superior performance on five datasets with four widely-used medical imaging modalities demonstrates the effectiveness and generalizability of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">625.Domain Generalized Stereo Matching via Hierarchical Visual Transformation</span><br>
                <span class="as">Chang, TianyuandYang, XunandZhang, TianzhuandWang, Meng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Domain_Generalized_Stereo_Matching_via_Hierarchical_Visual_Transformation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9559-9568.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度立体匹配网络容易学习到依赖于数据集的捷径，无法在未见过的现实数据集上进行良好的泛化。<br>
                    动机：为了解决这一问题，本文提出了一种针对领域泛化的立体匹配任务训练稳健模型的方法，主要关注从合成数据中学习捷径不变的表示以减轻领域偏移。<br>
                    方法：具体来说，我们提出了一个分层视觉转换（HVT）网络，首先将训练样本分层次地转换为来自三个级别的新领域：全局、局部和像素，然后最大化源领域和新领域之间的视觉差异，最小化跨领域特征不一致性，以捕获领域不变的特征。<br>
                    效果：通过在几个公共立体匹配基准数据集上集成我们的HVT网络并与最先进的立体匹配网络进行评估，大量实验清楚地表明，HVT网络可以显著提高现有立体匹配网络在合成到现实领域的泛化性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, deep Stereo Matching (SM) networks have shown impressive performance and attracted increasing attention in computer vision. However, existing deep SM networks are prone to learn dataset-dependent shortcuts, which fail to generalize well on unseen realistic datasets. This paper takes a step towards training robust models for the domain generalized SM task, which mainly focuses on learning shortcut-invariant representation from synthetic data to alleviate the domain shifts. Specifically, we propose a Hierarchical Visual Transformation (HVT) network to 1) first transform the training sample hierarchically into new domains with diverse distributions from three levels: Global, Local, and Pixel, 2) then maximize the visual discrepancy between the source domain and new domains, and minimize the cross-domain feature inconsistency to capture domain-invariant features. In this way, we can prevent the model from exploiting the artifacts of synthetic stereo images as shortcut features, thereby estimating the disparity maps more effectively based on the learned robust and shortcut-invariant representation. We integrate our proposed HVT network with SOTA SM networks and evaluate its effectiveness on several public SM benchmark datasets. Extensive experiments clearly show that the HVT network can substantially enhance the performance of existing SM networks in synthetic-to-realistic domain generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">626.Deep Semi-Supervised Metric Learning With Mixed Label Propagation</span><br>
                <span class="as">Zhuang, FurenandMoulin, Pierre</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_Deep_Semi-Supervised_Metric_Learning_With_Mixed_Label_Propagation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3429-3438.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过无标签数据进行有效的度量学习，特别是在寻找远隔相似对和近邻不相似对时。<br>
                    动机：传统的度量学习方法在无标签数据上难以找到远隔相似对和近邻不相似对，因为通常假设相近的数据对是相似的。<br>
                    方法：提出一种新的度量学习方法，通过在亲和矩阵中移除连接一对数据的边来获取不相似的标签，从而识别出困难负对，即使它们很接近也可以被识别出来。<br>
                    效果：这种方法显著提高了标签传播在识别远隔正对和近邻负对方面的能力，从而提高了半监督度量学习的性能，如内容基于信息检索（CBIR）应用的召回率、精度和归一化互信息（NMI）性能指标。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Metric learning requires the identification of far-apart similar pairs and close dissimilar pairs during training, and this is difficult to achieve with unlabeled data because pairs are typically assumed to be similar if they are close. We present a novel metric learning method which circumvents this issue by identifying hard negative pairs as those which obtain dissimilar labels via label propagation (LP), when the edge linking the pair of data is removed in the affinity matrix. In so doing, the negative pairs can be identified despite their proximity, and we are able to utilize this information to significantly improve LP's ability to identify far-apart positive pairs and close negative pairs. This results in a considerable improvement in semi-supervised metric learning performance as evidenced by recall, precision and Normalized Mutual Information (NMI) performance metrics on Content-based Information Retrieval (CBIR) applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">627.Unpaired Image-to-Image Translation With Shortest Path Regularization</span><br>
                <span class="as">Xie, ShaoanandXu, YanwuandGong, MingmingandZhang, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Unpaired_Image-to-Image_Translation_With_Shortest_Path_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10177-10187.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过无对图像翻译学习适当的映射，将一个领域的图像映射到另一个领域，同时保留输入图像的内容。<br>
                    动机：现有的方法将两个领域视为离散的，并提出了不同的假设来解决这个问题。本文从不同的角度出发，考虑了连接两个领域的路径。<br>
                    方法：假设输入和输出图像之间的最优路径长度应该是所有可能路径中最短的。基于这个假设，我们提出了一种新的方法，允许沿着路径生成图像，并提出了一种简单的方法来鼓励网络在没有配对信息的情况下找到最短路径。<br>
                    效果：大量的实验表明，我们的方法在各种任务上都表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unpaired image-to-image translation aims to learn proper mappings that can map images from one domain to another domain while preserving the content of the input image. However, with large enough capacities, the network can learn to map the inputs to any random permutation of images in another domain. Existing methods treat two domains as discrete and propose different assumptions to address this problem. In this paper, we start from a different perspective and consider the paths connecting the two domains. We assume that the optimal path length between the input and output image should be the shortest among all possible paths. Based on this assumption, we propose a new method to allow generating images along the path and present a simple way to encourage the network to find the shortest path without pair information. Extensive experiments on various tasks demonstrate the superiority of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">628.MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion</span><br>
                <span class="as">Jiang, Chiyu{\textquotedblleft</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9644-9653.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地预测多智能体的未来运动轨迹？<br>
                    动机：现有的模型在预测多智能体未来运动轨迹时，存在学习到的分布模式单一、需要依赖轨迹锚点以及无法对多智能体的运动进行置换不变性学习等问题。<br>
                    方法：提出了一种基于扩散的运动表示——MotionDiffuser，该模型通过学习高度多元的分布来捕捉未来的多种可能结果，采用简单的预测器设计，只需一个L2损失训练目标，无需依赖轨迹锚点，并能以置换不变的方式学习多个智能体的运动联合分布。同时，利用PCA压缩轨迹表示，提高了模型性能并允许高效计算精确样本概率。进一步提出了一种通用的约束采样框架，使得可以根据不同的可微成本函数进行受控的轨迹采样。<br>
                    效果：MotionDiffuser在Waymo开放运动数据集上的多智能体运动预测任务上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present MotionDiffuser, a diffusion based representation for the joint distribution of future trajectories over multiple agents. Such representation has several key advantages: first, our model learns a highly multimodal distribution that captures diverse future outcomes. Second, the simple predictor design requires only a single L2 loss training objective, and does not depend on trajectory anchors. Third, our model is capable of learning the joint distribution for the motion of multiple agents in a permutation-invariant manner. Furthermore, we utilize a compressed trajectory representation via PCA, which improves model performance and allows for efficient computation of the exact sample log probability. Subsequently, we propose a general constrained sampling framework that enables controlled trajectory sampling based on differentiable cost functions. This strategy enables a host of applications such as enforcing rules and physical priors, or creating tailored simulation scenarios. MotionDiffuser can be combined with existing backbone architectures to achieve top motion forecasting results. We obtain state-of-the-art results for multi-agent motion prediction on the Waymo Open Motion Dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">629.TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization</span><br>
                <span class="as">Liu, ZiquanandXu, YiandJi, XiangyangandChan, AntoniB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_TWINS_A_Fine-Tuning_Framework_for_Improved_Transferability_of_Adversarial_Robustness_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16436-16446.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地利用预训练模型在对抗性鲁棒性上的潜在价值，特别是在各种分类任务中的微调。<br>
                    动机：现有的研究表明，由于健壮的预训练模型已经学习了一个健壮的特征提取器，因此关键问题是如何在学习下游任务时保持预训练模型的鲁棒性。<br>
                    方法：我们研究了基于模型和数据的方法来实现这个目标，并发现这两种常见的方法无法同时提高泛化能力和对抗性鲁棒性。因此，我们提出了一种新的基于统计的方法——TWINS微调框架，它由两个神经网络组成，其中一个在批量归一化层中保持预训练数据的种群均值和方差。<br>
                    效果：TWINS不仅有效地转移了健壮信息，还提高了有效学习率，因为标准批量归一化层中的权重范数和梯度范数之间的关系被打破，从而更快地跳出次优初始化并减轻健壮过拟合。最后，TWINS在广泛的图像分类数据集上显示出在泛化和鲁棒性方面的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent years have seen the ever-increasing importance of pre-trained models and their downstream training in deep learning research and applications. At the same time, the defense for adversarial examples has been mainly investigated in the context of training from random initialization on simple classification tasks. To better exploit the potential of pre-trained models in adversarial robustness, this paper focuses on the fine-tuning of an adversarially pre-trained model in various classification tasks. Existing research has shown that since the robust pre-trained model has already learned a robust feature extractor, the crucial question is how to maintain the robustness in the pre-trained model when learning the downstream task. We study the model-based and data-based approaches for this goal and find that the two common approaches cannot achieve the objective of improving both generalization and adversarial robustness. Thus, we propose a novel statistics-based approach, Two-WIng NormliSation (TWINS) fine-tuning framework, which consists of two neural networks where one of them keeps the population means and variances of pre-training data in the batch normalization layers. Besides the robust information transfer, TWINS increases the effective learning rate without hurting the training stability since the relationship between a weight norm and its gradient norm in standard batch normalization layer is broken, resulting in a faster escape from the sub-optimal initialization and alleviating the robust overfitting. Finally, TWINS is shown to be effective on a wide range of image classification datasets in terms of both generalization and robustness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">630.Open-Set Semantic Segmentation for Point Clouds via Adversarial Prototype Framework</span><br>
                <span class="as">Li, JiananandDong, Qiulei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Open-Set_Semantic_Segmentation_for_Point_Clouds_via_Adversarial_Prototype_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9425-9434.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何识别训练集中未出现的3D对象类别，同时保持对已出现类别的分割性能。<br>
                    动机：大部分现有文献假设训练和测试点云具有相同的对象类别，但在许多真实场景中，这通常是无效的。<br>
                    方法：提出一种对抗原型框架（APF）来处理开放集3D语义分割任务，该框架由特征提取模块、原型约束模块和特征对抗模块组成。<br>
                    效果：实验结果表明，提出的APF在大多数情况下比比较方法有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, point cloud semantic segmentation has attracted much attention in computer vision. Most of the existing works in literature assume that the training and testing point clouds have the same object classes, but they are generally invalid in many real-world scenarios for identifying the 3D objects whose classes are not seen in the training set. To address this problem, we propose an Adversarial Prototype Framework (APF) for handling the open-set 3D semantic segmentation task, which aims to identify 3D unseen-class points while maintaining the segmentation performance on seen-class points. The proposed APF consists of a feature extraction module for extracting point features, a prototypical constraint module, and a feature adversarial module. The prototypical constraint module is designed to learn prototypes for each seen class from point features. The feature adversarial module utilizes generative adversarial networks to estimate the distribution of unseen-class features implicitly, and the synthetic unseen-class features are utilized to prompt the model to learn more effective point features and prototypes for discriminating unseen-class samples from the seen-class ones. Experimental results on two public datasets demonstrate that the proposed APF outperforms the comparative methods by a large margin in most cases.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">631.CR-FIQA: Face Image Quality Assessment by Learning Sample Relative Classifiability</span><br>
                <span class="as">Boutros, FadiandFang, MeilingandKlemt, MarcelandFu, BiyingandDamer, Naser</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5836-5845.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的面部图像质量评估（FIQA）方法，通过学习预测样本的相对可分类性来估计面部图像的质量。<br>
                    动机：现有的面部图像质量评估方法无法准确反映图像在实现可靠和准确识别性能方面的效用。<br>
                    方法：该方法通过在学习过程中探测内部网络观察结果并利用其预测未见过样本的质量，来学习训练样本特征表示在角度空间中相对于其类别中心和最近的负类别中心的分配与面部图像质量之间的相关性。<br>
                    效果：通过在八个基准和四个人脸识别模型上进行广泛的评估实验，证明了所提出的CR-FIQA方法优于最先进的FIQA算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face image quality assessment (FIQA) estimates the utility of the captured image in achieving reliable and accurate recognition performance. This work proposes a novel FIQA method, CR-FIQA, that estimates the face image quality of a sample by learning to predict its relative classifiability. This classifiability is measured based on the allocation of the training sample feature representation in angular space with respect to its class center and the nearest negative class center. We experimentally illustrate the correlation between the face image quality and the sample relative classifiability. As such property is only observable for the training dataset, we propose to learn this property by probing internal network observations during the training process and utilizing it to predict the quality of unseen samples. Through extensive evaluation experiments on eight benchmarks and four face recognition models, we demonstrate the superiority of our proposed CR-FIQA over state-of-the-art (SOTA) FIQA algorithms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">632.MetaViewer: Towards a Unified Multi-View Representation</span><br>
                <span class="as">Wang, RenandSun, HaoliangandMa, YulingandXi, XiaomingandYin, Yilong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MetaViewer_Towards_a_Unified_Multi-View_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11590-11599.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多视角表示学习方法通常遵循从特定到统一的管道，提取每个视角的潜在特征，然后融合或对齐它们以获得统一的对象表示。然而，手动预指定的融合函数和对齐标准可能会降低所得到表示的质量。<br>
                    动机：为了克服这个问题，我们提出了一种新的从元学习角度看的从统一到特定的多视角学习框架。在这个框架中，统一表示不再涉及手动操作，而是由一个名为MetaViewer的元学习器自动生成。<br>
                    方法：我们将视图特定潜在特征的提取和融合形式化为一个嵌套优化问题，并使用双层优化方案来解决它。通过这种方式，MetaViewer自动将视图特定特征融合成一个统一的表示，并通过观察所有视图从统一到特定的重构过程来学习最优的融合方案。<br>
                    效果：在下游分类和聚类任务中的大量实验结果表明了所提出方法的效率和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing multi-view representation learning methods typically follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specified fusion functions and aligning criteria could potentially degrade the quality of the derived representation. To overcome them, we propose a novel uniform-to-specific multi-view learning framework from a meta-learning perspective, where the unified representation no longer involves manual manipulation but is automatically derived from a meta-learner named MetaViewer. Specifically, we formulated the extraction and fusion of view-specific latent features as a nested optimization problem and solved it by using a bi-level optimization scheme. In this way, MetaViewer automatically fuses view-specific features into a unified one and learns the optimal fusion scheme by observing reconstruction processes from the unified to the specific over all views. Extensive experimental results in downstream classification and clustering tasks demonstrate the efficiency and effectiveness of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">633.Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction</span><br>
                <span class="as">Chen, GuangyiandChen, ZhenhaoandFan, ShunxingandZhang, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Unsupervised_Sampling_Promoting_for_Stochastic_Human_Trajectory_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17874-17884.png><br>
            
            <span class="tt"><span class="t0">研究问题：人类运动的不确定性需要轨迹预测系统使用概率模型来形成多模态现象，并推断出有限的未来轨迹。<br>
                    动机：大多数现有方法的推理过程依赖于蒙特卡罗随机采样，由于预测分布的长尾效应，这不足以覆盖有限样本的现实路径。<br>
                    方法：我们提出了一种名为BOsampler的新方法，通过贝叶斯优化在无监督的方式下自适应地挖掘潜在路径，作为一种新的序列设计策略，新的预测依赖于先前抽取的样本。<br>
                    效果：我们在各种基线方法上进行实验，结果表明我们的方法有效。源代码已在此链接中发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The indeterminate nature of human motion requires trajectory prediction systems to use a probabilistic model to formulate the multi-modality phenomenon and infer a finite set of future trajectories. However, the inference processes of most existing methods rely on Monte Carlo random sampling, which is insufficient to cover the realistic paths with finite samples, due to the long tail effect of the predicted distribution. To promote the sampling process of stochastic prediction, we propose a novel method, called BOsampler, to adaptively mine potential paths with Bayesian optimization in an unsupervised manner, as a sequential design strategy in which new prediction is dependent on the previously drawn samples. Specifically, we model the trajectory sampling as a Gaussian process and construct an acquisition function to measure the potential sampling value. This acquisition function applies the original distribution as prior and encourages exploring paths in the long-tail region. This sampling method can be integrated with existing stochastic predictive models without retraining. Experimental results on various baseline methods demonstrate the effectiveness of our method. The source code is released in this link.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">634.Robust Generalization Against Photon-Limited Corruptions via Worst-Case Sharpness Minimization</span><br>
                <span class="as">Huang, ZhuoandZhu, MiaoxiandXia, XiaoboandShen, LiandYu, JunandGong, ChenandHan, BoandDu, BoandLiu, Tongliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Robust_Generalization_Against_Photon-Limited_Corruptions_via_Worst-Case_Sharpness_Minimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16175-16185.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使预训练语言模型更好地利用结构化知识，以提升语言理解能力？<br>
                    动机：目前的预训练语言模型在处理丰富的结构化知识方面存在不足，而知识图谱中的有信息量的实体可以增强语言表示。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，该模型通过联合训练大规模文本语料库和知识图谱来捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust generalization aims to tackle the most challenging data distributions which are rare in the training set and contain severe noises, i.e., photon-limited corruptions. Common solutions such as distributionally robust optimization (DRO) focus on the worst-case empirical risk to ensure low training error on the uncommon noisy distributions. However, due to the over-parameterized model being optimized on scarce worst-case data, DRO fails to produce a smooth loss landscape, thus struggling on generalizing well to the test set. Therefore, instead of focusing on the worst-case risk minimization, we propose SharpDRO by penalizing the sharpness of the worst-case distribution, which measures the loss changes around the neighbor of learning parameters. Through worst-case sharpness minimization, the proposed method successfully produces a flat loss curve on the corrupted distributions, thus achieving robust generalization. Moreover, by considering whether the distribution annotation is available, we apply SharpDRO to two problem settings and design a worst-case selection process for robust generalization. Theoretically, we show that SharpDRO has a great convergence guarantee. Experimentally, we simulate photon-limited corruptions using CIFAR10/100 and ImageNet30 datasets and show that SharpDRO exhibits a strong generalization ability against severe corruptions and exceeds well-known baseline methods with large performance gains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">635.NICO++: Towards Better Benchmarking for Domain Generalization</span><br>
                <span class="as">Zhang, XingxuanandHe, YueandXu, RenzheandYu, HanandShen, ZheyanandCui, Peng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16036-16047.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管现代深度神经网络在独立同分布数据上取得了显著的性能，但在分布转移下可能会崩溃。<br>
                    动机：目前大多数领域泛化（DG）的评估方法采用留一策略作为对有限数量领域的妥协。<br>
                    方法：提出了一个大规模的标记领域基准测试NICO++以及更合理的评估方法，用于全面评估DG算法。<br>
                    效果：通过大量实验，NICO++显示出其优越的评估能力，优于目前的DG数据集，并在减轻模型选择中泄露的先知知识引起的不公平性方面做出了贡献。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable performance that modern deep neural networks have achieved on independent and identically distributed (I.I.D.) data, they can crash under distribution shifts. Most current evaluation methods for domain generalization (DG) adopt the leave-one-out strategy as a compromise on the limited number of domains. We propose a large-scale benchmark with extensive labeled domains named NICO++ along with more rational evaluation methods for comprehensively evaluating DG algorithms. To evaluate DG datasets, we propose two metrics to quantify covariate shift and concept shift, respectively. Two novel generalization bounds from the perspective of data construction are proposed to prove that limited concept shift and significant covariate shift favor the evaluation capability for generalization. Through extensive experiments, NICO++ shows its superior evaluation capability compared with current DG datasets and its contribution in alleviating unfairness caused by the leak of oracle knowledge in model selection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">636.Neural Dependencies Emerging From Learning Massive Categories</span><br>
                <span class="as">Feng, RuiliandZheng, KechengandZhu, KaiandShen, YujunandZhao, JianandHuang, YukunandZhao, DeliandZhou, JingrenandJordan, MichaelandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Neural_Dependencies_Emerging_From_Learning_Massive_Categories_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11711-11720.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了大规模图像分类神经网络中的两个惊人的发现，即神经依赖性的存在及其在模型间和模型内的表现。<br>
                    动机：作者发现，在一个训练良好的模型中，某些类别的预测结果可以通过线性组合其他几个类别的预测结果直接获得，这种现象被称为神经依赖性。并且这种神经依赖性不仅存在于单个模型中，也存在于独立学习的两个模型之间。<br>
                    方法：通过将识别神经依赖性的问题等价为解决协方差Lasso回归问题，作者们对此现象进行了理论分析。并通过研究问题解决方案的性质，确认了神经依赖性是由冗余对数协方差矩阵保证的。<br>
                    效果：实验结果表明，神经依赖性在理解内部数据关联、推广模型到未见过的类别以及通过依赖性导出的正则化提高模型鲁棒性方面具有潜力。作者还计划公开发布能够精确重现这项工作结果的代码。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work presents two astonishing findings on neural networks learned for large-scale image classification. 1) Given a well-trained model, the logits predicted for some category can be directly obtained by linearly combining the predictions of a few other categories, which we call neural dependency. 2) Neural dependencies exist not only within a single model, but even between two independently learned models, regardless of their architectures. Towards a theoretical analysis of such phenomena, we demonstrate that identifying neural dependencies is equivalent to solving the Covariance Lasso (CovLasso) regression problem proposed in this paper. Through investigating the properties of the problem solution, we confirm that neural dependency is guaranteed by a redundant logit covariance matrix, which condition is easily met given massive categories, and that neural dependency is sparse, which implies one category relates to only a few others. We further empirically show the potential of neural dependencies in understanding internal data correlations, generalizing models to unseen categories, and improving model robustness with a dependency-derived regularize. Code to exactly reproduce the results in this work will be released publicly.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">637.Constrained Evolutionary Diffusion Filter for Monocular Endoscope Tracking</span><br>
                <span class="as">Luo, Xiongbiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Constrained_Evolutionary_Diffusion_Filter_for_Monocular_Endoscope_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4747-4756.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善现有随机滤波方法在非线性优化问题上的探索与利用之间的不平衡。<br>
                    动机：现有的随机滤波方法由于粒子退化和贫化，导致局部最优解的问题，需要解决探索与利用的平衡问题。<br>
                    方法：提出一种新的约束进化扩散滤波器，通过开发空间状态约束和自适应历史回忆差异进化嵌入的进化随机扩散，以解决退化和贫化问题。<br>
                    效果：在单眼内窥镜3D跟踪的应用中，实验结果表明，提出的滤波器显著改善了探索与利用之间的平衡，比近期的3D跟踪方法效果更好，手术跟踪误差从4.03mm降低到2.59mm。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Stochastic filtering is widely used to deal with nonlinear optimization problems such as 3-D and visual tracking in various computer vision and augmented reality applications. Many current methods suffer from an imbalance between exploration and exploitation due to their particle degeneracy and impoverishment, resulting in local optimums. To address this imbalance, this work proposes a new constrained evolutionary diffusion filter for nonlinear optimization. Specifically, this filter develops spatial state constraints and adaptive history-recall differential evolution embedded evolutionary stochastic diffusion instead of sequential resampling to resolve the degeneracy and impoverishment problem. With application to monocular endoscope 3-D tracking, the experimental results show that the proposed filtering significantly improves the balance between exploration and exploitation and certainly works better than recent 3-D tracking methods. Particularly, the surgical tracking error was reduced from 4.03 mm to 2.59 mm.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">638.Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective</span><br>
                <span class="as">Zhu, JinjingandBai, HaotianandWang, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Patch-Mix_Transformer_for_Unsupervised_Domain_Adaptation_A_Game_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3561-3571.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行无监督领域适应（UDA）任务。<br>
                    动机：现有的使用视觉转换器（ViT）进行UDA的方法，在目标样本的伪标签质量不高时，效果会大打折扣。<br>
                    方法：提出一种名为PMTrans的新模型，通过构建一个中间域来连接源域和目标域。具体来说，提出了一种新的基于ViT的模块PatchMix，通过学习从两个域中采样补丁，根据博弈论模型建立中间域，即概率分布。<br>
                    效果：在四个基准数据集上进行的大量实验表明，PMTrans显著优于基于ViT和CNN的最新方法，分别在Office-Home、Office-31和DomainNet上提高了+3.6%、+1.4%和+17.7%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Endeavors have been recently made to leverage the vision transformer (ViT) for the challenging unsupervised domain adaptation (UDA) task. They typically adopt the cross-attention in ViT for direct domain alignment. However, as the performance of cross-attention highly relies on the quality of pseudo labels for targeted samples, it becomes less effective when the domain gap becomes large. We solve this problem from a game theory's perspective with the proposed model dubbed as PMTrans, which bridges source and target domains with an intermediate domain. Specifically, we propose a novel ViT-based module called PatchMix that effectively builds up the intermediate domain, i.e., probability distribution, by learning to sample patches from both domains based on the game-theoretical models. This way, it learns to mix the patches from the source and target domains to maximize the cross entropy (CE), while exploiting two semi-supervised mixup losses in the feature and label spaces to minimize it. As such, we interpret the process of UDA as a min-max CE game with three players, including the feature extractor, classifier, and PatchMix, to find the Nash Equilibria. Moreover, we leverage attention maps from ViT to re-weight the label of each patch by its importance, making it possible to obtain more domain-discriminative feature representations. We conduct extensive experiments on four benchmark datasets, and the results show that PMTrans significantly surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home, +1.4% on Office-31, and +17.7% on DomainNet, respectively. https://vlis2022.github.io/cvpr23/PMTrans</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">639.Improving Selective Visual Question Answering by Learning From Your Peers</span><br>
                <span class="as">Dancette, CorentinandWhitehead, SpencerandMaheshwary, RishabhandVedantam, RamakrishnaandScherer, StefanandChen, XinleiandCord, MatthieuandRohrbach, Marcus</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dancette_Improving_Selective_Visual_Question_Answering_by_Learning_From_Your_Peers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24049-24059.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管视觉问答（VQA）取得了进展，但模型评估自身正确性的能力仍未得到充分探索。<br>
                    动机：最近的研究表明，VQA模型在面对错误时往往难以选择不回答。这种选择性预测（Selective Prediction）在部署系统给用户时非常重要，例如为视障人士提供VQA助手。对于这些场景，当用户可能提供分布外（OOD）或对抗性输入使得错误答案更有可能时，不回答的选择尤为重要。<br>
                    方法：本研究在分布内（ID）和OOD场景中探索了选择性VQA，其中模型被呈现ID和OOD数据的混合。我们提出了一种简单而有效的从你的同龄人（LYP）学习的方法来训练多模态选择函数以做出不回答的决定。这种方法使用从不同训练数据子集上训练的模型的预测作为优化选择性VQA模型的目标，无需额外的手动标签或保留的数据。<br>
                    效果：在我们的广泛评估中，我们在选择性预测指标覆盖度上达到了32.92% C@1%，这比该任务上之前的最佳覆盖度15.79%提高了一倍。对于混合ID/OOD，即使只有10%的OOD示例，使用模型的softmax置信度进行不回答决策的性能非常差，C@1%的风险仅为5%，但使用LYP学习的选择函数可以将这一比例提高到25.38%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite advances in Visual Question Answering (VQA), the ability of models to assess their own correctness remains underexplored. Recent work has shown that VQA models, out-of-the-box, can have difficulties abstaining from answering when they are wrong. The option to abstain, also called Selective Prediction, is highly relevant when deploying systems to users who must trust the system's output (e.g., VQA assistants for users with visual impairments). For such scenarios, abstention can be especially important as users may provide out-of-distribution (OOD) or adversarial inputs that make incorrect answers more likely. In this work, we explore Selective VQA in both in-distribution (ID) and OOD scenarios, where models are presented with mixtures of ID and OOD data. The goal is to maximize the number of questions answered while minimizing the risk of error on those questions. We propose a simple yet effective Learning from Your Peers (LYP) approach for training multimodal selection functions for making abstention decisions. Our approach uses predictions from models trained on distinct subsets of the training data as targets for optimizing a Selective VQA model. It does not require additional manual labels or held-out data and provides a signal for identifying examples that are easy/difficult to generalize to. In our extensive evaluations, we show this benefits a number of models across different architectures and scales. Overall, for ID, we reach 32.92% in the selective prediction metric coverage at 1% risk of error (C@1%) which doubles the previous best coverage of 15.79% on this task. For mixed ID/OOD, using models' softmax confidences for abstention decisions performs very poorly, answering <5% of questions at 1% risk of error even when faced with only 10% OOD examples, but a learned selection function with LYP can increase that to 25.38% C@1%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">640.On Calibrating Semantic Segmentation Models: Analyses and an Algorithm</span><br>
                <span class="as">Wang, DongdongandGong, BoqingandWang, Liqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_Calibrating_Semantic_Segmentation_Models_Analyses_and_an_Algorithm_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23652-23662.png><br>
            
            <span class="tt"><span class="t0">研究问题：语义分割模型的校准问题。<br>
                    动机：尽管图像分类模型的置信度校准问题已有许多解决方案，但语义分割模型的置信度校准研究仍然有限。<br>
                    方法：我们提出了一种简单而有效的方法——选择性缩放，通过区分正确/错误的预测进行缩放，并更关注误预测的逻辑平滑。<br>
                    效果：在各种基准测试中，无论是在领域内还是领域转移的校准上，我们的选择性缩放方法都优于其他方法，表现出了一致的优秀性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of semantic segmentation calibration. Lots of solutions have been proposed to approach model miscalibration of confidence in image classification. However, to date, confidence calibration research on semantic segmentation is still limited. We provide a systematic study on the calibration of semantic segmentation models and propose a simple yet effective approach. First, we find that model capacity, crop size, multi-scale testing, and prediction correctness have impact on calibration. Among them, prediction correctness, especially misprediction, is more important to miscalibration due to over-confidence. Next, we propose a simple, unifying, and effective approach, namely selective scaling, by separating correct/incorrect prediction for scaling and more focusing on misprediction logit smoothing. Then, we study popular existing calibration methods and compare them with selective scaling on semantic segmentation calibration. We conduct extensive experiments with a variety of benchmarks on both in-domain and domain-shift calibration and show that selective scaling consistently outperforms other methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">641.Transductive Few-Shot Learning With Prototype-Based Label Propagation by Iterative Graph Refinement</span><br>
                <span class="as">Zhu, HaoandKoniusz, Piotr</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23996-24006.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改进少次学习（Few-shot learning）中的原型和图结构方法，以提高其在新类别适应上的性能。<br>
                    动机：现有的原型和图结构方法在原型估计和图构建中存在不准确和次优的问题，这影响了性能。<br>
                    方法：提出一种新的原型传播标签方法，通过原型与样本之间的关系而非样本之间的关系来构建图，并在更新原型时改变图。同时，对每个原型的标签进行估计，而不是将原型视为类中心。<br>
                    效果：在Mini-ImageNet、Tiered-ImageNet、CIFAR-FS和CUB数据集上，该方法在转导式FSL和半监督FSL中优于其他最先进的方法，特别是在有未标记数据伴随新少数几次任务时。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot learning (FSL) is popular due to its ability to adapt to novel classes. Compared with inductive few-shot learning, transductive models typically perform better as they leverage all samples of the query set. The two existing classes of methods, prototype-based and graph-based, have the disadvantages of inaccurate prototype estimation and sub-optimal graph construction with kernel functions, respectively. %, which hurt the performance. In this paper, we propose a novel prototype-based label propagation to solve these issues. Specifically, our graph construction is based on the relation between prototypes and samples rather than between samples. As prototypes are being updated, the graph changes.We also estimate the label of each prototype instead of considering a prototype be the class centre. On mini-ImageNet, tiered-ImageNet, CIFAR-FS and CUB datasets, we show the proposed method outperforms other state-of-the-art methods in transductive FSL and semi-supervised FSL when some unlabeled data accompanies the novel few-shot task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">642.Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations</span><br>
                <span class="as">Michaeli, HagayandMichaeli, TomerandSoudry, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Michaeli_Alias-Free_Convnets_Fractional_Shift_Invariance_via_Polynomial_Activations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16333-16342.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管卷积神经网络（CNN）被认为是对翻译不变的，但最近的研究表明，由于下采样层产生的混叠效应，这并非如此。<br>
                    动机：现有的防止混叠效应的架构解决方案是部分的，因为它们没有解决源于非线性层的混叠效应。<br>
                    方法：我们提出了一种扩展的反混叠方法，该方法解决了下采样和非线性层的问题，从而创建了真正无混叠、平移不变的CNNs。<br>
                    效果：我们展示出这种模型对于整数以及分数（即亚像素）的平移都是不变的，因此在对抗性平移方面的鲁棒性上超过了其他平移不变的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although CNNs are believed to be invariant to translations, recent works have shown this is not the case due to aliasing effects that stem from down-sampling layers. The existing architectural solutions to prevent the aliasing effects are partial since they do not solve those effects that originate in non-linearities. We propose an extended anti-aliasing method that tackles both down-sampling and non-linear layers, thus creating truly alias-free, shift-invariant CNNs. We show that the presented model is invariant to integer as well as fractional (i.e., sub-pixel) translations, thus outperforming other shift-invariant methods in terms of robustness to adversarial translations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">643.Initialization Noise in Image Gradients and Saliency Maps</span><br>
                <span class="as">Woerl, Ann-ChristinandDisselhoff, JanandWand, Michael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Woerl_Initialization_Noise_in_Image_Gradients_and_Saliency_Maps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1766-1775.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了图像分类CNNs的logits梯度与输入像素值的关系。<br>
                    动机：我们发现这些梯度会因训练随机性（如网络的随机初始化）而发生大幅度波动。<br>
                    方法：我们将研究扩展到通过GradCAM获得的中间层梯度，以及流行的网络显著性估计器，如DeepLIFT、SHAP、LIME、Integrated Gradients和SmoothGrad。<br>
                    效果：虽然经验噪声水平有所不同，但所有这些都可以对图像特征进行定性不同的归因，这对解释这些归因具有影响，特别是在寻求数据驱动的解释时。最后，我们证明可以通过简单的随机积分对初始化分布进行边缘化来消除观察到的artifacts。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we examine gradients of logits of image classification CNNs by input pixel values. We observe that these fluctuate considerably with training randomness, such as the random initialization of the networks. We extend our study to gradients of intermediate layers, obtained via GradCAM, as well as popular network saliency estimators such as DeepLIFT, SHAP, LIME, Integrated Gradients, and SmoothGrad. While empirical noise levels vary, qualitatively different attributions to image features are still possible with all of these, which comes with implications for interpreting such attributions, in particular when seeking data-driven explanations of the phenomenon generating the data. Finally, we demonstrate that the observed artefacts can be removed by marginalization over the initialization distribution by simple stochastic integration.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">644.Curricular Object Manipulation in LiDAR-Based Object Detection</span><br>
                <span class="as">Zhu, ZiyueandMeng, QiangandWang, XiaoandWang, KeandYan, LiujiangandYang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Curricular_Object_Manipulation_in_LiDAR-Based_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1125-1135.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索课程学习在基于激光雷达的3D物体检测中的潜力。<br>
                    动机：通过提出一个课程物体操作（COM）框架，将课程训练策略嵌入损失设计和增强过程中，以提高模型性能和泛化能力。<br>
                    方法：在损失设计中，提出了COMLoss来动态预测物体级别的难度，并根据训练阶段强调不同难度的物体。在常用的LiDAR检测任务增强技术GT-Aug的基础上，提出了一种新的COMAug策略，该策略首先根据精心设计的启发式方法对地面真实数据库中的物体进行聚类，然后预测并更新训练期间的组级别难度，以获得稳定的结果。<br>
                    效果：通过逐步增加更困难物体的采样和增强到训练点，可以改善模型性能和泛化能力。广泛的实验和消融研究表明了所提出的框架的优势和通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper explores the potential of curriculum learning in LiDAR-based 3D object detection by proposing a curricular object manipulation (COM) framework. The framework embeds the curricular training strategy into both the loss design and the augmentation process. For the loss design, we propose the COMLoss to dynamically predict object-level difficulties and emphasize objects of different difficulties based on training stages. On top of the widely-used augmentation technique called GT-Aug in LiDAR detection tasks, we propose a novel COMAug strategy which first clusters objects in ground-truth database based on well-designed heuristics. Group-level difficulties rather than individual ones are then predicted and updated during training for stable results. Model performance and generalization capabilities can be improved by sampling and augmenting progressively more difficult objects into the training points. Extensive experiments and ablation studies reveal the superior and generality of the proposed framework. The code is available at https://github.com/ZZY816/COM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">645.Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking</span><br>
                <span class="as">Tu, YuanpengandZhang, BoshenandLi, YuxiandLiu, LiangandLi, JianandZhang, JiangningandWang, YabiaoandWang, ChengjieandZhao, CaiRong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Learning_With_Noisy_Labels_via_Self-Supervised_Adversarial_Noisy_Masking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16186-16195.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地处理训练深度学习模型时由标注数据产生的噪声标签。<br>
                    动机：目前的处理噪声标签的方法主要是识别和移除噪声样本或根据训练样本的统计特性（如损失值）来修正其标签，但这些方法效果有限。<br>
                    方法：提出一种名为对抗性噪声掩蔽的新型鲁棒训练方法。该方法通过一个以标签质量为导向的掩蔽方案对深层特征进行正则化，自适应地同时调整输入数据和标签，防止模型过拟合噪声样本。此外，设计了一个辅助任务来重建输入数据，为深度模型提供无噪声的自我监督信号，从而增强其泛化能力。<br>
                    效果：在合成和真实世界的噪声数据集上测试了该方法，结果证明其在性能上显著优于现有的最先进方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Collecting large-scale datasets is crucial for training deep models, annotating the data, however, inevitably yields noisy labels, which poses challenges to deep learning algorithms. Previous efforts tend to mitigate this problem via identifying and removing noisy samples or correcting their labels according to the statistical properties (e.g., loss values) among training samples. In this paper, we aim to tackle this problem from a new perspective, delving into the deep feature maps, we empirically find that models trained with clean and mislabeled samples manifest distinguishable activation feature distributions. From this observation, a novel robust training approach termed adversarial noisy masking is proposed. The idea is to regularize deep features with a label quality guided masking scheme, which adaptively modulates the input data and label simultaneously, preventing the model to overfit noisy samples. Further, an auxiliary task is designed to reconstruct input data, it naturally provides noise-free self-supervised signals to reinforce the generalization ability of deep models. The proposed method is simple and flexible, it is tested on both synthetic and real-world noisy datasets, where significant improvements are achieved over previous state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">646.Instance-Aware Domain Generalization for Face Anti-Spoofing</span><br>
                <span class="as">Zhou, QianyuandZhang, Ke-YueandYao, TaipingandLu, XuequanandYi, RanandDing, ShouhongandMa, Lizhuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Instance-Aware_Domain_Generalization_for_Face_Anti-Spoofing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20453-20463.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高面部反欺诈系统在未见过的场景中的泛化能力。<br>
                    动机：现有的基于领域泛化的面部反欺诈方法主要依赖人工标注的领域标签来对齐每个领域的分布，但这种方法存在粗糙和主观的问题，无法准确反映真实的领域分布。<br>
                    方法：提出了一种无需领域标签，在实例级别对齐特征的新视角。具体来说，提出了一个名为“实例感知领域泛化”的框架，通过减弱特征对实例特定风格的敏感性来学习可泛化的特征。<br>
                    效果：实验结果和分析表明，该方法优于最先进的竞争对手。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face anti-spoofing (FAS) based on domain generalization (DG) has been recently studied to improve the generalization on unseen scenarios. Previous methods typically rely on domain labels to align the distribution of each domain for learning domain-invariant representations. However, artificial domain labels are coarse-grained and subjective, which cannot reflect real domain distributions accurately. Besides, such domain-aware methods focus on domain-level alignment, which is not fine-grained enough to ensure that learned representations are insensitive to domain styles. To address these issues, we propose a novel perspective for DG FAS that aligns features on the instance level without the need for domain labels. Specifically, Instance-Aware Domain Generalization framework is proposed to learn the generalizable feature by weakening the features' sensitivity to instance-specific styles. Concretely, we propose Asymmetric Instance Adaptive Whitening to adaptively eliminate the style-sensitive feature correlation, boosting the generalization. Moreover, Dynamic Kernel Generator and Categorical Style Assembly are proposed to first extract the instance-specific features and then generate the style-diversified features with large style shifts, respectively, further facilitating the learning of style-insensitive features. Extensive experiments and analysis demonstrate the superiority of our method over state-of-the-art competitors. Code will be publicly available at this link: https://github.com/qianyuzqy/IADG.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">647.Towards Domain Generalization for Multi-View 3D Object Detection in Bird-Eye-View</span><br>
                <span class="as">Wang, ShuoandZhao, XinhaiandXu, Hai-MingandChen, ZehuiandYu, DamengandChang, JiahaoandYang, ZhenandZhao, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Domain_Generalization_for_Multi-View_3D_Object_Detection_in_Bird-Eye-View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13333-13342.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低多视角三维物体检测（MV3D-Det）在输入图像领域与训练领域不同的情况下的性能下降。<br>
                    动机：大多数现有的仅依赖摄像头的三维物体检测算法，当输入图像领域与训练领域不同时，可能会面临性能急剧下降的风险。<br>
                    方法：通过将深度预测从相机的内在参数（即焦距）中解耦，并执行动态透视增强以增加外在参数（即相机姿态）的多样性，来获取稳健的深度预测。此外，修改焦距值以创建多个伪领域，并构建对抗性训练损失以鼓励特征表示更具领域不变性。<br>
                    效果：在未看到的目标领域中成功减轻了性能下降，同时没有损害源领域的精度。在Waymo、nuScenes和Lyft等数据集上的大量实验证明了该方法的泛化性和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-view 3D object detection (MV3D-Det) in Bird-Eye-View (BEV) has drawn extensive attention due to its low cost and high efficiency. Although new algorithms for camera-only 3D object detection have been continuously proposed, most of them may risk drastic performance degradation when the domain of input images differs from that of training. In this paper, we first analyze the causes of the domain gap for the MV3D-Det task. Based on the covariate shift assumption, we find that the gap mainly attributes to the feature distribution of BEV, which is determined by the quality of both depth estimation and 2D image's feature representation. To acquire a robust depth prediction, we propose to decouple the depth estimation from the intrinsic parameters of the camera (i.e. the focal length) through converting the prediction of metric depth to that of scale-invariant depth and perform dynamic perspective augmentation to increase the diversity of the extrinsic parameters (i.e. the camera poses) by utilizing homography. Moreover, we modify the focal length values to create multiple pseudo-domains and construct an adversarial training loss to encourage the feature representation to be more domain-agnostic. Without bells and whistles, our approach, namely DG-BEV, successfully alleviates the performance drop on the unseen target domain without impairing the accuracy of the source domain. Extensive experiments on Waymo, nuScenes, and Lyft, demonstrate the generalization and effectiveness of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">648.Robust and Scalable Gaussian Process Regression and Its Applications</span><br>
                <span class="as">Lu, YifanandMa, JiayiandFang, LeyuanandTian, XinandJiang, Junjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Robust_and_Scalable_Gaussian_Process_Regression_and_Its_Applications_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21950-21959.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将高斯过程回归（GPR）模型应用于大规模真实数据，特别是那些被异常值污染的数据。<br>
                    动机：现有的GPR模型在处理大规模和含有异常值的真实数据时存在困难。<br>
                    方法：本文提出了一种通过变分学习实现的鲁棒且可扩展的高斯过程回归模型。该模型采用混合似然模型来处理异常值，并推导出一种变分形式，通过最大化真实对数边际似然的下界来联合推断数据的模态（内点或外点）和超参数。<br>
                    效果：在两个具有挑战性的真实世界应用——特征匹配和密集基因表达填充中，实验表明，相比于现有的鲁棒GPR模型，新模型在鲁棒性和速度上都有显著优势。特别是在匹配4k个特征点时，其推理仅需几毫秒，且几乎没有错误匹配。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces a robust and scalable Gaussian process regression (GPR) model via variational learning. This enables the application of Gaussian processes to a wide range of real data, which are often large-scale and contaminated by outliers. Towards this end, we employ a mixture likelihood model where outliers are assumed to be sampled from a uniform distribution. We next derive a variational formulation that jointly infers the mode of data, i.e., inlier or outlier, as well as hyperparameters by maximizing a lower bound of the true log marginal likelihood. Compared to previous robust GPR, our formulation approximates the exact posterior distribution. The inducing variable approximation and stochastic variational inference are further introduced to our variational framework, extending our model to large-scale data. We apply our model to two challenging real-world applications, namely feature matching and dense gene expression imputation. Extensive experiments demonstrate the superiority of our model in terms of robustness and speed. Notably, when matching 4k feature points, its inference is completed in milliseconds with almost no false matches. The code is at https://github.com/YifanLu2000/Robust-Scalable-GPR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">649.Improving Robust Generalization by Direct PAC-Bayesian Bound Minimization</span><br>
                <span class="as">Wang, ZifanandDing, NanandLevinboim, TomerandChen, XiandSoricut, Radu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Improving_Robust_Generalization_by_Direct_PAC-Bayesian_Bound_Minimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16458-16468.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对抗攻击模型在训练集上表现出更高的鲁棒性，但在测试集上却表现出过拟合的现象。<br>
                    动机：尽管以前的工作通过对抗性测试误差的鲁棒PAC-贝叶斯边界提供了理论解释，但相关的算法推导与这个边界的联系并不紧密，这意味着他们的实证成功和我们对对抗性鲁棒性理论的理解之间存在差距。<br>
                    方法：本文考虑了鲁棒PAC-贝叶斯边界的不同形式，并直接最小化模型后验的期望。最优解的推导将PAC-贝叶斯学习与鲁棒损失表面的几何形状通过测量表面平坦度的迹Hessian（TrH）正则化器连接起来。<br>
                    效果：实验结果表明，TrH正则化可以提升ViT的鲁棒性，其性能要么与、要么超过了现有的最先进方法，同时需要的内存和计算成本更低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent research in robust optimization has shown an overfitting-like phenomenon in which models trained against adversarial attacks exhibit higher robustness on the training set compared to the test set. Although previous work provided theoretical explanations for this phenomenon using a robust PAC-Bayesian bound over the adversarial test error, related algorithmic derivations are at best only loosely connected to this bound, which implies that there is still a gap between their empirical success and our understanding of adversarial robustness theory. To close this gap, in this paper we consider a different form of the robust PAC-Bayesian bound and directly minimize it with respect to the model posterior. The derivation of the optimal solution connects PAC-Bayesian learning to the geometry of the robust loss surface through a Trace of Hessian (TrH) regularizer that measures the surface flatness. In practice, we restrict the TrH regularizer to the top layer only, which results in an analytical solution to the bound whose computational cost does not depend on the network depth. Finally, we evaluate our TrH regularization approach over CIFAR-10/100 and ImageNet using Vision Transformers (ViT) and compare against baseline adversarial robustness algorithms. Experimental results show that TrH regularization leads to improved ViT robustness that either matches or surpasses previous state-of-the-art approaches while at the same time requires less memory and computational cost.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">650.A Data-Based Perspective on Transfer Learning</span><br>
                <span class="as">Jain, SaachiandSalman, HadiandKhaddaj, AlaaandWong, EricandPark, SungMinandM\k{a</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_A_Data-Based_Perspective_on_Transfer_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3613-3622.png><br>
            
            <span class="tt"><span class="t0">研究问题：预训练数据对迁移学习性能的影响，以及如何通过改变源数据集的构成来提高迁移学习的性能。<br>
                    动机：尽管人们普遍认为更多的预训练数据可以提高迁移学习的性能，但最近的研究表明，从源数据集中删除某些数据实际上也可以帮助提高迁移学习的性能。<br>
                    方法：提出了一个框架来探索源数据集的构成对迁移学习性能的影响。该框架可以识别迁移学习的脆弱性，并检测源数据集中的数据泄露和误导性示例等问题。<br>
                    效果：实验表明，通过删除由该框架识别出的有害数据点，可以从ImageNet上的各种迁移任务中提高迁移学习的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It is commonly believed that more pre-training data leads to better transfer learning performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we present a framework for probing the impact of the source dataset's composition on transfer learning performance. Our framework facilitates new capabilities such as identifying transfer learning brittleness and detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer performance from ImageNet on a variety of transfer tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">651.Improved Test-Time Adaptation for Domain Generalization</span><br>
                <span class="as">Chen, LiangandZhang, YongandSong, YibingandShan, YingandLiu, Lingqiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24172-24182.png><br>
            
            <span class="tt"><span class="t0">研究问题：领域泛化（DG）的主要挑战在于处理训练数据和测试数据之间的分布偏移问题。<br>
                    动机：最近的研究表明，通过测试数据对已学习模型进行适应的测试时训练（TTT）可能是解决这个问题的一个有希望的方法。<br>
                    方法：本文提出了一种改进的测试时适应（ITTA）方法，该方法通过为TTT任务定义一个可学习的一致性损失来选择适当的辅助TTT任务，并引入额外的自适应参数，仅在测试阶段更新这些参数。<br>
                    效果：实验表明，这两种策略对已学习模型有益，ITTA可以在几个DG基准上实现优于当前最先进技术的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Generally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for updating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments indicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly considered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically defining an auxiliary objective, we propose a learnable consistency loss for the TTT task, which contains learnable parameters that can be adjusted toward better alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adaptive parameters during the test phase. Through extensive experiments, we show that the proposed two strategies are beneficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-arts on several DG benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">652.Adjustment and Alignment for Unbiased Open Set Domain Adaptation</span><br>
                <span class="as">Li, WuyangandLiu, JieandHan, BoandYuan, Yixuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adjustment_and_Alignment_for_Unbiased_Open_Set_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24110-24119.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将模型从标签丰富的领域转移到包含新类别样本的无标签领域，同时避免在新类别样本不可用的情况下产生的语义偏差。<br>
                    动机：现有的开放集领域适应（OSDA）工作忽视了源领域中隐藏的丰富新类别语义，导致模型学习偏颇和转移效果不佳。<br>
                    方法：提出了一种新颖的基于因果关系的解决方案，并利用前门调整理论进行实现，构建了一个名为Adjustment and Alignment (ANNA)的理论框架，以实现无偏的OSDA。ANNA由前门调整（FDA）和去耦因果对齐（DCA）两部分组成，前者通过深入细致的视觉块来发现隐藏在基本类别图像中的新类别区域，并通过实施因果关系去偏来纠正有偏的模型优化；后者则使用正交掩码分离基本类别和新类别区域，并对解耦分布进行调整，以实现无偏的模型转移。<br>
                    效果：大量实验表明，ANNA取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open Set Domain Adaptation (OSDA) transfers the model from a label-rich domain to a label-free one containing novel-class samples. Existing OSDA works overlook abundant novel-class semantics hidden in the source domain, leading to a biased model learning and transfer. Although the causality has been studied to remove the semantic-level bias, the non-available novel-class samples result in the failure of existing causal solutions in OSDA. To break through this barrier, we propose a novel causality-driven solution with the unexplored front-door adjustment theory, and then implement it with a theoretically grounded framework, coined AdjustmeNt aNd Alignment (ANNA), to achieve an unbiased OSDA. In a nutshell, ANNA consists of Front-Door Adjustment (FDA) to correct the biased learning in the source domain and Decoupled Causal Alignment (DCA) to transfer the model unbiasedly. On the one hand, FDA delves into fine-grained visual blocks to discover novel-class regions hidden in the base-class image. Then, it corrects the biased model optimization by implementing causal debiasing. On the other hand, DCA disentangles the base-class and novel-class regions with orthogonal masks, and then adapts the decoupled distribution for an unbiased model transfer. Extensive experiments show that ANNA achieves state-of-the-art results. The code is available at https://github.com/CityU-AIM-Group/Anna.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">653.Balancing Logit Variation for Long-Tailed Semantic Segmentation</span><br>
                <span class="as">Wang, YuchaoandFei, JingjingandWang, HaochenandLi, WeiandBao, TianpengandWu, LiweiandZhao, RuiandShen, Yujun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19561-19573.png><br>
            
            <span class="tt"><span class="t0">研究问题：语义分割中长尾数据分布的问题，即不同类别样本数量不平衡导致尾部类别的特征在特征空间中被挤压。<br>
                    动机：为了解决长尾数据分布问题，提出了一种在训练阶段引入类别间变化的网络预测方法，使实例不再被映射到一个特征点，而是一个小区域。<br>
                    方法：根据类别规模，为头部类别分配较小的变化，为尾部类别分配较大的变化，以此缩小不同类别特征区域的差距，实现更平衡的特征表示。值得注意的是，这种引入的变化在推理阶段会被丢弃，以便于进行准确的预测。<br>
                    效果：尽管这种方法的实现方式简单，但它在各种数据集和任务设置中表现出强大的泛化能力。大量实验表明，该方法可以很好地适用于一系列最新的方法，并在这些方法的基础上提高了性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic segmentation usually suffers from a long tail data distribution. Due to the imbalanced number of samples across categories, the features of those tail classes may get squeezed into a narrow area in the feature space. Towards a balanced feature distribution, we introduce category-wise variation into the network predictions in the training phase such that an instance is no longer projected to a feature point, but a small region instead. Such a perturbation is highly dependent on the category scale, which appears as assigning smaller variation to head classes and larger variation to tail classes. In this way, we manage to close the gap between the feature areas of different categories, resulting in a more balanced representation. It is noteworthy that the introduced variation is discarded at the inference stage to facilitate a confident prediction. Although with an embarrassingly simple implementation, our method manifests itself in strong generalizability to various datasets and task settings. Extensive experiments suggest that our plug-in design lends itself well to a range of state-of-the-art approaches and boosts the performance on top of them.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">654.Prompt-Guided Zero-Shot Anomaly Action Recognition Using Pretrained Deep Skeleton Features</span><br>
                <span class="as">Sato, FumiakiandHachiuma, RyoandSekii, Taiki</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sato_Prompt-Guided_Zero-Shot_Anomaly_Action_Recognition_Using_Pretrained_Deep_Skeleton_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6471-6480.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无监督异常动作识别的问题，即在没有异常样本的情况下，以无监督的方式识别视频级别的异常人类行为事件。<br>
                    动机：传统的基于骨架的方法存在三个局限性：目标领域依赖的DNN训练、对骨架错误的鲁棒性以及对正常样本的缺乏。<br>
                    方法：提出了一个统一的用户提示引导的零样本学习框架，使用目标领域独立的骨架特征提取器，该提取器在大规模动作识别数据集上进行预训练。特别是在使用正常样本的训练阶段，该方法在冻结DNN权重的同时，对正常动作的骨架特征分布进行建模，并在推理阶段使用此分布估计异常分数。此外，为了提高对骨架错误的鲁棒性，引入了一种受点云深度学习范式启发的DNN架构，该架构稀疏地在关节之间传播特征。另外，为了防止未观察到的正常动作被误识别为异常动作，将用户提示嵌入和在公共空间中对齐的骨架特征之间的相似度分数纳入异常分数，从而间接补充了正常动作。<br>
                    效果：在两个公开可用的数据集上进行实验，测试了所提出方法对于上述局限性的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This study investigates unsupervised anomaly action recognition, which identifies video-level abnormal-human-behavior events in an unsupervised manner without abnormal samples, and simultaneously addresses three limitations in the conventional skeleton-based approaches: target domain-dependent DNN training, robustness against skeleton errors, and a lack of normal samples. We present a unified, user prompt-guided zero-shot learning framework using a target domain-independent skeleton feature extractor, which is pretrained on a large-scale action recognition dataset. Particularly, during the training phase using normal samples, the method models the distribution of skeleton features of the normal actions while freezing the weights of the DNNs and estimates the anomaly score using this distribution in the inference phase. Additionally, to increase robustness against skeleton errors, we introduce a DNN architecture inspired by a point cloud deep learning paradigm, which sparsely propagates the features between joints. Furthermore, to prevent the unobserved normal actions from being misidentified as abnormal actions, we incorporate a similarity score between the user prompt embeddings and skeleton features aligned in the common space into the anomaly score, which indirectly supplements normal actions. On two publicly available datasets, we conduct experiments to test the effectiveness of the proposed method with respect to abovementioned limitations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">655.Dynamic Coarse-To-Fine Learning for Oriented Tiny Object Detection</span><br>
                <span class="as">Xu, ChangandDing, JianandWang, JinwangandYang, WenandYu, HuaiandYu, LeiandXia, Gui-Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dynamic_Coarse-To-Fine_Learning_for_Oriented_Tiny_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7318-7328.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何检测任意方向的微小物体，特别是在标签分配方面。<br>
                    动机：现有的检测器在面对极端几何形状和有限特征的微小定向物体时，存在严重的不匹配和不平衡问题。<br>
                    方法：提出一种动态先验和粗到细分配器的DCFL方法，通过动态建模先验、标签分配和物体表示来缓解不匹配问题，利用粗先验匹配和细后约束进行动态标签分配，为不同实例提供适当且相对平衡的监督。<br>
                    效果：在六个数据集上的大量实验表明，DCFL方法对基线有显著改进，并在DOTA-v1.5、DOTA-v2.0和DIOR-R数据集上获得了单尺度训练和测试下的最新性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detecting arbitrarily oriented tiny objects poses intense challenges to existing detectors, especially for label assignment. Despite the exploration of adaptive label assignment in recent oriented object detectors, the extreme geometry shape and limited feature of oriented tiny objects still induce severe mismatch and imbalance issues. Specifically, the position prior, positive sample feature, and instance are mismatched, and the learning of extreme-shaped objects is biased and unbalanced due to little proper feature supervision. To tackle these issues, we propose a dynamic prior along with the coarse-to-fine assigner, dubbed DCFL. For one thing, we model the prior, label assignment, and object representation all in a dynamic manner to alleviate the mismatch issue. For another, we leverage the coarse prior matching and finer posterior constraint to dynamically assign labels, providing appropriate and relatively balanced supervision for diverse instances. Extensive experiments on six datasets show substantial improvements to the baseline. Notably, we obtain the state-of-the-art performance for one-stage detectors on the DOTA-v1.5, DOTA-v2.0, and DIOR-R datasets under single-scale training and testing. Codes are available at https://github.com/Chasel-Tsui/mmrotate-dcfl.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">656.The Enemy of My Enemy Is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training</span><br>
                <span class="as">Dong, JunhaoandMoosavi-Dezfooli, Seyed-MohsenandLai, JianhuangandXie, Xiaohua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_The_Enemy_of_My_Enemy_Is_My_Friend_Exploring_Inverse_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24678-24687.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的深度学习技术在计算机视觉任务上表现优秀，但仍易受对抗性示例影响。<br>
                    动机：对抗性训练及其变体被认为是防御对抗性示例的最有效方法。然而，如果自然示例被误分类，这种方法可能会产生负面影响。<br>
                    方法：提出一种新的对抗性训练方案，鼓励模型为对抗性示例和其"逆对抗性"对应物产生相似的输出概率。特别是，对应物是通过最大化自然示例附近的似然生成的。<br>
                    效果：在各种视觉数据集和架构上的大量实验表明，我们的训练方法在鲁棒性和自然准确性方面都达到了最先进的水平。此外，使用通用的逆对抗性示例，我们以低计算成本提高了单步对抗性训练技术的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although current deep learning techniques have yielded superior performance on various computer vision tasks, yet they are still vulnerable to adversarial examples. Adversarial training and its variants have been shown to be the most effective approaches to defend against adversarial examples. A particular class of these methods regularize the difference between output probabilities for an adversarial and its corresponding natural example. However, it may have a negative impact if a natural example is misclassified. To circumvent this issue, we propose a novel adversarial training scheme that encourages the model to produce similar output probabilities for an adversarial example and its "inverse adversarial" counterpart. Particularly, the counterpart is generated by maximizing the likelihood in the neighborhood of the natural example. Extensive experiments on various vision datasets and architectures demonstrate that our training method achieves state-of-the-art robustness as well as natural accuracy among robust models. Furthermore, using a universal version of inverse adversarial examples, we improve the performance of single-step adversarial training techniques at a low computational cost.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">657.Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation</span><br>
                <span class="as">Zhou, KunandLi, WenboandHan, XiaoguangandLu, Jiangbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Exploring_Motion_Ambiguity_and_Alignment_for_High-Quality_Video_Frame_Interpolation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22169-22179.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频帧插值（VFI）中，现有的深度学习方法过于依赖地面真实（GT）的中间帧，忽视了从相邻帧判断运动并非唯一的可能性，导致生成的解不够清晰。<br>
                    动机：为了解决这个问题，我们提出了一种纹理一致性损失（TCL），放松了重构中间帧必须尽可能接近GT的要求。<br>
                    方法：我们开发了一个基于假设插值内容应与其在给定帧中的对应部分保持相似结构的TCL。尽管预测结果可能与预定义的GT不同，但我们鼓励满足这个约束的预测。此外，我们还设计了一种简单、高效且强大的O(N)引导跨尺度金字塔对齐（GCSPA）模块，充分利用多尺度信息。<br>
                    效果：实验证明，我们的策略在效率和有效性上都表现出色，能够持续提升现有VFI框架的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For video frame interpolation(VFI), existing deep-learning-based approaches strongly rely on the ground-truth (GT) intermediate frames, which sometimes ignore the non-unique nature of motion judging from the given adjacent frames. As a result, these methods tend to produce averaged solutions that are not clear enough. To alleviate this issue, we propose to relax the requirement of reconstructing an intermediate frame as close to the GT as possible. Towards this end, we develop a texture consistency loss (TCL) upon the assumption that the interpolated content should maintain similar structures with their counterparts in the given frames. Predictions satisfying this constraint are encouraged, though they may differ from the predefined GT. Without the bells and whistles, our plug-and-play TCL is capable of improving the performance of existing VFI frameworks consistently. On the other hand, previous methods usually adopt the cost volume or correlation map to achieve more accurate image or feature warping. However, the O(N^2) (N refers to the pixel count) computational complexity makes it infeasible for high-resolution cases. In this work, we design a simple, efficient O(N) yet powerful guided cross-scale pyramid alignment(GCSPA) module, where multi-scale information is highly exploited. Extensive experiments justify the efficiency and effectiveness of the proposed strategy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">658.Adaptive Annealing for Robust Geometric Estimation</span><br>
                <span class="as">Sidhartha, ChitturiandManam, LalitandGovindu, VenuMadhav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sidhartha_Adaptive_Annealing_for_Robust_Geometric_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21929-21939.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉中的几何估计问题通常通过最小化统计损失函数来解决，这些损失函数考虑了观察中的异常值。对应的能量景观通常有许多局部极小值。<br>
                    动机：许多方法试图通过使用诸如渐进非凸性（GNC）的方法来调整损失函数的规模参数来避免局部极小值。然而，对退火计划的关注往往被忽视，这通常是以固定的方式进行的，导致速度-准确性权衡不佳和无法可靠地收敛到全局最小值。<br>
                    方法：本文提出了一种自适应调整GNC规模的方法，通过跟踪成本函数海森矩阵的正定性（即局部凸性）。我们使用存在噪声和异常值的3D对应点注册的经典问题来说明我们的方法。我们还开发了显著加快我们方法的海森矩阵近似值。<br>
                    效果：我们的方法在一系列合成和真实数据集上与最先进的3D注册方法进行了比较，验证了其有效性。我们的方法准确且高效，比最先进的方法更可靠地收敛到全局解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Geometric estimation problems in vision are often solved via minimization of statistical loss functions which account for the presence of outliers in the observations. The corresponding energy landscape often has many local minima. Many approaches attempt to avoid local minima by annealing the scale parameter of loss functions using methods such as graduated non-convexity (GNC). However, little attention has been paid to the annealing schedule, which is often carried out in a fixed manner, resulting in a poor speed-accuracy trade-off and unreliable convergence to the global minimum. In this paper, we propose a principled approach for adaptively annealing the scale for GNC by tracking the positive-definiteness (i.e. local convexity) of the Hessian of the cost function. We illustrate our approach using the classic problem of registering 3D correspondences in the presence of noise and outliers. We also develop approximations to the Hessian that significantly speeds up our method. The effectiveness of our approach is validated by comparing its performance with state-of-the-art 3D registration approaches on a number of synthetic and real datasets. Our approach is accurate and efficient and converges to the global solution more reliably than the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">659.Upcycling Models Under Domain and Category Shift</span><br>
                <span class="as">Qu, SanqingandZou, TianpeiandR\&quot;ohrbein, FlorianandLu, CewuandChen, GuangandTao, DachengandJiang, Changjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Upcycling_Models_Under_Domain_and_Category_Shift_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20019-20028.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在面对领域和类别转移时表现不佳，如何改进并适应目标任务仍是一个重要未解决的问题。<br>
                    动机：现有的无监督领域适应（UDA）技术，特别是最近提出的源自由领域适应（SFDA），已成为解决这一问题的有希望的技术。然而，大多数现有的SFDA方法要求源领域和目标领域共享相同的标签空间，因此仅适用于普通的封闭设置。<br>
                    方法：我们进一步探索了源自由通用领域适应（SF-UniDA）。目标是识别出在领域和类别转移下的“已知”数据样本，并拒绝那些“未知”的数据样本（不在源类别中），只需要标准预训练的源模型的知识。为此，我们引入了一种创新的全局和局部聚类学习技术（GLC）。<br>
                    效果：我们在多个不同的类别转移场景下，包括部分集、开放集和开放部分集DA，对GLC的优势进行了检验。更值得注意的是，在最具挑战性的开放部分集DA场景中，GLC在VisDA基准测试上比UMAD高出14.8%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift. How to upcycle DNNs and adapt them to the target task remains an important open problem. Unsupervised Domain Adaptation (UDA), especially recently proposed Source-free Domain Adaptation (SFDA), has become a promising technology to address this issue. Nevertheless, most existing SFDA methods require that the source domain and target domain share the same label space, consequently being only applicable to the vanilla closed-set setting. In this paper, we take one step further and explore the Source-free Universal Domain Adaptation (SF-UniDA). The goal is to identify "known" data samples under both domain and category shift, and reject those "unknown" data samples (not present in source classes), with only the knowledge from standard pre-trained source model. To this end, we introduce an innovative global and local clustering learning technique (GLC). Specifically, we design a novel, adaptive one-vs-all global clustering algorithm to achieve the distinction across different target classes and introduce a local k-NN clustering strategy to alleviate negative transfer. We examine the superiority of our GLC on multiple benchmarks with different category shift scenarios, including partial-set, open-set, and open-partial-set DA. More remarkably, in the most challenging open-partial-set DA scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">660.Single Domain Generalization for LiDAR Semantic Segmentation</span><br>
                <span class="as">Kim, HyeonseongandKang, YoonsuandOh, ChanggyoonandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Single_Domain_Generalization_for_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17587-17598.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使深度学习模型在未见过的数据域中也能表现良好，特别是在LiDAR语义分割领域。<br>
                    动机：现有的3D深度学习模型在训练源数据域上表现良好，但在未见过的数据域（如不同的LiDAR传感器配置和场景分布）中性能下降，存在明显的领域差距。<br>
                    方法：提出一种名为DGLSS的LiDAR语义分割单域泛化方法，通过仅在源数据域上进行学习来确保在源数据域和未见过的数据域中都有良好的性能。主要通过模拟未见过的数据域来扩大训练领域，并引入两种约束条件来进行可泛化的特征学习：稀疏性不变特征一致性（SIFC）和语义相关性一致性（SCC）。<br>
                    效果：实验结果表明，与其它基线相比，该方法在未见过的数据域中的性能有所提高。即使没有目标数据域的访问权限，该方法的性能也优于领域适应方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the success of the 3D deep learning models, various perception technologies for autonomous driving have been developed in the LiDAR domain. While these models perform well in the trained source domain, they struggle in unseen domains with a domain gap. In this paper, we propose a single domain generalization method for LiDAR semantic segmentation (DGLSS) that aims to ensure good performance not only in the source domain but also in the unseen domain by learning only on the source domain. We mainly focus on generalizing from a dense source domain and target the domain shift from different LiDAR sensor configurations and scene distributions. To this end, we augment the domain to simulate the unseen domains by randomly subsampling the LiDAR scans. With the augmented domain, we introduce two constraints for generalizable representation learning: sparsity invariant feature consistency (SIFC) and semantic correlation consistency (SCC). The SIFC aligns sparse internal features of the source domain with the augmented domain based on the feature affinity. For SCC, we constrain the correlation between class prototypes to be similar for every LiDAR scan. We also establish a standardized training and evaluation setting for DGLSS. With the proposed evaluation setting, our method showed improved performance in the unseen domains compared to other baselines. Even without access to the target domain, our method performed better than the domain adaptation method. The code is available at https://github.com/gzgzys9887/DGLSS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">661.Balanced Energy Regularization Loss for Out-of-Distribution Detection</span><br>
                <span class="as">Choi, HyunjunandJeong, HawookandChoi, JinYoung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Balanced_Energy_Regularization_Loss_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15691-15700.png><br>
            
            <span class="tt"><span class="t0">研究问题：在OOD检测中，如何有效处理辅助数据分布不平衡的问题。<br>
                    动机：现有的方法对所有辅助数据等同对待，无法解决类别间的不平衡问题。<br>
                    方法：提出一种平衡能量正则化损失函数，利用各类别的先验概率对辅助数据进行不同程度的正则化，主要思想是对多数类别的辅助样本施加更重的正则化。<br>
                    效果：在语义分割、长尾图像分类和图像分类的OOD检测任务上，该方法均表现优于先前的能量正则化损失函数，并在语义分割和长尾图像分类的OOD检测任务上达到最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In the field of out-of-distribution (OOD) detection, a previous method that use auxiliary data as OOD data has shown promising performance. However, the method provides an equal loss to all auxiliary data to differentiate them from inliers. However, based on our observation, in various tasks, there is a general imbalance in the distribution of the auxiliary OOD data across classes. We propose a balanced energy regularization loss that is simple but generally effective for a variety of tasks. Our balanced energy regularization loss utilizes class-wise different prior probabilities for auxiliary data to address the class imbalance in OOD data. The main concept is to regularize auxiliary samples from majority classes, more heavily than those from minority classes. Our approach performs better for OOD detection in semantic segmentation, long-tailed image classification, and image classification than the prior energy regularization loss. Furthermore, our approach achieves state-of-the-art performance in two tasks: OOD detection in semantic segmentation and long-tailed image classification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">662.SLACK: Stable Learning of Augmentations With Cold-Start and KL Regularization</span><br>
                <span class="as">Marrie, JulietteandArbel, MichaelandLarlus, DianeandMairal, Julien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Marrie_SLACK_Stable_Learning_of_Augmentations_With_Cold-Start_and_KL_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24306-24314.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何自动进行数据增强以提高神经网络的泛化能力，同时避免依赖手动选择的转换集。<br>
                    动机：现有的自动数据增强方法大多依赖于一些先验信息，如预训练网络或强制将手动选择的默认转换作为自动数据增强算法学习的策略的一部分。<br>
                    方法：本文提出了一种直接学习增强策略的方法，不依赖这种先验知识。通过使用连续分布参数化大小和采用带有KL散度正则化的逐次冷启动策略，以解决双层优化问题的更大搜索空间和固有不稳定性问题。<br>
                    效果：尽管设置更具挑战性，但该方法在标准基准测试中取得了有竞争力的结果，并能够推广到自然图像之外。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Data augmentation is known to improve the generalization capabilities of neural networks, provided that the set of transformations is chosen with care, a selection often performed manually. Automatic data augmentation aims at automating this process. However, most recent approaches still rely on some prior information; they start from a small pool of manually-selected default transformations that are either used to pretrain the network or forced to be part of the policy learned by the automatic data augmentation algorithm. In this paper, we propose to directly learn the augmentation policy without leveraging such prior knowledge. The resulting bilevel optimization problem becomes more challenging due to the larger search space and the inherent instability of bilevel optimization algorithms. To mitigate these issues (i) we follow a successive cold-start strategy with a Kullback-Leibler regularization, and (ii) we parameterize magnitudes as continuous distributions. Our approach leads to competitive results on standard benchmarks despite a more challenging setting, and generalizes beyond natural images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">663.Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization</span><br>
                <span class="as">Zhang, XingxuanandXu, RenzheandYu, HanandZou, HaoandCui, Peng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Gradient_Norm_Aware_Minimization_Seeks_First-Order_Flatness_and_Improves_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20247-20257.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前模型优化器在寻找最小化损失函数时，如何更好地区分具有低泛化误差和高泛化误差的极小值。<br>
                    动机：现有的平坦度定义（如SAM）主要关注零阶平坦度，即在给定扰动半径内的最坏情况损失，但这种定义可能无法充分区分具有低和高泛化误差的极小值。<br>
                    方法：提出了一阶平坦度作为更强的平坦度度量，它关注在给定扰动半径内的最大梯度范数，可以同时约束局部极小点的海森矩阵的最大特征值和SAM的正则化函数。并设计了一种新的训练过程——梯度范数感知最小化（GAM），以寻找在所有方向上曲率均匀较小的极小值。<br>
                    效果：实验结果显示，GAM能提高各种数据集和网络下使用现有优化器（如SGD和AdamW）训练的模型的泛化能力。此外，GAM还能帮助SAM找到更平坦的极小值，实现更好的泛化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, flat minima are proven to be effective for improving generalization and sharpness-aware minimization (SAM) achieves state-of-the-art performance. Yet the current definition of flatness discussed in SAM and its follow-ups are limited to the zeroth-order flatness (i.e., the worst-case loss within a perturbation radius). We show that the zeroth-order flatness can be insufficient to discriminate minima with low generalization error from those with high generalization error both when there is a single minimum or multiple minima within the given perturbation radius. Thus we present first-order flatness, a stronger measure of flatness focusing on the maximal gradient norm within a perturbation radius which bounds both the maximal eigenvalue of Hessian at local minima and the regularization function of SAM. We also present a novel training procedure named Gradient norm Aware Minimization (GAM) to seek minima with uniformly small curvature across all directions. Experimental results show that GAM improves the generalization of models trained with current optimizers such as SGD and AdamW on various datasets and networks. Furthermore, we show that GAM can help SAM find flatter minima and achieve better generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">664.GraVoS: Voxel Selection for 3D Point-Cloud Detection</span><br>
                <span class="as">Shrout, OrenandBen-Shabat, YizhakandTal, Ayellet</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shrout_GraVoS_Voxel_Selection_for_3D_Point-Cloud_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21684-21693.png><br>
            
            <span class="tt"><span class="t0">研究问题：在大型3D场景中进行3D物体检测是一项挑战，不仅因为稀疏和不规则的3D点云，还因为前景-背景场景和类别的极度不平衡。<br>
                    动机：我们提出了一种通过移除元素（体素）而不是添加来修改场景的方法，以解决这两种类型的数据集不平衡问题。<br>
                    方法：我们选择“有意义的”体素，这种方法可以应用于任何基于体素的检测器，但体素的意义取决于网络。<br>
                    效果：我们的体素选择被证明可以提高几种突出的3D检测方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection within large 3D scenes is challenging not only due to the sparse and irregular 3D point clouds, but also due to both the extreme foreground-background scene imbalance and class imbalance. A common approach is to add ground-truth objects from other scenes. Differently, we propose to modify the scenes by removing elements (voxels), rather than adding ones. Our approach selects the "meaningful" voxels, in a manner that addresses both types of dataset imbalance. The approach is general and can be applied to any voxel-based detector, yet the meaningfulness of a voxel is network-dependent. Our voxel selection is shown to improve the performance of several prominent 3D detection methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">665.Rethinking Image Super Resolution From Long-Tailed Distribution Learning Perspective</span><br>
                <span class="as">Gou, YuanbiaoandHu, PengandLv, JianchengandZhu, HongyuanandPeng, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Rethinking_Image_Super_Resolution_From_Long-Tailed_Distribution_Learning_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14327-14336.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的预训练语言模型如何更好地利用知识图谱中的结构化知识。<br>
                    动机：目前的预训练语言模型在处理知识驱动任务时，对知识图谱的利用不足，而知识图谱中的有信息量的实体可以增强语言表示。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，该模型同时利用大规模文本语料库和知识图谱进行训练，能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing studies have empirically observed that the resolution of the low-frequency region is easier to enhance than that of the high-frequency one. Although plentiful works have been devoted to alleviating this problem, little understanding is given to explain it. In this paper, we try to give a feasible answer from a machine learning perspective, i.e., the twin fitting problem caused by the long-tailed pixel distribution in natural images. With this explanation, we reformulate image super resolution (SR) as a long-tailed distribution learning problem and solve it by bridging the gaps of the problem between in low- and high-level vision tasks. As a result, we design a long-tailed distribution learning solution, that rebalances the gradients from the pixels in the low- and high-frequency region, by introducing a static and a learnable structure prior. The learned SR model achieves better balance on the fitting of the low- and high-frequency region so that the overall performance is improved. In the experiments, we evaluate the solution on four CNN- and one Transformer-based SR models w.r.t. six datasets and three tasks, and experimental results demonstrate its superiority.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">666.On the Pitfall of Mixup for Uncertainty Calibration</span><br>
                <span class="as">Wang, Deng-BaoandLi, LanqingandZhao, PeilinandHeng, Pheng-AnnandZhang, Min-Ling</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_On_the_Pitfall_of_Mixup_for_Uncertainty_Calibration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7609-7618.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决混合训练可能导致模型校准性下降的问题。<br>
                    动机：虽然混合训练已被证明能提高预测精度，并使模型在不确定性校准上表现良好，但我们发现它通常会降低模型的校准性，这可能会对后验校准产生负面影响。<br>
                    方法：我们将混合过程分解为数据转换和随机扰动，并提出了一种名为mixup推理的训练策略，该策略采用简单的解耦原则，在网络前向传播结束时恢复原始样本的输出。<br>
                    效果：实验表明，这种策略在不牺牲预测性能的情况下，适当解决了混合训练的校准问题，甚至比原始混合训练提高了精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>By simply taking convex combinations between pairs of samples and their labels, mixup training has been shown to easily improve predictive accuracy. It has been recently found that models trained with mixup also perform well on uncertainty calibration. However, in this study, we found that mixup training usually makes models less calibratable than vanilla empirical risk minimization, which means that it would harm uncertainty estimation when post-hoc calibration is considered. By decomposing the mixup process into data transformation and random perturbation, we suggest that the confidence penalty nature of the data transformation is the reason of calibration degradation. To mitigate this problem, we first investigate the mixup inference strategy and found that despite it improves calibration on mixup, this ensemble-like strategy does not necessarily outperform simple ensemble. Then, we propose a general strategy named mixup inference in training, which adopts a simple decoupling principle for recovering the outputs of raw samples at the end of forward network pass. By embedding the mixup inference, models can be learned from the original one-hot labels and hence avoid the negative impact of confidence penalty. Our experiments show this strategy properly solves mixup's calibration issue without sacrificing the predictive performance, while even improves accuracy than vanilla mixup.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">667.C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation</span><br>
                <span class="as">Karim, NazmulandMithun, NiluthpolChowdhuryandRajvanshi, AbhinavandChiu, Han-pangandSamarasekera, SupunandRahnavard, Nazanin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24120-24131.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练的语言模型与知识图谱结合，以增强语言表示？<br>
                    动机：目前的预训练语言模型缺乏对结构化知识的利用，而知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱联合训练ERNIE模型，同时充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised domain adaptation (UDA) approaches focus on adapting models trained on a labeled source domain to an unlabeled target domain. In contrast to UDA, source-free domain adaptation (SFDA) is a more practical setup as access to source data is no longer required during adaptation. Recent state-of-the-art (SOTA) methods on SFDA mostly focus on pseudo-label refinement based self-training which generally suffers from two issues: i) inevitable occurrence of noisy pseudo-labels that could lead to early training time memorization, ii) refinement process requires maintaining a memory bank which creates a significant burden in resource constraint scenarios. To address these concerns, we propose C-SFDA, a curriculum learning aided self-training framework for SFDA that adapts efficiently and reliably to changes across domains based on selective pseudo-labeling. Specifically, we employ a curriculum learning scheme to promote learning from a restricted amount of pseudo labels selected based on their reliabilities. This simple yet effective step successfully prevents label noise propagation during different stages of adaptation and eliminates the need for costly memory-bank based label refinement. Our extensive experimental evaluations on both image recognition and semantic segmentation tasks confirm the effectiveness of our method. C-SFDA is also applicable to online test-time domain adaptation and outperforms previous SOTA methods in this task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">668.Improving Zero-Shot Generalization and Robustness of Multi-Modal Models</span><br>
                <span class="as">Ge, YunhaoandRen, JieandGallagher, AndrewandWang, YuxiaoandYang, Ming-HsuanandAdam, HartwigandItti, LaurentandLakshminarayanan, BalajiandZhao, Jiaping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Improving_Zero-Shot_Generalization_and_Robustness_of_Multi-Modal_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11093-11101.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态图像-文本模型如CLIP和LiT在图像分类基准测试中表现出色，但其零样本泛化能力存在显著的性能差距。<br>
                    动机：这些模型的零样本准确率虽然很高，但top-1准确率却低很多（在某些情况下差距超过25%）。研究发现，许多失败案例都是由文本提示的模糊性引起的。<br>
                    方法：我们开发了一种简单而有效的零样本后处理方法，通过测量预测与多个提示和图像变换的一致性，来识别其top-1预测可能错误的图像。我们还提出了一种利用WordNet层次结构提高这种不确定图像准确性的方法。<br>
                    效果：在CLIP和LiT模型上进行实验，结果显示，我们的方法是有效的，可以显著提高模型的top-1准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-modal image-text models such as CLIP and LiT have demonstrated impressive performance on image classification benchmarks and their zero-shot generalization ability is particularly exciting. While the top-5 zero-shot accuracies of these models are very high, the top-1 accuracies are much lower (over 25% gap in some cases). We investigate the reasons for this performance gap and find that many of the failure cases are caused by ambiguity in the text prompts. First, we develop a simple and efficient zero-shot post-hoc method to identify images whose top-1 prediction is likely to be incorrect, by measuring consistency of the predictions w.r.t. multiple prompts and image transformations. We show that our procedure better predicts mistakes, outperforming the popular max logit baseline on selective prediction tasks. Next, we propose a simple and efficient way to improve accuracy on such uncertain images by making use of the WordNet hierarchy; specifically we augment the original class by incorporating its parent and children from the semantic label hierarchy, and plug the augmentation into text prompts. We conduct experiments on both CLIP and LiT models with five different ImageNet- based datasets. For CLIP, our method improves the top-1 accuracy by 17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set. We also show that our method improves across ImageNet shifted datasets, four other datasets, and other model architectures such as LiT. Our proposed method is hyperparameter-free, requires no additional model training and can be easily scaled to other large multi-modal architectures. Code is available at https://github.com/gyhandy/Hierarchy-CLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">669.Modeling the Distributional Uncertainty for Salient Object Detection Models</span><br>
                <span class="as">Tian, XinyuandZhang, JingandXiang, MochuandDai, Yuchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Modeling_the_Distributional_Uncertainty_for_Salient_Object_Detection_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19660-19670.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的显著性目标检测（SOD）模型主要关注提高整体模型性能，没有明确解释训练和测试分布之间的差异。<br>
                    动机：本文研究了显著性目标检测中的一种特定类型的认识不确定性，即分布不确定性。<br>
                    方法：我们首次探索了现有的类别感知分布差距探索技术，如长尾学习、单一模型不确定性建模和测试时策略，并将其适应于我们的类别无关任务的分布不确定性模型。<br>
                    效果：通过广泛的实验结果验证了现有分布差距模型技术在SOD中的有效性，得出结论，训练时的单一模型不确定性估计技术和防止模型激活过度漂移的权重正则化解决方案是SOD分布不确定性建模有希望的方向。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most of the existing salient object detection (SOD) models focus on improving the overall model performance, without explicitly explaining the discrepancy between the training and testing distributions. In this paper, we investigate a particular type of epistemic uncertainty, namely distributional uncertainty, for salient object detection. Specifically, for the first time, we explore the existing class-aware distribution gap exploration techniques, i.e. long-tail learning, single-model uncertainty modeling and test-time strategies, and adapt them to model the distributional uncertainty for our class-agnostic task. We define test sample that is dissimilar to the training dataset as being "out-of-distribution" (OOD) samples. Different from the conventional OOD definition, where OOD samples are those not belonging to the closed-world training categories, OOD samples for SOD are those break the basic priors of saliency, i.e. center prior, color contrast prior, compactness prior and etc., indicating OOD as being "continuous" instead of being discrete for our task. We've carried out extensive experimental results to verify effectiveness of existing distribution gap modeling techniques for SOD, and conclude that both train-time single-model uncertainty estimation techniques and weight-regularization solutions that preventing model activation from drifting too much are promising directions for modeling distributional uncertainty for SOD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">670.Robust Model-Based Face Reconstruction Through Weakly-Supervised Outlier Segmentation</span><br>
                <span class="as">Li, ChunluandMorel-Forster, AndreasandVetter, ThomasandEgger, BernhardandKortylewski, Adam</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Robust_Model-Based_Face_Reconstruction_Through_Weakly-Supervised_Outlier_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/372-381.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过避免拟合模型到异常值（如遮挡物或化妆）来增强基于模型的面部重建。<br>
                    动机：异常值的高度变异性和难以标注性给局部化异常值带来了挑战。<br>
                    方法：提出了一种联合人脸自动编码器和异常值分割方法（FOCUS）。利用高质量模型无法良好拟合异常值的事实，可以很好地定位异常值。采用EM类型的训练策略，将人脸自动编码器与异常值分割网络联合训练，防止面部编码器拟合到异常值，提高重建质量。<br>
                    效果：在NoW测试集上的实验表明，FOCUS在所有未使用3D标注进行训练的基线上实现了最先进的3D面部重建性能。此外，在CelebA-HQ和AR数据库上的结果还显示，即使没有进行任何分割标注的训练，分割网络也能准确地定位遮挡物。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we aim to enhance model-based face reconstruction by avoiding fitting the model to outliers, i.e. regions that cannot be well-expressed by the model such as occluders or make-up. The core challenge for localizing outliers is that they are highly variable and difficult to annotate. To overcome this challenging problem, we introduce a joint Face-autoencoder and outlier segmentation approach (FOCUS).In particular, we exploit the fact that the outliers cannot be fitted well by the face model and hence can be localized well given a high-quality model fitting. The main challenge is that the model fitting and the outlier segmentation are mutually dependent on each other, and need to be inferred jointly. We resolve this chicken-and-egg problem with an EM-type training strategy, where a face autoencoder is trained jointly with an outlier segmentation network. This leads to a synergistic effect, in which the segmentation network prevents the face encoder from fitting to the outliers, enhancing the reconstruction quality. The improved 3D face reconstruction, in turn, enables the segmentation network to better predict the outliers. To resolve the ambiguity between outliers and regions that are difficult to fit, such as eyebrows, we build a statistical prior from synthetic data that measures the systematic bias in model fitting. Experiments on the NoW testset demonstrate that FOCUS achieves SOTA 3D face reconstruction performance among all baselines that are trained without 3D annotation. Moreover, our results on CelebA-HQ and the AR database show that the segmentation network can localize occluders accurately despite being trained without any segmentation annotation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">671.Exploring Incompatible Knowledge Transfer in Few-Shot Image Generation</span><br>
                <span class="as">Zhao, YunqingandDu, ChaoandAbdollahzadeh, MiladandPang, TianyuandLin, MinandYan, ShuichengandCheung, Ngai-Man</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7380-7391.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本图像生成（FSIG）中的知识不兼容转移问题，即研究问题：本文旨在解决小样本图像生成（FSIG）中的知识不兼容转移问题，即从源生成器向目标生成器传递知识时，由于最不显著的过滤器导致合成样本的真实性显著降低。<br>
                    动机：现有的FSIG方法通过选择、保留和转移来自相关领域的预训练源生成器的先验知识来学习目标生成器，但在此过程中存在一个被忽视的问题，即知识不兼容转移，这会严重影响生成图像的真实性。<br>
                    方法：为解决这个问题，本文提出了知识截断方法，作为知识保留的补充操作，并通过一种轻量级的基于剪枝的方法进行实现。<br>
                    效果：实验表明，知识截断方法简单有效，在各种挑战性设置下，包括源域和目标域距离较远的情况，都能取得最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot image generation (FSIG) learns to generate diverse and high-fidelity images from a target domain using a few (e.g., 10) reference samples. Existing FSIG methods select, preserve and transfer prior knowledge from a source generator (pretrained on a related domain) to learn the target generator. In this work, we investigate an underexplored issue in FSIG, dubbed as incompatible knowledge transfer, which would significantly degrade the realisticness of synthetic samples. Empirical observations show that the issue stems from the least significant filters from the source generator. To this end, we propose knowledge truncation to mitigate this issue in FSIG, which is a complementary operation to knowledge preservation and is implemented by a lightweight pruning-based method. Extensive experiments show that knowledge truncation is simple and effective, consistently achieving state-of-the-art performance, including challenging setups where the source and target domains are more distant. Project Page: https://yunqing-me.github.io/RICK.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">672.Zero-Shot Generative Model Adaptation via Image-Specific Prompt Learning</span><br>
                <span class="as">Guo, JiayiandWang, ChaofeiandWu, YouandZhang, EricandWang, KaiandXu, XingqianandSong, ShijiandShi, HumphreyandHuang, Gao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11494-11503.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高跨领域图像生成的质量与多样性，并解决模式崩溃问题。<br>
                    动机：现有的跨领域图像生成方法在质量和多样性上存在局限，且易受模式崩溃影响，主要原因是对所有跨领域图像对应用固定的适应方向导致监督信号相同。<br>
                    方法：提出一种针对特定图像的提示学习（IPL）方法，为每个源领域图像学习特定的提示向量，为每个跨领域图像对产生更精确的适应方向，增强目标领域生成器的灵活性。<br>
                    效果：实验证明，IPL能有效提升生成图像的质量和多样性，缓解模式崩溃问题，且独立于生成模型的结构，如生成对抗网络或扩散模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, CLIP-guided image synthesis has shown appealing performance on adapting a pre-trained source-domain generator to an unseen target domain. It does not require any target-domain samples but only the textual domain labels. The training is highly efficient, e.g., a few minutes. However, existing methods still have some limitations in the quality of generated images and may suffer from the mode collapse issue. A key reason is that a fixed adaptation direction is applied for all cross-domain image pairs, which leads to identical supervision signals. To address this issue, we propose an Image-specific Prompt Learning (IPL) method, which learns specific prompt vectors for each source-domain image. This produces a more precise adaptation direction for every cross-domain image pair, endowing the target-domain generator with greatly enhanced flexibility. Qualitative and quantitative evaluations on various domains demonstrate that IPL effectively improves the quality and diversity of synthesized images and alleviates the mode collapse. Moreover, IPL is independent of the structure of the generative model, such as generative adversarial networks or diffusion models. Code is available at https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">673.Hard Patches Mining for Masked Image Modeling</span><br>
                <span class="as">Wang, HaochenandSong, KaiyouandFan, JunsongandWang, YuxiandXie, JinandZhang, Zhaoxiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Hard_Patches_Mining_for_Masked_Image_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10375-10385.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练视觉模型（Masked Image Modeling，MIM）在预测被遮蔽图像内容时过于依赖预设遮蔽策略的问题。<br>
                    动机：目前的预训练视觉模型主要关注预测被遮蔽图像的具体内容，其性能与预设的遮蔽策略高度相关。作者认为，模型不仅应该专注于解决给定的问题，还应该扮演教师的角色，自己产生更具挑战性的问题。<br>
                    方法：提出了一种新的预训练框架——硬patches挖掘（Hard Patches Mining，HPM）。通过引入一个辅助的损失预测器，预测每个区域的重建损失，并决定下一个要遮蔽的区域。采用相对关系学习策略防止过拟合到精确的重建损失值。<br>
                    效果：实验表明，HPM在构造遮蔽图像方面非常有效。仅引入损失预测目标就足以产生强大的表示，验证了能够意识到难以重建的部分的能力的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked image modeling (MIM) has attracted much research attention due to its promising potential for learning scalable visual representations. In typical approaches, models usually focus on predicting specific contents of masked patches, and their performances are highly related to pre-defined mask strategies. Intuitively, this procedure can be considered as training a student (the model) on solving given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems, but also stand in the shoes of a teacher to produce a more challenging problem by itself. To this end, we propose Hard Patches Mining (HPM), a brand-new framework for MIM pre-training. We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next. It adopts a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Experiments under various settings demonstrate the effectiveness of HPM in constructing masked images. Furthermore, we empirically find that solely introducing the loss prediction objective leads to powerful representations, verifying the efficacy of the ability to be aware of where is hard to reconstruct.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">674.GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class Incremental Task</span><br>
                <span class="as">Zhuang, HuipingandWeng, ZhenyuandHe, RunandLin, ZhipingandZeng, Ziqian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7746-7755.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本增量学习中的灾难性遗忘问题。<br>
                    动机：在小样本学习设置中，类别增量学习会导致灾难性遗忘。<br>
                    方法：采用解析学习技术，将网络训练转化为线性问题，通过递归实现和权重相同属性来避免灾难性遗忘，提出高斯核嵌入解析学习方法（GKEAL）。<br>
                    效果：实验表明，GKEAL在几个基准数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot class incremental learning (FSCIL) aims to address catastrophic forgetting during class incremental learning in a few-shot learning setting. In this paper, we approach the FSCIL by adopting analytic learning, a technique that converts network training into linear problems. This is inspired by the fact that the recursive implementation (batch-by-batch learning) of analytic learning gives identical weights to that produced by training on the entire dataset at once. The recursive implementation and the weight-identical property highly resemble the FSCIL setting (phase-by-phase learning) and its goal of avoiding catastrophic forgetting. By bridging the FSCIL with the analytic learning, we propose a Gaussian kernel embedded analytic learning (GKEAL) for FSCIL. The key components of GKEAL include the kernel analytic module which allows the GKEAL to conduct FSCIL in a recursive manner, and the augmented feature concatenation module that balances the preference between old and new tasks especially effectively under the few-shot setting. Our experiments show that the GKEAL gives state-of-the-art performance on several benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">675.Active Exploration of Multimodal Complementarity for Few-Shot Action Recognition</span><br>
                <span class="as">Wanyan, YuyangandYang, XiaoshanandChen, ChaofanandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wanyan_Active_Exploration_of_Multimodal_Complementarity_for_Few-Shot_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6492-6502.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用多模态信息进行少次动作识别。<br>
                    动机：尽管少次动作识别已取得显著进步，但大部分方法主要依赖有限的单模态数据（如RGB帧），而对多模态信息的探索相对较少。<br>
                    方法：提出一种新的主动多模态少次动作识别（AMFAR）框架，该框架可以根据任务相关的上下文信息主动为每个样本找到可靠的模态，以提高少次推理过程。在元训练中，设计了一个主动样本选择（ASS）模块，将模态可靠性差异大的查询样本根据模态特定的后验分布组织成不同的组。此外，设计了一个主动互导蒸馏（AMD）模块，通过双向知识蒸馏从可靠的模态中捕获区分性的任务特定知识，以提高不可靠模态的表示学习。在元测试中，采用自适应多模态推理（AMI）模块，自适应地融合模态特定的后验分布，其中可靠的模态权重更大。<br>
                    效果：在四个公共基准测试集上的大量实验结果表明，我们的模型在现有的单模态和多模态方法上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, few-shot action recognition receives increasing attention and achieves remarkable progress. However, previous methods mainly rely on limited unimodal data (e.g., RGB frames) while the multimodal information remains relatively underexplored. In this paper, we propose a novel Active Multimodal Few-shot Action Recognition (AMFAR) framework, which can actively find the reliable modality for each sample based on task-dependent context information to improve few-shot reasoning procedure. In meta-training, we design an Active Sample Selection (ASS) module to organize query samples with large differences in the reliability of modalities into different groups based on modality-specific posterior distributions. In addition, we design an Active Mutual Distillation (AMD) module to capture discriminative task-specific knowledge from the reliable modality to improve the representation learning of unreliable modality by bidirectional knowledge distillation. In meta-test, we adopt Adaptive Multimodal Inference (AMI) module to adaptively fuse the modality-specific posterior distributions with a larger weight on the reliable modality. Extensive experimental results on four public benchmarks demonstrate that our model achieves significant improvements over existing unimodal and multimodal methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">676.Deep Learning of Partial Graph Matching via Differentiable Top-K</span><br>
                <span class="as">Wang, RunzhongandGuo, ZiaoandJiang, ShaofeiandYang, XiaokangandYan, Junchi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Learning_of_Partial_Graph_Matching_via_Differentiable_Top-K_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6272-6281.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图匹配（GM）中的NP-hard问题，特别是在存在离群点的情况下。<br>
                    动机：在图匹配中，离群点的存在是一个普遍的问题，尤其是在视觉问题上。现有的基于亲和力最大化的方法往往缺乏抑制错误匹配的机制，而依赖于手工设定的阈值来排除离群点。<br>
                    方法：本文将部分图匹配问题表述为给定/估计内联数量k的top-k选择任务。具体来说，我们设计了一个可微分的top-k模块，该模块可以在最优传输层上进行有效的梯度下降，可以方便地插入到包括二次匹配网络NGMv2和线性匹配网络GCAN在内的最新深GM管道中。同时，我们还开发了注意力融合的聚合层来估计k，以实现野外自动离群稳健匹配。<br>
                    效果：实验表明，我们的方法在流行的基准测试上优于其他部分匹配方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph matching (GM) aims at discovering node matching between graphs, by maximizing the node- and edge-wise affinities between the matched elements. As an NP-hard problem, its challenge is further pronounced in the existence of outlier nodes in both graphs which is ubiquitous in practice, especially for vision problems. However, popular affinity-maximization-based paradigms often lack a principled scheme to suppress the false matching and resort to handcrafted thresholding to dismiss the outliers. This limitation is also inherited by the neural GM solvers though they have shown superior performance in the ideal no-outlier setting. In this paper, we propose to formulate the partial GM problem as the top-k selection task with a given/estimated number of inliers k. Specifically, we devise a differentiable top-k module that enables effective gradient descent over the optimal-transport layer, which can be readily plugged into SOTA deep GM pipelines including the quadratic matching network NGMv2 as well as the linear matching network GCAN. Meanwhile, the attention-fused aggregation layers are developed to estimate k to enable automatic outlier-robust matching in the wild. Last but not least, we remake and release a new benchmark called IMC-PT-SparseGM, originating from the IMC-PT stereo-matching dataset. The new benchmark involves more scale-varying graphs and partial matching instances from the real world. Experiments show that our methods outperform other partial matching schemes on popular benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">677.Super-CLEVR: A Virtual Benchmark To Diagnose Domain Robustness in Visual Reasoning</span><br>
                <span class="as">Li, ZhuowanandWang, XingruiandStengel-Eskin, EliasandKortylewski, AdamandMa, WufeiandVanDurme, BenjaminandYuille, AlanL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Super-CLEVR_A_Virtual_Benchmark_To_Diagnose_Domain_Robustness_in_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14963-14973.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉问答（VQA）模型在分布外数据上表现不佳，且在领域泛化上存在困难。<br>
                    动机：由于VQA任务的多模态特性，多个变化因素交织在一起，使得泛化分析变得困难。因此，我们引入了一个虚拟基准——Super-CLEVR，以隔离VQA领域偏移中的不同因素，以便独立研究它们的影响。<br>
                    方法：我们在Super-CLEVR中考虑了四个因素：视觉复杂性、问题冗余、概念分布和概念复合性。通过控制生成的数据，Super-CLEVR使我们能够在测试数据与训练数据在这些轴上有所不同的情况下测试VQA方法。我们研究了四种现有方法，包括两种神经符号方法NSCL和NSVQA，以及两种非符号方法FiLM和mDETR；以及我们提出的方法，概率NSVQA（P-NSVQA），该方法通过不确定性推理扩展了NSVQA。P-NSVQA在四个领域偏移因素中的三个上优于其他方法。<br>
                    效果：我们的结果表明，将推理和感知解耦，结合概率不确定性，构成了一个强大的VQA模型，对领域偏移更具鲁棒性。数据集和代码发布在https://github.com/Lizw14/Super-CLEVR。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual Question Answering (VQA) models often perform poorly on out-of-distribution data and struggle on domain generalization. Due to the multi-modal nature of this task, multiple factors of variation are intertwined, making generalization difficult to analyze. This motivates us to introduce a virtual benchmark, Super-CLEVR, where different factors in VQA domain shifts can be isolated in order that their effects can be studied independently. Four factors are considered: visual complexity, question redundancy, concept distribution and concept compositionality. With controllably generated data, Super-CLEVR enables us to test VQA methods in situations where the test data differs from the training data along each of these axes. We study four existing methods, including two neural symbolic methods NSCL and NSVQA, and two non-symbolic methods FiLM and mDETR; and our proposed method, probabilistic NSVQA (P-NSVQA), which extends NSVQA with uncertainty reasoning. P-NSVQA outperforms other methods on three of the four domain shift factors. Our results suggest that disentangling reasoning and perception, combined with probabilistic uncertainty, form a strong VQA model that is more robust to domain shifts. The dataset and code are released at https://github.com/Lizw14/Super-CLEVR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">678.Domain Expansion of Image Generators</span><br>
                <span class="as">Nitzan, YotamandGharbi, Micha\&quot;elandZhang, RichardandPark, TaesungandZhu, Jun-YanandCohen-Or, DanielandShechtman, Eli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nitzan_Domain_Expansion_of_Image_Generators_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15933-15942.png><br>
            
            <span class="tt"><span class="t0">研究问题：能否在尊重已训练生成模型的现有结构和知识的情况下，向其注入新的概念？<br>
                    动机：为了解决这一问题，我们提出了一个新的任务——领域扩展。<br>
                    方法：我们提出了一种新方法，通过“重新利用”预训练生成模型中未使用的“休眠”轴，来表示新的领域，而无需干扰原始表示。<br>
                    效果：实验结果表明，预训练生成模型有能力添加多个甚至数百个新领域。使用我们的扩展技术，一个“扩展”的模型可以取代许多特定领域的模型，而无需扩大模型规模。此外，使用单个扩展生成器可以自然地支持领域之间的平滑过渡和组合。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Can one inject new concepts into an already trained generative model, while respecting its existing structure and knowledge? We propose a new task -- domain expansion -- to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new domains? Interestingly, we find that the latent space offers unused, "dormant" axes, which do not affect the output. This provides an opportunity -- by "repurposing" these axes, we are able to represent new domains, without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several -- even hundreds -- of new domains! Using our expansion technique, one "expanded" model can supersede numerous domain-specific models, without expanding model size. Additionally, using a single, expanded generator natively supports smooth transitions between and composition of domains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">679.FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding</span><br>
                <span class="as">Truong, Thanh-DatandLe, NganandRaj, BhikshaandCothren, JacksonandLuu, Khoa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Truong_FREDOM_Fairness_Domain_Adaptation_Approach_to_Semantic_Scene_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19988-19997.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管近年来在语义场景分割中的领域适应取得了显著的改进，但公平性问题尚未得到明确定义和解决。<br>
                    动机：公平性是部署分割模型到与人类相关的现实世界应用（如自动驾驶）中最关键的方面之一，因为任何不公平的预测都可能影响人类安全。<br>
                    方法：本文提出了一种新的公平性领域适应（FREDOM）方法来进行语义场景分割。具体来说，从提出的公平性目标出发，基于类别分布的公平处理引入了一个新的适应框架。此外，为了一般化地模拟结构依赖的上下文，引入了一种新的条件结构约束来强制预测分割的一致性。<br>
                    效果：通过消融研究，该方法已显示出分割模型的性能改进，并促进了模型预测的公平性。在两个标准基准测试（即SYNTHIA -> Cityscapes和GTA5 -> Cityscapes）上的实验结果表明，我们的方法实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA -> Cityscapes and GTA5 -> Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">680.SimpleNet: A Simple Network for Image Anomaly Detection and Localization</span><br>
                <span class="as">Liu, ZhikangandZhou, YimingandXu, YuanshengandWang, Zilei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SimpleNet_A_Simple_Network_for_Image_Anomaly_Detection_and_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20402-20411.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种简单且实用的网络（称为SimpleNet）用于检测和定位异常。<br>
                    动机：目前的网络在异常检测和定位上存在一些问题，如需要大量训练数据、计算资源消耗大等。<br>
                    方法：SimpleNet由四个组件组成：预训练的特征提取器生成局部特征；浅层特征适配器将局部特征转移到目标领域；简单的异常特征生成器通过向正常特征添加高斯噪声来伪造异常特征；二进制异常判别器区分异常特征和正常特征。在推理过程中，异常特征生成器将被丢弃。<br>
                    效果：实验结果表明，尽管简单，但SimpleNet在数量和质量上都优于先前的方法。在MVTec AD基准测试中，SimpleNet实现了99.6%的异常检测AUROC，与表现第二好的模型相比，误差降低了55.5%。此外，SimpleNet比现有方法更快，在3080ti GPU上的帧率高达77 FPS。此外，SimpleNet在一类新颖性检测任务上的性能也有显著提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a simple and application-friendly network (called SimpleNet) for detecting and localizing anomalies. SimpleNet consists of four components: (1) a pre-trained Feature Extractor that generates local features, (2) a shallow Feature Adapter that transfers local features towards target domain, (3) a simple Anomaly Feature Generator that counterfeits anomaly features by adding Gaussian noise to normal features, and (4) a binary Anomaly Discriminator that distinguishes anomaly features from normal features. During inference, the Anomaly Feature Generator would be discarded. Our approach is based on three intuitions. First, transforming pre-trained features to target-oriented features helps avoid domain bias. Second, generating synthetic anomalies in feature space is more effective, as defects may not have much commonality in the image space. Third, a simple discriminator is much efficient and practical. In spite of simplicity, SimpleNet outperforms previous methods quantitatively and qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly detection AUROC of 99.6%, reducing the error by 55.5% compared to the next best performing model. Furthermore, SimpleNet is faster than existing methods, with a high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet demonstrates significant improvements in performance on the One-Class Novelty Detection task. Code: https://github.com/DonaldRR/SimpleNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">681.Decoupling MaxLogit for Out-of-Distribution Detection</span><br>
                <span class="as">Zhang, ZihanandXiang, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Decoupling_MaxLogit_for_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3388-3397.png><br>
            
            <span class="tt"><span class="t0">研究问题：在机器学习中，标准训练对分布内（ID）和分布外（OOD）数据的输出信心异常高，因此检测OOD样本的能力对于模型部署至关重要。<br>
                    动机：为了解决这一问题，研究人员提出了一种基于日志的评分函数MaxLogit，但发现其性能受到MaxNorm的影响。<br>
                    方法：研究人员将日志转换为余弦相似度和日志范数，并提出了使用MaxCosine和MaxNorm的方法。同时，他们提出了Decoupling MaxLogit (DML) 方法来平衡MaxCosine和MaxNorm，并将其扩展到DML+，以进一步体现该方法的核心思想。<br>
                    效果：实验结果表明，他们在CIFAR-10、CIFAR-100和ImageNet上提出的基于日志的OOD检测方法非常有效，并建立了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In machine learning, it is often observed that standard training outputs anomalously high confidence for both in-distribution (ID) and out-of-distribution (OOD) data. Thus, the ability to detect OOD samples is critical to the model deployment. An essential step for OOD detection is post-hoc scoring. MaxLogit is one of the simplest scoring functions which uses the maximum logits as OOD score. To provide a new viewpoint to study the logit-based scoring function, we reformulate the logit into cosine similarity and logit norm and propose to use MaxCosine and MaxNorm. We empirically find that MaxCosine is a core factor in the effectiveness of MaxLogit. And the performance of MaxLogit is encumbered by MaxNorm. To tackle the problem, we propose the Decoupling MaxLogit (DML) for flexibility to balance MaxCosine and MaxNorm. To further embody the core of our method, we extend DML to DML+ based on the new insights that fewer hard samples and compact feature space are the key components to make logit-based methods effective. We demonstrate the effectiveness of our logit-based OOD detection methods on CIFAR-10, CIFAR-100 and ImageNet and establish state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">682.Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation</span><br>
                <span class="as">Zhao, LinglanandLu, JingandXu, YunluandCheng, ZhanzhanandGuo, DashanandNiu, YiandFang, Xiangzhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11838-11847.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本类别增量学习（FSCIL）中由于数据稀缺性带来的挑战，即如何基于少量训练样本持续学习新类别。<br>
                    动机：现有的类别增量学习方法在处理小样本类别增量学习时，由于数据稀缺，往往会出现过拟合新类别的问题。<br>
                    方法：提出一种新的知识蒸馏结构，通过从两个互补的教师那里获取知识来解决这个问题。其中一个是训练于丰富基础类别数据的模型，可以用于缓解当前新类别的过拟合；另一个是上一轮增量学习会话更新的模型，包含了之前新类别的适应知识，用于减轻其遗忘。同时，引入一个基于类别语义相似性的自适应策略来结合这两种指导。<br>
                    效果：在mini-ImageNet、CIFAR100和CUB200三个流行的FSCIL数据集上的大量实验表明，该方法显著超越了现有工作，证明了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-Shot Class-Incremental Learning (FSCIL) aims to continually learn novel classes based on only few training samples, which poses a more challenging task than the well-studied Class-Incremental Learning (CIL) due to data scarcity. While knowledge distillation, a prevailing technique in CIL, can alleviate the catastrophic forgetting of older classes by regularizing outputs between current and previous model, it fails to consider the overfitting risk of novel classes in FSCIL. To adapt the powerful distillation technique for FSCIL, we propose a novel distillation structure, by taking the unique challenge of overfitting into account. Concretely, we draw knowledge from two complementary teachers. One is the model trained on abundant data from base classes that carries rich general knowledge, which can be leveraged for easing the overfitting of current novel classes. The other is the updated model from last incremental session that contains the adapted knowledge of previous novel classes, which is used for alleviating their forgetting. To combine the guidances, an adaptive strategy conditioned on the class-wise semantic similarities is introduced. Besides, for better preserving base class knowledge when accommodating novel concepts, we adopt a two-branch network with an attention-based aggregation module to dynamically merge predictions from two complementary branches. Extensive experiments on 3 popular FSCIL datasets: mini-ImageNet, CIFAR100 and CUB200 validate the effectiveness of our method by surpassing existing works by a significant margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">683.Detection of Out-of-Distribution Samples Using Binary Neuron Activation Patterns</span><br>
                <span class="as">Olber, Bart{\l</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Olber_Detection_of_Out-of-Distribution_Samples_Using_Binary_Neuron_Activation_Patterns_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3378-3387.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在各种应用中表现出色，但面临分布外（OOD）样本的挑战。<br>
                    动机：识别并处理未见过的输入对于安全关键应用如自动驾驶汽车、无人机和机器人至关重要。<br>
                    方法：提出一种新颖的OOD检测方法，通过理论分析ReLU架构中的神经元激活模式，从卷积层提取二进制表示的激活模式，避免引入高计算开销。<br>
                    效果：经过广泛的实证评估，该方法在各种深度神经网络架构和七个图像数据集上表现出高性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNN) have outstanding performance in various applications. Despite numerous efforts of the research community, out-of-distribution (OOD) samples remain a significant limitation of DNN classifiers. The ability to identify previously unseen inputs as novel is crucial in safety-critical applications such as self-driving cars, unmanned aerial vehicles, and robots. Existing approaches to detect OOD samples treat a DNN as a black box and evaluate the confidence score of the output predictions. Unfortunately, this method frequently fails, because DNNs are not trained to reduce their confidence for OOD inputs. In this work, we introduce a novel method for OOD detection. Our method is motivated by theoretical analysis of neuron activation patterns (NAP) in ReLU-based architectures. The proposed method does not introduce a high computational overhead due to the binary representation of the activation patterns extracted from convolutional layers. The extensive empirical evaluation proves its high performance on various DNN architectures and seven image datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">684.Enlarging Instance-Specific and Class-Specific Information for Open-Set Action Recognition</span><br>
                <span class="as">Cen, JunandZhang, ShiweiandWang, XiangandPei, YixuanandQing, ZhiwuandZhang, YingyaandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cen_Enlarging_Instance-Specific_and_Class-Specific_Information_for_Open-Set_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15295-15304.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放集动作识别中的问题，即如何拒绝训练集分布之外的未知人类动作。<br>
                    动机：现有的方法主要关注学习更好的不确定性分数，但忽视了特征表示的重要性。我们发现，在相同的不确定性分数下，具有更丰富语义多样性的特征可以显著提高开放集的性能。<br>
                    方法：本文首先基于信息瓶颈理论分析开放集动作识别（OSAR）问题中的特征表示行为，并提出扩大特征中实例特定（IS）和类别特定（CS）信息以提高性能。为此，提出了一种新的原型相似性学习（PSL）框架，以保持同一类中的实例方差，从而保留更多的IS信息。此外，我们还注意到，与已知样本外观相似的未知样本容易被误分类为已知类别。为了解决这个问题，我们在PSL中进一步引入了视频混洗，以学习原始样本和混洗样本之间的独特时间信息，我们发现这扩大了CS信息。<br>
                    效果：大量实验证明，提出的PSL可以显著提高开放集和封闭集的性能，并在多个基准测试上取得了最先进的结果。代码可在https://github.com/Jun-CEN/PSL获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-set action recognition is to reject unknown human action cases which are out of the distribution of the training set. Existing methods mainly focus on learning better uncertainty scores but dismiss the importance of feature representations. We find that features with richer semantic diversity can significantly improve the open-set performance under the same uncertainty scores. In this paper, we begin with analyzing the feature representation behavior in the open-set action recognition (OSAR) problem based on the information bottleneck (IB) theory, and propose to enlarge the instance-specific (IS) and class-specific (CS) information contained in the feature for better performance. To this end, a novel Prototypical Similarity Learning (PSL) framework is proposed to keep the instance variance within the same class to retain more IS information. Besides, we notice that unknown samples sharing similar appearances to known samples are easily misclassified as known classes. To alleviate this issue, video shuffling is further introduced in our PSL to learn distinct temporal information between original and shuffled samples, which we find enlarges the CS information. Extensive experiments demonstrate that the proposed PSL can significantly boost both the open-set and closed-set performance and achieves state-of-the-art results on multiple benchmarks. Code is available at https://github.com/Jun-CEN/PSL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">685.Deep Hashing With Minimal-Distance-Separated Hash Centers</span><br>
                <span class="as">Wang, LiangdaoandPan, YanandLiu, CongandLai, HanjiangandYin, JianandLiu, Ye</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Hashing_With_Minimal-Distance-Separated_Hash_Centers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23455-23464.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高大规模图像检索的效率和效果。<br>
                    动机：现有的有监督深度学习哈希方法在训练效率、数据分布覆盖以及样本对不平衡问题上存在问题。<br>
                    方法：提出一种优化方法，通过使用编码理论中的吉尔伯特-瓦沙莫夫边界来获取大的最小距离，同时确保优化方法的实证可行性。并采用明确分离的哈希中心为每个图像类别分配一个哈希中心，并提出几种有效的损失函数来训练深度哈希网络。<br>
                    效果：在三个图像检索数据集上的大量实验表明，该方法在检索性能上超过了最先进的深度学习哈希方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep hashing is an appealing approach for large-scale image retrieval. Most existing supervised deep hashing methods learn hash functions using pairwise or triple image similarities in randomly sampled mini-batches. They suffer from low training efficiency, insufficient coverage of data distribution, and pair imbalance problems. Recently, central similarity quantization (CSQ) attacks the above problems by using "hash centers" as a global similarity metric, which encourages the hash codes of similar images to approach their common hash center and distance themselves from other hash centers. Although achieving SOTA retrieval performance, CSQ falls short of a worst-case guarantee on the minimal distance between its constructed hash centers, i.e. the hash centers can be arbitrarily close. This paper presents an optimization method that finds hash centers with a constraint on the minimal distance between any pair of hash centers, which is non-trivial due to the non-convex nature of the problem. More importantly, we adopt the Gilbert-Varshamov bound from coding theory, which helps us to obtain a large minimal distance while ensuring the empirical feasibility of our optimization approach. With these clearly-separated hash centers, each is assigned to one image class, we propose several effective loss functions to train deep hashing networks. Extensive experiments on three datasets for image retrieval demonstrate that the proposed method achieves superior retrieval performance over the state-of-the-art deep hashing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">686.GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection</span><br>
                <span class="as">Liu, XixiandLochman, YaroslavaandZach, Christopher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23946-23955.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行神经网络的OOD检测，特别是在大规模数据集上。<br>
                    动机：为了成功部署神经网络，特别是对于安全关键应用，需要进行OOD检测。在大规模数据集上进行OOD检测更接近现实，但也更具挑战性。<br>
                    方法：提出了一种通用的熵评分函数GEN，这是一种简单但有效的基于熵的评分函数，可以应用于任何预训练的基于softmax的分类器。<br>
                    效果：在大型ImageNet-1k OOD检测基准测试中，GEN的性能表现优越，其平均AUROC比六种常用的CNN和视觉变换器分类器的先进后处理方法提高了至少3.5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Out-of-distribution (OOD) detection has been extensively studied in order to successfully deploy neural networks, in particular, for safety-critical applications. Moreover, performing OOD detection on large-scale datasets is closer to reality, but is also more challenging. Several approaches need to either access the training data for score design or expose models to outliers during training. Some post-hoc methods are able to avoid the aforementioned constraints, but are less competitive. In this work, we propose Generalized ENtropy score (GEN), a simple but effective entropy-based score function, which can be applied to any pre-trained softmax-based classifier. Its performance is demonstrated on the large-scale ImageNet-1k OOD detection benchmark. It consistently improves the average AUROC across six commonly-used CNN-based and visual transformer classifiers over a number of state-of-the-art post-hoc methods. The average AUROC improvement is at least 3.5%. Furthermore, we used GEN on top of feature-based enhancing methods as well as methods using training statistics to further improve the OOD detection performance. The code is available at: https://github.com/XixiLiu95/GEN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">687.Sample-Level Multi-View Graph Clustering</span><br>
                <span class="as">Tan, YuzeandLiu, YixiandHuang, ShudongandFeng, WentaoandLv, Jiancheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Sample-Level_Multi-View_Graph_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23966-23975.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多视角聚类中存在的挑战，如忽视数据拓扑结构以及在探索聚类结构时无法完全保持不同视图之间的局部结构一致性。<br>
                    动机：现有的多视角聚类算法往往忽视了数据中的拓扑结构，且在探索聚类结构时无法完全保持不同视图之间的局部结构一致性。<br>
                    方法：本文提出了一种利用数据隐含的流形来学习数据拓扑结构的多视角聚类方法。同时，考虑到多个视图的一致性主要体现在相似的局部结构上，而不一致的结构是少数，因此进一步在样本级别探索了多个视图的交集，以更好地保持跨视图的一致性。<br>
                    效果：实验结果证明，该方法在各种多视角数据集上都表现出了有效性，并优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-view clustering have hitherto been studied due to their effectiveness in dealing with heterogeneous data. Despite the empirical success made by recent works, there still exists several severe challenges. Particularly, previous multi-view clustering algorithms seldom consider the topological structure in data, which is essential for clustering data on manifold. Moreover, existing methods cannot fully consistency the consistency of local structures between different views as they explore the clustering structure in a view-wise manner. In this paper, we propose to exploit the implied data manifold by learning the topological structure of data. Besides, considering that the consistency of multiple views is manifested in the generally similar local structure while the inconsistent structures are minority, we further explore the intersections of multiple views in the sample level such that the cross-view consistency can be better maintained. We model the above concerns in a unified framework and design an efficient algorithm to solve the corresponding optimization problem. Experimental results on various multi-view datasets certificate the effectiveness of the proposed method and verify its superiority over other SOTA approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">688.Curricular Contrastive Regularization for Physics-Aware Single Image Dehazing</span><br>
                <span class="as">Zheng, YuandZhan, JiahuiandHe, ShengfengandDong, JunyuandDu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Curricular_Contrastive_Regularization_for_Physics-Aware_Single_Image_Dehazing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5785-5794.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高图像去雾模型的性能和解释性。<br>
                    动机：目前的图像去雾模型存在欠拟合问题，对比度正则化方法引入了负样本信息作为下界，但负样本通常远离清晰（即正）图像，导致解决方案空间仍然受到限制。此外，对深度去雾模型的可解释性对于雾霾过程的物理机制的研究还不够充分。<br>
                    方法：本文提出了一种新的课程对比度正则化方法，针对一个一致的对比空间而不是一个不一致的对比空间。我们的负样本可以由1）有雾图像和2）其他现有方法的相应恢复组成，提供了更好的下界约束。此外，由于清晰图像和负样本嵌入之间的不同相似性，多个组件的学习难度本质上是不平衡的。为了解决这个问题，我们定制了一个课程学习策略来重新权衡不同负样本的重要性。另外，为了提高特征空间中的可解释性，我们根据大气散射模型构建了一个物理感知的双分支单元。<br>
                    效果：通过使用该单元以及课程对比度正则化，我们建立了我们的去雾网络C2PNet。大量实验表明，我们的C2PNet显著优于最先进的方法，在SOTS室内和SOTS室外数据集上分别实现了3.94dB和1.50dB的最大PSNR提升。代码可在https://github.com/YuZheng9/C2PNet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Considering the ill-posed nature, contrastive regularization has been developed for single image dehazing, introducing the information from negative images as a lower bound. However, the contrastive samples are nonconsensual, as the negatives are usually represented distantly from the clear (i.e., positive) image, leaving the solution space still under-constricted. Moreover, the interpretability of deep dehazing models is underexplored towards the physics of the hazing process. In this paper, we propose a novel curricular contrastive regularization targeted at a consensual contrastive space as opposed to a non-consensual one. Our negatives, which provide better lower-bound constraints, can be assembled from 1) the hazy image, and 2) corresponding restorations by other existing methods. Further, due to the different similarities between the embeddings of the clear image and negatives, the learning difficulty of the multiple components is intrinsically imbalanced. To tackle this issue, we customize a curriculum learning strategy to reweight the importance of different negatives. In addition, to improve the interpretability in the feature space, we build a physics-aware dual-branch unit according to the atmospheric scattering model. With the unit, as well as curricular contrastive regularization, we establish our dehazing network, named C2PNet. Extensive experiments demonstrate that our C2PNet significantly outperforms state-of-the-art methods, with extreme PSNR boosts of 3.94dB and 1.50dB, respectively, on SOTS-indoor and SOTS-outdoor datasets. Code is available at https://github.com/YuZheng9/C2PNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">689.Learning From Noisy Labels With Decoupled Meta Label Purifier</span><br>
                <span class="as">Tu, YuanpengandZhang, BoshenandLi, YuxiandLiu, LiangandLi, JianandWang, YabiaoandWang, ChengjieandZhao, CaiRong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Learning_From_Noisy_Labels_With_Decoupled_Meta_Label_Purifier_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19934-19943.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练深度神经网络时，由于标签噪声的存在，模型容易记住不准确的标签，导致泛化能力差。<br>
                    动机：为了解决这个问题，研究人员开始采用基于元学习的标签修正策略，通过使用一小部分干净的验证数据来识别和修正潜在的噪声标签。然而，这种双重优化过程（模型权重和超参数的优化）限制了模型表示能力和修正标签的准确性。<br>
                    方法：本文提出了一种新的多阶段标签净化器DMLP，它将标签修正过程解耦为无标签表示学习和一个简单的元标签净化器。这样，DMLP可以在两个不同的阶段专注于提取判别性特征和修正标签。<br>
                    效果：实验结果表明，DMLP在几个合成和真实世界的噪声数据集上取得了最先进的结果，特别是在高噪声水平下。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training deep neural networks (DNN) with noisy labels is challenging since DNN can easily memorize inaccurate labels, leading to poor generalization ability. Recently, the meta-learning based label correction strategy is widely adopted to tackle this problem via identifying and correcting potential noisy labels with the help of a small set of clean validation data. Although training with purified labels can effectively improve performance, solving the meta-learning problem inevitably involves a nested loop of bi-level optimization between model weights and hyperparameters (i.e., label distribution). As compromise, previous methods resort toa coupled learning process with alternating update. In this paper, we empirically find such simultaneous optimization over both model weights and label distribution can not achieve an optimal routine, consequently limiting the representation ability of backbone and accuracy of corrected labels. From this observation, a novel multi-stage label purifier named DMLP is proposed. DMLP decouples the label correction process into label-free representation learning and a simple meta label purifier, In this way, DMLP can focus on extracting discriminative feature and label correction in two distinctive stages. DMLP is a plug-and-play label purifier, the purified labels can be directly reused in naive end-to-end network retraining or other robust learning methods, where state-of-the-art results are obtained on several synthetic and real-world noisy datasets, especially under high noise levels.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">690.Sharpness-Aware Gradient Matching for Domain Generalization</span><br>
                <span class="as">Wang, PengfeiandZhang, ZhaoxiangandLei, ZhenandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3769-3778.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高模型从源领域到未见过的其他领域的泛化能力。<br>
                    动机：尽管现有的锐度感知最小化（SAM）方法在领域泛化（DG）上表现出色，但并不总是能收敛到期望的平坦区域和较小的损失值。<br>
                    方法：提出了两个条件来确保模型可以收敛到一个具有小损失的平坦最小值，并提出了锐度感知梯度匹配（SAGM）算法以满足这两个条件以增强模型的泛化能力。<br>
                    效果：实验结果表明，SAGM方法在五个DG基准测试中始终优于最先进的方法，包括PACS、VLCS、OfficeHome、TerraIncognita和DomainNet。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of domain generalization (DG) is to enhance the generalization capability of the model learned from a source domain to other unseen domains. The recently developed Sharpness-Aware Minimization (SAM) method aims to achieve this goal by minimizing the sharpness measure of the loss landscape. Though SAM and its variants have demonstrated impressive DG performance, they may not always converge to the desired flat region with a small loss value. In this paper, we present two conditions to ensure that the model could converge to a flat minimum with a small loss, and present an algorithm, named Sharpness-Aware Gradient Matching (SAGM), to meet the two conditions for improving model generalization capability. Specifically, the optimization objective of SAGM will simultaneously minimize the empirical risk, the perturbed loss (i.e., the maximum loss within a neighborhood in the parameter space), and the gap between them. By implicitly aligning the gradient directions between the empirical risk and the perturbed loss, SAGM improves the generalization capability over SAM and its variants without increasing the computational cost. Extensive experimental results show that our proposed SAGM method consistently outperforms the state-of-the-art methods on five DG benchmarks, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Codes are available at https://github.com/Wang-pengfei/SAGM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">691.Local Connectivity-Based Density Estimation for Face Clustering</span><br>
                <span class="as">Shin, JunhoandLee, Hyo-JunandKim, HyunseopandBaek, Jong-HyeonandKim, DaehyunandKoh, YeongJun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Local_Connectivity-Based_Density_Estimation_for_Face_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13621-13629.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于图的面部聚类方法存在预测大量边缘连接的问题，包括将不同类别节点链接的错误阳性边缘。<br>
                    动机：这些错误阳性边缘可能会在连接性被错误估计时将不同的簇合并在一起，导致面部聚类效果不佳。<br>
                    方法：本文提出了一种新的面部聚类方法，该方法采用基于密度的聚类，保持了较高密度的边缘。为此，我们提出了一种基于K最近邻（KNN）之间局部连接的可靠密度估计算法。<br>
                    效果：实验结果表明，所提出的聚类方法在大规模面部聚类数据集和时尚图像聚类数据集上显著优于最先进的聚类方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent graph-based face clustering methods predict the connectivity of enormous edges, including false positive edges that link nodes with different classes. However, those false positive edges, which connect negative node pairs, have the risk of integration of different clusters when their connectivity is incorrectly estimated. This paper proposes a novel face clustering method to address this problem. The proposed clustering method employs density-based clustering, which maintains edges that have higher density. For this purpose, we propose a reliable density estimation algorithm based on local connectivity between K nearest neighbors (KNN). We effectively exclude negative pairs from the KNN graph based on the reliable density while maintaining sufficient positive pairs. Furthermore, we develop a pairwise connectivity estimation network to predict the connectivity of the selected edges. Experimental results demonstrate that the proposed clustering method significantly outperforms the state-of-the-art clustering methods on large-scale face clustering datasets and fashion image clustering datasets. Our code is available at https://github.com/illian01/LCE-PCENet</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">692.Deep Deterministic Uncertainty: A New Simple Baseline</span><br>
                <span class="as">Mukhoti, JishnuandKirsch, AndreasandvanAmersfoort, JoostandTorr, PhilipH.S.andGal, Yarin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24384-24394.png><br>
            
            <span class="tt"><span class="t0">研究问题：寻找可靠的确定性单向前向传递模型的不确定性，因为传统的不确定性量化方法计算成本高。<br>
                    动机：对两种复杂的单向前向传递不确定性方法DUQ和SNGP进行研究，看它们是否主要依赖于良好规范化的特征空间。<br>
                    方法：通过残差连接和谱归一化实现具有这种规范化特征空间的单一softmax神经网络，无需使用更复杂的估计不确定性的方法，就优于DUQ和SNGP使用简单高斯判别分析作为单独特征空间密度估计器的先验知识不确定性预测。<br>
                    效果：概念上简单的深度确定性不确定性（DDU）基线也可以用于区分随机性和先验知识不确定性，并在几个OoD基准测试（CIFAR-10/100与SVHN/Tiny-ImageNet，ImageNet与ImageNet-O），不同模型架构的主动学习设置以及大规模视觉任务如语义分割等方面表现良好，同时计算成本更低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reliable uncertainty from deterministic single-forward pass models is sought after because conventional methods of uncertainty quantification are computationally expensive. We take two complex single-forward-pass uncertainty approaches, DUQ and SNGP, and examine whether they mainly rely on a well-regularized feature space. Crucially, without using their more complex methods for estimating uncertainty, we find that a single softmax neural net with such a regularized feature-space, achieved via residual connections and spectral normalization, outperforms DUQ and SNGP's epistemic uncertainty predictions using simple Gaussian Discriminant Analysis post-training as a separate feature-space density estimator---without fine-tuning on OoD data, feature ensembling, or input pre-procressing. Our conceptually simple Deep Deterministic Uncertainty (DDU) baseline can also be used to disentangle aleatoric and epistemic uncertainty and performs as well as Deep Ensembles, the state-of-the art for uncertainty prediction, on several OoD benchmarks (CIFAR-10/100 vs SVHN/Tiny-ImageNet, ImageNet vs ImageNet-O), active learning settings across different model architectures, as well as in large scale vision tasks like semantic segmentation, while being computationally cheaper.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">693.Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need</span><br>
                <span class="as">Wei, TongandGan, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Towards_Realistic_Long-Tailed_Semi-Supervised_Learning_Consistency_Is_All_You_Need_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3469-3478.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的长尾半监督学习算法通常假设标记和未标记的数据类别分布几乎相同，但在数据类别分布不匹配时会遭受严重影响。<br>
                    动机：为了解决这个问题，我们提出了一种新的简单方法，通过引入自适应一致性正则化器（ACR）来有效地利用未知类别分布的未标记数据。<br>
                    方法：ACR通过估计未标记数据的真实类别分布，以统一公式实现对各种分布的伪标签动态细化。<br>
                    效果：实验表明，ACR在多种标准的长尾半监督学习基准测试中取得了最先进的性能，例如，当标记和未标记数据的类别分布不匹配时，相对于现有算法，测试精度平均提高了10%。即使类别分布相同，ACR也始终优于许多复杂的长尾半监督学习算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While long-tailed semi-supervised learning (LTSSL) has received tremendous attention in many real-world classification problems, existing LTSSL algorithms typically assume that the class distributions of labeled and unlabeled data are almost identical. Those LTSSL algorithms built upon the assumption can severely suffer when the class distributions of labeled and unlabeled data are mismatched since they utilize biased pseudo-labels from the model. To alleviate this issue, we propose a new simple method that can effectively utilize unlabeled data of unknown class distributions by introducing the adaptive consistency regularizer (ACR). ACR realizes the dynamic refinery of pseudo-labels for various distributions in a unified formula by estimating the true class distribution of unlabeled data. Despite its simplicity, we show that ACR achieves state-of-the-art performance on a variety of standard LTSSL benchmarks, e.g., an averaged 10% absolute increase of test accuracy against existing algorithms when the class distributions of labeled and unlabeled data are mismatched. Even when the class distributions are identical, ACR consistently outperforms many sophisticated LTSSL algorithms. We carry out extensive ablation studies to tease apart the factors that are most important to ACR's success. Source code is available at https://github.com/Gank0078/ACR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">694.PartMix: Regularization Strategy To Learn Part Discovery for Visible-Infrared Person Re-Identification</span><br>
                <span class="as">Kim, MinsuandKim, SeungryongandPark, JunginandPark, SeongheonandSohn, Kwanghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_PartMix_Regularization_Strategy_To_Learn_Part_Discovery_for_Visible-Infrared_Person_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18621-18632.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索一种适用于基于部分的可见光-红外人体重识别（VI-ReID）模型的数据增强技术。<br>
                    动机：现有的数据增强技术在各种计算机视觉应用中可以防止模型过拟合，但针对基于部分的VI-ReID模型的适当数据增强技术尚未被探索。<br>
                    方法：我们提出了一种新的数据增强技术，称为PartMix，通过混合跨模态的部分描述符来合成增强的样本，以提高基于部分的VI-ReID模型的性能。我们还提出了一种基于熵的挖掘策略，以减弱不可靠正负样本的负面影响。<br>
                    效果：实验结果表明，PartMix在现有的基于部分的VI-ReID模型中的表现优于现有的VI-ReID方法，且具有稳定性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern data augmentation using a mixture-based technique can regularize the models from overfitting to the training data in various computer vision applications, but a proper data augmentation technique tailored for the part-based Visible-Infrared person Re-IDentification (VI-ReID) models remains unexplored. In this paper, we present a novel data augmentation technique, dubbed PartMix, that synthesizes the augmented samples by mixing the part descriptors across the modalities to improve the performance of part-based VI-ReID models. Especially, we synthesize the positive and negative samples within the same and across different identities and regularize the backbone model through contrastive learning. In addition, we also present an entropy-based mining strategy to weaken the adverse impact of unreliable positive and negative samples. When incorporated into existing part-based VI-ReID model, PartMix consistently boosts the performance. We conduct experiments to demonstrate the effectiveness of our PartMix over the existing VI-ReID methods and provide ablation studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">695.Learning Sample Relationship for Exposure Correction</span><br>
                <span class="as">Huang, JieandZhao, FengandZhou, ManandXiao, JieandZheng, NaishanandZheng, KaiwenandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Sample_Relationship_for_Exposure_Correction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9904-9913.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有曝光修正方法在优化过程中的不一致性问题。<br>
                    动机：尽管现有的曝光修正方法取得了很大的进步，但它们通常是通过训练小批量混合的欠曝和过曝样本来进行的，没有探索它们之间的关系来解决优化的不一致性。<br>
                    方法：本文提出了一种新的视角，通过在一个mini-batch中关联和约束修正过程的关系来连接它们的优化过程。该方法的核心设计包括两个步骤：1）通过一个与上下文无关的预训练任务来形成样本在整个批次维度上的曝光关系；2）将上述样本关系设计作为损失函数中的正则化项，以促进优化的一致性。<br>
                    效果：通过在多个代表性曝光修正基准上进行大量实验，证明了引入样本关系设计可以带来一致的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Exposure correction task aims to correct the underexposure and its adverse overexposure images to the normal exposure in a single network. As well recognized, the optimization flow is opposite. Despite the great advancement, existing exposure correction methods are usually trained with a mini-batch of both underexposure and overexposure mixed samples and have not explored the relationship between them to solve the optimization inconsistency. In this paper, we introduce a new perspective to conjunct their optimization processes by correlating and constraining the relationship of correction procedure in a mini-batch. The core designs of our framework consist of two steps: 1) formulating the exposure relationship of samples across the batch dimension via a context-irrelevant pretext task. 2) delivering the above sample relationship design as the regularization term within the loss function to promote optimization consistency. The proposed sample relationship design as a general term can be easily integrated into existing exposure correction methods without any computational burden in inference time. Extensive experiments over multiple representative exposure correction benchmarks demonstrate consistent performance gains by introducing our sample relationship design.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">696.TTA-COPE: Test-Time Adaptation for Category-Level Object Pose Estimation</span><br>
                <span class="as">Lee, TaeyeopandTremblay, JonathanandBlukis, ValtsandWen, BowenandLee, Byeong-UkandShin, InkyuandBirchfield, StanandKweon, InSoandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_TTA-COPE_Test-Time_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21285-21295.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过逐步更新模型，无需目标数据标签来解决源到目标领域差距。<br>
                    动机：解决类别级物体姿态估计的无监督领域适应问题。<br>
                    方法：提出一种名为TTA-COPE的测试时间适应方法，设计了一种带有自我训练损失的姿态集合方法，使用姿态感知置信度。<br>
                    效果：实验结果表明，所提出的姿态集合和自我训练损失在半监督和无监督设置下都能提高类别级物体姿态性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test-time adaptation methods have been gaining attention recently as a practical solution for addressing source-to-target domain gaps by gradually updating the model without requiring labels on the target data. In this paper, we propose a method of test-time adaptation for category-level object pose estimation called TTA-COPE. We design a pose ensemble approach with a self-training loss using pose-aware confidence. Unlike previous unsupervised domain adaptation methods for category-level object pose estimation, our approach processes the test data in a sequential, online manner, and it does not require access to the source domain at runtime. Extensive experimental results demonstrate that the proposed pose ensemble and the self-training loss improve category-level object pose performance during test time under both semi-supervised and unsupervised settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">697.Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic Segmentation</span><br>
                <span class="as">Yang, YuweiandHayat, MunawarandJin, ZhaoandRen, ChaoandLei, Yinjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Geometry_and_Uncertainty-Aware_3D_Point_Cloud_Class-Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21759-21768.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管3D点云语义分割在最近取得了显著进展，但当前的方法需要一次性训练所有类别的数据，不适合新类别不断被发现的真实场景。<br>
                    动机：为了使用先前的知识持续学习新类别，我们提出了一种3D点云的类别增量语义分割方法。由于3D点云无序且无结构，特别是在没有先前数据的情况下，存储和传递知识变得困难。<br>
                    方法：我们设计了一个几何感知的蒸馏模块来转移点特征关联的几何特性。为了应对由语义转移引起的遗忘问题，我们还开发了一种不确定性感知的伪标签方案，通过局部邻域内的标签传播消除了不确定伪标签中的噪声。<br>
                    效果：我们在S3DIS和ScanNet上进行了广泛的类别增量实验，结果令人印象深刻，与联合训练策略（上限）相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the significant recent progress made on 3D point cloud semantic segmentation, the current methods require training data for all classes at once, and are not suitable for real-life scenarios where new categories are being continuously discovered. Substantial memory storage and expensive re-training is required to update the model to sequentially arriving data for new concepts. In this paper, to continually learn new categories using previous knowledge, we introduce class-incremental semantic segmentation of 3D point cloud. Unlike 2D images, 3D point clouds are disordered and unstructured, making it difficult to store and transfer knowledge especially when the previous data is not available. We further face the challenge of semantic shift, where previous/future classes are indiscriminately collapsed and treated as the background in the current step, causing a dramatic performance drop on past classes. We exploit the structure of point cloud and propose two strategies to address these challenges. First, we design a geometry-aware distillation module that transfers point-wise feature associations in terms of their geometric characteristics. To counter forgetting caused by the semantic shift, we further develop an uncertainty-aware pseudo-labelling scheme that eliminates noise in uncertain pseudo-labels by label propagation within a local neighborhood. Our extensive experiments on S3DIS and ScanNet in a class-incremental setting show impressive results comparable to the joint training strategy (upper bound). Code is available at: https://github.com/leolyj/3DPC-CISS</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">698.Decompose, Adjust, Compose: Effective Normalization by Playing With Frequency for Domain Generalization</span><br>
                <span class="as">Lee, SangrokandBae, JongseongandKim, HaYoung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Decompose_Adjust_Compose_Effective_Normalization_by_Playing_With_Frequency_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11776-11785.png><br>
            
            <span class="tt"><span class="t0">研究问题：评估计算机视觉模型的鲁棒性，特别是在领域泛化任务中。<br>
                    动机：现有的领域泛化方法在去除风格时存在内容变化问题，因为内容和风格的边界不清晰。<br>
                    方法：从频率域的角度出发，将振幅和相位分别视为风格和内容，提出一种新的归一化方法PCNorm，并通过频谱分解只保留内容来消除风格。此外，还提出了改进的PCNorm变体CCNorm和SCNorm，它们可以分别调整内容和风格的变异程度，以学习领域无关的表示进行领域泛化。<br>
                    效果：使用这些归一化方法，我们提出了ResNet变体模型DAC-P和DAC-SC，它们对领域差距具有鲁棒性。所提出的模型在五个数据集上的表现优于其他最新的领域泛化方法，其中DAC-SC在PACS、VLCS、Office-Home、DomainNet和TerraIncognita五个数据集上的平均性能达到了65.6%，处于最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain generalization (DG) is a principal task to evaluate the robustness of computer vision models. Many previous studies have used normalization for DG. In normalization, statistics and normalized features are regarded as style and content, respectively. However, it has a content variation problem when removing style because the boundary between content and style is unclear. This study addresses this problem from the frequency domain perspective, where amplitude and phase are considered as style and content, respectively. First, we verify the quantitative phase variation of normalization through the mathematical derivation of the Fourier transform formula. Then, based on this, we propose a novel normalization method, PCNorm, which eliminates style only as the preserving content through spectral decomposition. Furthermore, we propose advanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees of variations in content and style, respectively. Thus, they can learn domain-agnostic representations for DG. With the normalization methods, we propose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domain gap. The proposed models outperform other recent DG methods. The DAC-SC achieves an average state-of-the-art performance of 65.6% on five datasets: PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">699.Multilateral Semantic Relations Modeling for Image Text Retrieval</span><br>
                <span class="as">Wang, ZhengandGao, ZhenweiandGuo, KangshuaiandYang, YangandWang, XiaomingandShen, HengTao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multilateral_Semantic_Relations_Modeling_for_Image_Text_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2830-2839.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像-文本检索中一对多对应关系的问题，即如何通过细粒度的配对将视觉和语言联系起来。<br>
                    动机：尽管现有的解决方案如多点映射、概率分布和几何嵌入等已取得一定进展，但一对多对应关系仍未得到充分探索。<br>
                    方法：本文提出了一种多边语义关系建模（MSRM）方法，通过超图模型捕捉多个样本与给定查询之间的一对多对应关系。具体来说，首先将给定的查询映射为概率嵌入，学习其基于马氏距离的真实语义分布；然后将每个候选实例视为一个具有平均语义的超图节点，而高斯查询则被建模为超边以捕捉候选点和查询之间的语义关联。<br>
                    效果：在两个广泛使用的数据集上的全面实验结果表明，我们的MSRM方法在解决多个匹配问题方面优于现有方法，同时在实例级别匹配方面保持了相当的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-text retrieval is a fundamental task to bridge vision and language by exploiting various strategies to fine-grained alignment between regions and words. This is still tough mainly because of one-to-many correspondence, where a set of matches from another modality can be accessed by a random query. While existing solutions to this problem including multi-point mapping, probabilistic distribution, and geometric embedding have made promising progress, one-to-many correspondence is still under-explored. In this work, we develop a Multilateral Semantic Relations Modeling (termed MSRM) for image-text retrieval to capture the one-to-many correspondence between multiple samples and a given query via hypergraph modeling. Specifically, a given query is first mapped as a probabilistic embedding to learn its true semantic distribution based on Mahalanobis distance. Then each candidate instance in a mini-batch is regarded as a hypergraph node with its mean semantics while a Gaussian query is modeled as a hyperedge to capture the semantic correlations beyond the pair between candidate points and the query. Comprehensive experimental results on two widely used datasets demonstrate that our MSRM method can outperform state-of-the-art methods in the settlement of multiple matches while still maintaining the comparable performance of instance-level matching. Our codes and checkpoints will be released soon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">700.Novel Class Discovery for 3D Point Cloud Semantic Segmentation</span><br>
                <span class="as">Riz, LuigiandSaltori, CristianoandRicci, ElisaandPoiesi, Fabio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Riz_Novel_Class_Discovery_for_3D_Point_Cloud_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9393-9402.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用标注的基础类别，对未标注的新类别进行语义分割。<br>
                    动机：尽管2D图像数据的新颖类别发现（NCD）问题已经得到了解决，但3D点云数据上的问题尚未得到解决。<br>
                    方法：我们提出了一种基于在线聚类的新型NCD方法，该方法利用不确定性量化来为新类别的点生成伪标签原型。<br>
                    效果：我们在SemanticKITTI和SemanticPOSS数据集上进行了全面评估，结果显示，我们的方法可以显著超越基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel class discovery (NCD) for semantic segmentation is the task of learning a model that can segment unlabelled (novel) classes using only the supervision from labelled (base) classes. This problem has recently been pioneered for 2D image data, but no work exists for 3D point cloud data. In fact, the assumptions made for 2D are loosely applicable to 3D in this case. This paper is presented to advance the state of the art on point cloud data analysis in four directions. Firstly, we address the new problem of NCD for point cloud semantic segmentation. Secondly, we show that the transposition of the only existing NCD method for 2D semantic segmentation to 3D data is suboptimal. Thirdly, we present a new method for NCD based on online clustering that exploits uncertainty quantification to produce prototypes for pseudo-labelling the points of the novel classes. Lastly, we introduce a new evaluation protocol to assess the performance of NCD for point cloud semantic segmentation. We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline. Project page: https://github.com/LuigiRiz/NOPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">701.Normalizing Flow Based Feature Synthesis for Outlier-Aware Object Detection</span><br>
                <span class="as">Kumar, Nishantand\v{S</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_Normalizing_Flow_Based_Feature_Synthesis_for_Outlier-Aware_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5156-5165.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高物体检测器对异常物体的识别能力。<br>
                    动机：目前通用的物体检测器如Faster R-CNN容易对异常物体给出过于自信的预测，这对于自动驾驶等应用来说十分关键。<br>
                    方法：提出一种新的异常物体检测框架，通过学习所有正常类别的联合数据分布以及可逆归一化流来区分异常物体和正常物体。<br>
                    效果：在图像和视频数据集上，该方法显著优于现有的异常物体检测技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-world deployment of reliable object detectors is crucial for applications such as autonomous driving. However, general-purpose object detectors like Faster R-CNN are prone to providing overconfident predictions for outlier objects. Recent outlier-aware object detection approaches estimate the density of instance-wide features with class-conditional Gaussians and train on synthesized outlier features from their low-likelihood regions. However, this strategy does not guarantee that the synthesized outlier features will have a low likelihood according to the other class-conditional Gaussians. We propose a novel outlier-aware object detection framework that distinguishes outliers from inlier objects by learning the joint data distribution of all inlier classes with an invertible normalizing flow. The appropriate sampling of the flow model ensures that the synthesized outliers have a lower likelihood than inliers of all object classes, thereby modeling a better decision boundary between inlier and outlier objects. Our approach significantly outperforms the state-of-the-art for outlier-aware object detection on both image and video datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">702.DivClust: Controlling Diversity in Deep Clustering</span><br>
                <span class="as">Metaxas, IoannisManiadisandTzimiropoulos, GeorgiosandPatras, Ioannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Metaxas_DivClust_Controlling_Diversity_in_Deep_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3418-3428.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习聚类方法无法有效生成具有多样性的数据集分区。<br>
                    动机：多样性的基聚类对于产生更优、更鲁棒的结果的共识聚类是必要的，但现有方法无法有效生成。<br>
                    方法：提出DivClust，一种可以整合到现有深度学习聚类框架中以生成具有期望多样性的多个聚类的多样性控制损失函数。<br>
                    效果：实验证明，该方法能有效控制不同框架和数据集的多样性，且其生成的聚类结果明显优于单一聚类基准，使用现有的共识聚类算法，DivClust产生的共识聚类结果也始终优于单一聚类基准，从而有效提升了基础深度学习聚类框架的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Clustering has been a major research topic in the field of machine learning, one to which Deep Learning has recently been applied with significant success. However, an aspect of clustering that is not addressed by existing deep clustering methods, is that of efficiently producing multiple, diverse partitionings for a given dataset. This is particularly important, as a diverse set of base clusterings are necessary for consensus clustering, which has been found to produce better and more robust results than relying on a single clustering. To address this gap, we propose DivClust, a diversity controlling loss that can be incorporated into existing deep clustering frameworks to produce multiple clusterings with the desired degree of diversity. We conduct experiments with multiple datasets and deep clustering frameworks and show that: a) our method effectively controls diversity across frameworks and datasets with very small additional computational cost, b) the sets of clusterings learned by DivClust include solutions that significantly outperform single-clustering baselines, and c) using an off-the-shelf consensus clustering algorithm, DivClust produces consensus clustering solutions that consistently outperform single-clustering baselines, effectively improving the performance of the base deep clustering framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">703.Bi-Directional Distribution Alignment for Transductive Zero-Shot Learning</span><br>
                <span class="as">Wang, ZhicaiandHao, YanbinandMu, TingtingandLi, OuxiangandWang, ShuoandHe, Xiangnan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-Directional_Distribution_Alignment_for_Transductive_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19893-19902.png><br>
            
            <span class="tt"><span class="t0">研究问题：零样本学习（ZSL）在处理未见过类别时，真实数据分布和学习到的数据分布不匹配的问题。<br>
                    动机：尽管转导式零样本学习（TZSL）试图通过允许使用未见过类别的无标签示例来改善这个问题，但仍存在高度的分布偏移。<br>
                    方法：我们提出了一种新的TZSL模型（命名为Bi-VAEGAN），通过加强视觉和辅助空间之间的分布对齐，从而大大改善了这种偏移。模型设计的关键提议包括：（1）双向分布对齐；（2）简单而有效的基于L_2范数的特征归一化方法；（3）更复杂的未见过类别先验估计方法。<br>
                    效果：在四个数据集上进行基准评估，Bi-VAEGAN在标准和广义TZSL设置下均取得了新的最先进的成果。代码可在https://github.com/Zhicaiwww/Bi-VAEGAN找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It is well-known that zero-shot learning (ZSL) can suffer severely from the problem of domain shift, where the true and learned data distributions for the unseen classes do not match. Although transductive ZSL (TZSL) attempts to improve this by allowing the use of unlabelled examples from the unseen classes, there is still a high level of distribution shift. We propose a novel TZSL model (named as Bi-VAEGAN), which largely improves the shift by a strengthened distribution alignment between the visual and auxiliary spaces. The key proposal of the model design includes (1) a bi-directional distribution alignment, (2) a simple but effective L_2-norm based feature normalization approach, and (3) a more sophisticated unseen class prior estimation approach. In benchmark evaluation using four datasets, Bi-VAEGAN achieves the new state of the arts under both the standard and generalized TZSL settings. Code could be found at https://github.com/Zhicaiwww/Bi-VAEGAN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">704.Adaptive Graph Convolutional Subspace Clustering</span><br>
                <span class="as">Wei, LaiandChen, ZhengweiandYin, JunandZhu, ChangmingandZhou, RiguiandLiu, Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Adaptive_Graph_Convolutional_Subspace_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6262-6271.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种自适应图卷积子空间聚类（AGCSC）算法，以改善现有谱系子空间聚类算法的性能。<br>
                    动机：现有的谱系子空间聚类算法主要关注于设计重构系数矩阵的约束或寻找原始数据样本潜在特征的特征提取方法。<br>
                    方法：受图卷积网络的启发，我们使用图卷积技术同时开发了一种特征提取方法和一种系数矩阵约束。在我们的提议的算法中，图卷积运算符是迭代和自适应更新的。<br>
                    效果：大量的子空间聚类实验证明了我们的结论，并表明AGCSC优于一些相关的方法以及一些深度模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that, by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering experiments prove our conclusions and show that AGCSC outperforms some related methods as well as some deep models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">705.Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization</span><br>
                <span class="as">Liu, AishanandTang, ShiyuandLiang, SiyuanandGong, RuihaoandWu, BoxiandLiu, XianglongandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Exploring_the_Relationship_Between_Architectural_Design_and_Adversarially_Robust_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4096-4107.png><br>
            
            <span class="tt"><span class="t0">研究问题：对抗训练已被证明是防御对抗性示例最有效的方法之一，但它在面对未见过的攻击者时，通常存在巨大的鲁棒性泛化差距，即对抗性鲁棒性泛化问题。<br>
                    动机：尽管对对抗性鲁棒性泛化有了初步的理解，但从架构的角度来看，我们对此知之甚少。为了填补这一空白，本文首次系统地研究了对抗性鲁棒性泛化与架构设计之间的关系。<br>
                    方法：我们在ImageNette和CIFAR-10数据集上，对20种最具代表性的对抗训练架构进行了全面评估，以应对多种l_p-范数对抗攻击。<br>
                    效果：实验发现，在相同的设置下，视觉转换器（如PVT、CoAtNet）在对抗性鲁棒性泛化方面表现更好，而CNNs往往在特定攻击上过拟合，无法在多个攻击者上进行泛化。理论分析揭示了更高的权重稀疏性对Transformers更好的对抗性鲁棒性泛化有显著贡献，这通常是通过专门设计的注意力块实现的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adversarial training has been demonstrated to be one of the most effective remedies for defending adversarial examples, yet it often suffers from the huge robustness generalization gap on unseen testing adversaries, deemed as the adversarially robust generalization problem. Despite the preliminary understandings devoted to adversarially robust generalization, little is known from the architectural perspective. To bridge the gap, this paper for the first time systematically investigated the relationship between adversarially robust generalization and architectural design. In particular, we comprehensively evaluated 20 most representative adversarially trained architectures on ImageNette and CIFAR-10 datasets towards multiple l_p-norm adversarial attacks. Based on the extensive experiments, we found that, under aligned settings, Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially robust generalization while CNNs tend to overfit on specific attacks and fail to generalize on multiple adversaries. To better understand the nature behind it, we conduct theoretical analysis via the lens of Rademacher complexity. We revealed the fact that the higher weight sparsity contributes significantly towards the better adversarially robust generalization of Transformers, which can be often achieved by the specially-designed attention blocks. We hope our paper could help to better understand the mechanism for designing robust DNNs. Our model weights can be found at http://robust.art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">706.FCC: Feature Clusters Compression for Long-Tailed Visual Recognition</span><br>
                <span class="as">Li, JianandMeng, ZiyaoandShi, DaqianandSong, RuiandDiao, XiaoleiandWang, JingwenandXu, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_FCC_Feature_Clusters_Compression_for_Long-Tailed_Visual_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24080-24089.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在处理长尾数据时存在限制，因为其对少数类别的表示不足。<br>
                    动机：现有的解决此问题的方法忽视了骨干特征（BFs）密度对此问题的影响。<br>
                    方法：提出一种简单通用的方法——特征聚类压缩（FCC），通过压缩骨干特征聚类来增加BFs的密度。该方法仅在训练阶段将原始BFs乘以一个缩放因子，建立了原始和乘法特征之间的线性压缩关系，并迫使深度神经网络将前者映射到更密集的聚类中。<br>
                    效果：实验结果充分验证了该方法的有效性和通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep Neural Networks (DNNs) are rather restrictive in long-tailed data, since they commonly exhibit an under-representation for minority classes. Various remedies have been proposed to tackle this problem from different perspectives, but they ignore the impact of the density of Backbone Features (BFs) on this issue. Through representation learning, DNNs can map BFs into dense clusters in feature space, while the features of minority classes often show sparse clusters. In practical applications, these features are discretely mapped or even cross the decision boundary resulting in misclassification. Inspired by this observation, we propose a simple and generic method, namely Feature Clusters Compression (FCC), to increase the density of BFs by compressing backbone feature clusters. The proposed FCC can be easily achieved by only multiplying original BFs by a scaling factor in training phase, which establishes a linear compression relationship between the original and multiplied features, and forces DNNs to map the former into denser clusters. In test phase, we directly feed original features without multiplying the factor to the classifier, such that BFs of test samples are mapped closer together and do not easily cross the decision boundary. Meanwhile, FCC can be friendly combined with existing long-tailed methods and further boost them. We apply FCC to numerous state-of-the-art methods and evaluate them on widely used long-tailed benchmark datasets. Extensive experiments fully verify the effectiveness and generality of our method. Code is available at https://github.com/lijian16/FCC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">707.Multi-Centroid Task Descriptor for Dynamic Class Incremental Inference</span><br>
                <span class="as">Cai, TenghaoandZhang, ZhizhongandTan, XinandQu, YanyunandJiang, GuannanandWang, ChengjieandXie, Yuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Multi-Centroid_Task_Descriptor_for_Dynamic_Class_Incremental_Inference_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7298-7307.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决增量学习中的类别和任务增量学习问题，特别是在评估过程中是否给出任务ID的问题。<br>
                    动机：作者发现在增量学习中，任务信息是一种强大的先验知识，可以显著提高类别增量学习的效果。<br>
                    方法：提出了一种门控网络来预测任务ID进行类别增量推理，并设计了一种多中心任务描述符来处理任务间没有明确语义关系的问题。<br>
                    效果：实验结果表明，该方法在CIFAR100-B0S50数据集上取得了72.41%的平均准确率，比DER提高了3.40%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Incremental learning could be roughly divided into two categories, i.e., class- and task-incremental learning. The main difference is whether the task ID is given during evaluation. In this paper, we show this task information is indeed a strong prior knowledge, which will bring significant improvement over class-incremental learning baseline, e.g., DER. Based on this observation, we propose a gate network to predict the task ID for class incremental inference. This is challenging as there is no explicit semantic relationship between categories in the concept of task. Therefore, we propose a multi-centroid task descriptor by assuming the data within a task can form multiple clusters. The cluster centers are optimized by pulling relevant sample-centroid pairs while pushing others away, which ensures that there is at least one centroid close to a given sample. To select relevant pairs, we use class prototypes as proxies and solve a bipartite matching problem, making the task descriptor representative yet not degenerate to uni-modal. As a result, our dynamic inference network is trained independently of baseline and provides a flexible, efficient solution to distinguish between tasks. Extensive experiments show our approach achieves state-of-the-art results, e.g., we achieve 72.41% average accuracy on CIFAR100-B0S50, outperforming DER by 3.40%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">708.Open-Set Likelihood Maximization for Few-Shot Learning</span><br>
                <span class="as">Boudiaf, MalikandBennequin, EtienneandTami, MyriamandToubhans, AntoineandPiantanida, PabloandHudelot, CelineandBenAyed, Ismail</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boudiaf_Open-Set_Likelihood_Maximization_for_Few-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24007-24016.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决小样本开放集识别（FSOSR）问题，即在只有少量标记样本的类别集合中进行分类，同时检测不属于任何已知类别的实例。<br>
                    动机：现有的转导式方法在开放集场景中表现不佳，因此提出一种最大似然原理的泛化，引入潜在分数来降低可能的异常值的影响。<br>
                    方法：提出了开放集似然优化（OSLO）方法，该方法将潜在分数和参数模型共同优化，从而从彼此中受益。<br>
                    效果：通过大量实验，证明该方法在开放集识别的两个方面，即内点分类和外点检测上超越了现有的归纳式和转导式方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We tackle the Few-Shot Open-Set Recognition (FSOSR) problem, i.e. classifying instances among a set of classes for which we only have a few labeled samples, while simultaneously detecting instances that do not belong to any known class. We explore the popular transductive setting, which leverages the unlabelled query instances at inference. Motivated by the observation that existing transductive methods perform poorly in open-set scenarios, we propose a generalization of the maximum likelihood principle, in which latent scores down-weighing the influence of potential outliers are introduced alongside the usual parametric model. Our formulation embeds supervision constraints from the support set and additional penalties discouraging overconfident predictions on the query set. We proceed with a block-coordinate descent, with the latent scores and parametric model co-optimized alternately, thereby benefiting from each other. We call our resulting formulation Open-Set Likelihood Optimization (OSLO). OSLO is interpretable and fully modular; it can be applied on top of any pre-trained model seamlessly. Through extensive experiments, we show that our method surpasses existing inductive and transductive methods on both aspects of open-set recognition, namely inlier classification and outlier detection. Code is available at https://github.com/ebennequin/few-shot-open-set.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">709.DiGeo: Discriminative Geometry-Aware Learning for Generalized Few-Shot Object Detection</span><br>
                <span class="as">Ma, JiaweiandNiu, YuleiandXu, JinchengandHuang, ShiyuanandHan, GuangxingandChang, Shih-Fu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_DiGeo_Discriminative_Geometry-Aware_Learning_for_Generalized_Few-Shot_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3208-3218.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决通用的少样本物体检测问题，即如何在有丰富标注的基础类别和只有少量训练数据的新颖类别上实现精确的检测。<br>
                    动机：现有的方法在提高少样本泛化能力时，往往会牺牲基础类别的性能，或者在保持基础类别高精度的同时，对新颖类别的适应能力提升有限。<br>
                    方法：本文提出了一种新的训练框架DiGeo，通过学习类间分离和类内紧凑的几何感知特征，以改善这一问题。具体来说，我们推导了一个离线的单纯形等角紧框架（ETF）分类器，其权重作为类中心并被最大化和等距分离。同时，我们还在分类损失中引入了自适应的特定类别间隔，以紧密每个类别的特征。<br>
                    效果：实验结果证明，该方法在两个少样本基准数据集（PASCAL VOC、MSCOCO）和一个长尾数据集（LVIS）上表现出色，可以在不损害基础类别检测性能的情况下，有效提高对新颖类别的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalized few-shot object detection aims to achieve precise detection on both base classes with abundant annotations and novel classes with limited training data. Existing approaches enhance few-shot generalization with the sacrifice of base-class performance, or maintain high precision in base-class detection with limited improvement in novel-class adaptation. In this paper, we point out the reason is insufficient Discriminative feature learning for all of the classes. As such, we propose a new training framework, DiGeo, to learn Geometry-aware features of inter-class separation and intra-class compactness. To guide the separation of feature clusters, we derive an offline simplex equiangular tight frame (ETF) classifier whose weights serve as class centers and are maximally and equally separated. To tighten the cluster for each class, we include adaptive class-specific margins into the classification loss and encourage the features close to the class centers. Experimental studies on two few-shot benchmark datasets (PASCAL VOC, MSCOCO) and one long-tail dataset (LVIS) demonstrate that, with a single model, our method can effectively improve generalization on novel classes without hurting the detection of base classes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">710.Fine-Grained Classification With Noisy Labels</span><br>
                <span class="as">Wei, QiandFeng, LeiandSun, HaoliangandWang, RenandGuo, ChenhuiandYin, Yilong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Fine-Grained_Classification_With_Noisy_Labels_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11651-11660.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在标签噪声存在的情况下，如何提高细粒度数据集上的模型泛化能力。<br>
                    动机：现有的学习方法在面对标签噪声时表现不佳，特别是在细粒度类别之间存在大量类间模糊性，导致标签噪声更为严重的情况下。<br>
                    方法：提出了一种新的框架——随机噪声容忍的有监督对比学习（SNSCL），通过鼓励区分性表示来应对标签噪声。具体来说，设计了一种噪声容忍的有监督对比学习损失函数，该函数包含一个加权机制用于纠正噪声标签和选择性更新动量队列列表。<br>
                    效果：实验结果表明，SNSCL在各种细粒度数据集上都能显著提高模型的泛化能力，且优于现有的处理方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning with noisy labels (LNL) aims to ensure model generalization given a label-corrupted training set. In this work, we investigate a rarely studied scenario of LNL on fine-grained datasets (LNL-FG), which is more practical and challenging as large inter-class ambiguities among fine-grained classes cause more noisy labels. We empirically show that existing methods that work well for LNL fail to achieve satisfying performance for LNL-FG, arising the practical need of effective solutions for LNL-FG. To this end, we propose a novel framework called stochastic noise-tolerated supervised contrastive learning (SNSCL) that confronts label noise by encouraging distinguishable representation. Specifically, we design a noise-tolerated supervised contrastive learning loss that incorporates a weight-aware mechanism for noisy label correction and selectively updating momentum queue lists. By this mechanism, we mitigate the effects of noisy anchors and avoid inserting noisy labels into the momentum-updated queue. Besides, to avoid manually-defined augmentation strategies in contrastive learning, we propose an efficient stochastic module that samples feature embeddings from a generated distribution, which can also enhance the representation ability of deep models. SNSCL is general and compatible with prevailing robust LNL strategies to improve their performance for LNL-FG. Extensive experiments demonstrate the effectiveness of SNSCL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">711.Hybrid Active Learning via Deep Clustering for Video Action Detection</span><br>
                <span class="as">Rana, AayushJ.andRawat, YogeshS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rana_Hybrid_Active_Learning_via_Deep_Clustering_for_Video_Action_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18867-18877.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在降低视频动作检测的标注成本，该任务需要昂贵的逐帧密集标注。<br>
                    动机：目前的标注方法成本高昂，因此需要一种有效的标注策略来降低成本。<br>
                    方法：提出了一种新的混合主动学习策略，通过样本内和样本间的选择进行高效的标注。这种混合策略从两个方面降低了标注成本。<br>
                    效果：在UCF-101-24和J-HMDB-21数据集上进行的实验表明，该方法能有效降低标注成本，并始终优于其他基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we focus on reducing the annotation cost for video action detection which requires costly frame-wise dense annotations. We study a novel hybrid active learning (AL) strategy which performs efficient labeling using both intra-sample and inter-sample selection. The intra-sample selection leads to labeling of fewer frames in a video as opposed to inter-sample selection which operates at video level. This hybrid strategy reduces the annotation cost from two different aspects leading to significant labeling cost reduction. The proposed approach utilize Clustering-Aware Uncertainty Scoring (CLAUS), a novel label acquisition strategy which relies on both informativeness and diversity for sample selection. We also propose a novel Spatio-Temporal Weighted (STeW) loss formulation, which helps in model training under limited annotations. The proposed approach is evaluated on UCF-101-24 and J-HMDB-21 datasets demonstrating its effectiveness in significantly reducing the annotation cost where it consistently outperforms other baselines. Project details available at https://sites.google.com/view/activesparselabeling/home</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">712.Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection</span><br>
                <span class="as">Lu, FanandZhu, KaiandZhai, WeiandZheng, KechengandCao, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Uncertainty-Aware_Optimal_Transport_for_Semantically_Coherent_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3282-3291.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何区分目标数据分布中的异常值（SCOOD检测）。<br>
                    动机：在存在分布内和分布外样本的情况下，如果没有进行区分，模型可能会过拟合。<br>
                    方法：提出一种新颖的不确定性感知最优传输方案，包括一个估计不确定性波动成本的能量传输机制和一个通过扩大相应间隔距离增强不同簇之间语义属性辨别性的簇间扩展策略。<br>
                    效果：在两个标准的SCOOD基准测试中，该方法的异常检测性能优于最先进方法，FPR@95分别提高了27.69%和34.4%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantically coherent out-of-distribution (SCOOD) detection aims to discern outliers from the intended data distribution with access to unlabeled extra set. The coexistence of in-distribution and out-of-distribution samples will exacerbate the model overfitting when no distinction is made. To address this problem, we propose a novel uncertainty-aware optimal transport scheme. Our scheme consists of an energy-based transport (ET) mechanism that estimates the fluctuating cost of uncertainty to promote the assignment of semantic-agnostic representation, and an inter-cluster extension strategy that enhances the discrimination of semantic property among different clusters by widening the corresponding margin distance. Furthermore, a T-energy score is presented to mitigate the magnitude gap between the parallel transport and classifier branches. Extensive experiments on two standard SCOOD benchmarks demonstrate the above-par OOD detection performance, outperforming the state-of-the-art methods by a margin of 27.69% and 34.4% on FPR@95, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">713.Hunting Sparsity: Density-Guided Contrastive Learning for Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Wang, XiaoyangandZhang, BingfengandYu, LiminandXiao, Jimin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Hunting_Sparsity_Density-Guided_Contrastive_Learning_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3114-3123.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过直接从特征空间的几何结构中提取适当的监督，提高模型在扰动不变训练中的泛化能力。<br>
                    动机：现有的半监督语义分割方法结合伪标签和一致性正则化来增强模型的泛化能力，但作者认为可以从特征空间的几何结构中直接提取适当的监督。<br>
                    方法：受基于密度的无监督聚类启发，提出利用特征密度定位由标签和伪标签定义的特征簇内的稀疏区域。假设低密度特征与高密度聚集的特征相比，往往训练不足。因此，提出通过解决稀疏性来对簇的结构进行正则化，以增加特征空间内的类内紧凑性。为此，提出了一种密度引导的对比学习（DGCL）策略，将锚定特征推向由高密度正键近似的簇中心的稀疏区域。该方法的核心是估计特征密度，定义为邻居密集度。设计了一个多尺度密度估计模块，从多个最近邻图中获得密度，以实现稳健的密度建模。此外，提出了一个统一的训练框架，将标签引导的自我训练和密度引导的几何正则化相结合，形成对未标记数据的互补监督。<br>
                    效果：在PASCAL VOC和Cityscapes的各种半监督设置下进行的实验结果表明，所提出的方法实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent semi-supervised semantic segmentation methods combine pseudo labeling and consistency regularization to enhance model generalization from perturbation-invariant training. In this work, we argue that adequate supervision can be extracted directly from the geometry of feature space. Inspired by density-based unsupervised clustering, we propose to leverage feature density to locate sparse regions within feature clusters defined by label and pseudo labels. The hypothesis is that lower-density features tend to be under-trained compared with those densely gathered. Therefore, we propose to apply regularization on the structure of the cluster by tackling the sparsity to increase intra-class compactness in feature space. With this goal, we present a Density-Guided Contrastive Learning (DGCL) strategy to push anchor features in sparse regions toward cluster centers approximated by high-density positive keys. The heart of our method is to estimate feature density which is defined as neighbor compactness. We design a multi-scale density estimation module to obtain the density from multiple nearest-neighbor graphs for robust density modeling. Moreover, a unified training framework is proposed to combine label-guided self-training and density-guided geometry regularization to form complementary supervision on unlabeled data. Experimental results on PASCAL VOC and Cityscapes under various semi-supervised settings demonstrate that our proposed method achieves state-of-the-art performances.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">714.Learning Analytical Posterior Probability for Human Mesh Recovery</span><br>
                <span class="as">Fang, QiandChen, KangandFan, YinghuiandShuai, QingandLi, JiefengandZhang, Weidong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Learning_Analytical_Posterior_Probability_for_Human_Mesh_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8781-8791.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的人体网格恢复模型的不确定性和模糊性建模方法精度有限，因为其关节研究问题：现有的人体网格恢复模型的不确定性和模糊性建模方法精度有限，因为其关节旋转的联合建模方式要么不约束在SO(3)上，要么对神经网络难以学习。<br>
                    动机：为了解决这个问题，我们提出了一种新的分析公式，以贝叶斯的方式学习人体关节旋转的条件概率分布，并基于此提出了一个新的后验引导的人体网格恢复框架。<br>
                    方法：我们的框架不仅优于现有的最佳基准，而且由于其贝叶斯性质，具有足够的灵活性，可以无缝地与其他传感器集成。<br>
                    效果：实验证明，我们的框架在多个基准测试中表现优越，代码已在GitHub上开源。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite various probabilistic methods for modeling the uncertainty and ambiguity in human mesh recovery, their overall precision is limited because existing formulations for joint rotations are either not constrained to SO(3) or difficult to learn for neural networks. To address such an issue, we derive a novel analytical formulation for learning posterior probability distributions of human joint rotations conditioned on bone directions in a Bayesian manner, and based on this, we propose a new posterior-guided framework for human mesh recovery. We demonstrate that our framework is not only superior to existing SOTA baselines on multiple benchmarks but also flexible enough to seamlessly incorporate with additional sensors due to its Bayesian nature. The code is available at https://github.com/NetEase-GameAI/ProPose.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">715.An Erudite Fine-Grained Visual Classification Model</span><br>
                <span class="as">Chang, DongliangandTong, YujunandDu, RuoyiandHospedales, TimothyandSong, Yi-ZheandMa, Zhanyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_An_Erudite_Fine-Grained_Visual_Classification_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7268-7277.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的细粒度视觉分类（FGVC）模型是孤立的，需要先识别出对象的粗粒度标签，再选择相应的FGVC模型进行识别，这限制了FGVC算法在现实场景中的应用。<br>
                    动机：为了解决这一问题，本文提出了一种联合训练的博学FGVC模型，该模型可以高效准确地预测对象在整个组合标签空间中的细粒度标签。<br>
                    方法：首先，我们提出了一个特征解耦模块和一个特征再融合模块，以减少不同数据集之间训练时的负迁移并增强正迁移。然后，我们提出了一个基于元学习的数据集无关的空间注意力层，以充分利用多数据集的训练数据。<br>
                    效果：实验结果表明，该方法在11个不同的混合数据集上取得了良好的效果，这些数据集基于四个不同的FGVC数据集构建。此外，该方法可以很容易地与现有的FGVC方法结合，获得最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current fine-grained visual classification (FGVC) models are isolated. In practice, we first need to identify the coarse-grained label of an object, then select the corresponding FGVC model for recognition. This hinders the application of the FGVC algorithm in real-life scenarios. In this paper, we propose an erudite FGVC model jointly trained by several different datasets, which can efficiently and accurately predict an object's fine-grained label across the combined label space. We found through a pilot study that positive and negative transfers co-occur when different datasets are mixed for training, i.e., the knowledge from other datasets is not always useful. Therefore, we first propose a feature disentanglement module and a feature re-fusion module to reduce negative transfer and boost positive transfer between different datasets. In detail, we reduce negative transfer by decoupling the deep features through many dataset-specific feature extractors. Subsequently, these are channel-wise re-fused to facilitate positive transfer. Finally, we propose a meta-learning based dataset-agnostic spatial attention layer to take full advantage of the multi-dataset training data, given that localisation is dataset-agnostic between different datasets. Experimental results across 11 different mixed-datasets built on four different FGVC datasets demonstrate the effectiveness of the proposed method. Furthermore, the proposed method can be easily combined with existing FGVC methods to obtain state-of-the-art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">716.RONO: Robust Discriminative Learning With Noisy Labels for 2D-3D Cross-Modal Retrieval</span><br>
                <span class="as">Feng, YanglinandZhu, HongyuanandPeng, DezhongandPeng, XiandHu, Peng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_RONO_Robust_Discriminative_Learning_With_Noisy_Labels_for_2D-3D_Cross-Modal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11610-11619.png><br>
            
            <span class="tt"><span class="t0">研究问题：随着Metaverse和AI生成内容的出现，跨模态检索在2D和3D数据中变得流行，但由于异构结构和语义差异，这个问题具有挑战性。<br>
                    动机：由于模糊的2D和3D内容，普遍存在不完美的标注，从而不可避免地产生噪声标签，降低学习性能。<br>
                    方法：本文提出了一种鲁棒的2D-3D检索框架（RONO），以从噪声多模态数据中稳健地学习。具体来说，提出了一种新的鲁棒判别中心学习机制（RDCL），以自适应地区分干净和噪声样本，分别提供它们正负优化方向，从而减轻噪声标签的负面影响。此外，还提出了一个共享空间一致性学习机制（SSCL），通过同时最小化公共空间和标签空间之间的跨模态和语义差异，捕获噪声数据中的固有信息。<br>
                    效果：通过与15种最先进的方法进行比较，在四个3D模型多模态数据集上进行了广泛的实验，验证了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, with the advent of Metaverse and AI Generated Content, cross-modal retrieval becomes popular with a burst of 2D and 3D data. However, this problem is challenging given the heterogeneous structure and semantic discrepancies. Moreover, imperfect annotations are ubiquitous given the ambiguous 2D and 3D content, thus inevitably producing noisy labels to degrade the learning performance. To tackle the problem, this paper proposes a robust 2D-3D retrieval framework (RONO) to robustly learn from noisy multimodal data. Specifically, one novel Robust Discriminative Center Learning mechanism (RDCL) is proposed in RONO to adaptively distinguish clean and noisy samples for respectively providing them with positive and negative optimization directions, thus mitigating the negative impact of noisy labels. Besides, we present a Shared Space Consistency Learning mechanism (SSCL) to capture the intrinsic information inside the noisy data by minimizing the cross-modal and semantic discrepancy between common space and label space simultaneously. Comprehensive mathematical analyses are given to theoretically prove the noise tolerance of the proposed method. Furthermore, we conduct extensive experiments on four 3D-model multimodal datasets to verify the effectiveness of our method by comparing it with 15 state-of-the-art methods. Code is available at https://github.com/penghu-cs/RONO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">717.DISC: Learning From Noisy Labels via Dynamic Instance-Specific Selection and Correction</span><br>
                <span class="as">Li, YifanandHan, HuandShan, ShiguangandChen, Xilin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24070-24079.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习网络会最终记住标签噪声，我们观察到每个实例对这种记忆的强度不同，并且可以通过置信度值来表示，该值在训练过程中会越来越大。<br>
                    动机：基于此，我们提出了一种从有噪声的标签中学习的动态实例特定选择和修正方法（DISC）。<br>
                    方法：首先，我们使用一个基于两个视图的分类模型进行图像分类，从两个视图中获得每个图像的置信度。然后，我们为每个实例提出一个动态阈值策略，根据前几个时期每个实例的记忆强度的动量来选择和修正有噪声的标签数据。<br>
                    效果：得益于动态阈值策略和双视图学习，我们可以有效地将每个实例分为三个子集之一（即干净、困难和净化），基于每个时期两个视图的预测一致性和差异。最后，我们采用不同的正则化策略来处理不同程度标签噪声的子集，提高整个网络的鲁棒性。在三个可控和四个真实世界的有噪声标签基准测试中，我们的方法优于最先进的方法，利用了有噪声数据中的有用信息，同时减轻了标签噪声的污染。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing studies indicate that deep neural networks (DNNs) can eventually memorize the label noise. We observe that the memorization strength of DNNs towards each instance is different and can be represented by the confidence value, which becomes larger and larger during the training process. Based on this, we propose a Dynamic Instance-specific Selection and Correction method (DISC) for learning from noisy labels (LNL). We first use a two-view-based backbone for image classification, obtaining confidence for each image from two views. Then we propose a dynamic threshold strategy for each instance, based on the momentum of each instance's memorization strength in previous epochs to select and correct noisy labeled data. Benefiting from the dynamic threshold strategy and two-view learning, we can effectively group each instance into one of the three subsets (i.e., clean, hard, and purified) based on the prediction consistency and discrepancy by two views at each epoch. Finally, we employ different regularization strategies to conquer subsets with different degrees of label noise, improving the whole network's robustness. Comprehensive evaluations on three controllable and four real-world LNL benchmarks show that our method outperforms the state-of-the-art (SOTA) methods to leverage useful information in noisy data while alleviating the pollution of label noise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">718.A Probabilistic Framework for Lifelong Test-Time Adaptation</span><br>
                <span class="as">Brahma, DhanajitandRai, Piyush</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brahma_A_Probabilistic_Framework_for_Lifelong_Test-Time_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3582-3591.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决测试时适应（TTA）的问题，即在推理时间给定来自不同目标域的测试输入时更新预训练源模型。<br>
                    动机：现有的TTA方法大多假设目标域是稳定的，即所有测试输入都来自单个目标域。然而，在许多实际情况下，测试输入分布可能会随时间发生持续变化。此外，现有的TTA方法也缺乏提供可靠不确定性估计的能力，这是源和目标域之间出现分布偏移时的关键。<br>
                    方法：我们提出了PETAL（概率终身测试时适应与自我训练先验），使用概率方法解决终身TTA，自然地产生了（1）学生-教师框架，其中教师模型是学生模型的指数移动平均，以及（2）使用源模型作为正则化器来正则化推理时的模型更新。为了防止终身/持续TTA设置中的模型漂移，我们还提出了一种数据驱动的参数恢复技术，通过仅恢复无关参数来减少误差累积并保持对最近领域的知识。<br>
                    效果：无论在预测错误率还是不确定性指标如Brier得分和负对数似然方面，我们的方法在各种基准上，如CIFAR-10C、CIFAR-100C、ImageNetC和ImageNet3DCC数据集，都比当前最先进的在线终身测试时适应方法取得了更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test-time adaptation (TTA) is the problem of updating a pre-trained source model at inference time given test input(s) from a different target domain. Most existing TTA approaches assume the setting in which the target domain is stationary, i.e., all the test inputs come from a single target domain. However, in many practical settings, the test input distribution might exhibit a lifelong/continual shift over time. Moreover, existing TTA approaches also lack the ability to provide reliable uncertainty estimates, which is crucial when distribution shifts occur between the source and target domain. To address these issues, we present PETAL (Probabilistic lifElong Test-time Adaptation with seLf-training prior), which solves lifelong TTA using a probabilistic approach, and naturally results in (1) a student-teacher framework, where the teacher model is an exponential moving average of the student model, and (2) regularizing the model updates at inference time using the source model as a regularizer. To prevent model drift in the lifelong/continual TTA setting, we also propose a data-driven parameter restoration technique which contributes to reducing the error accumulation and maintaining the knowledge of recent domains by restoring only the irrelevant parameters. In terms of predictive error rate as well as uncertainty based metrics such as Brier score and negative log-likelihood, our method achieves better results than the current state-of-the-art for online lifelong test-time adaptation across various benchmarks, such as CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC datasets. The source code for our approach is accessible at https://github.com/dhanajitb/petal.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">719.Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</span><br>
                <span class="as">Radenovic, FilipandDubey, AbhimanyuandKadian, AbhishekandMihaylov, TodorandVandenhende, SimonandPatel, YashandWen, YiandRamanathan, VigneshandMahajan, Dhruv</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6967-6977.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在改进对比预训练流程的三个主要方面：数据集噪声、模型初始化和训练目标。<br>
                    动机：在大规模嘈杂数据上进行对比学习的视觉语言模型，对于零样本识别问题越来越受欢迎。<br>
                    方法：提出了一种名为“复杂性、动作和文本定位”（CAT）的直接过滤策略，显著减少了数据集大小，同时提高了各种零样本视觉语言任务的性能。然后，提出了一种名为“概念蒸馏”的方法，利用强大的单模态表示进行对比训练，不增加训练复杂度，同时优于先前的工作。最后，修改了传统的对比对齐目标，并提出了一种新的重要采样方法来提高困难负例的重要性，而不增加额外的复杂性。<br>
                    效果：在29个任务的广泛零样本基准测试中，我们的“蒸馏和困难负例训练”（DiHT）方法在20个任务上超过了基线。此外，对于少数样本线性探测，我们提出了一种将零样本和少数样本性能之间的差距弥合的新方法，大大改善了先前的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-language models trained with contrastive learning on large-scale noisy data are becoming increasingly popular for zero-shot recognition problems. In this paper we improve the following three aspects of the contrastive pre-training pipeline: dataset noise, model initialization and the training objective. First, we propose a straightforward filtering strategy titled Complexity, Action, and Text-spotting (CAT) that significantly reduces dataset size, while achieving improved performance across zero-shot vision-language tasks. Next, we propose an approach titled Concept Distillation to leverage strong unimodal representations for contrastive training that does not increase training complexity while outperforming prior work. Finally, we modify the traditional contrastive alignment objective, and propose an importance-sampling approach to up-sample the importance of hard-negatives without adding additional complexity. On an extensive zero-shot benchmark of 29 tasks, our Distilled and Hard-negative Training (DiHT) approach improves on 20 tasks compared to the baseline. Furthermore, for few-shot linear probing, we propose a novel approach that bridges the gap between zero-shot and few-shot performance, substantially improving over prior work. Models are available at github.com/facebookresearch/diht.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">720.Meta Omnium: A Benchmark for General-Purpose Learning-To-Learn</span><br>
                <span class="as">Bohdal, OndrejandTian, YinbingandZong, YongshuoandChavhan, RuchikaandLi, DaandGouk, HenryandGuo, LiandHospedales, Timothy</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bohdal_Meta_Omnium_A_Benchmark_for_General-Purpose_Learning-To-Learn_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7693-7703.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究元学习和其他少样本学习方法是否能够跨多种视觉任务进行泛化。<br>
                    动机：元学习和少样本学习方法在图像识别等任务上已得到广泛应用，但其是否能适用于其他如姿态估计和密集预测等视觉任务尚待研究。<br>
                    方法：本文提出了Meta Omnium数据集，该数据集包含多个视觉任务，包括识别、关键点定位、语义分割和回归等。通过实验测试了流行的元学习基线在这些任务和知识转移方面的能力。<br>
                    效果：Meta Omnium使元学习研究者能够评估模型对比以前更广泛的任务的泛化能力，并为一系列视觉应用提供了一个统一的框架来评估元学习者的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Meta-learning and other approaches to few-shot learning are widely studied for image recognition, and are increasingly applied to other vision tasks such as pose estimation and dense prediction. This naturally raises the question of whether there is any few-shot meta-learning algorithm capable of generalizing across these diverse task types? To support the community in answering this question, we introduce Meta Omnium, a dataset-of-datasets spanning multiple vision tasks including recognition, keypoint localization, semantic segmentation and regression. We experiment with popular few-shot meta-learning baselines and analyze their ability to generalize across tasks and to transfer knowledge between them. Meta Omnium enables meta-learning researchers to evaluate model generalization to a much wider array of tasks than previously possible, and provides a single framework for evaluating meta-learners across a wide suite of vision applications in a consistent manner.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">721.Change-Aware Sampling and Contrastive Learning for Satellite Images</span><br>
                <span class="as">Mall, UtkarshandHariharan, BharathandBala, Kavita</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5261-5270.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用无标签的卫星图像数据进行有效的自监督学习。<br>
                    动机：虽然大量的时空卫星图像数据易于获取，但大部分未标记，这对监督学习算法来说并无太大用处。<br>
                    方法：利用卫星图像的独特特性，如时间信号和地理位置变化不大的特性，提出了一种新的对比损失函数——变化感知对比（CACo）损失函数，并设计了一种新的地理区域采样方法。<br>
                    效果：实验结果显示，该方法在各种下游任务中表现更好，例如，在语义分割和变化检测任务上，相对于最佳基线，性能分别提高了6.5%和8.5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic remote sensing tools can help inform many large-scale challenges such as disaster management, climate change, etc. While a vast amount of spatio-temporal satellite image data is readily available, most of it remains unlabelled. Without labels, this data is not very useful for supervised learning algorithms. Self-supervised learning instead provides a way to learn effective representations for various downstream tasks without labels. In this work, we leverage characteristics unique to satellite images to learn better self-supervised features. Specifically, we use the temporal signal to contrast images with long-term and short-term differences, and we leverage the fact that satellite images do not change frequently. Using these characteristics, we formulate a new loss contrastive loss called Change-Aware Contrastive (CACo) Loss. Further, we also present a novel method of sampling different geographical regions. We show that leveraging these properties leads to better performance on diverse downstream tasks. For example, we see a 6.5% relative improvement for semantic segmentation and an 8.5% relative improvement for change detection over the best-performing baseline with our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">722.Large-Scale Training Data Search for Object Re-Identification</span><br>
                <span class="as">Yao, YueandGedeon, TomandZheng, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Large-Scale_Training_Data_Search_for_Object_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15568-15578.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在无法获取实时训练数据标注的情况下，从大规模数据集中构建替代训练集以获得有竞争力的模型。<br>
                    动机：针对对象重识别（re-ID）应用，目标是匹配不同摄像头捕获的同一对象，但无法获取实时的训练数据标注。<br>
                    方法：提出一种搜索和剪枝（SnP）解决方案来解决这个问题，包括两个阶段：搜索阶段识别并合并与目标领域具有相似分布的源身份集群；第二阶段在预算限制下从第一阶段的输出中选择身份及其图像，以控制生成的训练集的大小，实现有效训练。<br>
                    效果：这种方法生成的训练集比源池小80%，同时达到相似的或更高的重识别准确性。这些训练集也优于现有的搜索方法，如随机采样和贪心采样，在相同的预算约束下。如果放开预算限制，仅第一阶段生成的训练集甚至能实现更高的重识别准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider a scenario where we have access to the target domain, but cannot afford on-the-fly training data annotation, and instead would like to construct an alternative training set from a large-scale data pool such that a competitive model can be obtained. We propose a search and pruning (SnP) solution to this training data search problem, tailored to object re-identification (re-ID), an application aiming to match the same object captured by different cameras. Specifically, the search stage identifies and merges clusters of source identities which exhibit similar distributions with the target domain. The second stage, subject to a budget, then selects identities and their images from the Stage I output, to control the size of the resulting training set for efficient training. The two steps provide us with training sets 80% smaller than the source pool while achieving a similar or even higher re-ID accuracy. These training sets are also shown to be superior to a few existing search methods such as random sampling and greedy sampling under the same budget on training data size. If we release the budget, training sets resulting from the first stage alone allow even higher re-ID accuracy. We provide interesting discussions on the specificity of our method to the re-ID problem and particularly its role in bridging the re-ID domain gap. The code is available at https://github.com/yorkeyao/SnP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">723.Uncertainty-Aware Unsupervised Image Deblurring With Deep Residual Prior</span><br>
                <span class="as">Tang, XiaoleandZhao, XileandLiu, JunandWang, JianliandMiao, YuchunandZeng, Tieyong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Uncertainty-Aware_Unsupervised_Image_Deblurring_With_Deep_Residual_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9883-9892.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一个适合的核（或诱导）误差先验，以处理实践中不可避免的核不确定性。<br>
                    动机：现有的非盲去模糊方法在准确的模糊核假设下表现良好，但在实际应用中，由于核（或诱导）误差的存在，这些方法的效果会受到影响。<br>
                    方法：提出了一种无需数据集的深度残差先验，用于表示由自定义未训练深度神经网络表达的核诱导误差（称为残差）。这种先验可以灵活适应真实场景中的不同模糊和图像。通过有机地整合深度先验和手工制作先验各自的优势，提出了一种无监督的半盲去模糊模型，该模型可以从模糊图像和不准确的模糊核中恢复潜在图像。<br>
                    效果：实验表明，与模型驱动和数据驱动的方法相比，该方法在图像质量和对不同类型的核误差的鲁棒性方面表现出良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Non-blind deblurring methods achieve decent performance under the accurate blur kernel assumption. Since the kernel uncertainty (i.e. kernel error) is inevitable in practice, semi-blind deblurring is suggested to handle it by introducing the prior of the kernel (or induced) error. However, how to design a suitable prior for the kernel (or induced) error remains challenging. Hand-crafted prior, incorporating domain knowledge, generally performs well but may lead to poor performance when kernel (or induced) error is complex. Data-driven prior, which excessively depends on the diversity and abundance of training data, is vulnerable to out-of-distribution blurs and images. To address this challenge, we suggest a dataset-free deep residual prior for the kernel induced error (termed as residual) expressed by a customized untrained deep neural network, which allows us to flexibly adapt to different blurs and images in real scenarios. By organically integrating the respective strengths of deep priors and hand-crafted priors, we propose an unsupervised semi-blind deblurring model which recovers the latent image from the blurry image and inaccurate blur kernel. To tackle the formulated model, an efficient alternating minimization algorithm is developed. Extensive experiments demonstrate the favorable performance of the proposed method as compared to model-driven and data-driven methods in terms of image quality and the robustness to different types of kernel error.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">724.Bridging the Gap Between Model Explanations in Partially Annotated Multi-Label Classification</span><br>
                <span class="as">Kim, YoungwookandKim, JaeMyungandJeong, JieunandSchmid, CordeliaandAkata, ZeynepandLee, Jungwoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Bridging_the_Gap_Between_Model_Explanations_in_Partially_Annotated_Multi-Label_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3408-3417.png><br>
            
            <span class="tt"><span class="t0">研究问题：多标签分类中，由于标注成本高，部分标注的情况越来越常见。如何减少未观察到的标签对模型解释的影响，提高模型性能是当前的研究问题。<br>
                    动机：在部分标注的多标签分类任务中，通常将未观察到的标签视为负标签，但这会引入标签噪声，形成假负标签。假负标签会对模型的解释产生影响，降低模型的性能。<br>
                    方法：通过对比全标和部分标训练的模型的解释，发现两者在相似区域有不同缩放，且后者的归一化得分较低。因此，提出一种提升部分标训练模型归一化得分的方法，使其解释更接近全标训练模型的解释。<br>
                    效果：该方法在三个不同的数据集上，单正标签设置和大规模部分标签设置下，均使多标签分类性能大幅提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the expensive costs of collecting labels in multi-label classification datasets, partially annotated multi-label classification has become an emerging field in computer vision. One baseline approach to this task is to assume unobserved labels as negative labels, but this assumption induces label noise as a form of false negative. To understand the negative impact caused by false negative labels, we study how these labels affect the model's explanation. We observe that the explanation of two models, trained with full and partial labels each, highlights similar regions but with different scaling, where the latter tends to have lower attribution scores. Based on these findings, we propose to boost the attribution scores of the model trained with partial labels to make its explanation resemble that of the model trained with full labels. Even with the conceptually simple approach, the multi-label classification performance improves by a large margin in three different datasets on a single positive label setting and one on a large-scale partial label setting. Code is available at https://github.com/youngwk/BridgeGapExplanationPAMC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">725.Learning Audio-Visual Source Localization via False Negative Aware Contrastive Learning</span><br>
                <span class="as">Sun, WeixuanandZhang, JiayiandWang, JianyuanandLiu, ZheyuanandZhong, YiranandFeng, TianpengandGuo, YandongandZhang, YanhaoandBarnes, Nick</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Learning_Audio-Visual_Source_Localization_via_False_Negative_Aware_Contrastive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6420-6429.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自监督视听源定位方法在训练中可能会受到假负样本的影响，导致模型学习到的表示效果不佳。<br>
                    动机：为了解决这个问题，我们提出了一种新的学习方法——False Negative Aware Contrastive（FNAC），通过利用模态内相似性来识别可能相似的样本，并构建相应的邻接矩阵来指导对比学习。<br>
                    方法：我们使用视觉特征强化真实负样本的作用，以帮助区分真实的声源区域。<br>
                    效果：实验结果表明，FNAC在Flickr-SoundNet、VGG-Sound和AVSBench等数据集上取得了最先进的性能，证明了我们的方法能有效缓解假负样本的问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised audio-visual source localization aims to locate sound-source objects in video frames without extra annotations. Recent methods often approach this goal with the help of contrastive learning, which assumes only the audio and visual contents from the same video are positive samples for each other. However, this assumption would suffer from false negative samples in real-world training. For example, for an audio sample, treating the frames from the same audio class as negative samples may mislead the model and therefore harm the learned representations (e.g., the audio of a siren wailing may reasonably correspond to the ambulances in multiple images). Based on this observation, we propose a new learning strategy named False Negative Aware Contrastive (FNAC) to mitigate the problem of misleading the training with such false negative samples. Specifically, we utilize the intra-modal similarities to identify potentially similar samples and construct corresponding adjacency matrices to guide contrastive learning. Further, we propose to strengthen the role of true negative samples by explicitly leveraging the visual features of sound sources to facilitate the differentiation of authentic sounding source regions. FNAC achieves state-of-the-art performances on Flickr-SoundNet, VGG-Sound, and AVSBench, which demonstrates the effectiveness of our method in mitigating the false negative issue. The code is available at https://github.com/OpenNLPLab/FNAC_AVL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">726.Improving the Transferability of Adversarial Samples by Path-Augmented Method</span><br>
                <span class="as">Zhang, JianpingandHuang, Jen-tseandWang, WenxuanandLi, YichenandWu, WeibinandWang, XiaosenandSu, YuxinandLyu, MichaelR.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Improving_the_Transferability_of_Adversarial_Samples_by_Path-Augmented_Method_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8173-8182.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度神经网络在各种视觉任务上取得了巨大成功，但对人眼无法察觉的对抗性噪声非常敏感，这限制了它们在实际场景，特别是安全相关场景中的应用。<br>
                    动机：为了评估目标模型在实践中的鲁棒性，基于转移的攻击通过本地模型制作对抗样本，由于其高效率而引起了研究者的广泛关注。<br>
                    方法：我们提出了路径增强方法（PAM），首先构造一个候选增强路径池，然后在生成对抗样本时使用贪婪搜索确定使用的增强路径。此外，为了避免增强语义不一致的图像，我们训练了一个语义预测器（SP）来约束增强路径的长度。<br>
                    效果：大量实验证明，与最先进的基线相比，PAM在攻击成功率方面平均提高了4.8%以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks have achieved unprecedented success on diverse vision tasks. However, they are vulnerable to adversarial noise that is imperceptible to humans. This phenomenon negatively affects their deployment in real-world scenarios, especially security-related ones. To evaluate the robustness of a target model in practice, transfer-based attacks craft adversarial samples with a local model and have attracted increasing attention from researchers due to their high efficiency. The state-of-the-art transfer-based attacks are generally based on data augmentation, which typically augments multiple training images from a linear path when learning adversarial samples. However, such methods selected the image augmentation path heuristically and may augment images that are semantics-inconsistent with the target images, which harms the transferability of the generated adversarial samples. To overcome the pitfall, we propose the Path-Augmented Method (PAM). Specifically, PAM first constructs a candidate augmentation path pool. It then settles the employed augmentation paths during adversarial sample generation with greedy search. Furthermore, to avoid augmenting semantics-inconsistent images, we train a Semantics Predictor (SP) to constrain the length of the augmentation path. Extensive experiments confirm that PAM can achieve an improvement of over 4.8% on average compared with the state-of-the-art baselines in terms of the attack success rates.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">727.Robust Mean Teacher for Continual and Gradual Test-Time Adaptation</span><br>
                <span class="as">D\&quot;obler, MarioandMarsden, RobertA.andYang, Bin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7704-7714.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在测试时处理领域转移，并解决由此产生的误差累积问题。<br>
                    动机：在实际应用中，测试时的领域转移是不可避免的，因此需要一种方法来适应这种转移。<br>
                    方法：提出了一种新的健壮均值教师（RMT）方法，该方法使用对称交叉熵作为一致性损失，并通过对比学习将测试特征空间拉近源域。<br>
                    效果：在CIFAR10C、CIFAR100C和Imagenet-C等连续和逐渐损坏的基准上取得了最先进的结果，并在新的持续DomainNet-126基准上取得了优秀的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Since experiencing domain shifts during test-time is inevitable in practice, test-time adaption (TTA) continues to adapt the model after deployment. Recently, the area of continual and gradual test-time adaptation (TTA) emerged. In contrast to standard TTA, continual TTA considers not only a single domain shift, but a sequence of shifts. Gradual TTA further exploits the property that some shifts evolve gradually over time. Since in both settings long test sequences are present, error accumulation needs to be addressed for methods relying on self-training. In this work, we propose and show that in the setting of TTA, the symmetric cross-entropy is better suited as a consistency loss for mean teachers compared to the commonly used cross-entropy. This is justified by our analysis with respect to the (symmetric) cross-entropy's gradient properties. To pull the test feature space closer to the source domain, where the pre-trained model is well posed, contrastive learning is leveraged. Since applications differ in their requirements, we address several settings, including having source data available and the more challenging source-free setting. We demonstrate the effectiveness of our proposed method "robust mean teacher" (RMT) on the continual and gradual corruption benchmarks CIFAR10C, CIFAR100C, and Imagenet-C. We further consider ImageNet-R and propose a new continual DomainNet-126 benchmark. State-of-the-art results are achieved on all benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">728.Understanding Imbalanced Semantic Segmentation Through Neural Collapse</span><br>
                <span class="as">Zhong, ZhishengandCui, JiequanandYang, YiboandWu, XiaoyangandQi, XiaojuanandZhang, XiangyuandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhong_Understanding_Imbalanced_Semantic_Segmentation_Through_Neural_Collapse_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19550-19560.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究探索了在语义分割中，最后一层特征中心和分类器的结构。<br>
                    动机：我们发现在语义分割中，类别间的上下文关联性和不平衡分布破坏了神经网络塌陷的等角最大分离结构，但对少数类有利。<br>
                    方法：提出了一种新的健壮均值教师（RMT）方法，该方法使用对称交叉熵作为一致性损失，并通过对比学习将测试特征空间拉近源域。<br>
                    效果：在CIFAR10C、CIFAR100C和Imagenet-C等连续和逐渐损坏的基准上取得了最先进的结果，并在新的持续DomainNet-126基准上取得了优秀的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A recent study has shown a phenomenon called neural collapse in that the within-class means of features and the classifier weight vectors converge to the vertices of a simplex equiangular tight frame at the terminal phase of training for classification. In this paper, we explore the corresponding structures of the last-layer feature centers and classifiers in semantic segmentation. Based on our empirical and theoretical analysis, we point out that semantic segmentation naturally brings contextual correlation and imbalanced distribution among classes, which breaks the equiangular and maximally separated structure of neural collapse for both feature centers and classifiers. However, such a symmetric structure is beneficial to discrimination for the minor classes. To preserve these advantages, we introduce a regularizer on feature centers to encourage the network to learn features closer to the appealing structure in imbalanced semantic segmentation. Experimental results show that our method can bring significant improvements on both 2D and 3D semantic segmentation benchmarks. Moreover, our method ranks first and sets a new record (+6.8% mIoU) on the ScanNet200 test leaderboard.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">729.Generalized UAV Object Detection via Frequency Domain Disentanglement</span><br>
                <span class="as">Wang, KunyuandFu, XueyangandHuang, YukunandCao, ChengzhiandShi, GegeandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Generalized_UAV_Object_Detection_via_Frequency_Domain_Disentanglement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1064-1073.png><br>
            
            <span class="tt"><span class="t0">研究问题：在将无人机目标检测（UAV-OD）网络部署到复杂和未见过的真实世界场景时，由于领域转移，通常会导致其泛化能力下降。<br>
                    动机：为了解决这个问题，本文提出了一种新的频域解耦方法来提高UAV-OD的泛化能力。<br>
                    方法：首先验证了图像中不同波段的频谱对UAV-OD泛化的影响不同。基于这个结论，设计了两种可学习的滤波器来提取领域不变的频谱和领域特定的频谱。前者可以用于训练UAV-OD网络并提高其泛化能力。此外，设计了一种新的实例级对比损失来指导网络训练。这种损失使网络能够专注于提取领域不变的频谱和领域特定的频谱，从而实现更好的解耦结果。<br>
                    效果：在三个未见过的目标领域的实验结果表明，我们的方法比基线方法和最先进的方法具有更好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When deploying the Unmanned Aerial Vehicles object detection (UAV-OD) network to complex and unseen real-world scenarios, the generalization ability is usually reduced due to the domain shift. To address this issue, this paper proposes a novel frequency domain disentanglement method to improve the UAV-OD generalization. Specifically, we first verified that the spectrum of different bands in the image has different effects to the UAV-OD generalization. Based on this conclusion, we design two learnable filters to extract domain-invariant spectrum and domain-specific spectrum, respectively. The former can be used to train the UAV-OD network and improve its capacity for generalization. In addition, we design a new instance-level contrastive loss to guide the network training. This loss enables the network to concentrate on extracting domain-invariant spectrum and domain-specific spectrum, so as to achieve better disentangling results. Experimental results on three unseen target domains demonstrate that our method has better generalization ability than both the baseline method and state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">730.Source-Free Adaptive Gaze Estimation by Uncertainty Reduction</span><br>
                <span class="as">Cai, XinandZeng, JiabeiandShan, ShiguangandChen, Xilin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Source-Free_Adaptive_Gaze_Estimation_by_Uncertainty_Reduction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22035-22045.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一种能在真实和多样化环境中使用的注视估计器，同时解决在源数据和目标数据之间进行联合训练的隐私和效率问题。<br>
                    动机：由于训练数据通常在受控条件下收集，而训练好的注视估计器需要在真实和多样化的环境中使用，因此需要探索跨领域的注视估计。<br>
                    方法：提出了一种无监督的源自由领域适应方法，通过降低样本和模型的不确定性来适应未标记的目标领域，无需源数据。<br>
                    效果：在六个跨领域任务上进行了广泛的实验，结果显示该方法优于其他最先进的跨领域注视估计方法，无论是否有源数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gaze estimation across domains has been explored recently because the training data are usually collected under controlled conditions while the trained gaze estimators are used in real and diverse environments. However, due to privacy and efficiency concerns, simultaneous access to annotated source data and to-be-predicted target data can be challenging. In light of this, we present an unsupervised source-free domain adaptation approach for gaze estimation, which adapts a source-trained gaze estimator to unlabeled target domains without source data. We propose the Uncertainty Reduction Gaze Adaptation (UnReGA) framework, which achieves adaptation by reducing both sample and model uncertainty. Sample uncertainty is mitigated by enhancing image quality and making them gaze-estimation-friendly, whereas model uncertainty is reduced by minimizing prediction variance on the same inputs. Extensive experiments are conducted on six cross-domain tasks, demonstrating the effectiveness of UnReGA and its components. Results show that UnReGA outperforms other state-of-the-art cross-domain gaze estimation methods under both protocols, with and without source data</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">731.SuperDisco: Super-Class Discovery Improves Visual Recognition for the Long-Tail</span><br>
                <span class="as">Du, YingjunandShen, JiayiandZhen, XiantongandSnoek, CeesG.M.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_SuperDisco_Super-Class_Discovery_Improves_Visual_Recognition_for_the_Long-Tail_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19944-19954.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善现代图像分类器在少数实例的尾部类别上的性能下降问题。<br>
                    动机：人类可以轻松处理长尾识别挑战，而现有的图像分类器则在尾部类别上性能显著下降。<br>
                    方法：提出SuperDisco算法，通过构建超类图模型来发现用于长尾识别的超类表示。通过在超类图上进行消息传递，根据语义相似性，修正和提炼与最相关实体相关的图像表示。<br>
                    效果：在CIFAR-100、ImageNet、Places和iNaturalist等长尾数据集上的实验表明，该方法可以有效改善长尾识别性能，并取得了一致的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern image classifiers perform well on populated classes while degrading considerably on tail classes with only a few instances. Humans, by contrast, effortlessly handle the long-tailed recognition challenge, since they can learn the tail representation based on different levels of semantic abstraction, making the learned tail features more discriminative. This phenomenon motivated us to propose SuperDisco, an algorithm that discovers super-class representations for long-tailed recognition using a graph model. We learn to construct the super-class graph to guide the representation learning to deal with long-tailed distributions. Through message passing on the super-class graph, image representations are rectified and refined by attending to the most relevant entities based on the semantic similarity among their super-classes. Moreover, we propose to meta-learn the super-class graph under the supervision of a prototype graph constructed from a small amount of imbalanced data. By doing so, we obtain a more robust super-class graph that further improves the long-tailed recognition performance. The consistent state-of-the-art experiments on the long-tailed CIFAR-100, ImageNet, Places, and iNaturalist demonstrate the benefit of the discovered super-class graph for dealing with long-tailed distributions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">732.Improving Generalization of Meta-Learning With Inverted Regularization at Inner-Level</span><br>
                <span class="as">Wang, LianzheandZhou, ShijiandZhang, ShanghangandChu, XuandChang, HengandZhu, Wenwu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Improving_Generalization_of_Meta-Learning_With_Inverted_Regularization_at_Inner-Level_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7826-7835.png><br>
            
            <span class="tt"><span class="t0">研究问题：元学习中的泛化问题是一个重要的挑战，尽管现有工作通过在元级别上正则化元损失来关注未见过的任务的元泛化，但忽视了适应模型可能在适应级别上无法泛化到任务领域。<br>
                    动机：本文提出了一种新的元学习正则化机制——Minimax-Meta Regularization，该机制在训练过程中在内循环使用反向正则化，在外循环使用普通正则化。<br>
                    方法：具体来说，内部反向正则化使得适应模型更难以泛化到任务领域；因此，优化外循环损失迫使元模型学习具有更好泛化的元知识。<br>
                    效果：理论上，我们证明反向正则化通过减少泛化错误来提高元测试性能。我们在代表性场景中进行了大量的实验，结果表明我们的方法始终能提高元学习算法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the broad interest in meta-learning, the generalization problem remains one of the significant challenges in this field. Existing works focus on meta-generalization to unseen tasks at the meta-level by regularizing the meta-loss, while ignoring that adapted models may not generalize to the task domains at the adaptation level. In this paper, we propose a new regularization mechanism for meta-learning -- Minimax-Meta Regularization, which employs inverted regularization at the inner loop and ordinary regularization at the outer loop during training. In particular, the inner inverted regularization makes the adapted model more difficult to generalize to task domains; thus, optimizing the outer-loop loss forces the meta-model to learn meta-knowledge with better generalization. Theoretically, we prove that inverted regularization improves the meta-testing performance by reducing generalization errors. We conduct extensive experiments on the representative scenarios, and the results show that our method consistently improves the performance of meta-learning algorithms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">733.Data-Efficient Large Scale Place Recognition With Graded Similarity Supervision</span><br>
                <span class="as">Leyva-Vallina, Mar{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Leyva-Vallina_Data-Efficient_Large_Scale_Place_Recognition_With_Graded_Similarity_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23487-23496.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉地点识别（VPR）是计算机视觉的基本任务，但现有的方法在训练研究问题：视觉地点识别（VPR）是计算机视觉的基本任务，但现有的方法在训练中使用的图像对只能表示同一地点或不同地点，这种二分法的指示并未考虑到同一地点在不同位置拍摄的图像之间的连续相似性。<br>
                    动机：由于相机位姿的差异，同一地点的两个图像只有部分共享的视觉线索。因此，我们提出了一种新的自动重新标注策略来重新标记VPR数据集。<br>
                    方法：我们根据可用的定位元数据计算图像对的分级相似性标签，并提出一种新的广义对比损失（GCL），该损失使用分级相似性标签来训练对比网络。<br>
                    效果：新的标签和GCL的使用使得我们可以摆脱硬对挖掘，训练出在VPR中表现更好的图像描述符，通过最近邻搜索获得优于或相当于需要昂贵硬对挖掘和重排技术的方法的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual place recognition (VPR) is a fundamental task of computer vision for visual localization. Existing methods are trained using image pairs that either depict the same place or not. Such a binary indication does not consider continuous relations of similarity between images of the same place taken from different positions, determined by the continuous nature of camera pose. The binary similarity induces a noisy supervision signal into the training of VPR methods, which stall in local minima and require expensive hard mining algorithms to guarantee convergence. Motivated by the fact that two images of the same place only partially share visual cues due to camera pose differences, we deploy an automatic re-annotation strategy to re-label VPR datasets. We compute graded similarity labels for image pairs based on available localization metadata. Furthermore, we propose a new Generalized Contrastive Loss (GCL) that uses graded similarity labels for training contrastive networks. We demonstrate that the use of the new labels and GCL allow to dispense from hard-pair mining, and to train image descriptors that perform better in VPR by nearest neighbor search, obtaining superior or comparable results than methods that require expensive hard-pair mining and re-ranking techniques.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">734.OpenMix: Exploring Outlier Samples for Misclassification Detection</span><br>
                <span class="as">Zhu, FeiandCheng, ZhenandZhang, Xu-YaoandLiu, Cheng-Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_OpenMix_Exploring_Outlier_Samples_for_Misclassification_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12074-12083.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络分类器的可靠置信度估计，特别是在高风险应用中。<br>
                    动机：现代深度神经网络经常对其错误的预测过于自信，这在高风险应用中是一个挑战。<br>
                    方法：利用易获取的异常样本（即来自非目标类别的未标记样本）来帮助检测误分类错误。特别是，我们发现著名的Outlier Exposure在识别未知类别的分布外（OOD）样本方面非常强大，但在识别误分类错误方面并未提供任何增益。基于这些观察，我们提出了一种名为OpenMix的新方法，该方法通过学习拒绝由异常转换生成的不确定伪样本来结合开放世界知识。<br>
                    效果：OpenMix在各种情况下显著提高了置信度的可靠性，为检测已知类别的误分类样本和未知类别的OOD样本建立了一个强大而统一的框架。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD samples from unknown classes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">735.Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection</span><br>
                <span class="as">Munir, MuhammadAkhtarandKhan, MuhammadHarisandKhan, SalmanandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Munir_Bridging_Precision_and_Confidence_A_Train-Time_Loss_for_Calibrating_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11474-11483.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度神经网络在视觉问题上取得了显著的进展，但往往预测过于自信，导致校准效果不佳。<br>
                    动机：大部分解决深度神经网络校准问题的研究都集中在分类任务上，对于作为许多视觉安全关键应用核心的基于深度神经网络的目标检测模型的校准问题，目前还鲜有研究。<br>
                    方法：受训练时校准方法的启发，本文提出了一种新的辅助损失函数形式，旨在明确将边界框的类别置信度与预测准确性（即精度）对齐。由于原始损失函数依赖于一个minibatch中真正例和假正例的数量，我们开发了一个可与其他特定于应用程序的损失函数一起使用的可微代理损失函数。<br>
                    效果：我们在具有挑战性的内域和外域场景下，使用包括MS-COCO、Cityscapes、Sim10k和BDD100k在内的六个基准数据集进行了广泛的实验。结果显示，我们的训练时损失在减少内域和外域场景的校准误差方面优于强大的校准基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) have enabled astounding progress in several vision-based problems. Despite showing high predictive accuracy, recently, several works have revealed that they tend to provide overconfident predictions and thus are poorly calibrated. The majority of the works addressing the miscalibration of DNNs fall under the scope of classification and consider only in-domain predictions. However, there is little to no progress in studying the calibration of DNN-based object detection models, which are central to many vision-based safety-critical applications. In this paper, inspired by the train-time calibration methods, we propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bounding boxes with the accurateness of predictions (i.e. precision). Since the original formulation of our loss depends on the counts of true positives and false positives in a minibatch, we develop a differentiable proxy of our loss that can be used during training with other application-specific loss functions. We perform extensive experiments on challenging in-domain and out-domain scenarios with six benchmark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Our results reveal that our train-time loss surpasses strong calibration baselines in reducing calibration error for both in and out-domain scenarios. Our source code and pre-trained models are available at https://github.com/akhtarvision/bpc_calibration</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">736.Adaptive Data-Free Quantization</span><br>
                <span class="as">Qian, BiaoandWang, YangandHong, RichangandWang, Meng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qian_Adaptive_Data-Free_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7960-7968.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何量化网络（Q）在没有原始数据的情况下恢复性能，同时生成的伪造样本是否对学习过程有益？<br>
                    动机：目前的无数据量化方法忽略了生成样本的知识适应性，导致泛化误差过大。<br>
                    方法：提出一种自适应无数据量化（AdaDFQ）方法，从零和游戏的角度重新审视样本适应性问题，优化生成样本与量化网络之间的边界，以解决过拟合和欠拟合问题。<br>
                    效果：实验证明，AdaDFQ优于现有技术，其生成的样本不仅应具有信息性，还应与训练数据的类别和分布信息相关。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Data-free quantization (DFQ) recovers the performance of quantized network (Q) without the original data, but generates the fake sample via a generator (G) by learning from full-precision network (P), which, however, is totally independent of Q, overlooking the adaptability of the knowledge from generated samples, i.e., informative or not to the learning process of Q, resulting into the overflow of generalization error. Building on this, several critical questions -- how to measure the sample adaptability to Q under varied bit-width scenarios? whether the largest adaptability is the best? how to generate the samples with adaptive adaptability to improve Q's generalization? To answer the above questions, in this paper, we propose an Adaptive Data-Free Quantization (AdaDFQ) method, which revisits DFQ from a zero-sum game perspective upon the sample adaptability between two players -- a generator and a quantized network. Following this viewpoint, we further define the disagreement and agreement samples to form two boundaries, where the margin between two boundaries is optimized to adaptively regulate the adaptability of generated samples to Q, so as to address the over-and-under fitting issues. Our AdaDFQ reveals: 1) the largest adaptability is NOT the best for sample generation to benefit Q's generalization; 2) the knowledge of the generated sample should not be informative to Q only, but also related to the category and distribution information of the training data for P. The theoretical and empirical analysis validate the advantages of AdaDFQ over the state-of-the-arts. Our code is available at https://github.com/hfutqian/AdaDFQ.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">737.Ground-Truth Free Meta-Learning for Deep Compressive Sampling</span><br>
                <span class="as">Qin, XinranandQuan, YuhuiandPang, TongyaoandJi, Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Ground-Truth_Free_Meta-Learning_for_Deep_Compressive_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9947-9956.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种无需真实值（GT）的元学习方法，用于压缩采样（CS）中的高质量图像重建。<br>
                    动机：目前的深度学习在压缩采样中重建图像方面起着重要作用，但需要大量的标注数据。本文提出了一种无需真实值的元学习方法，利用外部和内部学习进行无监督高质量图像重建。<br>
                    方法：该方法首先使用仅基于压缩采样测量的外部元学习训练深度模型，然后通过利用其内部特性对测试样本进行有效改进来适应已训练的模型。元学习和模型适应建立在改进的Stein无偏风险估计器（iSURE）上，为测量矩阵伴随范围空间中的准确预测提供有效的指导和高效的计算。<br>
                    效果：实验结果表明，提出的无需真实值的方法表现良好，甚至可以与基于监督学习的方方法竞争。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning has become an important tool for reconstructing images in compressive sampling (CS). This paper proposes a ground-truth (GT) free meta-learning method for CS, which leverages both external and internal learning for unsupervised high-quality image reconstruction. The proposed method first trains a deep model via external meta-learning using only CS measurements, and then efficiently adapts the trained model to a test sample for further improvement by exploiting its internal characteristics. The meta-learning and model adaptation are built on an improved Stein's unbiased risk estimator (iSURE) that provides efficient computation and effective guidance for accurate prediction in the range space of the adjoint of the measurement matrix. To further improve the learning on the null space of the measurement matrix, a modified model-agnostic meta-learning scheme is proposed, along with a null-space-consistent loss and a bias-adaptive deep unrolling network to improve and accelerate model adaption in test time. Experimental results have demonstrated that the proposed GT-free method performs well, and can even compete with supervised learning-based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">738.DistractFlow: Improving Optical Flow Estimation via Realistic Distractions and Pseudo-Labeling</span><br>
                <span class="as">Jeong, JisooandCai, HongandGarrepalli, RisheekandPorikli, Fatih</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_DistractFlow_Improving_Optical_Flow_Estimation_via_Realistic_Distractions_and_Pseudo-Labeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13691-13700.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过引入真实的干扰来训练光流估计模型。<br>
                    动机：现有的数据增强方法主要关注低层次的修改，而我们的方法通过使用语义上有意义的干扰物，使模型能够学习相关的变化并提高对挑战性偏差的鲁棒性。<br>
                    方法：我们提出了一种新的数据增强方法DistractFlow，通过将一对帧中的一个与描绘相似领域的干扰图像结合，引入与自然物体和场景一致的视觉扰动。我们还在原始对的估计流和其真实流之间以及干扰对的流和原始对的真实流之间定义了两个监督损失。<br>
                    效果：我们在多个基准测试中进行了广泛的评估，包括Sintel、KITTI和SlowFlow，结果显示DistractFlow能够持续改善现有模型，超越最新的技术状态。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel data augmentation approach, DistractFlow, for training optical flow estimation models by introducing realistic distractions to the input frames. Based on a mixing ratio, we combine one of the frames in the pair with a distractor image depicting a similar domain, which allows for inducing visual perturbations congruent with natural objects and scenes. We refer to such pairs as distracted pairs. Our intuition is that using semantically meaningful distractors enables the model to learn related variations and attain robustness against challenging deviations, compared to conventional augmentation schemes focusing only on low-level aspects and modifications. More specifically, in addition to the supervised loss computed between the estimated flow for the original pair and its ground-truth flow, we include a second supervised loss defined between the distracted pair's flow and the original pair's ground-truth flow, weighted with the same mixing ratio. Furthermore, when unlabeled data is available, we extend our augmentation approach to self-supervised settings through pseudo-labeling and cross-consistency regularization. Given an original pair and its distracted version, we enforce the estimated flow on the distracted pair to agree with the flow of the original pair. Our approach allows increasing the number of available training pairs significantly without requiring additional annotations. It is agnostic to the model architecture and can be applied to training any optical flow estimation models. Our extensive evaluations on multiple benchmarks, including Sintel, KITTI, and SlowFlow, show that DistractFlow improves existing models consistently, outperforming the latest state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">739.Flexible-Cm GAN: Towards Precise 3D Dose Prediction in Radiotherapy</span><br>
                <span class="as">Gao, RiqiangandLou, BinandXu, ZhoubingandComaniciu, DorinandKamen, Ali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Flexible-Cm_GAN_Towards_Precise_3D_Dose_Prediction_in_Radiotherapy_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/715-725.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习进行知识基础的放射治疗规划，以适应不同的临床场景。<br>
                    动机：现有的深度学习方法主要适用于简单的场景，如固定的治疗类型或一致的射束角度配置，限制了其通用性和实用性。<br>
                    方法：提出了一种名为Flexible-C^m GAN的新型条件生成模型，利用额外的治疗类型和各种射束几何信息。并设计了一种失配一致性损失函数来处理输入数据条件有限的问题。<br>
                    效果：通过实验验证，该方法在实际应用中的性能优于现有的深度学习方法，可以灵活地选择特定的治疗类型和射束角度以满足临床需求。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning has been utilized in knowledge-based radiotherapy planning in which a system trained with a set of clinically approved plans is employed to infer a three-dimensional dose map for a given new patient. However, previous deep methods are primarily limited to simple scenarios, e.g., a fixed planning type or a consistent beam angle configuration. This in fact limits the usability of such approaches and makes them not generalizable over a larger set of clinical scenarios. Herein, we propose a novel conditional generative model, Flexible-C^m GAN, utilizing additional information regarding planning types and various beam geometries. A miss-consistency loss is proposed to deal with the challenge of having a limited set of conditions on the input data, e.g., incomplete training samples. To address the challenges of including clinical preferences, we derive a differentiable shift-dose-volume loss to incorporate the well-known dose-volume histogram constraints. During inference, users can flexibly choose a specific planning type and a set of beam angles to meet the clinical requirements. We conduct experiments on an illustrative face dataset to show the motivation of Flexible-C^m GAN and further validate our model's potential clinical values with two radiotherapy datasets. The results demonstrate the superior performance of the proposed method in a practical heterogeneous radiotherapy planning application compared to existing deep learning-based approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">740.Learning To Measure the Point Cloud Reconstruction Loss in a Representation Space</span><br>
                <span class="as">Huang, TianxinandDing, ZhongganandZhang, JiangningandTai, YingandZhang, ZhenyuandChen, MingangandWang, ChengjieandLiu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_To_Measure_the_Point_Cloud_Reconstruction_Loss_in_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12208-12217.png><br>
            
            <span class="tt"><span class="t0">研究问题：针对点云重建相关任务，如何更准确地评估重建结果与真实值之间的形状差异。<br>
                    动机：现有的方法通常使用点到点的欧氏距离来度量训练损失，但这种方法可能引入额外的缺陷，因为预定义的匹配规则可能偏离真实的形状差异。<br>
                    方法：本文提出了一种基于学习的对比对抗损失（CALoss）方法，通过结合对比约束和对抗策略，在非线性表示空间中动态地度量点云重建损失。具体来说，我们使用对比约束来帮助CALoss学习具有形状相似性的表现空间，同时引入对抗策略来帮助CALoss挖掘重建结果与真实值之间的差异。<br>
                    效果：实验结果表明，CALoss可以帮助任务网络提高重建性能并学习更具代表性的表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For point cloud reconstruction-related tasks, the reconstruction losses to evaluate the shape differences between reconstructed results and the ground truths are typically used to train the task networks. Most existing works measure the training loss with point-to-point distance, which may introduce extra defects as predefined matching rules may deviate from the real shape differences. Although some learning-based works have been proposed to overcome the weaknesses of manually-defined rules, they still measure the shape differences in 3D Euclidean space, which may limit their ability to capture defects in reconstructed shapes. In this work, we propose a learning-based Contrastive Adversarial Loss (CALoss) to measure the point cloud reconstruction loss dynamically in a non-linear representation space by combining the contrastive constraint with the adversarial strategy. Specifically, we use the contrastive constraint to help CALoss learn a representation space with shape similarity, while we introduce the adversarial strategy to help CALoss mine differences between reconstructed results and ground truths. According to experiments on reconstruction-related tasks, CALoss can help task networks improve reconstruction performances and learn more representative representations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">741.Back to the Source: Diffusion-Driven Adaptation To Test-Time Corruption</span><br>
                <span class="as">Gao, JinandZhang, JialingandLiu, XihuiandDarrell, TrevorandShelhamer, EvanandWang, Dequan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11786-11796.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用测试输入来提高在目标数据上训练的模型的准确性？<br>
                    动机：大多数方法通过重新训练源模型来更新源数据，但这种方法对数据的数量和顺序以及优化的超参数非常敏感。<br>
                    方法：我们更新目标数据，并使用生成性扩散模型将所有测试输入投影到源域。我们的扩散驱动适应（DDA）方法在所有领域共享其分类和生成模型，先在源上进行训练，然后对所有目标进行冻结，以避免昂贵的领域特定再训练。<br>
                    效果：在ImageNet-C基准测试中，DDA的输入适应比各种破坏、模型和数据模式下的模型适应更具鲁棒性。通过输入方式的更新，DDA在数据太少（小批量）、依赖数据（相关顺序）或混合数据（多种破坏）的情况下成功，而模型适应在这些情况下会退化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test-time adaptation harnesses test inputs to improve the accuracy of a model trained on source data when tested on shifted target data. Most methods update the source model by (re-)training on each target domain. While re-training can help, it is sensitive to the amount and order of the data and the hyperparameters for optimization. We update the target data instead, and project all test inputs toward the source domain with a generative diffusion model. Our diffusion-driven adaptation (DDA) method shares its models for classification and generation across all domains, training both on source then freezing them for all targets, to avoid expensive domain-wise re-training. We augment diffusion with image guidance and classifier self-ensembling to automatically decide how much to adapt. Input adaptation by DDA is more robust than model adaptation across a variety of corruptions, models, and data regimes on the ImageNet-C benchmark. With its input-wise updates, DDA succeeds where model adaptation degrades on too little data (small batches), on dependent data (correlated orders), or on mixed data (multiple corruptions).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">742.Regularizing Second-Order Influences for Continual Learning</span><br>
                <span class="as">Sun, ZhichengandMu, YadongandHua, Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20166-20175.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决持续学习中遗忘先前知识的问题。<br>
                    动机：现有的重放方法通过重演已看到的数据来解决此挑战，但需要仔细选择样本，而现有的选择方案通常只关注当前选择的最大效用，忽视了连续选择轮次之间的干扰。<br>
                    方法：本文在影响函数的框架下，分析了连续选择步骤之间的交互作用，并提出了一种新的二阶影响类别来逐渐放大偶然性偏差，以规范二阶效应。同时，还提出了一种新颖的选择目标，并与两种广泛采用的标准有明确的联系。<br>
                    效果：实验证明，该方法在多个持续学习基准测试中优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning aims to learn on non-stationary data streams without catastrophically forgetting previous knowledge. Prevalent replay-based methods address this challenge by rehearsing on a small buffer holding the seen data, for which a delicate sample selection strategy is required. However, existing selection schemes typically seek only to maximize the utility of the ongoing selection, overlooking the interference between successive rounds of selection. Motivated by this, we dissect the interaction of sequential selection steps within a framework built on influence functions. We manage to identify a new class of second-order influences that will gradually amplify incidental bias in the replay buffer and compromise the selection process. To regularize the second-order effects, a novel selection objective is proposed, which also has clear connections to two widely adopted criteria. Furthermore, we present an efficient implementation for optimizing the proposed criterion. Experiments on multiple continual learning benchmarks demonstrate the advantage of our approach over state-of-the-art methods. Code is available at https://github.com/feifeiobama/InfluenceCL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">743.GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency</span><br>
                <span class="as">Tian, LinandGreer, HastingsandVialard, Fran\c{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_GradICON_Approximate_Diffeomorphisms_via_Gradient_Inverse_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18084-18094.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在医学图像配准中学习图像对之间的规则空间变换。<br>
                    动机：与优化基的配准技术和许多现代基于学习的方法不同，我们不直接惩罚转换不规则性，而是通过反向一致性惩罚来促进转换规则性。<br>
                    方法：我们使用神经网络预测源图像和目标图像之间的映射以及交换源图像和目标图像时的映射。不同于现有方法，我们将这两个结果映射组合并规范其雅可比矩阵的偏离。<br>
                    效果：我们的注册模型训练时，这种规范化器——GradICON——比直接推广映射组合的反向一致性同时保留后者的显式规范化效应，能更好地收敛。我们在各种真实世界的医疗图像数据集上实现了一流的注册性能，使用一组超参数和一个非特定于数据集的训练协议。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present an approach to learning regular spatial transformations between image pairs in the context of medical image registration. Contrary to optimization-based registration techniques and many modern learning-based methods, we do not directly penalize transformation irregularities but instead promote transformation regularity via an inverse consistency penalty. We use a neural network to predict a map between a source and a target image as well as the map when swapping the source and target images. Different from existing approaches, we compose these two resulting maps and regularize deviations of the Jacobian of this composition from the identity matrix. This regularizer -- GradICON -- results in much better convergence when training registration models compared to promoting inverse consistency of the composition of maps directly while retaining the desirable implicit regularization effects of the latter. We achieve state-of-the-art registration performance on a variety of real-world medical image datasets using a single set of hyperparameters and a single non-dataset-specific training protocol. The code is available at https://github.com/uncbiag/ICON.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">744.Distribution Shift Inversion for Out-of-Distribution Prediction</span><br>
                <span class="as">Yu, RunpengandLiu, SonghuaandYang, XingyiandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Distribution_Shift_Inversion_for_Out-of-Distribution_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3592-3602.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接减轻未见过测试集中的分布偏移，由于在训练阶段无法获取测试分布，因此训练一个在训练和测试分布之间进行映射的分布转换器是不可能的。<br>
                    动机：解决现有算法在处理未见过的分布（OoD）问题上的挑战，即通过寻找统一的预测器或不变的特征表示来处理训练和测试分布之间的分布偏移。<br>
                    方法：提出一种便携式的分布偏移反转（DSI）算法，该算法首先将OoD测试样本与额外的高斯噪声线性组合，然后使用仅在源分布上训练的扩散模型将其转回训练分布。<br>
                    效果：理论分析和实验结果表明，该方法在多种领域泛化数据集和单一领域泛化数据集上都取得了良好的性能提升，可以广泛应用于常见的OoD算法中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Machine learning society has witnessed the emergence of a myriad of Out-of-Distribution (OoD) algorithms, which address the distribution shift between the training and the testing distribution by searching for a unified predictor or invariant feature representation. However, the task of directly mitigating the distribution shift in the unseen testing set is rarely investigated, due to the unavailability of the testing distribution during the training phase and thus the impossibility of training a distribution translator mapping between the training and testing distribution. In this paper, we explore how to bypass the requirement of testing distribution for distribution translator training and make the distribution translation useful for OoD prediction. We propose a portable Distribution Shift Inversion (DSI) algorithm, in which, before being fed into the prediction model, the OoD testing samples are first linearly combined with additional Gaussian noise and then transferred back towards the training distribution using a diffusion model trained only on the source distribution. Theoretical analysis reveals the feasibility of our method. Experimental results, on both multiple-domain generalization datasets and single-domain generalization datasets, show that our method provides a general performance gain when plugged into a wide range of commonly used OoD algorithms. Our code is available at https://github.com/yu-rp/Distribution-Shift-Iverson  https://github.com/yu-rp/Distribution-Shift-Iverson.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">745.Soft Augmentation for Image Classification</span><br>
                <span class="as">Liu, YangandYan, ShenandLeal-Taix\&#x27;e, LauraandHays, JamesandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Soft_Augmentation_for_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16241-16250.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过数据增强来减少过拟合并提高泛化能力？<br>
                    动机：目前的神经网络模型存在过度参数化的问题，需要强大的正则化方法如数据增强和权重衰减来改善。<br>
                    方法：从人类视觉分类研究中获取灵感，提出将数据增强的不变变换推广为软增强，即学习目标随着样本变换程度的非线性软化。例如，更激进的图像裁剪增强会产生较低置信度的学习目标。<br>
                    效果：实验证明，软目标允许进行更激进的数据增强，提供更稳健的性能提升，可以与其他增强策略一起使用，并且能产生更准确校准的模型。结合现有的激进增强策略，软目标在Cifar-10、Cifar-100、ImageNet-1K和ImageNet-V2上将top-1准确率提高了一倍，模型遮挡性能提高了4倍，预期校准误差（ECE）降低了一半。最后，我们证明了软增强可以推广到自我监督分类任务中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmentation and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sample. We draw inspiration from human visual classification studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learning target softens non-linearly as a function of the degree of the transform applied to the sample: e.g., more aggressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation policies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing aggressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2, 2) improve model occlusion performance by up to 4x, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation generalizes to self-supervised classification tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">746.Probabilistic Knowledge Distillation of Face Ensembles</span><br>
                <span class="as">Xu, JianqingandLi, ShenandDeng, AilinandXiong, MiaoandWu, JiayingandWu, JiaxiangandDing, ShouhongandHooi, Bryan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Probabilistic_Knowledge_Distillation_of_Face_Ensembles_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3489-3498.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过结合大规模文本语料库和知识图谱来训练一种增强的语言表示模型（ERNIE）？<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，本文提出利用知识图谱中的有信息量的实体来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出ERNIE模型，该模型能同时充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mean ensemble (i.e. averaging predictions from multiple models) is a commonly-used technique in machine learning that improves the performance of each individual model. We formalize it as feature alignment for ensemble in open-set face recognition and generalize it into Bayesian Ensemble Averaging (BEA) through the lens of probabilistic modeling. This generalization brings up two practical benefits that existing methods could not provide: (1) the uncertainty of a face image can be evaluated and further decomposed into aleatoric uncertainty and epistemic uncertainty, the latter of which can be used as a measure for out-of-distribution detection of faceness; (2) a BEA statistic provably reflects the aleatoric uncertainty of a face image, acting as a measure for face image quality to improve recognition performance. To inherit the uncertainty estimation capability from BEA without the loss of inference efficiency, we propose BEA-KD, a student model to distill knowledge from BEA. BEA-KD mimics the overall behavior of ensemble members and consistently outperforms SOTA knowledge distillation methods on various challenging benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">747.Twin Contrastive Learning With Noisy Labels</span><br>
                <span class="as">Huang, ZhizhongandZhang, JunpingandShan, Hongming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11661-11670.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从有噪声的数据中学习，以改善模型性能。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：提出一种新颖的双胞胎对比学习模型TCL，通过将监督模型预测结果注入高斯混合模型（GMM）来链接标签自由的潜在变量和标签噪声注释，从而学习鲁棒的表示并处理分类的噪声标签。<br>
                    效果：实验结果表明，TCL在几种标准基准和真实世界数据集上表现出优越的性能，特别是在CIFAR-10上实现了7.5%的改进，这是一个非常嘈杂的场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning from noisy data is a challenging task that significantly degenerates the model performance. In this paper, we present TCL, a novel twin contrastive learning model to learn robust representations and handle noisy labels for classification. Specifically, we construct a Gaussian mixture model (GMM) over the representations by injecting the supervised model predictions into GMM to link label-free latent variables in GMM with label-noisy annotations. Then, TCL detects the examples with wrong labels as the out-of-distribution examples by another two-component GMM, taking into account the data distribution. We further propose a cross-supervision with an entropy regularization loss that bootstraps the true targets from model predictions to handle the noisy labels. As a result, TCL can learn discriminative representations aligned with estimated labels through mixup and contrastive learning. Extensive experimental results on several standard benchmarks and real-world datasets demonstrate the superior performance of TCL. In particular, TCL achieves 7.5% improvements on CIFAR-10 with 90% noisy label---an extremely noisy scenario. The source code is available at https://github.com/Hzzone/TCL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">748.Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection</span><br>
                <span class="as">Hu, QianjiangandLiu, DaizongandHu, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Density-Insensitive_Unsupervised_Domain_Adaption_on_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17556-17566.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何减少3D物体检测中由于光束密度不同导致的领域差距，提高模型的泛化能力。<br>
                    动机：现有的方法在处理光束密度不同的问题上效果不佳，且标记成本高，难以适应未知数据。<br>
                    方法：提出一种密度不敏感的领域适应框架，通过随机光束重采样增强源领域训练的3D探测器对光束密度变化的鲁棒性，并设计任务特定的教师-学生框架预测高质量的伪标签。<br>
                    效果：实验结果表明，该方法在三个广泛使用的3D物体检测数据集上优于最先进的方法，特别是在变化密度的数据上表现优秀。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection from point clouds is crucial in safety-critical autonomous driving. Although many works have made great efforts and achieved significant progress on this task, most of them suffer from expensive annotation cost and poor transferability to unknown data due to the domain gap. Recently, few works attempt to tackle the domain gap in objects, but still fail to adapt to the gap of varying beam-densities between two domains, which is critical to mitigate the characteristic differences of the LiDAR collectors. To this end, we make the attempt to propose a density-insensitive domain adaption framework to address the density-induced domain gap. In particular, we first introduce Random Beam Re-Sampling (RBRS) to enhance the robustness of 3D detectors trained on the source domain to the varying beam-density. Then, we take this pre-trained detector as the backbone model, and feed the unlabeled target domain data into our newly designed task-specific teacher-student framework for predicting its high-quality pseudo labels. To further adapt the property of density-insensitive into the target domain, we feed the teacher and student branches with the same sample of different densities, and propose an Object Graph Alignment (OGA) module to construct two object-graphs between the two branches for enforcing the consistency in both the attribute and relation of cross-density objects. Experimental results on three widely adopted 3D object detection datasets demonstrate that our proposed domain adaption method outperforms the state-of-the-art methods, especially over varying-density data. Code is available at https://github.com/WoodwindHu/DTS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">749.On-the-Fly Category Discovery</span><br>
                <span class="as">Du, RuoyiandChang, DongliangandLiang, KongmingandHospedales, TimothyandSong, Yi-ZheandMa, Zhanyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_On-the-Fly_Category_Discovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11691-11700.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管机器在视觉识别问题上超越了人类，但它们仍然只能提供封闭的解答。与机器不同，人类可以在第一次观察时就认知到新的类别。<br>
                    动机：目前的新颖类别发现（NCD）技术通过将已知类别的知识转移到未知类别来区分，旨在弥合这一差距。然而，当前的NCD方法假设了一个转导学习和离线推理范例，这限制了它们只能在预定义的查询集上工作，并且无法提供即时反馈。<br>
                    方法：本文研究了一种即时类别发现（OCD）方法，该方法使模型能够立即意识到新的类别样本（即实现归纳学习和流式推理）。我们首先设计了一个基于哈希编码的可扩展识别模型作为实用的基线。然后，注意到哈希码对类别内变化的敏感性，我们进一步提出了一种新的符号幅度差异（SMILE）架构来减轻它带来的干扰。<br>
                    效果：实验结果表明，SMILE相对于我们的基线模型和现有技术具有优越性。我们的代码将在https://github.com/PRIS-CV/On-the-fly-Category-Discovery上公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although machines have surpassed humans on visual recognition problems, they are still limited to providing closed-set answers. Unlike machines, humans can cognize novel categories at the first observation. Novel category discovery (NCD) techniques, transferring knowledge from seen categories to distinguish unseen categories, aim to bridge the gap. However, current NCD methods assume a transductive learning and offline inference paradigm, which restricts them to a pre-defined query set and renders them unable to deliver instant feedback. In this paper, we study on-the-fly category discovery (OCD) aimed at making the model instantaneously aware of novel category samples (i.e., enabling inductive learning and streaming inference). We first design a hash coding-based expandable recognition model as a practical baseline. Afterwards, noticing the sensitivity of hash codes to intra-category variance, we further propose a novel Sign-Magnitude dIsentangLEment (SMILE) architecture to alleviate the disturbance it brings. Our experimental results demonstrate the superiority of SMILE against our baseline model and prior art. Our code will be made publicly available. Our code is available at https://github.com/PRIS-CV/On-the-fly-Category-Discovery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">750.Test Time Adaptation With Regularized Loss for Weakly Supervised Salient Object Detection</span><br>
                <span class="as">Veksler, Olga</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Veksler_Test_Time_Adaptation_With_Regularized_Loss_for_Weakly_Supervised_Salient_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7360-7369.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何应对卷积神经网络（CNN）在训练数据上过度拟合的问题。<br>
                    动机：CNN往往对训练数据过度拟合，而测试时适应是一种解决过度拟合的极端方法。然而，主要困难在于无法获取真实标签。<br>
                    方法：提出了一种基于正则化损失函数的测试时显著目标检测（SOD）方法，该方法可以在像素级精确的真实标签不可用的情况下训练CNN。正则化损失倾向于为更可能的目标段赋予较低的值，因此可以用于微调已训练的CNN以适应给定的测试图像。<br>
                    效果：开发了一种特别适合测试时适应的正则化损失函数，并证明该方法在弱监督SOD方面明显优于先前的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It is well known that CNNs tend to overfit to the training data. Test-time adaptation is an extreme approach to deal with overfitting: given a test image, the aim is to adapt the trained model to that image. Indeed nothing can be closer to the test data than the test image itself. The main difficulty of test-time adaptation is that the ground truth is not available. Thus test-time adaptation, while intriguing, applies to only a few scenarios where one can design an effective loss function that does not require ground truth. We propose the first approach for test-time Salient Object Detection (SOD) in the context of weak supervision. Our approach is based on a so called regularized loss function, which can be used for training CNN when pixel precise ground truth is unavailable. Regularized loss tends to have lower values for the more likely object segments, and thus it can be used to fine-tune an already trained CNN to a given test image, adapting to images unseen during training. We develop a regularized loss function particularly suitable for test-time adaptation and show that our approach significantly outperforms prior work for weakly supervised SOD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">751.Guiding Pseudo-Labels With Uncertainty Estimation for Source-Free Unsupervised Domain Adaptation</span><br>
                <span class="as">Litrico, MattiaandDelBue, AlessioandMorerio, Pietro</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Litrico_Guiding_Pseudo-Labels_With_Uncertainty_Estimation_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7640-7650.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无监督领域适应（UDA）中源数据不可用的问题，特别是源自由无监督领域适应（SF-UDA）。<br>
                    动机：在许多实际应用中，源数据可能无法获取。因此，研究如何在没有源数据的情况下进行领域适应具有重要的实际意义。<br>
                    方法：提出了一种基于损失重加权策略的新颖方法，该方法通过估计伪标签的不确定性来度量其可靠性，并据此对分类损失进行重加权。同时，利用自我监督对比框架作为目标空间正则化器来增强这种知识聚合。此外，还提出了一种新的负对排除策略，以识别和排除共享相同类的样本对。<br>
                    效果：在三个主要基准测试中，该方法均大幅超越了先前的方法。在VisDA-C和DomainNet上，性能提升了1.8%，在PACS上，单源设置下的性能提升了12.3%，多目标适应下的性能提升了6.6%。额外的分析表明，该方法对噪声具有鲁棒性，生成的伪标签比最先进的方法更准确。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples sharing the same class, even in presence of some noise in the pseudo-labels. Our method outperforms previous methods on three major benchmarks by a large margin. We set the new SF-UDA state-of-the-art on VisDA-C and DomainNet with a performance gain of +1.8% on both benchmarks and on PACS with +12.3% in the single-source setting and +6.6% in multi-target adaptation. Additional analyses demonstrate that the proposed approach is robust to the noise, which results in significantly more accurate pseudo-labels compared to state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">752.Generalizable Local Feature Pre-Training for Deformable Shape Analysis</span><br>
                <span class="as">Attaiki, SouhaibandLi, LeiandOvsjanikov, Maks</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Attaiki_Generalizable_Local_Feature_Pre-Training_for_Deformable_Shape_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13650-13661.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练特征解决3D形状识别中的问题，特别是在处理变形有机形状等新类别时。<br>
                    动机：现有的迁移学习方法通常在完整的3D对象或场景级别上操作，无法泛化到新的类别，如变形的有机形状。同时，对于预训练特征在不同3D形状类别之间可转移的原因，目前还缺乏理解。<br>
                    方法：通过分析特征局部性和任务可转移性之间的关系，比较不同的骨干网络和损失函数进行局部特征预训练。提出一种可微的方法来优化3D迁移学习中的感知野大小。<br>
                    效果：实验结果表明，这种方法可以成功地推广到人类和动物等未见过的形状类别，并在分割、形状对应和分类等下游任务上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transfer learning is fundamental for addressing problems in settings with little training data. While several transfer learning approaches have been proposed in 3D, unfortunately, these solutions typically operate on an entire 3D object or even scene-level and thus, as we show, fail to generalize to new classes, such as deformable organic shapes. In addition, there is currently a lack of understanding of what makes pre-trained features transferable across significantly different 3D shape categories. In this paper, we make a step toward addressing these challenges. First, we analyze the link between feature locality and transferability in tasks involving deformable 3D objects, while also comparing different backbones and losses for local feature pre-training. We observe that with proper training, learned features can be useful in such tasks, but, crucially, only with an appropriate choice of the receptive field size. We then propose a differentiable method for optimizing the receptive field within 3D transfer learning. Jointly, this leads to the first learnable features that can successfully generalize to unseen classes of 3D shapes such as humans and animals. Our extensive experiments show that this approach leads to state-of-the-art results on several downstream tasks such as segmentation, shape correspondence, and classification. Our code is available at https://github.com/pvnieo/vader.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">753.Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval</span><br>
                <span class="as">Hao, XiaoshuaiandZhang, WanqianandWu, DayanandZhu, FeiandLi, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Dual_Alignment_Unsupervised_Domain_Adaptation_for_Video-Text_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18962-18972.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无监督领域适应视频-文本检索（UDAVR）中的困难任务，即训练和测试数据来自不同分布。<br>
                    动机：先前的工作仅缓解了领域偏移的问题，但忽视了目标领域中的配对错配问题，即目标视频和文本之间不存在语义关系。<br>
                    方法：我们提出了一种名为双重对齐领域适应（DADA）的新方法。具体来说，我们首先引入跨模态语义嵌入以在联合嵌入空间中生成判别性源特征。此外，我们利用视频和文本领域的适应来平滑地平衡最小化领域偏移。为了解决目标领域中的配对错配问题，我们引入了双重对齐一致性（DAC），以充分利用目标领域中两种模态的语义信息。<br>
                    效果：与最先进的方法相比，DADA在TGIF->MSRVTT和TGIF->MSVD设置下分别实现了20.18％和18.61％的R@1相对改进，证明了我们的方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video-text retrieval is an emerging stream in both computer vision and natural language processing communities, which aims to find relevant videos given text queries. In this paper, we study the notoriously challenging task, i.e., Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), wherein training and testing data come from different distributions. Previous works merely alleviate the domain shift, which however overlook the pairwise misalignment issue in target domain, i.e., there exist no semantic relationships between target videos and texts. To tackle this, we propose a novel method named Dual Alignment Domain Adaptation (DADA). Specifically, we first introduce the cross-modal semantic embedding to generate discriminative source features in a joint embedding space. Besides, we utilize the video and text domain adaptations to smoothly balance the minimization of the domain shifts. To tackle the pairwise misalignment in target domain, we introduce the Dual Alignment Consistency (DAC) to fully exploit the semantic information of both modalities in target domain. The proposed DAC adaptively aligns the video-text pairs which are more likely to be relevant in target domain, enabling that positive pairs are increasing progressively and the noisy ones will potentially be aligned in the later stages. To that end, our method can generate more truly aligned target pairs and ensure the discriminality of target features.Compared with the state-of-the-art methods, DADA achieves 20.18% and 18.61% relative improvements on R@1 under the setting of TGIF->MSRVTT and TGIF->MSVD respectively, demonstrating the superiority of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">754.Hubs and Hyperspheres: Reducing Hubness and Improving Transductive Few-Shot Learning With Hyperspherical Embeddings</span><br>
                <span class="as">Trosten, DanielJ.andChakraborty, RwiddhiandL{\o</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Trosten_Hubs_and_Hyperspheres_Reducing_Hubness_and_Improving_Transductive_Few-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7527-7536.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何消除转导式少数镜头学习（FSL）中由于图像表示的高维性导致的"中心性问题"，即一些点（中心）频繁出现在其他点的多个最近邻列表中。<br>
                    动机：中心性问题会负面影响基于距离的分类，当一个类别的中心经常出现在另一个类别的点的最近邻中时，会降低分类器的性能。<br>
                    方法：我们首先证明通过在超球体上均匀分布表示可以消除中心性问题。然后，我们提出了两种新的在超球体上嵌入表示的方法，这两种方法被证明可以在均匀性和局部相似性保留之间优化平衡——减少中心性问题的同时保持类结构。<br>
                    效果：实验表明，所提出的方法减少了中心性问题，并在广泛的分类器上显著提高了转导式FSL的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Distance-based classification is frequently used in transductive few-shot learning (FSL). However, due to the high-dimensionality of image representations, FSL classifiers are prone to suffer from the hubness problem, where a few points (hubs) occur frequently in multiple nearest neighbour lists of other points. Hubness negatively impacts distance-based classification when hubs from one class appear often among the nearest neighbors of points from another class, degrading the classifier's performance. To address the hubness problem in FSL, we first prove that hubness can be eliminated by distributing representations uniformly on the hypersphere. We then propose two new approaches to embed representations on the hypersphere, which we prove optimize a tradeoff between uniformity and local similarity preservation -- reducing hubness while retaining class structure. Our experiments show that the proposed methods reduce hubness, and significantly improves transductive FSL accuracy for a wide range of classifiers.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">755.Architecture, Dataset and Model-Scale Agnostic Data-Free Meta-Learning</span><br>
                <span class="as">Hu, ZixuanandShen, LiandWang, ZhenyiandLiu, TongliangandYuan, ChunandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Architecture_Dataset_and_Model-Scale_Agnostic_Data-Free_Meta-Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7736-7745.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的无数据元学习只解决了参数空间的问题，忽视了预训练模型中的数据知识，无法扩展到大规模的预训练模型，并且只能元学习具有相同网络架构的预训练模型。<br>
                    动机：提出一种统一的框架PURER，通过无数据元训练中的episode课程反转（ECI）和元测试中的反转校准后续内部循环（ICFIL），以解决上述问题。<br>
                    方法：在元训练阶段，我们提出了ECI进行伪剧集训练，以快速适应新的未见过的任务。具体来说，我们逐步合成一系列伪剧集，从每个预训练模型中提炼训练数据。根据元模型的实时反馈，ECI自适应地增加伪剧集的难度级别。我们将带有ECI的元训练优化过程形式化为端到端的对抗形式。在元测试阶段，我们进一步提出了一种简单的即插即用补充——ICFIL——仅在元测试期间使用，以缩小元训练和元测试任务分布之间的差距。<br>
                    效果：广泛的实验表明，我们的方法在各种真实场景中表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes according to the real-time feedback of the meta model. We formulate the optimization process of meta training with ECI as an adversarial form in an end-to-end manner. During meta testing, we further propose a simple plug-and-play supplement--ICFIL--only used during meta testing to narrow the gap between meta training and meta testing task distribution. Extensive experiments in various real-world scenarios show the superior performance of ours.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">756.On the Stability-Plasticity Dilemma of Class-Incremental Learning</span><br>
                <span class="as">Kim, DongwanandHan, Bohyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20196-20204.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决类增量学习中的稳定性和可塑性之间的平衡问题，即模型既要稳定地保留从已见过的类别中学到的知识，又要具有足够的可塑性来学习新类别的概念。<br>
                    动机：尽管先前的研究在类增量基准测试上表现出强大的性能，但尚不清楚他们的成功是来自于模型的稳定性、可塑性，还是两者的混合。<br>
                    方法：本文建立了测量特征表示稳定性和可塑性的分析工具，并在大规模类增量基准测试上使用这些工具对各种算法训练的模型进行研究。<br>
                    效果：令人惊讶的是，我们发现大多数类增量学习算法严重倾向于稳定性而非可塑性，以至于在初始类别集上训练的模型的特征提取器的效果并不比最终的增量模型差。我们的观察结果不仅启发了两种强调特征表示分析重要性的简单算法，还表明类增量学习方法应努力改善特征表示学习。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A primary goal of class-incremental learning is to strike a balance between stability and plasticity, where models should be both stable enough to retain knowledge learned from previously seen classes, and plastic enough to learn concepts from new classes. While previous works demonstrate strong performance on class-incremental benchmarks, it is not clear whether their success comes from the models being stable, plastic, or a mixture of both. This paper aims to shed light on how effectively recent class-incremental learning algorithms address the stability-plasticity trade-off. We establish analytical tools that measure the stability and plasticity of feature representations, and employ such tools to investigate models trained with various algorithms on large-scale class-incremental benchmarks. Surprisingly, we find that the majority of class-incremental learning algorithms heavily favor stability over plasticity, to the extent that the feature extractor of a model trained on the initial set of classes is no less effective than that of the final incremental model. Our observations not only inspire two simple algorithms that highlight the importance of feature representation analysis, but also suggest that class-incremental learning approaches, in general, should strive for better feature representation learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">757.Generalization Matters: Loss Minima Flattening via Parameter Hybridization for Efficient Online Knowledge Distillation</span><br>
                <span class="as">Zhang, TianliandXue, MengqiandZhang, JiangtaoandZhang, HaofeiandWang, YuandCheng, LechaoandSong, JieandSong, Mingli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Generalization_Matters_Loss_Minima_Flattening_via_Parameter_Hybridization_for_Efficient_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20176-20185.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的在线知识蒸馏技术通常需要复杂的模块来产生多样化的知识以提高学生的泛化能力。<br>
                    动机：本文旨在充分利用多模型设置，而不是精心设计的模块，以实现具有优秀泛化性能的蒸馏效果。<br>
                    方法：通过在每个训练批次中线性加权学生模型的参数，构建了一个混合权重模型（HWM）来表示涉及的学生周围的参数。将HWM的损失集成到学生的训练中，并提出了一种新的在线知识蒸馏框架，即参数混合在线知识蒸馏（OKDPH），以促进更平坦的极小值并获得稳健的解决方案。<br>
                    效果：与最先进的在线知识蒸馏方法和寻求平坦极小值的方法相比，我们的OKDPH在更少的参数下实现了更高的性能，使在线知识蒸馏具有轻量级和鲁棒的特性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing online knowledge distillation(OKD) techniques typically require sophisticated modules to produce diverse knowledge for improving students' generalization ability. In this paper, we strive to fully utilize multi-model settings instead of well-designed modules to achieve a distillation effect with excellent generalization performance. Generally, model generalization can be reflected in the flatness of the loss landscape. Since averaging parameters of multiple models can find flatter minima, we are inspired to extend the process to the sampled convex combinations of multi-student models in OKD. Specifically, by linearly weighting students' parameters in each training batch, we construct a Hybrid-Weight Model(HWM) to represent the parameters surrounding involved students. The supervision loss of HWM can estimate the landscape's curvature of the whole region around students to measure the generalization explicitly. Hence we integrate HWM's loss into students' training and propose a novel OKD framework via parameter hybridization(OKDPH) to promote flatter minima and obtain robust solutions. Considering the redundancy of parameters could lead to the collapse of HWM, we further introduce a fusion operation to keep the high similarity of students. Compared to the state-of-the-art(SOTA) OKD methods and SOTA methods of seeking flat minima, our OKDPH achieves higher performance with fewer parameters, benefiting OKD with lightweight and robust characteristics. Our code is publicly available at https://github.com/tianlizhang/OKDPH.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">758.Gaussian Label Distribution Learning for Spherical Image Object Detection</span><br>
                <span class="as">Xu, HangandLiu, XinyuanandZhao, QiangandMa, YikeandYan, ChenggangandDai, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Gaussian_Label_Distribution_Learning_for_Spherical_Image_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1033-1042.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的球形图像检测器使用ln-范数损失进行球形边界框的回归，存在研究问题：现有的球形图像检测器使用ln-范数损失进行球形边界框的回归，存在参数独立优化和度量（以IoU为主）与损失不一致的问题。<br>
                    动机：这些问题在平面图像检测中也存在，但在球形图像检测中更为严重。由于Spherical IoU（SphIoU）的不可微性，现有的基于IoU损失和相关变体的解决方案无法应用于球形图像目标检测。<br>
                    方法：本文设计了一种简单而有效的基于高斯标签分布学习（GLDL）的回归损失函数用于球形图像目标检测。同时，由于球形图像中物体的大小差异巨大，不同类别物体之间的差异使得基于SphIoU的样本选择策略具有挑战性，因此提出了GLDL-ATSS作为更好的球形图像目标训练样本选择策略，以缓解基于IoU阈值的策略在尺度样本不平衡问题上的缺陷。<br>
                    效果：在两个具有不同基线检测器的数据集上进行的大量实验表明，该方法是有效的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spherical image object detection emerges in many applications from virtual reality to robotics and automatic driving, while many existing detectors use ln-norms loss for regression of spherical bounding boxes. There are two intrinsic flaws for ln-norms loss, i.e., independent optimization of parameters and inconsistency between metric (dominated by IoU) and loss. These problems are common in planar image detection but more significant in spherical image detection. Solution for these problems has been extensively discussed in planar image detection by using IoU loss and related variants. However, these solutions cannot be migrated to spherical image object detection due to the undifferentiable of the Spherical IoU (SphIoU). In this paper, we design a simple but effective regression loss based on Gaussian Label Distribution Learning (GLDL) for spherical image object detection. Besides, we observe that the scale of the object in a spherical image varies greatly. The huge differences among objects from different categories make the sample selection strategy based on SphIoU challenging. Therefore, we propose GLDL-ATSS as a better training sample selection strategy for objects of the spherical image, which can alleviate the drawback of IoU threshold-based strategy of scale-sample imbalance. Extensive results on various two datasets with different baseline detectors show the effectiveness of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">759.On the Effects of Self-Supervision and Contrastive Alignment in Deep Multi-View Clustering</span><br>
                <span class="as">Trosten, DanielJ.andL{\o</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Trosten_On_the_Effects_of_Self-Supervision_and_Contrastive_Alignment_in_Deep_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23976-23985.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习多视角聚类（MVC）中，自我监督学习方法发展不均的问题。<br>
                    动机：作者发现在深度学习多视角聚类中，自我监督学习方法的发展存在很大差异，这可能会阻碍该领域的进展。<br>
                    方法：作者提出了一个统一的深度MVC框架DeepMVC，并将许多最近的方法作为实例。通过这个框架，作者对自我监督的影响进行了关键观察，特别是对比学习对表示的对齐的缺点。<br>
                    效果：实验结果表明，(i)与理论发现一致，对比对齐会降低多视角数据集的性能；(ii)所有方法都从某种形式的自我监督中受益；(iii)新实例在几个数据集上优于以前的方法。根据结果，作者为未来的研究提供了几个有希望的方向。为了增强领域的开放性，作者提供了一个开源的DeepMVC实现，包括最近的模型和新的实例。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning is a central component in recent approaches to deep multi-view clustering (MVC). However, we find large variations in the development of self-supervision-based methods for deep MVC, potentially slowing the progress of the field. To address this, we present DeepMVC, a unified framework for deep MVC that includes many recent methods as instances. We leverage our framework to make key observations about the effect of self-supervision, and in particular, drawbacks of aligning representations with contrastive learning. Further, we prove that contrastive alignment can negatively influence cluster separability, and that this effect becomes worse when the number of views increases. Motivated by our findings, we develop several new DeepMVC instances with new forms of self-supervision. We conduct extensive experiments and find that (i) in line with our theoretical findings, contrastive alignments decreases performance on datasets with many views; (ii) all methods benefit from some form of self-supervision; and (iii) our new instances outperform previous methods on several datasets. Based on our results, we suggest several promising directions for future research. To enhance the openness of the field, we provide an open-source implementation of DeepMVC, including recent models and our new instances. Our implementation includes a consistent evaluation protocol, facilitating fair and accurate evaluation of methods and components.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">760.DARE-GRAM: Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices</span><br>
                <span class="as">Nejjar, IsmailandWang, QinandFink, Olga</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nejjar_DARE-GRAM_Unsupervised_Domain_Adaptation_Regression_by_Aligning_Inverse_Gram_Matrices_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11744-11754.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无监督领域适应回归（DAR）问题，即如何缩小有标签源数据集和无标签目标数据集之间的领域差距。<br>
                    动机：现有的方法主要通过最小化源特征和目标特征之间的差异来学习深度特征编码器。然而，作者提出了一种新的视角，通过分析线性回归器在深度领域适应背景下的闭型普通最小二乘（OLS）解。<br>
                    方法：作者提出的方法不是对原始特征嵌入空间进行对齐，而是对特征的逆Gram矩阵进行对齐。这是由其在OLS解中的存在性和Gram矩阵捕捉特征相关性的能力所驱动的。具体来说，作者提出了一种简单而有效的DAR方法，该方法利用伪逆低秩特性在一个由两个领域的伪逆Gram矩阵生成的选定子空间中对尺度和角度进行对齐。<br>
                    效果：实验结果表明，该方法在三个领域适应回归基准测试中实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. In this work, we present a different perspective for the DAR problem by analyzing the closed-form ordinary least square (OLS) solution to the linear regressor in the deep domain adaptation context. Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. Specifically, we propose a simple yet effective DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three domain adaptation regression benchmarks. Experimental results demonstrate that our method achieves state-of-the-art performance. Our code is available at https://github.com/ismailnejjar/DARE-GRAM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">761.Probabilistic Debiasing of Scene Graphs</span><br>
                <span class="as">Biswas, BashirulAzamandJi, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Biswas_Probabilistic_Debiasing_of_Scene_Graphs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10429-10438.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有最先进的场景图生成模型由于关系和其父母对象对的长尾特性，导致生成的场景图质量降低。<br>
                    动机：训练场景图的过程中，大部分关系和主要对象对占据了主导地位，导致在训练收敛后，少数对象对的关系分布没有被保留下来，从而使得模型存在偏差。<br>
                    方法：我们提出了一种在三元组贝叶斯网络中引入虚拟证据的方法，以保留关系标签的对象条件分布并消除由关系边缘概率产生的偏差。同时，为了解决少数类关系样本不足的问题，我们在语义空间中从相邻的三元组借用少数类三元组样本进行嵌入增强。<br>
                    效果：我们在两个不同的数据集上进行了实验，发现该方法显著提高了关系的平均召回率，并且与现有的最优场景图模型去偏技术相比，取得了更好的召回率和平均召回率性能平衡。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The quality of scene graphs generated by the state-of-the-art (SOTA) models is compromised due to the long-tail nature of the relationships and their parent object pairs. Training of the scene graphs is dominated by the majority relationships of the majority pairs and, therefore, the object-conditional distributions of relationship in the minority pairs are not preserved after the training is converged. Consequently, the biased model performs well on more frequent relationships in the marginal distribution of relationships such as 'on' and 'wearing', and performs poorly on the less frequent relationships such as 'eating' or 'hanging from'. In this work, we propose virtual evidence incorporated within-triplet Bayesian Network (BN) to preserve the object-conditional distribution of the relationship label and to eradicate the bias created by the marginal probability of the relationships. The insufficient number of relationships in the minority classes poses a significant problem in learning the within-triplet Bayesian network. We address this insufficiency by embedding-based augmentation of triplets where we borrow samples of the minority triplet classes from its neighboring triplets in the semantic space. We perform experiments on two different datasets and achieve a significant improvement in the mean recall of the relationships. We also achieve a better balance between recall and mean recall performance compared to the SOTA de-biasing techniques of scene graph models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">762.OSAN: A One-Stage Alignment Network To Unify Multimodal Alignment and Unsupervised Domain Adaptation</span><br>
                <span class="as">Liu, YeandQiao, LingfengandLu, ChangchongandYin, DiandLin, ChenandPeng, HaoyuanandRen, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_OSAN_A_One-Stage_Alignment_Network_To_Unify_Multimodal_Alignment_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3551-3560.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何进行无监督的多模态领域适应，特别是在领域适应和模态对齐这两个主要问题上。<br>
                    动机：现有的两阶段研究中，领域和模态并未关联，且二者的关系未被利用，这为多模态领域适应带来了挑战。<br>
                    方法：本文将这两个阶段统一起来，同时进行领域和模态的对齐。提出了一种基于张量的对齐模块（TAL）来探索领域和模态之间的关系，并建立了一个动态领域生成器（DDG）模块，通过混合两个领域的共享信息，以自监督的方式构建过渡样本，帮助模型学习领域不变的公共表示空间。<br>
                    效果：实验证明，该方法在两个实际应用中都能取得优异的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Extending from unimodal to multimodal is a critical challenge for unsupervised domain adaptation (UDA). Two major problems emerge in unsupervised multimodal domain adaptation: domain adaptation and modality alignment. An intuitive way to handle these two problems is to fulfill these tasks in two separate stages: aligning modalities followed by domain adaptation, or vice versa. However, domains and modalities are not associated in most existing two-stage studies, and the relationship between them is not leveraged which can provide complementary information to each other. In this paper, we unify these two stages into one to align domains and modalities simultaneously. In our model, a tensor-based alignment module (TAL) is presented to explore the relationship between domains and modalities. By this means, domains and modalities can interact sufficiently and guide them to utilize complementary information for better results. Furthermore, to establish a bridge between domains, a dynamic domain generator (DDG) module is proposed to build transitional samples by mixing the shared information of two domains in a self-supervised manner, which helps our model learn a domain-invariant common representation space. Extensive experiments prove that our method can achieve superior performance in two real-world applications. The code will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">763.Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models</span><br>
                <span class="as">Chung, HyungjinandRyu, DohoonandMcCann, MichaelT.andKlasky, MarcL.andYe, JongChul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22542-22551.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地解决三维医学图像重建问题，如稀疏视图断层扫描、有限角度断层扫描和压缩感知MRI。<br>
                    动机：传统的基于模型的迭代重建方法和现代的扩散模型在二维图像重建中表现出色，但在三维重建中由于高维度（与数据维度相同）导致内存和计算成本极高，尚未得到广泛应用。<br>
                    方法：本文将传统基于模型的迭代重建方法和现代扩散模型相结合，提出一种有效的方法，通过在测试时用基于模型的先验信息补充二维扩散模型中的剩余方向，实现在所有维度上的协调重建。<br>
                    效果：该方法可以在单台普通GPU上运行，并在极端情况下（如2视图3D断层扫描）实现高保真度和准确性的重建。此外，该方法具有很高的泛化能力，可以用于重建与训练数据集完全不同的体积。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility. They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost. In this paper, we combine the ideas from the conventional model-based iterative reconstruction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from pre-trained 2D diffusion models. In essence, we propose to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all dimensions. Our method can be run in a single commodity GPU, and establishes the new state-of-the-art, showing that the proposed method can perform reconstructions of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">764.Federated Domain Generalization With Generalization Adjustment</span><br>
                <span class="as">Zhang, RuipengandXu, QinweiandYao, JiangchaoandZhang, YaandTian, QiandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3954-3963.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在保护隐私的同时，学习一个能良好泛化到新客户端的全局模型，特别是在可能存在领域偏移的情况下。<br>
                    动机：现有的方法主要关注在每个单独的领域中设计无偏的训练策略，但在没有多领域数据联合支持的批量训练中，几乎所有的方法都不能保证在领域偏移下的泛化。<br>
                    方法：提出了一种结合了新的方差减少正则化器的新型全局目标，以鼓励公平性。并提出了一种新的联邦适应（FL-friendly）方法——通用调整（GA），通过动态校准聚合权重来优化上述目标。<br>
                    效果：理论分析表明，使用显式重新加权的聚合可以实现更紧的泛化边界，取代仅适用于传统DG设置的隐式多领域数据共享。此外，该方法是通用的，可以与任何基于局部客户端训练的方法结合使用。在几个基准数据集上的大量实验表明了该方法的有效性，当与其他FedDG算法结合使用时，可以获得一致的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Domain Generalization (FedDG) attempts to learn a global model in a privacy-preserving manner that generalizes well to new clients possibly with domain shift. Recent exploration mainly focuses on designing an unbiased training strategy within each individual domain. However, without the support of multi-domain data jointly in the mini-batch training, almost all methods cannot guarantee the generalization under domain shift. To overcome this problem, we propose a novel global objective incorporating a new variance reduction regularizer to encourage fairness. A novel FL-friendly method named Generalization Adjustment (GA) is proposed to optimize the above objective by dynamically calibrating the aggregation weights. The theoretical analysis of GA demonstrates the possibility to achieve a tighter generalization bound with an explicit re-weighted aggregation, substituting the implicit multi-domain data sharing that is only applicable to the conventional DG settings. Besides, the proposed algorithm is generic and can be combined with any local client training-based methods. Extensive experiments on several benchmark datasets have shown the effectiveness of the proposed method, with consistent improvements over several FedDG algorithms when used in combination. The source code is released at https://github.com/MediaBrain-SJTU/FedDG-GA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">765.Learning the Distribution of Errors in Stereo Matching for Joint Disparity and Uncertainty Estimation</span><br>
                <span class="as">Chen, LiyanandWang, WeihanandMordohai, Philippos</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_the_Distribution_of_Errors_in_Stereo_Matching_for_Joint_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17235-17244.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的损失函数，用于深度立体匹配中的联合视差和不确定性估计。<br>
                    动机：由于需要精确的不确定性估计，并且多任务学习通常会在所有任务中提高性能，因此我们进行了这项工作。<br>
                    方法：通过在网络的损失函数中加入KL散度项，要求不确定性分布与视差误差分布相匹配，从而实现联合视差和不确定性估计。使用可微分的软直方图技术来近似这些分布，以便在损失中使用。<br>
                    效果：我们在大型数据集上对该方法的效果进行实验评估，观察到在视差和不确定性预测方面都有显著改进。我们的代码可以在https://github.com/lly00412/SEDNet.git获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a new loss function for joint disparity and uncertainty estimation in deep stereo matching. Our work is motivated by the need for precise uncertainty estimates and the observation that multi-task learning often leads to improved performance in all tasks. We show that this can be achieved by requiring the distribution of uncertainty to match the distribution of disparity errors via a KL divergence term in the network's loss function. A differentiable soft-histogramming technique is used to approximate the distributions so that they can be used in the loss. We experimentally assess the effectiveness of our approach and observe significant improvements in both disparity and uncertainty prediction on large datasets. Our code is available at https://github.com/lly00412/SEDNet.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">766.Learning Correspondence Uncertainty via Differentiable Nonlinear Least Squares</span><br>
                <span class="as">Muhle, DominikandKoestler, LukasandJatavallabhula, KrishnaMurthyandCremers, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Muhle_Learning_Correspondence_Uncertainty_via_Differentiable_Nonlinear_Least_Squares_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13102-13112.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种可微非线性最小二乘框架，用于从特征对应关系中估计相对位姿的不确定性。<br>
                    动机：现有的方法在处理相对位姿估计的不确定性时存在不足，需要一种新的方法来提高精度和稳定性。<br>
                    方法：引入对称的概率正外极约束和通过微分相机位姿估计过程来估计特征位置协方差的方法，构建一个可微非线性最小二乘框架。<br>
                    效果：在合成数据集以及KITTI和EuRoC真实世界数据集上的实验表明，该方法能够准确逼近真实的噪声分布，并在真实世界实验中始终优于最先进的非概率和概率方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a differentiable nonlinear least squares framework to account for uncertainty in relative pose estimation from feature correspondences. Specifically, we introduce a symmetric version of the probabilistic normal epipolar constraint, and an approach to estimate the covariance of feature positions by differentiating through the camera pose estimation procedure. We evaluate our approach on synthetic, as well as the KITTI and EuRoC real-world datasets. On the synthetic dataset, we confirm that our learned covariances accurately approximate the true noise distribution. In real world experiments, we find that our approach consistently outperforms state-of-the-art non-probabilistic and probabilistic approaches, regardless of the feature extraction algorithm of choice.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">767.Samples With Low Loss Curvature Improve Data Efficiency</span><br>
                <span class="as">Garg, IshaandRoy, Kaushik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20290-20300.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了深度神经网络训练损失的二阶特性，以理解损失表面在训练数据点附近的曲率。<br>
                    动机：研究发现，训练数据中存在一种意料之外的低曲率样本集中现象。这些低曲率样本在不同架构中具有高度一致性，且在训练早期即可识别。<br>
                    方法：作者提出了SLo-Curves算法，该算法将低曲率样本视为更高效的数据，并在其附近添加了一个额外的正则化项，用于惩罚高曲率的损失表面。<br>
                    效果：在CIFAR-10和CIFAR-100数据集上，SLo-Curves算法表现出色，其在小型核心集大小上比最先进的核心集选择方法高出9%。所识别的核心集可以跨架构进行泛化，因此可以预先计算生成用于下游任务的数据集压缩版本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study the second order properties of the loss of trained deep neural networks with respect to the training data points to understand the curvature of the loss surface in the vicinity of these points. We find that there is an unexpected concentration of samples with very low curvature. We note that these low curvature samples are largely consistent across completely different architectures, and identifiable in the early epochs of training. We show that the curvature relates to the 'cleanliness' of the data points, with low curvatures samples corresponding to clean, higher clarity samples, representative of their category. Alternatively, high curvature samples are often occluded, have conflicting features and visually atypical of their category. Armed with this insight, we introduce SLo-Curves, a novel coreset identification and training algorithm. SLo-curves identifies the samples with low curvatures as being more data-efficient and trains on them with an additional regularizer that penalizes high curvature of the loss surface in their vicinity. We demonstrate the efficacy of SLo-Curves on CIFAR-10 and CIFAR-100 datasets, where it outperforms state of the art coreset selection methods at small coreset sizes by up to 9%. The identified coresets generalize across architectures, and hence can be pre-computed to generate condensed versions of datasets for use in downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">768.Re-Basin via Implicit Sinkhorn Differentiation</span><br>
                <span class="as">Pe\~na, FidelA.GuerreroandMedeiros, HeitorRapelaandDubail, ThomasandAminbeidokhti, MasihandGranger, EricandPedersoli, Marco</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20237-20246.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何找到最小化目标的排列模型，并整合到基于梯度的优化中。<br>
                    动机：当前优化技术不具有可微分性，导致难以找到最优解。<br>
                    方法：提出Sinkhorn再盆地网络，通过利用线性模式连通性属性进行增量学习。<br>
                    效果：在最优传输和线性模式连通性等多个条件下，与文献中的类似方法相比，该方法在常见基准数据集上的表现与最先进的技术相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent emergence of new algorithms for permuting models into functionally equivalent regions of the solution space has shed some light on the complexity of error surfaces and some promising properties like mode connectivity. However, finding the permutation that minimizes some objectives is challenging, and current optimization techniques are not differentiable, which makes it difficult to integrate into a gradient-based optimization, and often leads to sub-optimal solutions. In this paper, we propose a Sinkhorn re-basin network with the ability to obtain the transportation plan that better suits a given objective. Unlike the current state-of-art, our method is differentiable and, therefore, easy to adapt to any task within the deep learning domain. Furthermore, we show the advantage of our re-basin method by proposing a new cost function that allows performing incremental learning by exploiting the linear mode connectivity property. The benefit of our method is compared against similar approaches from the literature under several conditions for both optimal transport and linear mode connectivity. The effectiveness of our continual learning method based on re-basin is also shown for several common benchmark datasets, providing experimental results that are competitive with the state-of-art. The source code is provided at https://github.com/fagp/sinkhorn-rebasin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">769.Layout-Based Causal Inference for Object Navigation</span><br>
                <span class="as">Zhang, SixianandSong, XinhangandLi, WeijieandBai, YubingandYu, XinyaoandJiang, Shuqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Layout-Based_Causal_Inference_for_Object_Navigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10792-10802.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用先验知识在训练环境中进行导航，同时解决布局差距对导航效果的负面影响。<br>
                    动机：先前的工作试图通过学习视觉输入和目标之间的关联（如关系图）来进行导航任务，但当测试环境和训练环境的布局差距较大时，这种先验知识会对导航产生负面影响。<br>
                    方法：提出了基于因果关系的布局基础软总直接效应（L-sTDE）框架来调整导航策略的预测。具体来说，我们计算对象布局的后验分布和先验分布之间的KL散度作为布局差距，然后根据布局差距提出sTDE来适当控制经验的影响。<br>
                    效果：在AI2THOR、RoboTHOR和Habitat等实验数据集上的实验结果表明，该方法能有效提高导航性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous works for ObjectNav task attempt to learn the association (e.g. relation graph) between the visual inputs and the goal during training. Such association contains the prior knowledge of navigating in training environments, which is denoted as the experience. The experience performs a positive effect on helping the agent infer the likely location of the goal when the layout gap between the unseen environments of the test and the prior knowledge obtained in training is minor. However, when the layout gap is significant, the experience exerts a negative effect on navigation. Motivated by keeping the positive effect and removing the negative effect of the experience, we propose the layout-based soft Total Direct Effect (L-sTDE) framework based on the causal inference to adjust the prediction of the navigation policy. In particular, we propose to calculate the layout gap which is defined as the KL divergence between the posterior and the prior distribution of the object layout. Then the sTDE is proposed to appropriately control the effect of the experience based on the layout gap. Experimental results on AI2THOR, RoboTHOR, and Habitat demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">770.Source-Free Video Domain Adaptation With Spatial-Temporal-Historical Consistency Learning</span><br>
                <span class="as">Li, KaiandPatel, DeepandKruus, ErikandMin, MartinRenqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Source-Free_Video_Domain_Adaptation_With_Spatial-Temporal-Historical_Consistency_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14643-14652.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用未标记的目标数据对预训练的源模型进行适应，特别是在视频领域。<br>
                    动机：现有的源自由领域适应方法主要针对图像，对于视频的处理效果不佳。我们提出的方法考虑了视频的空间、时间和历史特性，以解决这一问题。<br>
                    方法：我们提出了一种简单而灵活的源自由视频领域适应（SFVDA）方法，通过模拟空间和时间变化来克服领域偏移，并鼓励模型对视频及其增强版本做出一致的预测。<br>
                    效果：实验表明，我们的方法在所有设置中都达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Source-free domain adaptation (SFDA) is an emerging research topic that studies how to adapt a pretrained source model using unlabeled target data. It is derived from unsupervised domain adaptation but has the advantage of not requiring labeled source data to learn adaptive models. This makes it particularly useful in real-world applications where access to source data is restricted. While there has been some SFDA work for images, little attention has been paid to videos. Naively extending image-based methods to videos without considering the unique properties of videos often leads to unsatisfactory results. In this paper, we propose a simple and highly flexible method for Source-Free Video Domain Adaptation (SFVDA), which extensively exploits consistency learning for videos from spatial, temporal, and historical perspectives. Our method is based on the assumption that videos of the same action category are drawn from the same low-dimensional space, regardless of the spatio-temporal variations in the high-dimensional space that cause domain shifts. To overcome domain shifts, we simulate spatio-temporal variations by applying spatial and temporal augmentations on target videos, and encourage the model to make consistent predictions from a video and its augmented versions. Due to the simple design, our method can be applied to various SFVDA settings, and experiments show that our method achieves state-of-the-art performance for all the settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">771.MELTR: Meta Loss Transformer for Learning To Fine-Tune Video Foundation Models</span><br>
                <span class="as">Ko, DohwanandChoi, JoonmyungandChoi, HyeongKyuandOn, Kyoung-WoonandRoh, ByungseokandKim, HyunwooJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ko_MELTR_Meta_Loss_Transformer_for_Learning_To_Fine-Tune_Video_Foundation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20105-20115.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基础模型在微调阶段主要关注单一任务损失的最小化，没有充分利用其他可能对目标任务有益的损失。<br>
                    动机：提出一种可以自动非线性组合各种损失函数的插件模块，通过辅助学习来帮助学习目标任务。<br>
                    方法：将辅助学习形式化为双层优化问题，并提出了一种基于近似隐式微分（AID）的高效优化算法。<br>
                    效果：在多种视频基础模型上应用该框架，并在四个下游任务中取得了显著的性能提升。定性分析表明，MELTR能够有效地转换和融合各个损失函数为一个有效的统一损失。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Foundation models have shown outstanding performance and generalization capabilities across domains. Since most studies on foundation models mainly focus on the pretraining phase, a naive strategy to minimize a single task-specific loss is adopted for fine-tuning. However, such fine-tuning methods do not fully leverage other losses that are potentially beneficial for the target task. Therefore, we propose MEta Loss TRansformer (MELTR), a plug-in module that automatically and non-linearly combines various loss functions to aid learning the target task via auxiliary learning. We formulate the auxiliary learning as a bi-level optimization problem and present an efficient optimization algorithm based on Approximate Implicit Differentiation (AID). For evaluation, we apply our framework to various video foundation models (UniVL, Violet and All-in-one), and show significant performance gain on all four downstream tasks: text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis. Our qualitative analyses demonstrate that MELTR adequately 'transforms' individual loss functions and 'melts' them into an effective unified loss. Code is available at https://github.com/mlvlab/MELTR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">772.Ambiguous Medical Image Segmentation Using Diffusion Models</span><br>
                <span class="as">Rahman, AimonandValanarasu, JeyaMariaJoseandHacihaliloglu, IlkerandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Ambiguous_Medical_Image_Segmentation_Using_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11536-11546.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用集体专家的见解来提高医学图像分割任务的诊断效果。<br>
                    动机：现有的AI模型主要模仿个体专家的表现，而忽视了集体专家的力量。<br>
                    方法：提出一种基于单一扩散模型的方法，通过学习群体见解的分布来生成多个可能的输出。该方法利用扩散的固有随机采样过程，仅需要最小的额外学习就能生成分割掩模的分布。<br>
                    效果：在CT、超声波和MRI三种不同的医学图像模态上进行测试，结果显示该方法能够生成多种可能的变体，同时捕捉其发生的频率。在准确性方面，该方法优于现有的最先进的模糊分割网络，同时保留了自然发生的变异。此外，还提出了一种新的评估指标，用于评估分割预测的多样性和准确性，以符合临床实践的集体见解的利益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Collective insights from a group of experts have always proven to outperform an individual's best diagnostic for clinical tasks. For the task of medical image segmentation, existing research on AI-based alternatives focuses more on developing models that can imitate the best individual rather than harnessing the power of expert groups. In this paper, we introduce a single diffusion model-based approach that produces multiple plausible outputs by learning a distribution over group insights. Our proposed model generates a distribution of segmentation masks by leveraging the inherent stochastic sampling process of diffusion using only minimal additional learning. We demonstrate on three different medical image modalities- CT, ultrasound, and MRI that our model is capable of producing several possible variants while capturing the frequencies of their occurrences. Comprehensive results show that our proposed approach outperforms existing state-of-the-art ambiguous segmentation networks in terms of accuracy while preserving naturally occurring variation. We also propose a new metric to evaluate the diversity as well as the accuracy of segmentation predictions that aligns with the interest of clinical practice of collective insights. Implementation code will be released publicly after the review process.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">773.Make Landscape Flatter in Differentially Private Federated Learning</span><br>
                <span class="as">Shi, YifanandLiu, YingqiandWei, KangandShen, LiandWang, XueqianandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Make_Landscape_Flatter_in_Differentially_Private_Federated_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24552-24562.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在联邦学习中保护隐私并减少敏感信息泄露？<br>
                    动机：现有的联邦学习方法在保护隐私时会导致损失函数更尖锐，权重扰动的鲁棒性较差，从而严重影响性能。<br>
                    方法：提出一种新的联邦学习方法DP-FedSAM，通过梯度扰动来减轻DP的负面影响。具体来说，DP-FedSAM集成了Sharpness Aware Minimization（SAM）优化器，生成具有更好稳定性和权重扰动鲁棒性的局部平坦模型，从而使局部更新的范数较小，对DP噪声具有鲁棒性，从而提高性能。<br>
                    效果：从理论角度详细分析了DP-FedSAM如何减轻由DP引起的性能下降。同时，我们提供了严格的Renyi DP隐私保证，并对局部更新进行了敏感性分析。最后，我们通过实证证明，与现有的联邦学习方法相比，我们的算法实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigorous privacy guarantees with Renyi DP and present the sensitivity analysis of local updates. At last, we empirically confirm that our algorithm achieves state-of-the-art (SOTA) performance compared with existing SOTA baselines in DPFL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">774.Towards Better Stability and Adaptability: Improve Online Self-Training for Model Adaptation in Semantic Segmentation</span><br>
                <span class="as">Zhao, DongandWang, ShuangandZang, QiandQuan, DouandYe, XiutiaoandJiao, Licheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Towards_Better_Stability_and_Adaptability_Improve_Online_Self-Training_for_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11733-11743.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决语义分割中的无监督领域适应问题，特别是在涉及隐私、产权保护和保密性的适应场景中。<br>
                    动机：传统的无监督领域适应（UDA）需要访问源数据的标签信息，这在涉及隐私等问题时无法使用。因此，本文关注无需访问源数据的无监督模型适应（UMA）。<br>
                    方法：本文发现在线自我训练方法可以应用于UMA，但缺乏源领域损失会大大降低该方法的稳定性和适应性。基于此，我们提出了动态教师更新机制和基于训练一致性的重采样策略来提高在线自我训练的稳定性和适应性。<br>
                    效果：在多个模型适应基准测试中，我们的方法获得了新的最先进的性能，与最先进的UDA方法相当甚至更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised domain adaptation (UDA) in semantic segmentation transfers the knowledge of the source domain to the target one to improve the adaptability of the segmentation model in the target domain. The need to access labeled source data makes UDA unable to handle adaptation scenarios involving privacy, property rights protection, and confidentiality. In this paper, we focus on unsupervised model adaptation (UMA), also called source-free domain adaptation, which adapts a source-trained model to the target domain without accessing source data. We find that the online self-training method has the potential to be deployed in UMA, but the lack of source domain loss will greatly weaken the stability and adaptability of the method. We analyze the two possible reasons for the degradation of online self-training, i.e. inopportune updates of the teacher model and biased knowledge from source-trained model. Based on this, we propose a dynamic teacher update mechanism and a training-consistency based resampling strategy to improve the stability and adaptability of online self training. On multiple model adaptation benchmarks, our method obtains new state-of-the-art performance, which is comparable or even better than state-of-the-art UDA methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">775.Ranking Regularization for Critical Rare Classes: Minimizing False Positives at a High True Positive Rate</span><br>
                <span class="as">Mohammadi, KiarashandZhao, HeandZhai, MengyaoandTung, Frederick</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mohammadi_Ranking_Regularization_for_Critical_Rare_Classes_Minimizing_False_Positives_at_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15783-15792.png><br>
            
            <span class="tt"><span class="t0">研究问题：在许多真实世界的场景中，关键类别的实例非常罕见，而漏检的代价却极高。如何在这种需要高真阳性率的场景下，降低假阳性率是一个挑战。<br>
                    动机：例如在医疗诊断和银行交易欺诈检测等场景中，误报的后果严重，因此需要寻找方法降低假阳性率。<br>
                    方法：本文提出了一种基于排序的正则化（RankReg）方法，该方法易于实施，并能有效减少假阳性，同时补充了传统的不平衡学习损失。<br>
                    效果：通过在CIFAR-10&100和Melanoma等多个数据集上进行实验，证明该方法显著提高了先前最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In many real-world settings, the critical class is rare and a missed detection carries a disproportionately high cost. For example, tumors are rare and a false negative diagnosis could have severe consequences on treatment outcomes; fraudulent banking transactions are rare and an undetected occurrence could result in significant losses or legal penalties. In such contexts, systems are often operated at a high true positive rate, which may require tolerating high false positives. In this paper, we present a novel approach to address the challenge of minimizing false positives for systems that need to operate at a high true positive rate. We propose a ranking-based regularization (RankReg) approach that is easy to implement, and show empirically that it not only effectively reduces false positives, but also complements conventional imbalanced learning losses. With this novel technique in hand, we conduct a series of experiments on three broadly explored datasets (CIFAR-10&100 and Melanoma) and show that our approach lifts the previous state-of-the-art performance by notable margins.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">776.Rethinking Feature-Based Knowledge Distillation for Face Recognition</span><br>
                <span class="as">Li, JingzhiandGuo, ZidongandLi, HuiandHan, SeungjuandBaek, Ji-wonandYang, MinandYang, RanandSuh, Sungjoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Rethinking_Feature-Based_Knowledge_Distillation_for_Face_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20156-20165.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在不使用身份监督的情况下，通过特征蒸馏进行大规模人脸识别。<br>
                    动机：在大规模人脸识别中，由于数据集的持续扩大，特征蒸馏方法盛行。然而，直接去除身份监督会导致蒸馏效果下降。<br>
                    方法：通过反向蒸馏缩小教师模型的搜索空间，以减小内在维度的差距，从而释放特征蒸馏的潜力。同时设计了一个学生代理来更好地弥合内在差距。<br>
                    效果：该方法在各种人脸识别基准测试中超越了最先进的有监督特征蒸馏技术，并且改进效果在不同的教师-学生对之间是一致的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the continual expansion of face datasets, feature-based distillation prevails for large-scale face recognition. In this work, we attempt to remove identity supervision in student training, to spare the GPU memory from saving massive class centers. However, this naive removal leads to inferior distillation result. We carefully inspect the performance degradation from the perspective of intrinsic dimension, and argue that the gap in intrinsic dimension, namely the intrinsic gap, is intimately connected to the infamous capacity gap problem. By constraining the teacher's search space with reverse distillation, we narrow the intrinsic gap and unleash the potential of feature-only distillation. Remarkably, the proposed reverse distillation creates universally student-friendly teacher that demonstrates outstanding student improvement. We further enhance its effectiveness by designing a student proxy to better bridge the intrinsic gap. As a result, the proposed method surpasses state-of-the-art distillation techniques with identity supervision on various face recognition benchmarks, and the improvements are consistent across different teacher-student pairs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">777.Revisiting Reverse Distillation for Anomaly Detection</span><br>
                <span class="as">Tien, TranDinhandNguyen, AnhTuanandTran, NguyenHoangandHuy, TaDucandDuong, SoanT.M.andNguyen, ChanhD.Tr.andTruong, StevenQ.H.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24511-24520.png><br>
            
            <span class="tt"><span class="t0">研究问题：大型工业制造中异常检测的重要应用。<br>
                    动机：现有的方法虽然准确性高，但存在延迟的问题。<br>
                    方法：通过改进反转蒸馏（RD）的方法，建立了一种新的、在具有挑战性的MVTec数据集上进行异常检测和定位的最先进的基准。<br>
                    效果：提出的方法运行速度比PatchCore快六倍，比CFA快两倍，但与RD相比引入了可忽略的延迟。该方法在BTAD和视网膜OCT数据集上进行了实验，证明了其泛化能力，并通过重要的消融实验提供了对其配置的见解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Anomaly detection is an important application in large-scale industrial manufacturing. Recent methods for this task have demonstrated excellent accuracy but come with a latency trade-off. Memory based approaches with dominant performances like PatchCore or Coupled-hypersphere-based Feature Adaptation (CFA) require an external memory bank, which significantly lengthens the execution time. Another approach that employs Reversed Distillation (RD) can perform well while maintaining low latency. In this paper, we revisit this idea to improve its performance, establishing a new state-of-the-art benchmark on the challenging MVTec dataset for both anomaly detection and localization. The proposed method, called RD++, runs six times faster than PatchCore, and two times faster than CFA but introduces a negligible latency compared to RD. We also experiment on the BTAD and Retinal OCT datasets to demonstrate our method's generalizability and conduct important ablation experiments to provide insights into its configurations. Source code will be available at https://github.com/tientrandinh/Revisiting-Reverse-Distillation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">778.Meta-Causal Learning for Single Domain Generalization</span><br>
                <span class="as">Chen, JinandGao, ZhiandWu, XinxiaoandLuo, Jiebo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Meta-Causal_Learning_for_Single_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7683-7692.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单个训练领域（源领域）学习模型并将其应用于多个未见过的目标领域。<br>
                    动机：现有的方法主要关注扩大训练领域的分布以覆盖目标领域，但没有估计源领域和目标领域之间的领域转移。<br>
                    方法：提出了一种新的学习范式——模拟-分析-减少，首先通过构建辅助领域作为目标领域来模拟领域转移，然后学习分析领域转移的原因，最后学习减少领域转移以进行模型适应。在这个范式下，提出了一种元因果关系学习方法来学习元知识，即如何在训练过程中推断辅助领域和源领域之间的领域转移的原因。<br>
                    效果：在几个图像分类基准测试上进行的大量实验表明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single domain generalization aims to learn a model from a single training domain (source domain) and apply it to multiple unseen test domains (target domains). Existing methods focus on expanding the distribution of the training domain to cover the target domains, but without estimating the domain shift between the source and target domains. In this paper, we propose a new learning paradigm, namely simulate-analyze-reduce, which first simulates the domain shift by building an auxiliary domain as the target domain, then learns to analyze the causes of domain shift, and finally learns to reduce the domain shift for model adaptation. Under this paradigm, we propose a meta-causal learning method to learn meta-knowledge, that is, how to infer the causes of domain shift between the auxiliary and source domains during training. We use the meta-knowledge to analyze the shift between the target and source domains during testing. Specifically, we perform multiple transformations on source data to generate the auxiliary domain, perform counterfactual inference to learn to discover the causal factors of the shift between the auxiliary and source domains, and incorporate the inferred causality into factor-aware domain alignments. Extensive experiments on several benchmarks of image classification show the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">779.FEND: A Future Enhanced Distribution-Aware Contrastive Learning Framework for Long-Tail Trajectory Prediction</span><br>
                <span class="as">Wang, YuningandZhang, PuandBai, LeiandXue, Jianru</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_FEND_A_Future_Enhanced_Distribution-Aware_Contrastive_Learning_Framework_for_Long-Tail_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1400-1409.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自动驾驶中交通代理的未来轨迹预测问题，特别是长尾数据中的复杂和安全关键性问题。<br>
                    动机：现有的轨迹预测方法没有考虑到长尾数据中多样的运动模式，且长尾数据通常更复杂、更具安全性。<br>
                    方法：提出了一种未来增强的对比学习框架来识别尾部轨迹模式并形成具有独立模式簇的特征空间。此外，还提出了一个分布感知的超级预测器以更好地利用形成的特征空间。<br>
                    效果：实验结果表明，该方法在尾部样本上比最先进的尾部预测方法提高了9.5%的ADE和8.5%的FDE，同时保持或略微提高了平均性能。该方法还在轨迹预测任务上超越了许多长尾技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting the future trajectories of the traffic agents is a gordian technique in autonomous driving. However, trajectory prediction suffers from data imbalance in the prevalent datasets, and the tailed data is often more complicated and safety-critical. In this paper, we focus on dealing with the long-tail phenomenon in trajectory prediction. Previous methods dealing with long-tail data did not take into account the variety of motion patterns in the tailed data. In this paper, we put forward a future enhanced contrastive learning framework to recognize tail trajectory patterns and form a feature space with separate pattern clusters.Furthermore, a distribution aware hyper predictor is brought up to better utilize the shaped feature space.Our method is a model-agnostic framework and can be plugged into many well-known baselines. Experimental results show that our framework outperforms the state-of-the-art long-tail prediction method on tailed samples by 9.5% on ADE and 8.5% on FDE, while maintaining or slightly improving the averaged performance. Our method also surpasses many long-tail techniques on trajectory prediction task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">780.Reliability in Semantic Segmentation: Are We on the Right Track?</span><br>
                <span class="as">deJorge, PauandVolpi, RiccardoandTorr, PhilipH.S.andRogez, Gr\&#x27;egory</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/de_Jorge_Reliability_in_Semantic_Segmentation_Are_We_on_the_Right_Track_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7173-7182.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在探索现代语义分割模型在鲁棒性和不确定性估计方面的性能，以提升模型的可靠性。<br>
                    动机：尽管现有的预训练语言模型在领域内的性能不断提高，但其对模型的鲁棒性和不确定性估计等方面的探索较少，这引发了对模型可靠性提升的疑虑。<br>
                    方法：本研究对多种模型进行了广泛分析，包括从早期的基于ResNet的架构到新的转换器，并根据四个指标（鲁棒性、校准、误分类检测和分布外(OOD)检测）评估了它们的可靠性。<br>
                    效果：研究发现，虽然最新的模型在鲁棒性方面有了显著提高，但在不确定性估计方面并不总是更可靠。进一步的探索发现，提高校准度也可以帮助改善其他不确定性指标，如误分类或OOD检测。这是首个关注于现代分割模型在鲁棒性和不确定性估计方面的研究，希望能帮助对此感兴趣的实践者和研究者。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Motivated by the increasing popularity of transformers in computer vision, in recent times there has been a rapid development of novel architectures. While in-domain performance follows a constant, upward trend, properties like robustness or uncertainty estimation are less explored -leaving doubts about advances in model reliability. Studies along these axes exist, but they are mainly limited to classification models. In contrast, we carry out a study on semantic segmentation, a relevant task for many real-world applications where model reliability is paramount. We analyze a broad variety of models, spanning from older ResNet-based architectures to novel transformers and assess their reliability based on four metrics: robustness, calibration, misclassification detection and out-of-distribution (OOD) detection. We find that while recent models are significantly more robust, they are not overall more reliable in terms of uncertainty estimation. We further explore methods that can come to the rescue and show that improving calibration can also help with other uncertainty metrics such as misclassification or OOD detection. This is the first study on modern segmentation models focused on both robustness and uncertainty estimation and we hope it will help practitioners and researchers interested in this fundamental vision task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">781.Video Test-Time Adaptation for Action Recognition</span><br>
                <span class="as">Lin, WeiandMirza, MuhammadJehanzebandKozinski, MateuszandPossegger, HorstandKuehne, HildeandBischof, Horst</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Video_Test-Time_Adaptation_for_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22952-22961.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的动作识别系统在面对未预期的数据分布变化时表现脆弱。<br>
                    动机：尽管现有的模型在评估时可以达到最佳性能，但它们对测试数据中未预期的分布变化非常敏感。<br>
                    方法：提出一种适用于时空模型的方法，该方法能够在单个视频样本上进行适应。该方法包括一个特征分布对齐技术，该技术将在线估计的测试集统计量与训练统计量对齐。此外，我们还对同一测试视频样本的临时增强视图实施预测一致性。<br>
                    效果：在三个基准动作识别数据集上的评估表明，我们提出的方法对架构无关，并能显著提高现有最先进的卷积架构TANet和Video Swin Transformer的性能。我们的方法在单个分布偏移的评估和具有挑战性的随机分布偏移的情况下，比现有的测试时间适应方法表现出显著的性能增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although action recognition systems can achieve top performance when evaluated on in-distribution test points, they are vulnerable to unanticipated distribution shifts in test data. However, test-time adaptation of video action recognition models against common distribution shifts has so far not been demonstrated. We propose to address this problem with an approach tailored to spatio-temporal models that is capable of adaptation on a single video sample at a step. It consists in a feature distribution alignment technique that aligns online estimates of test set statistics towards the training statistics. We further enforce prediction consistency over temporally augmented views of the same test video sample. Evaluations on three benchmark action recognition datasets show that our proposed technique is architecture-agnostic and able to significantly boost the performance on both, the state of the art convolutional architecture TANet and the Video Swin Transformer. Our proposed method demonstrates a substantial performance gain over existing test-time adaptation approaches in both evaluations of a single distribution shift and the challenging case of random distribution shifts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">782.Bi-Level Meta-Learning for Few-Shot Domain Generalization</span><br>
                <span class="as">Qin, XiaorongandSong, XinhangandJiang, Shuqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Bi-Level_Meta-Learning_for_Few-Shot_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15900-15910.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本学习中的领域泛化问题，即如何在只有少量样本的情况下，实现从已知领域到未知领域的泛化。<br>
                    动机：现有的小样本学习方法主要集中在特定领域的泛化上，但在实际应用中，更常见的情况是需要跨领域的泛化。<br>
                    方法：本文通过元学习两个层次的元知识来解决小样本领域泛化问题。较低层次的元知识是特定领域的嵌入空间，作为基础空间的子空间进行域内泛化；较高层次的元知识是基础空间和先验的特定领域空间子空间，用于域间泛化。通过双层优化来求解这两个层次的元知识学习问题，并进一步开发了一种无需海森矩阵信息的最优化算法。<br>
                    效果：通过在广泛使用的基准Meta-Dataset上进行评估，证明该方法显著优于先前的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of few-shot learning is to learn the generalizability from seen to unseen data with only a few samples. Most previous few-shot learning focus on learning generalizability within particular domains. However, the more practical scenarios may also require generalizability across domains. In this paper, we study the problem of Few-shot domain generalization (FSDG), which is a more challenging variant of few-shot classification. FSDG requires additional generalization with larger gap from seen domains to unseen domains. We address FSDG problem by meta-learning two levels of meta-knowledge, where the lower-level meta-knowledge are domain-specific embedding spaces as subspaces of a base space for intra-domain generalization, and the upper-level meta-knowledge is the base space and a prior subspace over domain-specific spaces for inter-domain generalization. We formulate the two levels of meta-knowledge learning problem with bi-level optimization, and further develop an optimization algorithm without Hessian information to solve it. We demonstrate our method is significantly superior to the previous works by evaluating it on the widely used benchmark Meta-Dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">783.Class Relationship Embedded Learning for Source-Free Unsupervised Domain Adaptation</span><br>
                <span class="as">Zhang, YixinandWang, ZileiandHe, Weinan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Class_Relationship_Embedded_Learning_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7619-7629.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决源无监督领域适应（SFUDA）这一实际知识转移任务，即只有训练好的源模型和未标记的目标数据可用。<br>
                    动机：为了充分利用源知识，作者提出转移类别关系，这是一种领域不变的但之前的研究尚未充分探索的类别关系。<br>
                    方法：首先将源模型的分类器权重视为类别原型来计算类别关系，然后通过嵌入源领域的类别关系来提出一种新的基于概率的相似性，称为类别关系嵌入相似性（CRS）。最后，作者建议将CRS统一嵌入对比学习中。<br>
                    效果：广泛的实验结果表明，由于类别关系的领域不变性转移，该方法可以实现最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work focuses on a practical knowledge transfer task defined as Source-Free Unsupervised Domain Adaptation (SFUDA), where only a well-trained source model and unlabeled target data are available. To fully utilize source knowledge, we propose to transfer the class relationship, which is domain-invariant but still under-explored in previous works. To this end, we first regard the classifier weights of the source model as class prototypes to compute class relationship, and then propose a novel probability-based similarity between target-domain samples by embedding the source-domain class relationship, resulting in Class Relationship embedded Similarity (CRS). Here the inter-class term is particularly considered in order to more accurately represent the similarity between two samples, in which the source prior of class relationship is utilized by weighting. Finally, we propose to embed CRS into contrastive learning in a unified form. Here both class-aware and instance discrimination contrastive losses are employed, which are complementary to each other. We combine the proposed method with existing representative methods to evaluate its efficacy in multiple SFUDA settings. Extensive experimental results reveal that our method can achieve state-of-the-art performance due to the transfer of domain-invariant class relationship.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">784.Spatio-Temporal Pixel-Level Contrastive Learning-Based Source-Free Domain Adaptation for Video Semantic Segmentation</span><br>
                <span class="as">Lo, Shao-YuanandOza, PoojanandChennupati, SumanthandGalindo, AlejandroandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lo_Spatio-Temporal_Pixel-Level_Contrastive_Learning-Based_Source-Free_Domain_Adaptation_for_Video_Semantic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10534-10543.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在无法访问源数据的情况下，将已标记的源知识转移到未标记的目标领域。<br>
                    动机：在现实世界的场景中，对源数据的访问通常受到限制或不可行，这使得无监督域适应（UDA）在实际应用中的效果并不理想。<br>
                    方法：本文提出了一种新颖的方法——时空像素级（STPL）对比学习，该方法充分利用了时空信息来解决视频应用中的视频适应问题。<br>
                    效果：实验表明，与当前的UDA和SFDA方法相比，STPL在VSS基准测试上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised Domain Adaptation (UDA) of semantic segmentation transfers labeled source knowledge to an unlabeled target domain by relying on accessing both the source and target data. However, the access to source data is often restricted or infeasible in real-world scenarios. Under the source data restrictive circumstances, UDA is less practical. To address this, recent works have explored solutions under the Source-Free Domain Adaptation (SFDA) setup, which aims to adapt a source-trained model to the target domain without accessing source data. Still, existing SFDA approaches use only image-level information for adaptation, making them sub-optimal in video applications. This paper studies SFDA for Video Semantic Segmentation (VSS), where temporal information is leveraged to address video adaptation. Specifically, we propose Spatio-Temporal Pixel-Level (STPL) contrastive learning, a novel method that takes full advantage of spatio-temporal information to tackle the absence of source data better. STPL explicitly learns semantic correlations among pixels in the spatio-temporal space, providing strong self-supervision for adaptation to the unlabeled target domain. Extensive experiments show that STPL achieves state-of-the-art performance on VSS benchmarks compared to current UDA and SFDA approaches. Code is available at: https://github.com/shaoyuanlo/STPL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">785.Mind the Label Shift of Augmentation-Based Graph OOD Generalization</span><br>
                <span class="as">Yu, JunchiandLiang, JianandHe, Ran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Mind_the_Label_Shift_of_Augmentation-Based_Graph_OOD_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11620-11630.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高图神经网络（GNNs）的分布外（OOD）泛化能力。<br>
                    动机：现有的方法通过编辑图形结构生成增强环境以学习一个不变的GNN进行泛化，但这会改变图形标签，导致增强环境中的标签偏移和不一致的预测关系。<br>
                    方法：提出LiSA，利用训练图中的标签不变子图构建增强环境，而不是依赖图形编辑。具体来说，LiSA首先设计变分子图生成器来有效提取局部预测模式并构造多个标签不变的子图。然后，由不同生成器产生的子图被收集起来构建不同的增强环境。为了促进增强环境之间的多样性，LiSA进一步引入了一种易于处理的能量基线正则化，以扩大环境分布之间的成对距离。<br>
                    效果：在节点级和图级的OOB基准测试上进行的大量实验表明，LiSA在不同的GNN骨干网络上实现了令人印象深刻的泛化性能。代码可在https://github.com/Samyu0304/LiSA获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Out-of-distribution (OOD) generalization is an important issue for Graph Neural Networks (GNNs). Recent works employ different graph editions to generate augmented environments and learn an invariant GNN for generalization. However, the graph structural edition inevitably alters the graph label. This causes the label shift in augmentations and brings inconsistent predictive relationships among augmented environments. To address this issue, we propose LiSA, which generates label-invariant augmentations to facilitate graph OOD generalization. Instead of resorting to graph editions, LiSA exploits Label-invariant Subgraphs of the training graphs to construct Augmented environments. Specifically, LiSA first designs the variational subgraph generators to efficiently extract locally predictive patterns and construct multiple label-invariant subgraphs. Then, the subgraphs produced by different generators are collected to build different augmented environments. To promote diversity among augmented environments, LiSA further introduces a tractable energy-based regularization to enlarge pair-wise distances between the distributions of environments. In this manner, LiSA generates diverse augmented environments with a consistent predictive relationship to facilitate learning an invariant GNN. Extensive experiments on node-level and graph-level OOD benchmarks show that LiSA achieves impressive generalization performance with different GNN backbones. Code is available on https://github.com/Samyu0304/LiSA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">786.Confidence-Aware Personalized Federated Learning via Variational Expectation Maximization</span><br>
                <span class="as">Zhu, JunyiandMa, XingchenandBlaschko, MatthewB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Confidence-Aware_Personalized_Federated_Learning_via_Variational_Expectation_Maximization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24542-24551.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决联邦学习中各客户端数据分布不均和规模不同的问题。<br>
                    动机：个性化联邦学习通过局部适应模型来解决这个问题。<br>
                    方法：提出了一种基于分层贝叶斯建模和变分推理的个性化联邦学习新框架，引入全局模型作为潜在变量，优化基于最大化边缘似然性原则，使用变分期望最大化进行计算。<br>
                    效果：在多种数据集上进行了广泛的实证研究，实验结果表明，该方法在轻度异构环境下表现良好，在高度异构环境下显著优于最先进的个性化联邦学习框架。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Learning (FL) is a distributed learning scheme to train a shared model across clients. One common and fundamental challenge in FL is that the sets of data across clients could be non-identically distributed and have different sizes. Personalized Federated Learning (PFL) attempts to solve this challenge via locally adapted models. In this work, we present a novel framework for PFL based on hierarchical Bayesian modeling and variational inference. A global model is introduced as a latent variable to augment the joint distribution of clients' parameters and capture the common trends of different clients, optimization is derived based on the principle of maximizing the marginal likelihood and conducted using variational expectation maximization. Our algorithm gives rise to a closed-form estimation of a confidence value which comprises the uncertainty of clients' parameters and local model deviations from the global model. The confidence value is used to weigh clients' parameters in the aggregation stage and adjust the regularization effect of the global model. We evaluate our method through extensive empirical studies on multiple datasets. Experimental results show that our approach obtains competitive results under mild heterogeneous circumstances while significantly outperforming state-of-the-art PFL frameworks in highly heterogeneous settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">787.AdaptiveMix: Improving GAN Training via Feature Space Shrinkage</span><br>
                <span class="as">Liu, HaozheandZhang, WentianandLi, BingandWu, HaoqianandHe, NanjunandHuang, YawenandLi, YuexiangandGhanem, BernardandZheng, Yefeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_AdaptiveMix_Improving_GAN_Training_via_Feature_Space_Shrinkage_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16219-16229.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练GANs的困难在于判别器的训练分布是动态的，导致图像表示不稳定。<br>
                    动机：受稳健图像表示研究的启发，提出一种简单有效的模块——AdaptiveMix，用于缩小判别器在图像表示空间中训练数据的区域。<br>
                    方法：通过混合一对训练图像来构造硬样本，缩小硬样本和易样本之间的特征距离。<br>
                    效果：实验结果表明，AdaptiveMix可以促进GAN的训练，有效提高生成样本的图像质量。此外，将其与最先进的方法结合，可以进一步应用于图像分类和OOD检测任务，显著提高基线性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the outstanding capability for data generation, Generative Adversarial Networks (GANs) have attracted considerable attention in unsupervised learning. However, training GANs is difficult, since the training distribution is dynamic for the discriminator, leading to unstable image representation. In this paper, we address the problem of training GANs from a novel perspective, i.e., robust image classification. Motivated by studies on robust image representation, we propose a simple yet effective module, namely AdaptiveMix, for GANs, which shrinks the regions of training data in the image representation space of the discriminator. Considering it is intractable to directly bound feature space, we propose to construct hard samples and narrow down the feature distance between hard and easy samples. The hard samples are constructed by mixing a pair of training images. We evaluate the effectiveness of our AdaptiveMix with widely-used and state-of-the-art GAN architectures. The evaluation results demonstrate that our AdaptiveMix can facilitate the training of GANs and effectively improve the image quality of generated samples. We also show that our AdaptiveMix can be further applied to image classification and Out-Of-Distribution (OOD) detection tasks, by equipping it with state-of-the-art methods. Extensive experiments on seven publicly available datasets show that our method effectively boosts the performance of baselines. The code is publicly available at https://github.com/WentianZhang-ML/AdaptiveMix.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">788.Angelic Patches for Improving Third-Party Object Detector Performance</span><br>
                <span class="as">Si, WenwenandLi, ShuoandPark, SangdonandLee, InsupandBastani, Osbert</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Si_Angelic_Patches_for_Improving_Third-Party_Object_Detector_Performance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24638-24647.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习模型对简单扰动和空间变换表现出极度的脆弱性，本文探索是否可以研究问题：深度学习模型对简单扰动和空间变换表现出极度的脆弱性，本文探索是否可以采用对抗攻击方法的特性来提高目标检测的扰动鲁棒性。<br>
                    动机：在现实的目标检测设置中，目标对象可以控制其外观，我们提出一种反向快速梯度符号方法（FGSM）来获取这些天使补丁，即使没有预知扰动也能显著增加检测概率。<br>
                    方法：我们将补丁同时应用于每个目标实例，不仅加强分类，还提高边界框的准确性。<br>
                    效果：实验证明部分覆盖的补丁在解决复杂的边界框问题上是有效的。更重要的是，即使在严重的仿射变换和变形形状下，性能也可以转移到不同的检测模型上。据我们所知，我们是第一个实现跨模型和多补丁效果的目标检测补丁。在真实世界的实验中，我们观察到平均准确率提高了30%，带来了巨大的社会价值。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning models have shown extreme vulnerability to simple perturbations and spatial transformations. In this work, we explore whether we can adopt the characteristics of adversarial attack methods to help improve perturbation robustness for object detection. We study a class of realistic object detection settings wherein the target objects have control over their appearance. To this end, we propose a reversed Fast Gradient Sign Method (FGSM) to obtain these angelic patches that significantly increase the detection probability, even without pre-knowledge of the perturbations. In detail, we apply the patch to each object instance simultaneously, strengthen not only classification but also bounding box accuracy. Experiments demonstrate the efficacy of the partial-covering patch in solving the complex bounding box problem. More importantly, the performance is also transferable to different detection models even under severe affine transformations and deformable shapes. To our knowledge, we are the first (object detection) patch that achieves both cross-model and multiple-patch efficacy. We observed average accuracy improvements of 30% in the real-world experiments, which brings large social value. Our code is available at: https://github.com/averysi224/angelic_patches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">789.Bi3D: Bi-Domain Active Learning for Cross-Domain 3D Object Detection</span><br>
                <span class="as">Yuan, JiakangandZhang, BoandYan, XiangchaoandChen, TaoandShi, BotianandLi, YikangandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Bi3D_Bi-Domain_Active_Learning_for_Cross-Domain_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15599-15608.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过选择部分重要目标数据并对其进行标注，以实现高性能和低标注成本之间的良好平衡。<br>
                    动机：尽管无监督领域适应（UDA）技术在3D跨领域任务中取得了初步进展，但基于UDA的3D模型与使用完全注释的目标领域的有监督模型之间的性能差距仍然很大。<br>
                    方法：我们提出了一种双域主动学习方法，即Bi3D，来解决跨领域的3D对象检测任务。Bi3D首先开发了一种领域感知的源采样策略，从源领域中识别出类似目标领域的样本，以避免模型受到无关源数据的干扰。然后，我们开发了一种基于多样性的目标采样策略，该策略选择了最具信息量的目标子集，以最少的标注预算提高模型对目标领域的适应性。<br>
                    效果：我们在典型的跨领域适应场景中进行了实验，包括跨激光束、跨国界和跨传感器，其中Bi3D实现了令人鼓舞的目标领域检测精度（在KITTI上为89.63%），超过了基于UDA的工作（84.29%），甚至超过了在全标记目标领域上训练的检测器（88.98%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised Domain Adaptation (UDA) technique has been explored in 3D cross-domain tasks recently. Though preliminary progress has been made, the performance gap between the UDA-based 3D model and the supervised one trained with fully annotated target domain is still large. This motivates us to consider selecting partial-yet-important target data and labeling them at a minimum cost, to achieve a good trade-off between high performance and low annotation cost. To this end, we propose a Bi-domain active learning approach, namely Bi3D, to solve the cross-domain 3D object detection task. The Bi3D first develops a domainness-aware source sampling strategy, which identifies target-domain-like samples from the source domain to avoid the model being interfered by irrelevant source data. Then a diversity-based target sampling strategy is developed, which selects the most informative subset of target domain to improve the model adaptability to the target domain using as little annotation budget as possible. Experiments are conducted on typical cross-domain adaptation scenarios including cross-LiDAR-beam, cross-country, and cross-sensor, where Bi3D achieves a promising target-domain detection accuracy (89.63% on KITTI) compared with UDA-based work (84.29%), even surpassing the detector trained on the full set of the labeled target domain (88.98%).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">790.Highly Confident Local Structure Based Consensus Graph Learning for Incomplete Multi-View Clustering</span><br>
                <span class="as">Wen, JieandLiu, ChengliangandXu, GehuiandWu, ZhihaoandHuang, ChaoandFei, LunkeandXu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Highly_Confident_Local_Structure_Based_Consensus_Graph_Learning_for_Incomplete_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15712-15721.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行基于图的多视角聚类，特别是在存在大量不完整数据的情况下。<br>
                    动机：现有的方法通常利用从原始数据构建的图来帮助学习一致的表示，而我们的方法直接学习跨视图的共识图进行聚类。<br>
                    方法：我们设计了一种新的置信图，并将其嵌入以形成一种由置信结构驱动的共识图学习模型。我们的置信图基于直观的相似最近邻假设，不需要任何额外信息，可以帮助模型获得高质量的共识图以实现更好的聚类。<br>
                    效果：通过大量的实验，我们的方法被证明是有效的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph-based multi-view clustering has attracted extensive attention because of the powerful clustering-structure representation ability and noise robustness. Considering the reality of a large amount of incomplete data, in this paper, we propose a simple but effective method for incomplete multi-view clustering based on consensus graph learning, termed as HCLS_CGL. Unlike existing methods that utilize graph constructed from raw data to aid in the learning of consistent representation, our method directly learns a consensus graph across views for clustering. Specifically, we design a novel confidence graph and embed it to form a confidence structure driven consensus graph learning model. Our confidence graph is based on an intuitive similar-nearest-neighbor hypothesis, which does not require any additional information and can help the model to obtain a high-quality consensus graph for better clustering. Numerous experiments are performed to confirm the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">791.CafeBoost: Causal Feature Boost To Eliminate Task-Induced Bias for Class Incremental Learning</span><br>
                <span class="as">Qiu, BenliuandLi, HongliangandWen, HaitaoandQiu, HeqianandWang, LanxiaoandMeng, FanmanandWu, QingboandPan, Lili</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_CafeBoost_Causal_Feature_Boost_To_Eliminate_Task-Induced_Bias_for_Class_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16016-16025.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决连续学习中的灾难性遗忘问题，并发现其中出现的一种新类型的偏差——任务诱导偏差。<br>
                    动机：在连续学习中，模型需要逐步学习一系列任务，但在此过程中会出现灾难性遗忘的问题。同时，作者发现在任务和领域增量学习中存在两种可以自然减少任务诱导偏差的机制，但在类别增量学习（CIL）中并不存在这种机制。<br>
                    方法：作者设计了一种因果干预操作来切断导致任务诱导偏差的因果关系，并将其实现为一个因果去偏模块，该模块可以将有偏的特征转换为无偏的特征。此外，作者还提出了一种训练流程，将这个新的模块整合到现有的方法中，并对整个架构进行联合优化。<br>
                    效果：在CIFAR-100和ImageNet上的大量实验表明，该方法可以显著提高准确率，并大幅减少已有方法的遗忘程度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning requires a model to incrementally learn a sequence of tasks and aims to predict well on all the learned tasks so far, which notoriously suffers from the catastrophic forgetting problem. In this paper, we find a new type of bias appearing in continual learning, coined as task-induced bias. We place continual learning into a causal framework, based on which we find the task-induced bias is reduced naturally by two underlying mechanisms in task and domain incremental learning. However, these mechanisms do not exist in class incremental learning (CIL), in which each task contains a unique subset of classes. To eliminate the task-induced bias in CIL, we devise a causal intervention operation so as to cut off the causal path that causes the task-induced bias, and then implement it as a causal debias module that transforms biased features into unbiased ones. In addition, we propose a training pipeline to incorporate the novel module into existing methods and jointly optimize the entire architecture. Our overall approach does not rely on data replay, and is simple and convenient to plug into existing methods. Extensive empirical study on CIFAR-100 and ImageNet shows that our approach can improve accuracy and reduce forgetting of well-established methods by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">792.Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning</span><br>
                <span class="as">Song, ZeyinandZhao, YifanandShi, YujunandPeng, PeixiandYuan, LiandTian, Yonghong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24183-24192.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本类别增量学习（FSCIL）中的问题，即如何在有限的样本中持续学习新类别，同时不忘记旧类别。<br>
                    动机：目前的FSCIL主流框架在基础会话训练阶段采用交叉熵损失函数，但发现其对类间表示的分离效果不佳，进一步影响了对新类别的泛化能力。<br>
                    方法：受此启发，本文提出了一种新颖的方法——语义感知虚拟对比模型（SAVC）。该方法通过在对比学习中引入虚拟类别来促进新旧类别之间的分离。这些虚拟类别是通过预定义的转换生成的，不仅在表示空间中作为未见过类别的占位符，还提供了多样化的语义信息。<br>
                    效果：实验结果表明，SAVC显著提高了基础类别的分离度和新类别的泛化能力，在三个广泛使用的FSCIL基准数据集上取得了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot class-incremental learning (FSCIL) aims at learning to classify new classes continually from limited samples without forgetting the old classes. The mainstream framework tackling FSCIL is first to adopt the cross-entropy (CE) loss for training at the base session, then freeze the feature extractor to adapt to new classes. However, in this work, we find that the CE loss is not ideal for the base session training as it suffers poor class separation in terms of representations, which further degrades generalization to novel classes. One tempting method to mitigate this problem is to apply an additional naive supervised contrastive learning (SCL) in the base session. Unfortunately, we find that although SCL can create a slightly better representation separation among different base classes, it still struggles to separate base classes and new classes. Inspired by the observations made, we propose Semantic-Aware Virtual Contrastive model (SAVC), a novel method that facilitates separation between new classes and base classes by introducing virtual classes to SCL. These virtual classes, which are generated via pre-defined transformations, not only act as placeholders for unseen classes in the representation space but also provide diverse semantic information. By learning to recognize and contrast in the fantasy space fostered by virtual classes, our SAVC significantly boosts base class separation and novel class generalization, achieving new state-of-the-art performance on the three widely-used FSCIL benchmark datasets. Code is available at: https://github.com/zysong0113/SAVC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">793.Learning Partial Correlation Based Deep Visual Representation for Image Classification</span><br>
                <span class="as">Rahman, SaimunurandKoniusz, PiotrandWang, LeiandZhou, LupingandMoghadam, PeymanandSun, Changming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Learning_Partial_Correlation_Based_Deep_Visual_Representation_for_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6231-6240.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将"部分相关性"估计过程有效地整合到CNN中，以获取更准确的深度视觉表示。<br>
                    动机：现有的基于协方差矩阵的视觉表示方法在存在其他与两个目标通道相关联的通道时会出现“混淆”效应，导致估计结果失真。因此，需要引入可以消除这种混淆效应的“部分相关性”估计。<br>
                    方法：我们将SICE（稀疏逆协方差估计）定义为一种新的CNN结构化层，并开发了一种迭代方法，在正向和反向传播步骤中解决上述矩阵优化问题，以确保模型的端到端可训练性。<br>
                    效果：我们的工作获得了一种基于部分相关性的深度视觉表示，并解决了CNN中经常遇到的协方差矩阵估计的小样本问题。实验表明，我们的深度视觉表示在分类性能上优于基于协方差矩阵的对应方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual representation based on covariance matrix has demonstrates its efficacy for image classification by characterising the pairwise correlation of different channels in convolutional feature maps. However, pairwise correlation will become misleading once there is another channel correlating with both channels of interest, resulting in the "confounding" effect. For this case, "partial correlation" which removes the confounding effect shall be estimated instead. Nevertheless, reliably estimating partial correlation requires to solve a symmetric positive definite matrix optimisation, known as sparse inverse covariance estimation (SICE). How to incorporate this process into CNN remains an open issue. In this work, we formulate SICE as a novel structured layer of CNN. To ensure end-to-end trainability, we develop an iterative method to solve the above matrix optimisation during forward and backward propagation steps. Our work obtains a partial correlation based deep visual representation and mitigates the small sample problem often encountered by covariance matrix estimation in CNN. Computationally, our model can be effectively trained with GPU and works well with a large number of channels of advanced CNNs. Experiments show the efficacy and superior classification performance of our deep visual representation compared to covariance matrix based counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">794.IterativePFN: True Iterative Point Cloud Filtering</span><br>
                <span class="as">deSilvaEdirimuni, DasithandLu, XuequanandShao, ZhiwenandLi, GangandRobles-Kelly, AntonioandHe, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/de_Silva_Edirimuni_IterativePFN_True_Iterative_Point_Cloud_Filtering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13530-13539.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高点云质量，即去除捕获过程中引入的噪声。<br>
                    动机：现有的基于学习的方法主要通过训练神经网络来推断过滤后的位移，并将有噪声的点直接转移到底层清洁表面，但这种方法在高噪声环境下效果不佳。<br>
                    方法：提出一种迭代点云滤波网络（IterativePFN），该网络内部包含多个迭代模块，模拟真实的迭代滤波过程。并使用一种新的损失函数进行训练，该函数在每次迭代时利用一个自适应的地面真实目标，以捕捉训练过程中中间滤波结果之间的关系。<br>
                    效果：实验结果表明，该方法比现有技术具有更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The quality of point clouds is often limited by noise introduced during their capture process. Consequently, a fundamental 3D vision task is the removal of noise, known as point cloud filtering or denoising. State-of-the-art learning based methods focus on training neural networks to infer filtered displacements and directly shift noisy points onto the underlying clean surfaces. In high noise conditions, they iterate the filtering process. However, this iterative filtering is only done at test time and is less effective at ensuring points converge quickly onto the clean surfaces. We propose IterativePFN (iterative point cloud filtering network), which consists of multiple IterationModules that model the true iterative filtering process internally, within a single network. We train our IterativePFN network using a novel loss function that utilizes an adaptive ground truth target at each iteration to capture the relationship between intermediate filtering results during training. This ensures that the filtered results converge faster to the clean surfaces. Our method is able to obtain better performance compared to state-of-the-art methods. The source code can be found at: https://github.com/ddsediri/IterativePFN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">795.On the Convergence of IRLS and Its Variants in Outlier-Robust Estimation</span><br>
                <span class="as">Peng, LiangzuandK\&quot;ummerle, ChristianandVidal, Ren\&#x27;e</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_On_the_Convergence_of_IRLS_and_Its_Variants_in_Outlier-Robust_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17808-17818.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在存在异常值的情况下，从数据样本中估计一些参数（如3D旋转），并解决其非凸和非光滑的问题。<br>
                    动机：经典的迭代加权最小二乘法及其变体在处理这个问题上表现出了优秀的性能，但对其为何能如此有效的原因尚不明确。<br>
                    方法：将主要化和分级非凸性（GNC）引入到迭代加权最小二乘法框架中，证明得到的变体是一个收敛的异常值稳健估计方法。<br>
                    效果：实验结果证实了我们的理论，并表明对于典型的异常值稳健估计问题实例，所提出的方法在5-10次迭代内收敛，而最先进的方法至少需要30次迭代。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Outlier-robust estimation involves estimating some parameters (e.g., 3D rotations) from data samples in the presence of outliers, and is typically formulated as a non-convex and non-smooth problem. For this problem, the classical method called iteratively reweighted least-squares (IRLS) and its variants have shown impressive performance. This paper makes several contributions towards understanding why these algorithms work so well. First, we incorporate majorization and graduated non-convexity (GNC) into the IRLS framework and prove that the resulting IRLS variant is a convergent method for outlier-robust estimation. Moreover, in the robust regression context with a constant fraction of outliers, we prove this IRLS variant converges to the ground truth at a global linear and local quadratic rate for a random Gaussian feature matrix with high probability. Experiments corroborate our theory and show that the proposed IRLS variant converges within 5-10 iterations for typical problem instances of outlier-robust estimation, while state-of-the-art methods need at least 30 iterations. A basic implementation of our method is provided: https://github.com/liangzu/IRLS-CVPR2023</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">796.Deep Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype Alignment</span><br>
                <span class="as">Jin, JiaqiandWang, SiweiandDong, ZhibinandLiu, XinwangandZhu, En</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Deep_Incomplete_Multi-View_Clustering_With_Cross-View_Partial_Sample_and_Prototype_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11600-11609.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多视角聚类方法在面对样本不完整时表现不佳，且现有解决不完整多视角聚类（IMVC）的方法存在忽视视图差异和表示灵活性以及可能导致错误融合的问题。<br>
                    动机：由于数据损坏或传感器故障等原因，现实中的多视角样本往往是部分可用的，这导致现有的多视角聚类方法无法很好地处理这种情况。<br>
                    方法：我们提出了一种名为Cross-view Partial Sample and Prototype Alignment Network (CPSPAN)的深度不完整多视角聚类方法。该方法通过使用对观察到的数据对齐作为'代理监督信号'来指导视图之间的实例到实例对应关系的构建，并针对IMVC中偏移的原型设计了一种原型对齐模块来实现跨视图的不完整分布校准。<br>
                    效果：大量的实验结果表明，我们提出的模块非常有效，与现有的IMVC竞争对手相比，在基准数据集上取得了显著的性能改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following drawbacks: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pair-observed data alignment as 'proxy supervised signals' to guide instance-to-instance correspondence construction among views. Then, regarding of the shifted prototypes in IMVC, we further propose a prototype alignment module to achieve incomplete distribution calibration across views. Extensive experimental results showcase the effectiveness of our proposed modules, attaining noteworthy performance improvements when compared to existing IMVC competitors on benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">797.Object Pose Estimation With Statistical Guarantees: Conformal Keypoint Detection and Geometric Uncertainty Propagation</span><br>
                <span class="as">Yang, HengandPavone, Marco</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Object_Pose_Estimation_With_Statistical_Guarantees_Conformal_Keypoint_Detection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8947-8958.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的两阶段物体姿态估计方法在标准基准上表现良好，但对估计的质量和不确定性没有提供可证明的保证。<br>
                    动机：为了解决这一问题，本文对两阶段范式进行了两个根本性的改变，即保形关键点检测和几何不确定性传播，并提出了第一个能够提供可证明和可计算的最坏情况误差界限的姿态估计器。<br>
                    方法：首先，保形关键点检测将归纳性保形预测的统计机制应用于将启发式关键点检测转化为覆盖真实关键点的用户指定边缘概率（例如90%）的圆形或椭圆形预测集。其次，几何不确定性传播将关键点上的几何约束传播到6D对象姿态，从而产生一个保证以相同概率覆盖真实姿态的姿态不确定性集（PURSE）。然后，通过随机样本平均（RANSAG）计算平均姿态，并通过半定松弛法上界最坏情况误差。<br>
                    效果：在LineMOD遮挡数据集上，我们证明了：（i）PURSE以有效的概率覆盖了真实姿态；（ii）最坏情况误差界限提供了正确的不确定性量化；（iii）平均姿态实现了与基于稀疏关键点的代表方法相当或更好的精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The two-stage object pose estimation paradigm first detects semantic keypoints on the image and then estimates the 6D pose by minimizing reprojection errors. Despite performing well on standard benchmarks, existing techniques offer no provable guarantees on the quality and uncertainty of the estimation. In this paper, we inject two fundamental changes, namely conformal keypoint detection and geometric uncertainty propagation, into the two-stage paradigm and propose the first pose estimator that endows an estimation with provable and computable worst-case error bounds. On one hand, conformal keypoint detection applies the statistical machinery of inductive conformal prediction to convert heuristic keypoint detections into circular or elliptical prediction sets that cover the groundtruth keypoints with a user-specified marginal probability (e.g., 90%). Geometric uncertainty propagation, on the other, propagates the geometric constraints on the keypoints to the 6D object pose, leading to a Pose UnceRtainty SEt (PURSE) that guarantees coverage of the groundtruth pose with the same probability. The PURSE, however, is a nonconvex set that does not directly lead to estimated poses and uncertainties. Therefore, we develop RANdom SAmple averaGing (RANSAG) to compute an average pose and apply semidefinite relaxation to upper bound the worst-case errors between the average pose and the groundtruth. On the LineMOD Occlusion dataset we demonstrate: (i) the PURSE covers the groundtruth with valid probabilities; (ii) the worst-case error bounds provide correct uncertainty quantification; and (iii) the average pose achieves better or similar accuracy as representative methods based on sparse keypoints.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">798.Learning Joint Latent Space EBM Prior Model for Multi-Layer Generator</span><br>
                <span class="as">Cui, JialiandWu, YingNianandHan, Tian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Learning_Joint_Latent_Space_EBM_Prior_Model_for_Multi-Layer_Generator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3603-3612.png><br>
            
            <span class="tt"><span class="t0">研究问题：学习多层生成器模型的基本问题。<br>
                    动机：现有的多层生成器模型通过在生成器顶部建立多个潜变量层作为先验模型，有助于学习复杂的数据分布和分层表示。然而，这种先验模型通常通过假设非信息性（条件）高斯分布来建模潜变量层之间的相互关系，这可能在模型表现力上存在限制。<br>
                    方法：提出一种基于能量的模型（EBM），在多层生成器的所有潜变量层的联合潜空间上进行训练。这种联合潜空间EBM先验模型通过逐层的能量项捕获每层的层内上下文关系，并联合校正不同层的潜变量。开发了一种基于最大似然估计（MLE）的联合训练方案，其中涉及从不同层的潜在变量中进行马尔科夫链蒙特卡罗（MCMC）采样的先验和后验分布。为确保高效的推理和学习，进一步提出了一种变分训练方案，其中使用推理模型来摊销昂贵的后验MCMC采样。<br>
                    效果：实验结果表明，学习到的模型在生成高质量图像和捕获分层特征以进行更好的异常检测方面具有表现力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior distributions of the latent variables from different layers. To ensure efficient inference and learning, we further propose a variational training scheme where an inference model is used to amortize the costly posterior MCMC sampling. Our experiments demonstrate that the learned model can be expressive in generating high-quality images and capturing hierarchical features for better outlier detection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">799.Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning</span><br>
                <span class="as">Wu, ZesenandYe, Mang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Unsupervised_Visible-Infrared_Person_Re-Identification_via_Progressive_Graph_Matching_and_Alternate_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9548-9558.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决无监督可见光-红外跨模态行人重识别问题，由于模态差距大和缺乏跨模态对应关系。<br>
                    动机：跨模态对应关系对于弥合模态差距至关重要。现有的一些工作尝试挖掘跨模态对应关系，但只关注局部信息，没有充分利用全局身份关系，限制了挖掘的对应关系的质量。<br>
                    方法：设计了一种渐进式图匹配方法（PGM）来在聚类不平衡的情况下全局挖掘跨模态对应关系。PGM将对应关系挖掘视为图匹配过程，通过最小化全局匹配成本（衡量聚类之间的差异）来考虑全局信息。此外，PGM采用渐进策略通过多个动态匹配过程来解决不平衡问题。基于PGM，设计了一个交替交叉对比学习（ACCL）模块，利用挖掘的跨模态对应关系减小模态差距，并通过交替方案减轻对应关系中的噪声影响。<br>
                    效果：大量实验证明生成的对应关系的可靠性以及该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised visible-infrared person re-identification is a challenging task due to the large modality gap and the unavailability of cross-modality correspondences. Cross-modality correspondences are very crucial to bridge the modality gap. Some existing works try to mine cross-modality correspondences, but they focus only on local information. They do not fully exploit the global relationship across identities, thus limiting the quality of the mined correspondences. Worse still, the number of clusters of the two modalities is often inconsistent, exacerbating the unreliability of the generated correspondences. In response, we devise a Progressive Graph Matching method to globally mine cross-modality correspondences under cluster imbalance scenarios. PGM formulates correspondences mining as a graph matching process and considers the global information by minimizing the global matching cost, where the matching cost measures the dissimilarity of clusters. Besides, PGM adopts a progressive strategy to address the imbalance issue with multiple dynamic matching processes. Based on PGM, we design an Alternate Cross Contrastive Learning (ACCL) module to reduce the modality gap with the mined cross-modality correspondences, while mitigating the effect of noise in correspondences through an alternate scheme. Extensive experiments demonstrate the reliability of the generated correspondences and the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">800.Training Debiased Subnetworks With Contrastive Weight Pruning</span><br>
                <span class="as">Park, GeonYeongandLee, SangminandLee, SangWanandYe, JongChul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Training_Debiased_Subnetworks_With_Contrastive_Weight_Pruning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7929-7938.png><br>
            
            <span class="tt"><span class="t0">研究问题：在严重偏置的网络中，是否存在最优无偏的功能性子网络？如果存在，如何提取这样的子网络？<br>
                    动机：神经网络往往偏向于误导性相关的特征，这引发了一个问题：在严重偏置的网络中，是否存在最优无偏的功能性子网络？如果存在，如何提取这样的子网络？<br>
                    方法：我们提出了一种去偏对比权重剪枝（DCWP）算法，该算法在没有昂贵群体注释的情况下探索无偏子网络。<br>
                    效果：实验结果表明，尽管我们的方法是参数数量的显著减少，但它显著优于最先进的去偏方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: "Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbiased subnetworks without expensive group annotations. Experimental results demonstrate that our approach significantly outperforms state-of-the-art debiasing methods despite its considerable reduction in the number of parameters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">801.STAR Loss: Reducing Semantic Ambiguity in Facial Landmark Detection</span><br>
                <span class="as">Zhou, ZhenglinandLi, HuaxiaandLiu, HongandWang, NanyangandYu, GangandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_STAR_Loss_Reducing_Semantic_Ambiguity_in_Facial_Landmark_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15475-15484.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习在面部关键点检测中取得了显著进步，但语义歧义问题降低了检测性能。<br>
                    动机：语义歧义导致标注不一致，影响模型的收敛，使预测准确性和稳定性下降。<br>
                    方法：提出一种自适应歧义消减（STAR）损失函数，利用语义歧义的特性，通过设计衡量预测分布各向异性的损失函数来解决这个问题。<br>
                    效果：实验证明，STAR损失函数在三个基准测试中优于现有方法，且计算开销可忽略不计。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, deep learning-based facial landmark detection has achieved significant improvement. However, the semantic ambiguity problem degrades detection performance. Specifically, the semantic ambiguity causes inconsistent annotation and negatively affects the model's convergence, leading to worse accuracy and instability prediction. To solve this problem, we propose a Self-adapTive Ambiguity Reduction (STAR) loss by exploiting the properties of semantic ambiguity. We find that semantic ambiguity results in the anisotropic predicted distribution, which inspires us to use predicted distribution to represent semantic ambiguity. Based on this, we design the STAR loss that measures the anisotropism of the predicted distribution. Compared with the standard regression loss, STAR loss is encouraged to be small when the predicted distribution is anisotropic and thus adaptively mitigates the impact of semantic ambiguity. Moreover, we propose two kinds of eigenvalue restriction methods that could avoid both distribution's abnormal change and the model's premature convergence. Finally, the comprehensive experiments demonstrate that STAR loss outperforms the state-of-the-art methods on three benchmarks, i.e., COFW, 300W, and WFLW, with negligible computation overhead. Code is at https://github.com/ZhenglinZhou/STAR</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">802.A Meta-Learning Approach to Predicting Performance and Data Requirements</span><br>
                <span class="as">Jain, AchinandSwaminathan, GurumurthyandFavaro, PaoloandYang, HaoandRavichandran, AvinashandHarutyunyan, HrayrandAchille, AlessandroandDabeer, OnkarandSchiele, BerntandSwaminathan, AshwinandSoatto, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_A_Meta-Learning_Approach_to_Predicting_Performance_and_Data_Requirements_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3623-3632.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确估计模型达到目标性能所需的样本数量。<br>
                    动机：目前的模型性能评估原则——幂定律，在小数据集（如每个类别5个样本）外推时会导致大误差。<br>
                    方法：提出一种新的分段幂定律(PPL)来处理两种数据情况，并使用元学习训练的随机森林回归器来估计PPL的参数，该回归器可以适用于各种分类/检测任务和不同的网络架构及初始化方式。<br>
                    效果：在16个分类数据集和10个检测数据集中，PPL相比幂定律平均提高了37%和33%的性能估计精度，同时通过提供置信区间和使用它来限制预测范围，将分类和检测数据集的数据过度估计降低了76%和91%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose an approach to estimate the number of samples required for a model to reach a target performance. We find that the power law, the de facto principle to estimate model performance, leads to large error when using a small dataset (e.g., 5 samples per class) for extrapolation. This is because the log-performance error against the log-dataset size follows a nonlinear progression in the few-shot regime followed by a linear progression in the high-shot regime. We introduce a novel piecewise power law (PPL) that handles the two data regimes differently. To estimate the parameters of the PPL, we introduce a random forest regressor trained via meta learning that generalizes across classification/detection tasks, ResNet/ViT based architectures, and random/pre-trained initializations. The PPL improves the performance estimation on average by 37% across 16 classification datasets and 33% across 10 detection datasets, compared to the power law. We further extend the PPL to provide a confidence bound and use it to limit the prediction horizon that reduces over-estimation of data by 76% on classification and 91% on detection datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">803.Semi-Supervised Domain Adaptation With Source Label Adaptation</span><br>
                <span class="as">Yu, Yu-ChuandLin, Hsuan-Tien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Semi-Supervised_Domain_Adaptation_With_Source_Label_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24100-24109.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督领域适应方法（SSDA）通常通过特征空间映射和伪标签分配将目标数据与标记的源数据对齐，但这种方法可能会将目标数据错误地对齐到错误的类别上，降低分类性能。<br>
                    动机：本文提出了一种新的源适应范式，将源数据视为理想目标数据的有噪声版本，并设计了一个从目标角度出发的鲁棒清理组件来动态清理标签噪声。<br>
                    方法：通过在源数据上应用这个清理组件，使源数据更好地匹配目标数据。由于这种范式与现有SSDA方法的核心思想非常不同，因此可以很容易地将其与现有方法结合以提高其性能。<br>
                    效果：在两个最先进的SSDA方法上进行实证测试，结果表明，该方法能有效清理源标签中的噪声，并在基准数据集上表现出优于这些方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-Supervised Domain Adaptation (SSDA) involves learning to classify unseen target data with a few labeled and lots of unlabeled target data, along with many labeled source data from a related domain. Current SSDA approaches usually aim at aligning the target data to the labeled source data with feature space mapping and pseudo-label assignments. Nevertheless, such a source-oriented model can sometimes align the target data to source data of the wrong classes, degrading the classification performance. This paper presents a novel source-adaptive paradigm that adapts the source data to match the target data. Our key idea is to view the source data as a noisily-labeled version of the ideal target data. Then, we propose an SSDA model that cleans up the label noise dynamically with the help of a robust cleaner component designed from the target perspective. Since the paradigm is very different from the core ideas behind existing SSDA approaches, our proposed model can be easily coupled with them to improve their performance. Empirical results on two state-of-the-art SSDA approaches demonstrate that the proposed model effectively cleans up the noise within the source labels and exhibits superior performance over those approaches across benchmark datasets. Our code is available at https://github.com/chu0802/SLA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">804.Conflict-Based Cross-View Consistency for Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Wang, ZichengandZhao, ZhenandXing, XiaoxiaandXu, DongandKong, XiangyuandZhou, Luping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Conflict-Based_Cross-View_Consistency_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19585-19595.png><br>
            
            <span class="tt"><span class="t0">研究问题：半监督语义分割（SSS）需要大量全标注的训练数据，但现有方法常受伪标签过程的确认偏差影响。<br>
                    动机：为了减少对大量全标注训练数据的依赖，并解决伪标签过程中的确认偏差问题，本文提出了一种基于共同训练框架的冲突基于跨视图一致性（CCVC）方法。<br>
                    方法：首先，提出一种新的跨视图一致性（CVC）策略，通过引入特征差异损失，鼓励两个子网络从同一输入中学习不同的特征，同时期望这些不同特征生成一致的输入预测分数。其次，进一步提出一种冲突基于伪标签（CPL）方法，确保模型从冲突的预测中学习更多有用的信息，从而实现稳定的训练过程。<br>
                    效果：在SSS基准数据集上验证了新的CCVC方法，该方法实现了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised semantic segmentation (SSS) has recently gained increasing research interest as it can reduce the requirement for large-scale fully-annotated training data. The current methods often suffer from the confirmation bias from the pseudo-labelling process, which can be alleviated by the co-training framework. The current co-training-based SSS methods rely on hand-crafted perturbations to prevent the different sub-nets from collapsing into each other, but these artificial perturbations cannot lead to the optimal solution. In this work, we propose a new conflict-based cross-view consistency (CCVC) method based on a two-branch co-training framework which aims at enforcing the two sub-nets to learn informative features from irrelevant views. In particular, we first propose a new cross-view consistency (CVC) strategy that encourages the two sub-nets to learn distinct features from the same input by introducing a feature discrepancy loss, while these distinct features are expected to generate consistent prediction scores of the input. The CVC strategy helps to prevent the two sub-nets from stepping into the collapse. In addition, we further propose a conflict-based pseudo-labelling (CPL) method to guarantee the model will learn more useful information from conflicting predictions, which will lead to a stable training process. We validate our new CCVC approach on the SSS benchmark datasets where our method achieves new state-of-the-art performance. Our code is available at https://github.com/xiaoyao3302/CCVC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">805.Boosting Transductive Few-Shot Fine-Tuning With Margin-Based Uncertainty Weighting and Probability Regularization</span><br>
                <span class="as">Tao, RanandChen, HaoandSavvides, Marios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Boosting_Transductive_Few-Shot_Fine-Tuning_With_Margin-Based_Uncertainty_Weighting_and_Probability_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15752-15761.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量数据进行高效的模型微调，特别是在处理类别不平衡和分布外数据时。<br>
                    动机：近年来，Few-Shot Learning（FSL）快速发展，可能消除了对大量数据获取的需求。我们发现FSL方法在学习过程中存在类别边际分布不平衡的问题，这进一步激发我们提出新的解决方法。<br>
                    方法：我们提出了一种基于边缘不确定性权重和概率正则化的转导微调方法（TF-MP）。该方法首先根据边缘不确定性分数对测试数据进行样本加权，然后对每个测试样本的分类概率进行正则化。<br>
                    效果：在Meta数据集的内/外分布评估中，TF-MP实现了最先进的性能，并且比之前的转导方法大幅提高了性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-Shot Learning (FSL) has been rapidly developed in recent years, potentially eliminating the requirement for significant data acquisition. Few-shot fine-tuning has been demonstrated to be practically efficient and helpful, especially for out-of-distribution datum. In this work, we first observe that the few-shot fine-tuned methods are learned with the imbalanced class marginal distribution. This observation further motivates us to propose the Transductive Fine-tuning with Margin-based uncertainty weighting and Probability regularization (TF-MP), which learns a more balanced class marginal distribution. We first conduct sample weighting on the testing data with margin-based uncertainty scores and further regularize each test sample's categorical probability. TF-MP achieves state-of-the-art performance on in- / out-of-distribution evaluations of Meta-Dataset and surpasses previous transductive methods by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">806.PRISE: Demystifying Deep Lucas-Kanade With Strongly Star-Convex Constraints for Multimodel Image Alignment</span><br>
                <span class="as">Zhang, YiqingandHuang, XinmingandZhang, Ziming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PRISE_Demystifying_Deep_Lucas-Kanade_With_Strongly_Star-Convex_Constraints_for_Multimodel_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13187-13197.png><br>
            
            <span class="tt"><span class="t0">研究问题：针对图像对齐中经典的Lucas-Kanade (LK) 方法在面对大变形图像对时易陷入局部最优的问题。<br>
                    动机：提出一种新的Deep Star-Convexified Lucas-Kanade (PRISE) 方法，通过将强星形凸约束引入优化问题，解决多模型图像对齐问题。<br>
                    方法：利用神经网络近似学习真实值周围的星形凸损失景观，以促进LK方法通过由网络定义的高维空间向真实值的收敛。同时，为了训练，将由于强星形凸性定义产生的对比（铰链）损失附加到原始损失上。<br>
                    效果：在MSCOCO、GoogleEarth和GoogleMap等基准数据集上进行评估，实验结果展示出PRISE方法具有优秀的性能，尤其在小像素误差方面表现突出。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The Lucas-Kanade (LK) method is a classic iterative homography estimation algorithm for image alignment, but often suffers from poor local optimality especially when image pairs have large distortions. To address this challenge, in this paper we propose a novel Deep Star-Convexified Lucas-Kanade (PRISE)  method for multimodel image alignment by introducing strongly star-convex constraints into the optimization problem. Our basic idea is to enforce the neural network to approximately learn a star-convex loss landscape around the ground truth give any data to facilitate the convergence of the LK method to the ground truth through the high dimensional space defined by the network. This leads to a minimax learning problem, with contrastive (hinge) losses due to the definition of strong star-convexity that are appended to the original loss for training. We also provide an efficient sampling based algorithm to leverage the training cost, as well as some analysis on the quality of the solutions from PRISE. We further evaluate our approach on benchmark datasets such as MSCOCO, GoogleEarth, and GoogleMap, and demonstrate state-of-the-art results, especially for small pixel errors. Demo code is attached.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">807.BiasAdv: Bias-Adversarial Augmentation for Model Debiasing</span><br>
                <span class="as">Lim, JonginandKim, YoungdongandKim, ByungjaiandAhn, ChanhoandShin, JinwooandYang, EunhoandHan, Seungju</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lim_BiasAdv_Bias-Adversarial_Augmentation_for_Model_Debiasing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3832-3841.png><br>
            
            <span class="tt"><span class="t0">研究问题：神经网络容易受到数据集中固有的虚假相关性的影响，无法进行无偏的泛化。<br>
                    动机：解决此问题的一个关键挑战是缺乏与偏见冲突的训练数据（即没有虚假相关性的样本）。<br>
                    方法：本文提出了一种名为“偏见对抗性增强”（BiasAdv）的新型数据增强方法，通过生成对抗性图像来补充与偏见冲突的样本。<br>
                    效果：实验结果表明，BiasAdv可以生成令人惊讶的有用的合成偏见冲突样本，使去偏模型能够学习可泛化的表示。此外，BiasAdv不需要任何偏见注释或对偏见类型的先验知识，这使其能够广泛应用于现有的去偏方法以改善其性能。在四个流行的基准数据集上进行的广泛实验表明，BiasAdv具有优越的性能，在各种偏见领域中实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural networks are often prone to bias toward spurious correlations inherent in a dataset, thus failing to generalize unbiased test criteria. A key challenge to resolving the issue is the significant lack of bias-conflicting training data (i.e., samples without spurious correlations). In this paper, we propose a novel data augmentation approach termed Bias-Adversarial augmentation (BiasAdv) that supplements bias-conflicting samples with adversarial images. Our key idea is that an adversarial attack on a biased model that makes decisions based on spurious correlations may generate synthetic bias-conflicting samples, which can then be used as augmented training data for learning a debiased model. Specifically, we formulate an optimization problem for generating adversarial images that attack the predictions of an auxiliary biased model without ruining the predictions of the desired debiased model. Despite its simplicity, we find that BiasAdv can generate surprisingly useful synthetic bias-conflicting samples, allowing the debiased model to learn generalizable representations. Furthermore, BiasAdv does not require any bias annotations or prior knowledge of the bias type, which enables its broad applicability to existing debiasing methods to improve their performances. Our extensive experimental results demonstrate the superiority of BiasAdv, achieving state-of-the-art performance on four popular benchmark datasets across various bias domains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">808.Learning To Retain While Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation</span><br>
                <span class="as">Patel, GauravandMopuri, KondaReddyandQiu, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Patel_Learning_To_Retain_While_Acquiring_Combating_Distribution-Shift_in_Adversarial_Data-Free_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7786-7794.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在没有训练数据的情况下，通过教师神经网络向学生神经网络进行知识转移。<br>
                    动机：在对抗性DFKD框架中，由于多个生成器更新的伪样本分布的非平稳性，学生网络的准确性会受到影响。<br>
                    方法：提出一种元学习启发式框架，将知识获取（从新生成的样本中学习）和知识保留（保留以前遇到的例子的知识）作为元训练和元测试，分别进行处理。<br>
                    效果：通过在多个数据集上与先前的艺术进行广泛的评估和比较，证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Data-free Knowledge Distillation (DFKD) has gained popularity recently, with the fundamental idea of carrying out knowledge transfer from a Teacher neural network to a Student neural network in the absence of training data. However, in the Adversarial DFKD framework, the student network's accuracy, suffers due to the non-stationary distribution of the pseudo-samples under multiple generator updates. To this end, at every generator update, we aim to maintain the student's performance on previously encountered examples while acquiring knowledge from samples of the current distribution. Thus, we propose a meta-learning inspired framework by treating the task of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously met samples) as meta-train and meta-test, respectively. Hence, we dub our method as Learning to Retain while Acquiring. Moreover, we identify an implicit aligning factor between the Knowledge-Retention and Knowledge-Acquisition tasks indicating that the proposed student update strategy enforces a common gradient direction for both tasks, alleviating interference between the two objectives. Finally, we support our hypothesis by exhibiting extensive evaluation and comparison of our method with prior arts on multiple datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">809.Why Is the Winner the Best?</span><br>
                <span class="as">Eisenmann, MatthiasandReinke, AnnikaandWeru, ViviennandTizabi, MinuD.andIsensee, FabianandAdler, TimJ.andAli, SharibandAndrearczyk, VincentandAubreville, MarcandBaid, UjjwalandBakas, SpyridonandBalu, NiranjanandBano, SophiaandBernal, JorgeandBodenstedt, SebastianandCasella, AlessandroandCheplygina, VeronikaandDaum, MarieanddeBruijne, MarleenandDepeursinge, AdrienandDorent, ReubenandEgger, JanandEllis, DavidG.andEngelhardt, SandyandGanz, MelanieandGhatwary, NohaandGirard, GabrielandGodau, PatrickandGupta, AnubhaandHansen, LasseandHarada, KanakoandHeinrich, MattiasP.andHeller, NicholasandHering, AlessaandHuaulm\&#x27;e, ArnaudandJannin, PierreandKavur, AliEmreandKodym, Old\v{r</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenmann_Why_Is_the_Winner_the_Best_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19955-19966.png><br>
            
            <span class="tt"><span class="t0">研究问题：国际基准竞赛在图像分析方法的比较性能评估中起着基本作用，但关于这些竞赛能带来什么学习成果的研究却鲜有关注。<br>
                    动机：本文旨在填补这一研究空白，通过对所有在IEEE ISBI 2021和MICCAI 2021上进行的80个竞赛进行多中心研究，探讨获胜解决方案优于竞争方法的原因。<br>
                    方法：通过对提交的算法及其排名的全面描述进行统计分析，揭示获胜解决方案的共同特征。<br>
                    效果：研究发现，获胜方案通常包括使用多任务学习和/或多阶段管道（各占63%和61%），并注重增强（100%）、图像预处理（97%）、数据策划（79%）和后处理（66%）。此外，团队领导通常是具有博士学位、五年生物医学图像分析经验和四年深度学习经验的计算机科学家。对于排名较高的团队，两个核心通用开发策略是：将指标反映在方法设计和专注于分析和处理失败案例。根据组织者的说法，43%的获胜算法超过了现有技术水平，但只有11%完全解决了各自的领域问题。本研究的见解可以帮助研究人员(1)改进解决新问题的算法开发策略；(2)关注这项工作揭示的开放研究问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>International benchmarking competitions have become fundamental for the comparative performance assessment of image analysis methods. However, little attention has been given to investigating what can be learnt from these competitions. Do they really generate scientific progress? What are common and successful participation strategies? What makes a solution superior to a competing method? To address this gap in the literature, we performed a multi-center study with all 80 competitions that were conducted in the scope of IEEE ISBI 2021 and MICCAI 2021. Statistical analyses performed based on comprehensive descriptions of the submitted algorithms linked to their rank as well as the underlying participation strategies revealed common characteristics of winning solutions. These typically include the use of multi-task learning (63%) and/or multi-stage pipelines (61%), and a focus on augmentation (100%), image preprocessing (97%), data curation (79%), and postprocessing (66%). The "typical" lead of a winning team is a computer scientist with a doctoral degree, five years of experience in biomedical image analysis, and four years of experience in deep learning. Two core general development strategies stood out for highly-ranked teams: the reflection of the metrics in the method design and the focus on analyzing and handling failure cases. According to the organizers, 43% of the winning algorithms exceeded the state of the art but only 11% completely solved the respective domain problem. The insights of our study could help researchers (1) improve algorithm development strategies when approaching new problems, and (2) focus on open research questions revealed by this work.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">810.Good Is Bad: Causality Inspired Cloth-Debiasing for Cloth-Changing Person Re-Identification</span><br>
                <span class="as">Yang, ZhengweiandLin, MengandZhong, XianandWu, YuandWang, Zheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1472-1481.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在传统的人员再识别（ReID）中消除服装对身份的负面影响，以实现鲁棒的换衣人员再识别（CC-ReID）。<br>
                    动机：由于缺乏理论和难以准确分离影响，消除服装对身份的负面影响仍然具有挑战性。<br>
                    方法：提出了一种基于因果关系的自动干预模型（AIM），通过分析服装对模型推理的影响并采用双分支模型模拟因果干预，逐步自动消除服装偏见。<br>
                    效果：在两个标准的CC-ReID数据集上进行的大量实验表明，所提出的AIM优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Entangled representation of clothing and identity (ID)-intrinsic clues are potentially concomitant in conventional person Re-IDentification (ReID). Nevertheless, eliminating the negative impact of clothing on ID remains challenging due to the lack of theory and the difficulty of isolating the exact implications. In this paper, a causality-based Auto-Intervention Model, referred to as AIM, is first proposed to mitigate clothing bias for robust cloth-changing person ReID (CC-ReID). Specifically, we analyze the effect of clothing on the model inference and adopt a dual-branch model to simulate causal intervention. Progressively, clothing bias is eliminated automatically with model training. AIM is encouraged to learn more discriminative ID clues that are free from clothing bias. Extensive experiments on two standard CC-ReID datasets demonstrate the superiority of the proposed AIM over other state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">811.Use Your Head: Improving Long-Tail Video Recognition</span><br>
                <span class="as">Perrett, TobyandSinha, SaptarshiandBurghardt, TiloandMirmehdi, MajidandDamen, Dima</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Perrett_Use_Your_Head_Improving_Long-Tail_Video_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2415-2425.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前视频基准测试在多个长尾属性上存在不足，特别是在尾部缺乏少数镜头类别。<br>
                    动机：为了解决这一问题，我们提出了新的视频基准测试，并开发了一种名为“长尾混合重建”（LMR）的方法。<br>
                    方法：通过从SSv2和VideoLT两个数据集中采样子集来更好地评估长尾识别。然后，我们提出一种方法，即通过将少数镜头类别的样本重建为头部类别样本的加权组合来减少对少数镜头类别实例的过拟合。最后，LMR采用标签混合来学习稳健的决策边界。<br>
                    效果：实验结果表明，LMR在EPIC-KITCHENS以及我们提出的SSv2-LT和VideoLT-LT上取得了最先进的平均类别准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents an investigation into long-tail video recognition. We demonstrate that, unlike naturally-collected video datasets and existing long-tail image benchmarks, current video benchmarks fall short on multiple long-tailed properties. Most critically, they lack few-shot classes in their tails. In response, we propose new video benchmarks that better assess long-tail recognition, by sampling subsets from two datasets: SSv2 and VideoLT. We then propose a method, Long-Tail Mixed Reconstruction (LMR), which reduces overfitting to instances from few-shot classes by reconstructing them as weighted combinations of samples from head classes. LMR then employs label mixing to learn robust decision boundaries. It achieves state-of-the-art average class accuracy on EPIC-KITCHENS and the proposed SSv2-LT and VideoLT-LT. Benchmarks and code at: github.com/tobyperrett/lmr</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">812.Beyond mAP: Towards Better Evaluation of Instance Segmentation</span><br>
                <span class="as">Jena, RohitandZhornyak, LukasandDoiphode, NehalandChaudhari, PratikandBuch, VivekandGee, JamesandShi, Jianbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jena_Beyond_mAP_Towards_Better_Evaluation_of_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11309-11318.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前实例分割的准确性评估方法存在缺陷，无法有效区分正确定位但分类错误的实例。<br>
                    动机：为了解决这一问题，我们提出了一种新的度量方法，并设计了一种基于像素占用匹配方案的模块来消除这些重复预测。<br>
                    方法：我们回顾了文献中的替代度量方法，并提出了两种新的方法来明确测量空间和类别重复预测的数量。我们还提出了一种语义排序和NMS模块来消除这些重复预测。<br>
                    效果：实验表明，现代分割网络在AP上取得了显著的增益，但也包含了相当数量的重复预测。我们的语义排序和NMS可以作为一个即插即用的模块来减轻过度预测并保持AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Correctness of instance segmentation constitutes counting the number of objects, correctly localizing all predictions and classifying each localized prediction. Average Precision is the de-facto metric used to measure all these constituents of segmentation. However, this metric does not penalize duplicate predictions in the high-recall range, and cannot distinguish instances that are localized correctly but categorized incorrectly. This weakness has inadvertently led to network designs that achieve significant gains in AP but also introduce a large number of false positives. We therefore cannot rely on AP to choose a model that provides an optimal tradeoff between false positives and high recall. To resolve this dilemma, we review alternative metrics in the literature and propose two new measures to explicitly measure the amount of both spatial and categorical duplicate predictions. We also propose a Semantic Sorting and NMS module to remove these duplicates based on a pixel occupancy matching scheme. Experiments show that modern segmentation networks have significant gains in AP, but also contain a considerable amount of duplicates. Our Semantic Sorting and NMS can be added as a plug-and-play module to mitigate hedged predictions and preserve AP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">813.Diversity-Measurable Anomaly Detection</span><br>
                <span class="as">Liu, WenruiandChang, HongandMa, BingpengandShan, ShiguangandChen, Xilin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Diversity-Measurable_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12147-12156.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决重建型异常检测模型在处理正常模式多样性不足和异常信息传递的问题。<br>
                    动机：目前的重建型异常检测模型在处理正常模式多样性和异常信息传递上存在问题，影响了其效果。<br>
                    方法：本文提出了一种可测量多样性的异常检测（DMAD）框架，设计了金字塔形变模块（PDM），通过估计从重建参考到原始输入的多尺度形变场来建模多样化的正常模式并衡量异常的严重性。<br>
                    效果：实验结果表明，该方法在监控视频和工业图像上都表现出良好的效果，并且在面对被污染的数据和类似异常的正常样本时也能同样有效工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstruction-based anomaly detection models achieve their purpose by suppressing the generalization ability for anomaly. However, diverse normal patterns are consequently not well reconstructed as well. Although some efforts have been made to alleviate this problem by modeling sample diversity, they suffer from shortcut learning due to undesired transmission of abnormal information. In this paper, to better solve the tradeoff problem, we propose Diversity-Measurable Anomaly Detection (DMAD) framework to enhance reconstruction diversity while avoid the undesired generalization on anomalies. To this end, we design Pyramid Deformation Module (PDM), which models diverse normals and measures the severity of anomaly by estimating multi-scale deformation fields from reconstructed reference to original input. Integrated with an information compression module, PDM essentially decouples deformation from prototypical embedding and makes the final anomaly score more reliable. Experimental results on both surveillance videos and industrial images demonstrate the effectiveness of our method. In addition, DMAD works equally well in front of contaminated data and anomaly-like normal samples.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">814.FreeNeRF: Improving Few-Shot Neural Rendering With Free Frequency Regularization</span><br>
                <span class="as">Yang, JiaweiandPavone, MarcoandWang, Yue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8254-8263.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用稀疏输入进行新颖视图合成是神经辐射场（NeRF）面临的挑战。<br>
                    动机：最近的一些努力通过引入外部监督，如预训练模型和额外的深度信号，或使用非平凡的基于补丁的渲染来缓解这个问题。<br>
                    方法：我们提出了频率规整NeRF（FreeNeRF），这是一个令人惊讶的简单基线，通过最小限度地修改普通的NeRF就能超越之前的方法。我们分析了少量神经渲染的关键挑战，发现频率在NeRF的训练中起着重要的作用。基于这个分析，我们提出了两种正则化项：一种用于规范NeRF输入的频率范围，另一种用于惩罚近摄像机密度场。这两种技术都是“免费午餐”，不需要额外的计算成本。<br>
                    效果：即使只改变一行代码，原始的NeRF也能在少量设置中实现与其他复杂方法相当的性能。FreeNeRF在包括Blender、DTU和LLFF在内的各种数据集上实现了最先进的性能。我们希望这个简单的基线能重新思考频率在NeRF训练中的基本作用，无论是在低数据环境下还是超出其范围之外。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel view synthesis with sparse inputs is a challenging problem for neural radiance fields (NeRF). Recent efforts alleviate this challenge by introducing external supervision, such as pre-trained models and extra depth signals, or by using non-trivial patch-based rendering. In this paper, we present Frequency regularized NeRF (FreeNeRF), a surprisingly simple baseline that outperforms previous methods with minimal modifications to plain NeRF. We analyze the key challenges in few-shot neural rendering and find that frequency plays an important role in NeRF's training. Based on this analysis, we propose two regularization terms: one to regularize the frequency range of NeRF's inputs, and the other to penalize the near-camera density fields. Both techniques are "free lunches" that come at no additional computational cost. We demonstrate that even with just one line of code change, the original NeRF can achieve similar performance to other complicated methods in the few-shot setting. FreeNeRF achieves state-of-the-art performance across diverse datasets, including Blender, DTU, and LLFF. We hope that this simple baseline will motivate a rethinking of the fundamental role of frequency in NeRF's training, under both the low-data regime and beyond. This project is released at https://jiawei-yang.github.io/FreeNeRF/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">815.VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution</span><br>
                <span class="as">Kim, JaeillandKang, SuhyunandHwang, DuhunandShin, JungwookandRhee, Wonjong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_VNE_An_Effective_Method_for_Improving_Deep_Representation_by_Manipulating_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3799-3810.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善深度学习中表示的质量，如去相关、白化、解纠缠、排名、各向同性和互信息等。<br>
                    动机：现有的表示属性操作在实施效果和通用性方面具有挑战性。<br>
                    方法：提出对表示的冯·诺依曼熵进行正则化（VNE）。首先，证明了VNE在有效操纵表示自相关矩阵的特征值方面的优越性。然后，通过研究领域泛化、元学习、自我监督学习和生成模型，证明其在改进最先进的算法或流行的基准算法方面的广泛应用性。此外，还与表示的排名、解纠缠和各向同性建立了理论联系。最后，讨论了VNE的维度控制和与香农熵的关系。<br>
                    效果：实验结果表明，VNE在各种任务上都取得了显著的改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy (VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discussions on the dimension control of VNE and the relationship with Shannon entropy. Code is available at: https://github.com/jaeill/CVPR23-VNE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">816.Divide and Adapt: Active Domain Adaptation via Customized Learning</span><br>
                <span class="as">Huang, DuojunandLi, JichangandChen, WeikaiandHuang, JunshiandChai, ZhenhuaandLi, Guanbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Divide_and_Adapt_Active_Domain_Adaptation_via_Customized_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7651-7660.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的预训练语言模型对结构化知识的利用不足，以及在领域适应中如何识别真正有价值的样本的问题。<br>
                    动机：通过结合主动学习和领域适应技术，提高模型的适应性能。<br>
                    方法：提出了一种名为Divide-and-Adapt（DiaNA）的新ADA框架，该框架将目标实例分为四类，并采用基于不确定性和领域性的新数据划分协议，以准确识别最有益的样本。<br>
                    效果：实验结果表明，DiaNA可以处理具有大范围域差距的数据，并能推广到不同的领域适应设置，如无监督领域适应（UDA）、半监督领域适应（SSDA）、源自由领域适应（SFDA）等。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Active domain adaptation (ADA) aims to improve the model adaptation performance by incorporating the active learning (AL) techniques to label a maximally-informative subset of target samples. Conventional AL methods do not consider the existence of domain shift, and hence, fail to identify the truly valuable samples in the context of domain adaptation. To accommodate active learning and domain adaption, the two naturally different tasks, in a collaborative framework, we advocate that a customized learning strategy for the target data is the key to the success of ADA solutions. We present Divide-and-Adapt (DiaNA), a new ADA framework that partitions the target instances into four categories with stratified transferable properties. With a novel data subdivision protocol based on uncertainty and domainness, DiaNA can accurately recognize the most gainful samples. While sending the informative instances for annotation, DiaNA employs tailored learning strategies for the remaining categories. Furthermore, we propose an informativeness score that unifies the data partitioning criteria. This enables the use of a Gaussian mixture model (GMM) to automatically sample unlabeled data into the proposed four categories. Thanks to the "divide-and-adapt" spirit, DiaNA can handle data with large variations of domain gap. In addition, we show that DiaNA can generalize to different domain adaptation settings, such as unsupervised domain adaptation (UDA), semi-supervised domain adaptation (SSDA), source-free domain adaptation (SFDA), etc.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">817.Towards Bridging the Performance Gaps of Joint Energy-Based Models</span><br>
                <span class="as">Yang, XiulongandSu, QingandJi, Shihao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Towards_Bridging_the_Performance_Gaps_of_Joint_Energy-Based_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15732-15741.png><br>
            
            <span class="tt"><span class="t0">研究问题：我们能否用一个单一的网络训练出混合判别式-生成式模型？<br>
                    动机：尽管最近取得了进展，但联合能量模型（JEM）在分类准确性和图像生成质量上仍存在两个性能差距。<br>
                    方法：我们引入了多种训练技术来填补JEM的准确性差距和生成质量差距。包括1) 将最近提出的锐度感知最小化（SAM）框架融入JEM的训练中，以提升JEM的能量景观平滑性和泛化能力；2) 从JEM的最大似然估计管道中排除数据增强，并减轻数据增强对图像生成质量的负面影响。<br>
                    效果：我们在多个数据集上的大量实验表明，我们的SADA-JEM实现了最先进的性能，并在图像分类、图像生成、校准、分布外检测和对抗鲁棒性方面以显著的优势超越了JEM。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Can we train a hybrid discriminative-generative model with a single network? This question has recently been answered in the affirmative, introducing the field of Joint Energy-based Model (JEM), which achieves high classification accuracy and image generation quality simultaneously. Despite recent advances, there remain two performance gaps: the accuracy gap to the standard softmax classifier, and the generation quality gap to state-of-the-art generative models. In this paper, we introduce a variety of training techniques to bridge the accuracy gap and the generation quality gap of JEM. 1) We incorporate a recently proposed sharpness-aware minimization (SAM) framework to train JEM, which promotes the energy landscape smoothness and the generalization of JEM. 2) We exclude data augmentation from the maximum likelihood estimate pipeline of JEM, and mitigate the negative impact of data augmentation to image generation quality. Extensive experiments on multiple datasets demonstrate our SADA-JEM achieves state-of-the-art performances and outperforms JEM in image classification, image generation, calibration, out-of-distribution detection and adversarial robustness by a notable margin. Our code is available at https://github.com/sndnyang/SADAJEM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">818.Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation</span><br>
                <span class="as">Zheng, XuandZhu, JinjingandLiu, YexinandCao, ZidongandFu, ChongandWang, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1285-1295.png><br>
            
            <span class="tt"><span class="t0">研究问题：全景图像语义分割的性能受到等距投影（ERP）扭曲和像素级注释缺乏的影响。<br>
                    动机：现有的方法将ERP和针孔图像视为同等，并通过无监督领域适应（UDA）从针孔图像向ERP图像转移知识，但这种方法无法处理由1)相机传感器和捕获场景的内在差异；2)不同的图像格式（如ERP和针孔图像）引起的域差距。<br>
                    方法：本文提出了一种新的灵活的双路径UDA框架DPPASS，将ERP和切线投影（TP）图像作为输入。为了减小域差距，我们提出了跨投影和 intra-projection 训练。跨投影训练包括tangent-wise特征对比训练和预测一致性训练。<br>
                    效果：实验结果表明，我们的DPPASS在两个基准测试上比最先进的方法提高了+1.06%的mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability of scene understanding has sparked active research for panoramic image semantic segmentation. However, the performance is hampered by distortion of the equirectangular projection (ERP) and a lack of pixel-wise annotations. For this reason, some works treat the ERP and pinhole images equally and transfer knowledge from the pinhole to ERP images via unsupervised domain adaptation (UDA). However, they fail to handle the domain gaps caused by: 1) the inherent differences between camera sensors and captured scenes; 2) the distinct image formats (e.g., ERP and pinhole images). In this paper, we propose a novel yet flexible dual-path UDA framework, DPPASS, taking ERP and tangent projection (TP) images as inputs. To reduce the domain gaps, we propose cross-projection and intra-projection training. The cross-projection training includes tangent-wise feature contrastive training and prediction consistency training. That is, the former formulates the features with the same projection locations as positive examples and vice versa, for the models' awareness of distortion, while the latter ensures the consistency of cross-model predictions between the ERP and TP. Moreover, adversarial intra-projection training is proposed to reduce the inherent gap, between the features of the pinhole images and those of the ERP and TP images, respectively. Importantly, the TP path can be freely removed after training, leading to no additional inference cost. Extensive experiments on two benchmarks show that our DPPASS achieves +1.06% mIoU increment than the state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">819.Learning Debiased Representations via Conditional Attribute Interpolation</span><br>
                <span class="as">Zhang, Yi-KaiandWang, Qi-WeiandZhan, De-ChuanandYe, Han-Jia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Debiased_Representations_via_Conditional_Attribute_Interpolation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7599-7608.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善深度神经网络在有偏数据集上的表现，防止其过度依赖与目标标签无关的属性进行预测。<br>
                    动机：当数据集存在偏差，即大部分样本的属性与目标标签存在虚假相关性时，深度神经网络容易通过“非预期”的属性进行预测，特别是当这些属性更容易学习时。<br>
                    方法：提出一种chi^2模型来学习无偏表示。首先设计一个chi形状模式以匹配深度神经网络的训练动态，并找到中间属性样本（IASs）——即接近属性决策边界的样本，它们表明一个属性的值从一个极端到另一个极端的变化。然后使用chi结构的度量学习目标对表示进行修正。条件插值在IASs之间消除了边缘属性的负面影响，有助于保持类内的紧凑性。<br>
                    效果：实验表明，chi^2模型有效地学习了无偏表示，并在各种数据集上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>An image is usually described by more than one attribute like "shape" and "color". When a dataset is biased, i.e., most samples have attributes spuriously correlated with the target label, a Deep Neural Network (DNN) is prone to make predictions by the "unintended" attribute, especially if it is easier to learn. To improve the generalization ability when training on such a biased dataset, we propose a chi^2-model to learn debiased representations. First, we design a chi-shape pattern to match the training dynamics of a DNN and find Intermediate Attribute Samples (IASs) --- samples near the attribute decision boundaries, which indicate how the value of an attribute changes from one extreme to another. Then we rectify the representation with a chi-structured metric learning objective. Conditional interpolation among IASs eliminates the negative effect of peripheral attributes and facilitates retaining the intra-class compactness. Experiments show that chi^2-model learns debiased representation effectively and achieves remarkable improvements on various datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">820.Modeling Inter-Class and Intra-Class Constraints in Novel Class Discovery</span><br>
                <span class="as">Li, WenbinandFan, ZhichenandHuo, JingandGao, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Modeling_Inter-Class_and_Intra-Class_Constraints_in_Novel_Class_Discovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3449-3458.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的类发现方法没有充分利用类发现设置的本质。<br>
                    动机：提出一种新的模型，通过在未标记的数据集中发现新的类别（簇），将一个类别不相交的标记数据集的公共知识转移到另一个未标记的数据集中。<br>
                    方法：基于对称Kullback-Leibler散度（sKLD）在类发现中对类间和类内约束进行建模。提出了一种类间sKLD约束，以有效利用标记和未标记类别之间的不相交关系，并在嵌入空间中强制不同类别的可分离性。此外，还提出了一种类内sKLD约束，以明确约束样本与其增强之间的关系，同时确保训练过程的稳定性。<br>
                    效果：在流行的CIFAR10、CIFAR100和ImageNet基准上进行了广泛的实验，成功地证明了该方法可以建立新的最先进的状态，并可以实现显著的性能改进。例如，在CIFAR100-50数据集分割下的任务感知/无关评估协议中，比之前最先进的方法提高了3.5%/3.7%的聚类准确率。代码可在https://github.com/FanZhichen/NCD-IIC获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel class discovery (NCD) aims at learning a model that transfers the common knowledge from a class-disjoint labelled dataset to another unlabelled dataset and discovers new classes (clusters) within it. Many methods, as well as elaborate training pipelines and appropriate objectives, have been proposed and considerably boosted performance on NCD tasks. Despite all this, we find that the existing methods do not sufficiently take advantage of the essence of the NCD setting. To this end, in this paper, we propose to model both inter-class and intra-class constraints in NCD based on the symmetric Kullback-Leibler divergence (sKLD). Specifically, we propose an inter-class sKLD constraint to effectively exploit the disjoint relationship between labelled and unlabelled classes, enforcing the separability for different classes in the embedding space. In addition, we present an intra-class sKLD constraint to explicitly constrain the intra-relationship between a sample and its augmentations and ensure the stability of the training process at the same time. We conduct extensive experiments on the popular CIFAR10, CIFAR100 and ImageNet benchmarks and successfully demonstrate that our method can establish a new state of the art and can achieve significant performance improvements, e.g., 3.5%/3.7% clustering accuracy improvements on CIFAR100-50 dataset split under the task-aware/-agnostic evaluation protocol, over previous state-of-the-art methods. Code is available at https://github.com/FanZhichen/NCD-IIC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">821.Bayesian Posterior Approximation With Stochastic Ensembles</span><br>
                <span class="as">Balabanov, OleksandrandMehlig, BernhardandLinander, Hampus</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Balabanov_Bayesian_Posterior_Approximation_With_Stochastic_Ensembles_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13701-13711.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用随机神经网络近似贝叶斯后验，并结合随机方法如dropout和深度集成。<br>
                    动机：目前的贝叶斯推理方法需要大量计算资源，而随机神经网络可以更高效地进行近似。<br>
                    方法：提出基于蒙特卡洛dropout、DropConnect和非参数化dropout的随机集成方法，并通过变分推断训练它们来近似贝叶斯后验。<br>
                    效果：在玩具问题和CIFAR图像分类任务上，随机集成方法比其他流行基线提供了更准确的后验估计。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">822.Modality-Agnostic Debiasing for Single Domain Generalization</span><br>
                <span class="as">Qu, SanqingandPan, YingweiandChen, GuangandYao, TingandJiang, ChangjunandMei, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Modality-Agnostic_Debiasing_for_Single_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24142-24151.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在分布外（OOD）数据上通常泛化能力不佳，特别是在从单个领域到多个未见过领域的单域泛化（single-DG）的极端情况下。<br>
                    动机：现有的单域泛化技术通常设计各种数据增强算法，并重塑多源领域泛化方法以学习领域通用（语义）特征。然而，这些方法通常是模态特定的，因此仅适用于一种单一模态（例如图像）。<br>
                    方法：我们针对一种通用的模态无关去偏（MAD）框架进行单域泛化，该框架能够实现不同模态的泛化。技术上，MAD引入了一种新的双分支分类器：一个有偏分支鼓励分类器识别特定于领域的（表面的）特征，而一个通用分支则基于有偏分支的知识捕获领域通用的特征。<br>
                    效果：我们在各种不同模态的单域泛化场景中验证了MAD的优越性，包括对一维文本、二维图像、三维点云的识别，以及对二维图像的语义分割。更值得注意的是，对于三维点云的识别和二维图像的语义分割，MAD将准确率和mIOU分别提高了2.82%和1.5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) usually fail to generalize well to outside of distribution (OOD) data, especially in the extreme case of single domain generalization (single-DG) that transfers DNNs from single domain to multiple unseen domains. Existing single-DG techniques commonly devise various data-augmentation algorithms, and remould the multi-source domain generalization methodology to learn domain-generalized (semantic) features. Nevertheless, these methods are typically modality-specific, thereby being only applicable to one single modality (e.g., image). In contrast, we target a versatile Modality-Agnostic Debiasing (MAD) framework for single-DG, that enables generalization for different modalities. Technically, MAD introduces a novel two-branch classifier: a biased-branch encourages the classifier to identify the domain-specific (superficial) features, and a general-branch captures domain-generalized features based on the knowledge from biased-branch. Our MAD is appealing in view that it is pluggable to most single-DG models. We validate the superiority of our MAD in a variety of single-DG scenarios with different modalities, including recognition on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. More remarkably, for recognition on 3D point clouds and semantic segmentation on 2D images, MAD improves DSU by 2.82% and 1.5% in accuracy and mIOU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">823.Difficulty-Based Sampling for Debiased Contrastive Representation Learning</span><br>
                <span class="as">Jang, TaeukandWang, Xiaoqian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jang_Difficulty-Based_Sampling_for_Debiased_Contrastive_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24039-24048.png><br>
            
            <span class="tt"><span class="t0">研究问题：对比学习是一种自我监督表示学习方法，在各种分类任务中取得了里程碑式的性能。然而，由于其无监督的方式，它遭受了假负样本问题的影响。<br>
                    动机：假负样本问题会降低对比学习的性能，因为它与对比语义相似和不相似的对的动机相矛盾。因此，找到合法的负样本并区分真假负样本以及易难负样本的重要性引起了人们的关注。<br>
                    方法：本文提出了一种去偏对比学习方法，通过参考放大偏差的对应部分来探索难负样本。我们提出了三元组损失来训练一个偏向编码器，该编码器更关注易负样本。<br>
                    效果：理论上，我们证明了三元组损失会放大自我监督表示学习中的偏差。最后，我们通过实证表明，所提出的方法提高了下游分类性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive learning is a self-supervised representation learning method that achieves milestone performance in various classification tasks. However, due to its unsupervised fashion, it suffers from the false negative sample problem: randomly drawn negative samples that are assumed to have a different label but actually have the same label as the anchor. This deteriorates the performance of contrastive learning as it contradicts the motivation of contrasting semantically similar and dissimilar pairs. This raised the attention and the importance of finding legitimate negative samples, which should be addressed by distinguishing between 1) true vs. false negatives; 2) easy vs. hard negatives. However, previous works were limited to the statistical approach to handle false negative and hard negative samples with hyperparameters tuning. In this paper, we go beyond the statistical approach and explore the connection between hard negative samples and data bias. We introduce a novel debiased contrastive learning method to explore hard negatives by relative difficulty referencing the bias-amplifying counterpart. We propose triplet loss for training a biased encoder that focuses more on easy negative samples. We theoretically show that the triplet loss amplifies the bias in self-supervised representation learning. Finally, we empirically show the proposed method improves downstream classification performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">824.Zero-Shot Model Diagnosis</span><br>
                <span class="as">Luo, JinqiandWang, ZhaoningandWu, ChenHenryandHuang, DongandDelaTorre, Fernando</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Zero-Shot_Model_Diagnosis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11631-11640.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何评估深度视觉模型对任意视觉属性的敏感性，而无需使用标注的测试集？<br>
                    动机：创建平衡的测试集既耗时又昂贵，且容易出错。因此，我们需要一种不需要测试集或标签的方法来评估深度学习模型的敏感性。<br>
                    方法：本文提出了一种名为Zero-shot Model Diagnosis（ZOOM）的方法，该方法不依赖测试集和标签。通过使用生成模型和CLIP，用户可以选择一个与问题相关的提示集，系统将自动搜索语义反事实图像（即在二元分类器的情况下翻转预测的合成图像）。<br>
                    效果：通过在多个视觉领域中进行多种视觉任务（分类、关键点检测和分割）的评估，实验表明该方法能够生成反事实图像，并在无需测试集的情况下提供模型敏感性分析。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When it comes to deploying deep vision models, the behavior of these systems must be explicable to ensure confidence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it performs. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is often time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensitivity of deep learning models to arbitrary visual attributes without an annotated test set? This paper argues the case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the problem) and our system will automatically search for semantic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classification, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our methodology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">825.Re-Thinking Federated Active Learning Based on Inter-Class Diversity</span><br>
                <span class="as">Kim, SangMookandBae, SangminandSong, HwanjunandYun, Se-Young</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Re-Thinking_Federated_Active_Learning_Based_on_Inter-Class_Diversity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3944-3953.png><br>
            
            <span class="tt"><span class="t0">研究问题：在联邦学习中，如何有效利用大量未标注的数据？<br>
                    动机：尽管联邦学习取得了显著的进步，但大多数研究都假设客户端的数据是完全标注的。然而，在现实世界中，每个客户端可能都有大量的未标注实例。<br>
                    方法：提出了一种联邦主动学习框架，并探讨了全局和局部仅模型的性能优势及其原因。基于这些发现，我们提出了LoGo，这是一种能够适应不同局部异质性和全局不平衡比例的FAL采样策略，通过两步主动选择方案整合了这两种模型。<br>
                    效果：实验结果表明，LoGo在38个实验设置中始终优于其他六种主动学习策略。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although federated learning has made awe-inspiring advances, most studies have assumed that the client's data are fully labeled. However, in a real-world scenario, every client may have a significant amount of unlabeled instances. Among the various approaches to utilizing unlabeled data, a federated active learning framework has emerged as a promising solution. In the decentralized setting, there are two types of available query selector models, namely 'global' and 'local-only' models, but little literature discusses their performance dominance and its causes. In this work, we first demonstrate that the superiority of two selector models depends on the global and local inter-class diversity. Furthermore, we observe that the global and local-only models are the keys to resolving the imbalance of each side. Based on our findings, we propose LoGo, a FAL sampling strategy robust to varying local heterogeneity levels and global imbalance ratio, that integrates both models by two steps of active selection scheme. LoGo consistently outperforms six active learning strategies in the total number of 38 experimental settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">826.Out-of-Distributed Semantic Pruning for Robust Semi-Supervised Learning</span><br>
                <span class="as">Wang, YuandQiao, PengchongandLiu, ChangandSong, GuoliandZheng, XiawuandChen, Jie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Out-of-Distributed_Semantic_Pruning_for_Robust_Semi-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23849-23858.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有鲁棒半监督学习方法在语义层面存在OOD信息污染的问题。<br>
                    动机：当前的方法主要在样本层面过滤掉OOD信息，但在语义层面的OOD信息污染问题尚未得到充分关注，限制了该领域的发展。<br>
                    方法：提出一种名为OOD语义剪枝（OSP）的统一框架，通过将ID特征与具有语义重叠的OOD样本配对，并设计软正交性正则化来抑制ID特征中与配对的OOD样本语义分量共线的语义成分，从而剪除OOD语义。<br>
                    效果：在TinyImageNet数据集上，OSP在ID分类和OOD检测任务上的表现均优于先前最先进的方法，准确率提高了13.7%，AUROC提高了5.9%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in robust semi-supervised learning (SSL) typical filters out-of-distribution (OOD) information at the sample level. We argue that an overlooked problem of robust SSL is its corrupted information on semantic level, practically limiting the development of the field. In this paper, we take an initiative step to explore and propose a unified framework termed as OOD Semantic Pruning (OSP), aims at pruning OOD semantics out from the in-distribution (ID) features. Specifically, (i) we propose an aliasing OOD matching module to pair each ID sample with an OOD sample with semantic overlap. (ii) We design a soft orthogonality regularization, which first transforms each ID feature by suppressing its semantic component that is collinear with paired OOD sample. It then forces the predictions before and after soft orthogonality transformation to be consistent. Being practically simple, our method shows a strong performance in OOD detection and ID classification on challenging benchmarks. In particular, OSP surpasses the previous state-of-the-art by 13.7% on accuracy for ID classification and 5.9% on AUROC for OOD detection on TinyImageNet dataset. Codes are available in the supplementary material.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">827.Understanding and Improving Visual Prompting: A Label-Mapping Perspective</span><br>
                <span class="as">Chen, AochuanandYao, YuguangandChen, Pin-YuandZhang, YihuaandLiu, Sijia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19133-19143.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探讨视觉提示（VP）与标签映射（LM）之间的关系，以及如何利用这种关系来提高VP在目标任务上的准确性。<br>
                    动机：尽管视觉提示（VP）是一种有效的预训练源模型重编程技术，但其在目标任务上的有效性仍然取决于源类别和目标类别之间的无规则标签映射（LM）。因此，本文试图理解LM如何影响VP，并探索如何优化LM以提高VP的效果。<br>
                    方法：本文提出了一种新的VP框架，称为ILM-VP（基于迭代标签映射的视觉提示），该框架自动重新映射源标签到目标标签，并逐步提高VP在目标任务上的准确性。此外，当使用对比性语言-图像预训练（CLIP）模型时，本文还提出了一种集成LM过程的方法，以帮助选择CLIP的文本提示并提高目标任务的准确性。<br>
                    效果：大量实验表明，本文提出的方法显著优于现有的VP方法。例如，当将ImageNet预训练的ResNet-18重编程为13个目标任务时，该方法在迁移学习到目标花102和CIFAR100数据集上分别提高了7.9%和6.7%的准确性。此外，本文在CLIP基础上的VP建议在花102和DTD上分别提高了13.7%和7.1%的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We revisit and advance visual prompting (VP), an input prompting technique for vision tasks. VP can reprogram a fixed, pre-trained source model to accomplish downstream tasks in the target domain by simply incorporating universal prompts (in terms of input perturbation patterns) into downstream data points. Yet, it remains elusive why VP stays effective even given a ruleless label mapping (LM) between the source classes and the target classes. Inspired by the above, we ask: How is LM interrelated with VP? And how to exploit such a relationship to improve its accuracy on target tasks? We peer into the influence of LM on VP and provide an affirmative answer that a better 'quality' of LM (assessed by mapping precision and explanation) can consistently improve the effectiveness of VP. This is in contrast to the prior art where the factor of LM was missing. To optimize LM, we propose a new VP framework, termed ILM-VP (iterative label mapping-based visual prompting), which automatically re-maps the source labels to the target labels and progressively improves the target task accuracy of VP. Further, when using a contrastive language-image pretrained (CLIP) model, we propose to integrate an LM process to assist the text prompt selection of CLIP and to improve the target task accuracy. Extensive experiments demonstrate that our proposal significantly outperforms state-of-the-art VP methods. As highlighted below, we show that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target tasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and 6.7% accuracy improvements in transfer learning to the target Flowers102 and CIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and 7.1% accuracy improvements on Flowers102 and DTD respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">828.Understanding Deep Generative Models With Generalized Empirical Likelihoods</span><br>
                <span class="as">Ravuri, SumanandRey, M\&#x27;elanieandMohamed, ShakirandDeisenroth, MarcPeter</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ravuri_Understanding_Deep_Generative_Models_With_Generalized_Empirical_Likelihoods_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24395-24405.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确评估深度生成模型对高维数据分布的捕捉能力，特别是在GANs和扩散模型等模型无法计算精确似然的情况下。<br>
                    动机：当前对于深度生成模型（DGMs）的评估方法存在诸多不足，需要一种能够全面诊断模型缺陷的工具。<br>
                    方法：提出了广义经验似然（GEL）方法，通过设定合适的矩条件，可以识别出哪些模式被丢弃、模型的模式失衡程度以及模型是否充分捕捉了类内多样性。<br>
                    效果：结合最大均值差异和广义经验似然的技术，创建了具有样本解释性的分布测试，同时也包括了标签信息的度量。实验表明，这种方法在预测模式丢弃和模式失衡程度上比改进的精度/召回率等指标提高了60%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dropping and mode imbalance up to 60% better than metrics such as improved precision/recall.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">829.Weakly-Supervised Domain Adaptive Semantic Segmentation With Prototypical Contrastive Learning</span><br>
                <span class="as">Das, AnuragandXian, YongqinandDai, DengxinandSchiele, Bernt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Weakly-Supervised_Domain_Adaptive_Semantic_Segmentation_With_Prototypical_Contrastive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15434-15443.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过使用来自目标领域的不同弱标签（如图像、点和粗略标签）来减小无监督领域适应在语义分割任务中的性能差距。<br>
                    动机：尽管在改善无监督领域适应的语义分割任务性能方面已经做了很多努力，但与有监督学习相比，其性能仍有巨大差距。<br>
                    方法：提出了一个通用框架，利用这些弱标签来学习更具代表性的类别特征原型，并通过对比对齐类别特征来进行改进。具体来说，我们执行了两种不同的特征对齐，首先，我们在每个域内将像素特征与原型进行对齐；其次，我们以非对称方式将源域的像素特征与目标域的原型进行对齐。这种非对称对齐在训练过程中保留了目标特征，当弱标签来自目标领域时，这是至关重要的。<br>
                    效果：在标准基准测试上进行的实验表明，我们的框架与现有工作相比取得了显著的改进，并能够减小与有监督学习的性能差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>There has been a lot of effort in improving the performance of unsupervised domain adaptation for semantic segmentation task, however there is still a huge gap in performance when compared with supervised learning. In this work, we propose a common framework to use different weak labels, e.g. image, point and coarse labels from target domain to reduce this performance gap. Specifically, we propose to learn better prototypes that are representative class features, by exploiting these weak labels. We use these improved prototypes for contrastive alignment of class features. In particular, we perform two different feature alignments, first, we align pixel features with prototypes within each domain and second, we align pixel features from source to prototype of target domain in an asymmetric way. This asymmetric alignment is beneficial as it preserves the target features during training, which is essential when weak labels are available from target domain. Our experiments on standard benchmarks shows that our framework achieves significant improvement compared to existing works and is able to reduce the performance gap with supervised learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">830.Neuro-Modulated Hebbian Learning for Fully Test-Time Adaptation</span><br>
                <span class="as">Tang, YushunandZhang, CeandXu, HengandChen, ShuoshuoandCheng, JieandLeng, LuziweiandGuo, QinghaiandHe, Zhihai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Neuro-Modulated_Hebbian_Learning_for_Fully_Test-Time_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3728-3738.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度神经网络在跨领域性能下降的问题。<br>
                    动机：借鉴生物学习理论，设计了一种基于前馈学习的软Hebbian学习过程，以实现完全的测试时适应。<br>
                    方法：通过引入反馈神经调制层，改进了前馈Hebbian学习的性能，形成了神经调制Hebbian学习方法。<br>
                    效果：实验结果表明，该方法能显著提高网络模型的适应性能，优于现有的最先进方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fully test-time adaptation aims to adapt the network model based on sequential analysis of input samples during the inference stage to address the cross-domain performance degradation problem of deep neural networks. We take inspiration from the biological plausibility learning where the neuron responses are tuned based on a local synapse-change procedure and activated by competitive lateral inhibition rules. Based on these feed-forward learning rules, we design a soft Hebbian learning process which provides an unsupervised and effective mechanism for online adaptation. We observe that the performance of this feed-forward Hebbian learning for fully test-time adaptation can be significantly improved by incorporating a feedback neuro-modulation layer. It is able to fine-tune the neuron responses based on the external feedback generated by the error back-propagation from the top inference layers. This leads to our proposed neuro-modulated Hebbian learning (NHL) method for fully test-time adaptation. With the unsupervised feed-forward soft Hebbian learning being combined with a learned neuro-modulator to capture feedback from external responses, the source model can be effectively adapted during the testing process. Experimental results on benchmark datasets demonstrate that our proposed method can significantly improve the adaptation performance of network models and outperforms existing state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">831.Label Information Bottleneck for Label Enhancement</span><br>
                <span class="as">Zheng, QinghaiandZhu, JihuaandTang, Haoyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Label_Information_Bottleneck_for_Label_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7497-7506.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决标签增强（Label Enhancement，LE）问题，即如何从逻辑标签中精确恢复标签分布。<br>
                    动机：在恢复标签分布的过程中，数据集中的标签无关信息可能导致恢复性能不佳。为了解决这个问题，我们尝试挖掘关键的标签相关信息以提高恢复性能。<br>
                    方法：我们提出了一种新的标签信息瓶颈（Label Information Bottleneck，LIB）方法。该方法将LE问题分为两个联合过程：1) 学习包含关键标签相关信息的表示；2) 根据学到的表示恢复标签分布。我们可以基于学到的表示形成的“瓶颈”来挖掘标签相关信息。<br>
                    效果：我们在几个基准标签分布学习数据集上进行的评估实验验证了LIB方法的有效性和竞争力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we focus on the challenging problem of Label Enhancement (LE), which aims to exactly recover label distributions from logical labels, and present a novel Label Information Bottleneck (LIB) method for LE. For the recovery process of label distributions, the label irrelevant information contained in the dataset may lead to unsatisfactory recovery performance. To address this limitation, we make efforts to excavate the essential label relevant information to improve the recovery performance. Our method formulates the LE problem as the following two joint processes: 1) learning the representation with the essential label relevant information, 2) recovering label distributions based on the learned representation. The label relevant information can be excavated based on the "bottleneck" formed by the learned representation. Significantly, both the label relevant information about the label assignments and the label relevant information about the label gaps can be explored in our method. Evaluation experiments conducted on several benchmark label distribution learning datasets verify the effectiveness and competitiveness of LIB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">832.Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-World</span><br>
                <span class="as">Gan, YuluandPan, MingjieandZhang, RongyuandLing, ZijianandZhao, LingranandLiu, JiamingandZhang, Shanghang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Cloud-Device_Collaborative_Adaptation_to_Continual_Changing_Environments_in_the_Real-World_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12157-12166.png><br>
            
            <span class="tt"><span class="t0">研究问题：在现实世界中，面对不断变化的环境，客户端设备的轻量级模型在分布变化下性能严重下降。<br>
                    动机：现有的设备模型的主要限制在于（1）由于设备的计算限制无法更新，（2）轻量级模型的泛化能力有限。同时，最近的大模型在云上显示出强大的泛化能力，但由于计算约束无法部署在客户端设备上。<br>
                    方法：我们提出了一种新的学习范式——云-设备协作持续适应，以使设备模型能够应对不断变化的环境。在这种范式中，我们提出了一种基于不确定性的视觉提示适应（U-VPA）的教师-学生模型，以鼓励云和设备之间的协作并提高设备模型的泛化能力。<br>
                    效果：我们在两个持续变化的环境中的对象检测数据集上进行了广泛的实验。我们的U-VPA教师-学生框架优于先前最先进的测试时间适应和设备-云协作方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When facing changing environments in the real world, the lightweight model on client devices suffer from severe performance drop under distribution shifts. The main limitations of existing device model lie in: (1) unable to update due to the computation limit of the device, (2) limited generalization ability of the lightweight model. Meanwhile, recent large models have shown strong generalization capability on cloud while they can not be deployed on client devices due to the poor computation constraint. To enable the device model to deal with changing environments, we propose a new learning paradigm of Cloud-Device Collaborative Continual Adaptation. To encourage collaboration between cloud and device and improve the generalization of device model, we propose an Uncertainty-based Visual Prompt Adapted (U-VPA) teacher-student model in such paradigm. Specifically, we first design the Uncertainty Guided Sampling (UGS) to screen out challenging data continuously and transmit the most out-of-distribution samples from the device to the cloud. To further transfer the generalization capability of the large model on the cloud to the device model, we propose a Visual Prompt Learning Strategy with Uncertainty guided updating (VPLU) to specifically deal with the selected samples with more distribution shifts. Then, we transmit the visual prompts to the device and concatenate them with the incoming data to pull the device testing distribution closer to the cloud training distribution. We conduct extensive experiments on two object detection datasets with continually changing environments. Our proposed U-VPA teacher-student framework outperforms previous state-of-the-art test time adaptation and device-cloud collaboration methods. The code and datasets will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">833.Ingredient-Oriented Multi-Degradation Learning for Image Restoration</span><br>
                <span class="as">Zhang, JinghaoandHuang, JieandYao, MingdeandYang, ZizhengandYu, HuandZhou, ManandZhao, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5825-5835.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用图像恢复任务之间的关系，挖掘退化的内在成分。<br>
                    动机：现有的一体化方法在处理多种图像退化时，往往忽视了任务之间的关联性，导致可扩展性差。<br>
                    方法：提出一种新颖的、以成分为导向的退化重构框架（IDR），包括任务导向的知识收集和成分导向的知识整合两个阶段。第一阶段根据物理原理对不同的退化进行特设操作，并为每种类型的退化建立相应的先验枢纽。第二阶段通过学习主成分分析逐步将前一阶段的任务导向枢纽转化为单一成分导向枢纽，并采用动态路由机制进行概率未知退化消除。<br>
                    效果：实验证明，该方法在各种图像恢复任务上具有有效性和可扩展性，且对未知下游任务具有良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning to leverage the relationship among diverse image restoration tasks is quite beneficial for unraveling the intrinsic ingredients behind the degradation. Recent years have witnessed the flourish of various All-in-one methods, which handle multiple image degradations within a single model. In practice, however, few attempts have been made to excavate task correlations in that exploring the underlying fundamental ingredients of various image degradations, resulting in poor scalability as more tasks are involved. In this paper, we propose a novel perspective to delve into the degradation via an ingredients-oriented rather than previous task-oriented manner for scalable learning. Specifically, our method, named Ingredients-oriented Degradation Reformulation framework (IDR), consists of two stages, namely task-oriented knowledge collection and ingredients-oriented knowledge integration. In the first stage, we conduct ad hoc operations on different degradations according to the underlying physics principles, and establish the corresponding prior hubs for each type of degradation. While the second stage progressively reformulates the preceding task-oriented hubs into single ingredients-oriented hub via learnable Principal Component Analysis (PCA), and employs a dynamic routing mechanism for probabilistic unknown degradation removal. Extensive experiments on various image restoration tasks demonstrate the effectiveness and scalability of our method. More importantly, our IDR exhibits the favorable generalization ability to unknown downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">834.How To Prevent the Continuous Damage of Noises To Model Training?</span><br>
                <span class="as">Yu, XiaotianandJiang, YangandShi, TianqiandFeng, ZunleiandWang, YuexuanandSong, MingliandSun, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_How_To_Prevent_the_Continuous_Damage_of_Noises_To_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12054-12063.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习中存在噪声标签的问题在许多情况下是挑战性的，且不可避免。<br>
                    动机：现有的方法通过降低不确定样本的损失权重或过滤潜在的噪声样本来减少噪声样本的影响，但这高度依赖于模型识别噪声样本的优越判别能力。然而，训练阶段的模型是不完美的，会错过许多噪声样本，从而对模型训练造成持续的伤害。<br>
                    方法：本文提出了一种梯度切换策略（GSS），通过将每个样本的当前梯度方向切换到从包含所有类别梯度方向的新梯度方向池中选择的新方向，来防止噪声样本对分类器的持续伤害。<br>
                    效果：实验表明，使用GSS训练的模型可以达到与使用干净数据训练的模型相当的性能。此外，提出的GSS可以插入到现有的噪声标签学习框架中。这可以为未来的噪声标签学习提供新的视角。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning with noisy labels is challenging and inevitable in many circumstances. Existing methods reduce the impact of noise samples by reducing loss weights of uncertain samples or by filtering out potential noise samples, which highly rely on the model's superior discriminative power for identifying noise samples. However, in the training stage, the trainee model is imperfect will miss many noise samples, which cause continuous damage to the model training. Consequently, there is a large performance gap between existing anti-noise models trained with noisy samples and models trained with clean samples. In this paper, we put forward a Gradient Switching Strategy (GSS) to prevent the continuous damage of noise samples to the classifier. Theoretical analysis shows that the damage comes from the misleading gradient direction computed from the noise samples. The trainee model will deviate from the correct optimization direction under the influence of the accumulated misleading gradient of noise samples. To address this problem, the proposed GSS alleviates the damage by switching the current gradient direction of each sample to a new direction selected from a gradient direction pool, which contains all-class gradient directions with different probabilities. During training, the trainee model is optimized along switched gradient directions generated by GSS, which assigns higher probabilities to potential principal directions for high-confidence samples. Conversely, uncertain samples have a relatively uniform probability distribution for all gradient directions, which can cancel out the misleading gradient directions. Extensive experiments show that a model trained with GSS can achieve comparable performance with a model trained with clean data. Moreover, the proposed GSS is pluggable for existing frameworks for noisy-label learning. This work can provide a new perspective for future noisy-label learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">835.ActMAD: Activation Matching To Align Distributions for Test-Time-Training</span><br>
                <span class="as">Mirza, MuhammadJehanzebandSoneira, PolJan\&#x27;eandLin, WeiandKozinski, MateuszandPossegger, HorstandBischof, Horst</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mirza_ActMAD_Activation_Matching_To_Align_Distributions_for_Test-Time-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24152-24161.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过在测试时训练（TTT）来应对分布外（OOD）数据，并适应测试时发生的分布变化。<br>
                    动机：现有的方法主要对特征提取器最内层的所有通道的分布进行建模，而我们的方法旨在更精细地对网络中各层的每个特征的分布进行建模。<br>
                    方法：我们提出了一种名为激活匹配（ActMAD）的方法，通过分析模型的激活情况并将OOD测试数据的激活统计与训练数据的激活统计进行对齐来进行模型调整。<br>
                    效果：实验结果表明，ActMAD在CIFAR-100C和Imagenet-C上取得了最先进的性能，并且在KITTI-trained物体检测器在KITTI-Fog上的评估中比之前的方法提高了15.4%。此外，ActMAD可以应用于在线适应现实场景，且只需少量数据即可达到其全部性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test-Time-Training (TTT) is an approach to cope with out-of-distribution (OOD) data by adapting a trained model to distribution shifts occurring at test-time. We propose to perform this adaptation via Activation Matching (ActMAD): We analyze activations of the model and align activation statistics of the OOD test data to those of the training data. In contrast to existing methods, which model the distribution of entire channels in the ultimate layer of the feature extractor, we model the distribution of each feature in multiple layers across the network. This results in a more fine-grained supervision and makes ActMAD attain state of the art performance on CIFAR-100C and Imagenet-C. ActMAD is also architecture- and task-agnostic, which lets us go beyond image classification, and score 15.4% improvement over previous approaches when evaluating a KITTI-trained object detector on KITTI-Fog. Our experiments highlight that ActMAD can be applied to online adaptation in realistic scenarios, requiring little data to attain its full performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">836.Guided Recommendation for Model Fine-Tuning</span><br>
                <span class="as">Li, HaoandFowlkes, CharlessandYang, HaoandDabeer, OnkarandTu, ZhuowenandSoatto, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Guided_Recommendation_for_Model_Fine-Tuning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3633-3642.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在大规模模型库中选择最适合的预训练模型进行下游任务。<br>
                    动机：现有的手工设计模型选择标准由于无效的假设和内在限制可能失败，且难以将关于模型容量和数据集的先验知识整合到现有标准中。<br>
                    方法：将模型选择转化为推荐问题，并从过去的训练历史中学习。具体来说，我们将数据集和模型的元信息特征化，并使用其迁移学习性能作为指导分数。通过数千个历史训练作业，可以学习一个推荐系统，以预测给定数据集和模型特征的模型选择分数。<br>
                    效果：通过22个预训练模型和40个下游任务的广泛评估，我们的方法在有相关训练历史可用时，可以显著优于先前的手工设计模型选择方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model selection is essential for reducing the search cost of the best pre-trained model over a large-scale model zoo for a downstream task. After analyzing recent hand-designed model selection criteria with 400+ ImageNet pre-trained models and 40 downstream tasks, we find that they can fail due to invalid assumptions and intrinsic limitations. The prior knowledge on model capacity and dataset also can not be easily integrated into the existing criteria. To address these issues, we propose to convert model selection as a recommendation problem and to learn from the past training history. Specifically, we characterize the meta information of datasets and models as features, and use their transfer learning performance as the guided score. With thousands of historical training jobs, a recommendation system can be learned to predict the model selection score given the features of the dataset and the model as input. Our approach enables integrating existing model selection scores as additional features and scales with more historical data. We evaluate the prediction accuracy with 22 pre-trained models over 40 downstream tasks. With extensive evaluations, we show that the learned approach can outperform prior hand-designed model selection methods significantly when relevant training history is available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">837.Masked Image Training for Generalizable Deep Image Denoising</span><br>
                <span class="as">Chen, HaoyuandGu, JinjinandLiu, YihaoandMagid, SalmaAbdelandDong, ChaoandWang, QiongandPfister, HanspeterandZhu, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Masked_Image_Training_for_Generalizable_Deep_Image_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1692-1703.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度学习模型在图像去噪任务上的泛化能力。<br>
                    动机：现有的深度学习方法在处理不同噪声分布的图像去噪任务时，往往表现不佳。<br>
                    方法：提出一种名为“掩蔽训练”的新方法，通过在训练过程中随机遮蔽输入图像的像素并重建缺失信息，以及遮蔽自注意力层的特征，避免训练-测试不一致的影响。<br>
                    效果：实验表明，该方法比其他深度学习模型具有更好的泛化能力，并且可以直接应用于真实世界的场景。同时，我们的可解释性分析也证明了该方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When capturing and storing images, devices inevitably introduce noise. Reducing this noise is a critical task called image denoising. Deep learning has become the de facto method for image denoising, especially with the emergence of Transformer-based models that have achieved notable state-of-the-art results on various image tasks. However, deep learning-based methods often suffer from a lack of generalization ability. For example, deep models trained on Gaussian noise may perform poorly when tested on other noise distributions. To address this issue, we present a novel approach to enhance the generalization performance of denoising networks, known as masked training. Our method involves masking random pixels of the input image and reconstructing the missing information during training. We also mask out the features in the self-attention layers to avoid the impact of training-testing inconsistency. Our approach exhibits better generalization ability than other deep learning models and is directly applicable to real-world scenarios. Additionally, our interpretability analysis demonstrates the superiority of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">838.OT-Filter: An Optimal Transport Filter for Learning With Noisy Labels</span><br>
                <span class="as">Feng, ChuanwenandRen, YilongandXie, Xike</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_OT-Filter_An_Optimal_Transport_Filter_for_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16164-16174.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过优化传输理论改进深度学习模型在有噪声标签数据上的性能。<br>
                    动机：现有的深度学习模型在有噪声标签的数据上性能会显著下降，因为网络会因记忆噪声标签而产生确认偏误。<br>
                    方法：提出一种基于最优传输理论的样本选择方法，称为OT-Filter，该方法通过提供几何有意义的距离和保留分布模式来测量数据差异，从而减轻确认偏误。<br>
                    效果：在Clothing1M、ANIMAL-10N等基准测试中，OT-Filter的性能优于其他方法。同时，在CIFAR-10/100等具有合成标签的基准测试中，OT-Filter在处理高噪声数据标签方面表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of deep learning is largely attributed to the training over clean data. However, data is often coupled with noisy labels in practice. Learning with noisy labels is challenging because the performance of the deep neural networks (DNN) drastically degenerates, due to confirmation bias caused by the network memorization over noisy labels. To alleviate that, a recent prominent direction is on sample selection, which retrieves clean data samples from noisy samples, so as to enhance the model's robustness and tolerance to noisy labels. In this paper, we revamp the sample selection from the perspective of optimal transport theory and propose a novel method, called the OT-Filter. The OT-Filter provides geometrically meaningful distances and preserves distribution patterns to measure the data discrepancy, thus alleviating the confirmation bias. Extensive experiments on benchmarks, such as Clothing1M and ANIMAL-10N, show that the performance of the OT- Filter outperforms its counterparts. Meanwhile, results on benchmarks with synthetic labels, such as CIFAR-10/100, show the superiority of the OT-Filter in handling data labels of high noise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">839.Rebalancing Batch Normalization for Exemplar-Based Class-Incremental Learning</span><br>
                <span class="as">Cha, SungminandCho, SungjunandHwang, DasolandHong, SunwonandLee, MoontaeandMoon, Taesup</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cha_Rebalancing_Batch_Normalization_for_Exemplar-Based_Class-Incremental_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20127-20136.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在解决在持续学习中，批量归一化（BN）的问题。<br>
                    动机：尽管批量归一化在各种计算机视觉任务中被广泛研究，但在持续学习中的应用却相对较少。特别是在基于样例的类别增量学习（CIL）中，BN的主要问题是在一个小批量中，当前任务和过去任务的训练数据不平衡，导致BN的经验均值和方差以及可学习的仿射变换参数严重偏向当前任务，从而引发对过去任务的遗忘。<br>
                    方法：我们开发了一种新的BN更新补丁，专门针对基于样例的类别增量学习（CIL）。我们提出了一种无超参数的变体，称为任务平衡BN（TBBN），通过在训练过程中使用重塑和重复操作进行水平拼接的任务平衡批次，以更准确地解决数据不平衡问题。<br>
                    效果：我们在CIFAR-100、ImageNet-100和五个不同任务数据集上进行的实验表明，TBBN在推理时与普通BN完全相同，易于应用于大多数现有的基于样例的离线CIL算法，并始终优于其他BN变体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Batch Normalization (BN) and its variants has been extensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we develop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the empirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task --- contributing to the forgetting of past tasks. While one of the recent BN variants has been developed for "online" CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for "offline" CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies in not fully addressing the data imbalance issue, especially in computing the gradients for learning the affine transformation parameters of BN. Accordingly, our new hyperparameter-free variant, dubbed as Task-Balanced BN (TBBN), is proposed to more correctly resolve the imbalance issue by making a horizontally-concatenated task-balanced batch using both reshape and repeat operations during training. Based on our experiments on class incremental learning of CIFAR-100, ImageNet-100, and five dissimilar task datasets, we demonstrate that our TBBN, which works exactly the same as the vanilla BN in the inference time, is easily applicable to most existing exemplar-based offline CIL algorithms and consistently outperforms other BN variants.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">840.The Treasure Beneath Multiple Annotations: An Uncertainty-Aware Edge Detector</span><br>
                <span class="as">Zhou, CaixiaandHuang, YapingandPu, MengyangandGuan, QingjiandHuang, LiandLing, Haibin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_The_Treasure_Beneath_Multiple_Annotations_An_Uncertainty-Aware_Edge_Detector_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15507-15517.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习的边缘检测器严重依赖像素级的标签，这些标签通常由多个注释者提供。<br>
                    动机：现有的方法通过简单的投票过程融合多个注释，忽略了边缘的固有模糊性和注释者的标注偏见。<br>
                    方法：本文提出了一种新的不确定性感知边缘检测器（UAED），该检测器利用不确定性来研究不同注释的主观性和模糊性。具体来说，我们首先将确定性标签空间转换为可学习的高斯分布，其方差度量不同注释之间的模糊度。然后，我们将学习的方差视为预测边缘图的估计不确定性，具有较高不确定性的像素可能是边缘检测的难题。因此，我们设计了一个自适应加权损失来强调从那些具有高不确定性的像素中学习，这有助于网络逐渐集中在重要的像素上。<br>
                    效果：UAED可以与各种编码器-解码器骨干结合使用，广泛的实验证明，UAED在多个边缘检测基准测试中始终表现出优越的性能。源代码可在https://github.com/ZhouCX117/UAED获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning-based edge detectors heavily rely on pixel-wise labels which are often provided by multiple annotators. Existing methods fuse multiple annotations using a simple voting process, ignoring the inherent ambiguity of edges and labeling bias of annotators. In this paper, we propose a novel uncertainty-aware edge detector (UAED), which employs uncertainty to investigate the subjectivity and ambiguity of diverse annotations. Specifically, we first convert the deterministic label space into a learnable Gaussian distribution, whose variance measures the degree of ambiguity among different annotations. Then we regard the learned variance as the estimated uncertainty of the predicted edge maps, and pixels with higher uncertainty are likely to be hard samples for edge detection. Therefore we design an adaptive weighting loss to emphasize the learning from those pixels with high uncertainty, which helps the network to gradually concentrate on the important pixels. UAED can be combined with various encoder-decoder backbones, and the extensive experiments demonstrate that UAED achieves superior performance consistently across multiple edge detection benchmarks. The source code is available at https://github.com/ZhouCX117/UAED.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">841.Fair Federated Medical Image Segmentation via Client Contribution Estimation</span><br>
                <span class="as">Jiang, MeiruiandRoth, HolgerR.andLi, WenqiandYang, DongandZhao, CanandNath, VishweshandXu, DaguangandDou, QiandXu, Ziyue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Fair_Federated_Medical_Image_Segmentation_via_Client_Contribution_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16302-16311.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在联邦学习中确保公平性，包括贡献公平性和性能公平性。<br>
                    动机：尽管已有研究在单独考虑贡献公平性和性能公平性时取得了进展，但我们认为同时考虑这两者对于激励更多多样化的客户端参与并生成高质量的全局模型至关重要。<br>
                    方法：我们提出了一种优化这两种公平性的方法，即通过估计贡献来执行联邦学习（FedCE）。具体来说，我们在梯度空间和数据空间中估计客户端的贡献。在梯度空间中，我们监控每个客户端相对于其他客户端的梯度方向差异。在数据空间中，我们使用辅助模型测量客户端数据的预测误差。基于这种贡献估计，我们提出了一种联邦学习方法，即使用估计作为全局模型聚合权重。<br>
                    效果：我们的理论分析和实验评估表明，我们的方法在两个真实世界的医疗数据集上取得了显著的性能改进，更好的协作公平性和性能公平性，以及全面的分析研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model aggregation weights. We have theoretically analyzed our method and empirically evaluated it on two real-world medical datasets. The effectiveness of our approach has been validated with significant performance improvements, better collaboration fairness, better performance fairness, and comprehensive analytical studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">842.AsyFOD: An Asymmetric Adaptation Paradigm for Few-Shot Domain Adaptive Object Detection</span><br>
                <span class="as">Gao, YipengandLin, Kun-YuandYan, JunkaiandWang, YaoweiandZheng, Wei-Shi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_AsyFOD_An_Asymmetric_Adaptation_Paradigm_for_Few-Shot_Domain_Adaptive_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3261-3271.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在只有少量目标标注图像的情况下进行领域自适应物体检测的问题。<br>
                    动机：在少数目标标注图像的情况下，源域和目标域之间的数据不平衡可能导致过度适应，这是由于传统的特征对齐方法无法有效处理这个问题。<br>
                    方法：提出了一种不对称适应的范式，即AsyFOD，通过利用源域和目标域的不同视角来解决数据不平衡问题。具体来说，AsyFOD首先通过目标分布估计识别出与目标相似的源实例，用于扩充有限的目标任务实例。然后，我们对与目标不相似的源实例和增强的目标实例进行异步对齐，这种方法简单而有效，可以缓解过度适应的问题。<br>
                    效果：实验表明，所提出的AsyFOD在所有四个FSDAOD基准测试中都优于所有最先进的方法，例如，在Cityscapes-to-FoggyCityscapes上提高了3.1%的mAP，在Sim10k-to-Cityscapes上提高了2.9%的mAP。代码可在https://github.com/Hlings/AsyFOD获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we study few-shot domain adaptive object detection (FSDAOD), where only a few target labeled images are available for training in addition to sufficient source labeled images. Critically, in FSDAOD, the data-scarcity in the target domain leads to an extreme data imbalance between the source and target domains, which potentially causes over-adaptation in traditional feature alignment. To address the data imbalance problem, we propose an asymmetric adaptation paradigm, namely AsyFOD, which leverages the source and target instances from different perspectives. Specifically, by using target distribution estimation, the AsyFOD first identifies the target-similar source instances, which serves for augmenting the limited target instances. Then, we conduct asynchronous alignment between target-dissimilar source instances and augmented target instances, which is simple yet effective for alleviating the over-adaptation. Extensive experiments demonstrate that the proposed AsyFOD outperforms all state-of-the-art methods on four FSDAOD benchmarks with various environmental variances, e.g., 3.1% mAP improvement on Cityscapes-to-FoggyCityscapes and 2.9% mAP increase on Sim10k-to-Cityscapes. The code is available at https://github.com/Hlings/AsyFOD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">843.Block Selection Method for Using Feature Norm in Out-of-Distribution Detection</span><br>
                <span class="as">Yu, YeongukandShin, SunghoandLee, SeongjuandJun, ChanghyunandLee, Kyoobin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Block_Selection_Method_for_Using_Feature_Norm_in_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15701-15711.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地检测神经网络推理阶段的分布外（OOD）输入？<br>
                    动机：目前的方法主要依赖于网络输出的高激活特征图，但这种方法存在局限性。<br>
                    方法：本文提出了一个简单框架，包括特征图的范数（FeatureNorm）和ID与OOD的范数比（NormRatio），用于测量每个块的OOD检测性能。通过创建伪OOD的jigsaw拼图从ID训练样本中，并计算NormRatio来选择提供ID和OOD特征图范数最大差异的块。<br>
                    效果：实验结果表明，使用FeatureNorm进行OOD检测比其他方法更有效，在CIFAR10基准测试中FPR95降低了最多52.77%，在ImageNet基准测试中降低了最多48.53%。此外，该框架可以适用于各种架构，并且块选择的重要性可以提高先前的OOD检测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detecting out-of-distribution (OOD) inputs during the inference stage is crucial for deploying neural networks in the real world. Previous methods commonly relied on the output of a network derived from the highly activated feature map. In this study, we first revealed that a norm of the feature map obtained from the other block than the last block can be a better indicator of OOD detection. Motivated by this, we propose a simple framework consisting of FeatureNorm: a norm of the feature map and NormRatio: a ratio of FeatureNorm for ID and OOD to measure the OOD detection performance of each block. In particular, to select the block that provides the largest difference between FeatureNorm of ID and FeatureNorm of OOD, we create jigsaw puzzles as pseudo OOD from ID training samples and calculate NormRatio, and the block with the largest value is selected. After the suitable block is selected, OOD detection with the FeatureNorm outperforms other OOD detection methods by reducing FPR95 by up to 52.77% on CIFAR10 benchmark and by up to 48.53% on ImageNet benchmark. We demonstrate that our framework can generalize to various architectures and the importance of block selection, which can improve previous OOD detection methods as well.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">844.Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning</span><br>
                <span class="as">He, QiangandSu, HuangyuanandZhang, JieyuandHou, Xinwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Frustratingly_Easy_Regularization_on_Representation_Can_Boost_Deep_Reinforcement_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20215-20225.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度强化学习中的Q网络及其目标网络的表示是否应满足良好的区分性表示属性。<br>
                    动机：当前的深度强化学习代理可能会违反这一属性，导致次优策略。<br>
                    方法：提出了一种名为“Policy Evaluation with Easy Regularization on Representation”的简单有效的正则化器PEER，通过显式地对内部表示进行正则化来保持区分性表示属性。<br>
                    效果：实验证明，将PEER引入深度强化学习可以显著提高性能和样本效率。在PyBullet的所有4个环境中，DMControl的12个任务中的9个以及Atari的26个游戏中的19个，PEER都达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the Q-network and its target Q-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate guarantee of PEER. Implementing PEER requires only one line of code. Our experiments demonstrate that incorporating PEER into DRL can significantly improve performance and sample efficiency. Comprehensive experiments show that PEER achieves state-of-the-art performance on all 4 environments on PyBullet, 9 out of 12 tasks on DMControl, and 19 out of 26 games on Atari. To the best of our knowledge, PEER is the first work to study the inherent representation property of Q-network and its target. Our code is available at https://sites.google.com/view/peer-cvpr2023/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">845.StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning</span><br>
                <span class="as">Fu, YuqianandXie, YuandFu, YanweiandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_StyleAdv_Meta_Style_Adversarial_Training_for_Cross-Domain_Few-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24575-24584.png><br>
            
            <span class="tt"><span class="t0">研究问题：跨领域少样本学习（CD-FSL）是一种新的任务，旨在将源数据集上学到的先验知识转移到新的目标任务集上。<br>
                    动机：CD-FSL任务面临的主要挑战是不同数据集之间的巨大域差距，这种差距主要来自视觉风格的改变。现有的方法通过交换两个图像的风格来解决这个问题，但这种方法生成的风格仍然属于源风格集，因此效果有限。<br>
                    方法：受传统对抗性学习的启发，我们提出了一种模型无关的元风格对抗训练（StyleAdv）方法和一种新的风格对抗攻击方法。特别是，我们的风格攻击方法通过使用有符号的风格梯度对原始风格进行扰动，为模型训练合成了"虚拟"和"硬"的对抗风格。<br>
                    效果：我们在八个不同的目标数据集上进行了广泛的实验，无论是基于ResNet还是ViT，我们都取得了新的最先进的结果，证明了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cross-Domain Few-Shot Learning (CD-FSL) is a recently emerging task that tackles few-shot learning across different domains. It aims at transferring prior knowledge learned on the source dataset to novel target datasets. The CD-FSL task is especially challenged by the huge domain gap between different datasets. Critically, such a domain gap actually comes from the changes of visual styles, and wave-SAN empirically shows that spanning the style distribution of the source data helps alleviate this issue. However, wave-SAN simply swaps styles of two images. Such a vanilla operation makes the generated styles "real" and "easy", which still fall into the original set of the source styles. Thus, inspired by vanilla adversarial learning, a novel model-agnostic meta Style Adversarial training (StyleAdv) method together with a novel style adversarial attack method is proposed for CD-FSL. Particularly, our style attack method synthesizes both "virtual" and "hard" adversarial styles for model training. This is achieved by perturbing the original style with the signed style gradients. By continually attacking styles and forcing the model to recognize these challenging adversarial styles, our model is gradually robust to the visual styles, thus boosting the generalization ability for novel target datasets. Besides the typical CNN-based backbone, we also employ our StyleAdv method on large-scale pretrained vision transformer. Extensive experiments conducted on eight various target datasets show the effectiveness of our method. Whether built upon ResNet or ViT, we achieve the new state of the art for CD-FSL. Code is available at https://github.com/lovelyqian/StyleAdv-CDFSL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">846.Long-Tailed Visual Recognition via Self-Heterogeneous Integration With Knowledge Excavation</span><br>
                <span class="as">Jin, YanandLi, MengkeandLu, YangandCheung, Yiu-mingandWang, Hanzi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Long-Tailed_Visual_Recognition_via_Self-Heterogeneous_Integration_With_Knowledge_Excavation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23695-23704.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习模型在处理长尾分布的现实世界数据时，往往对多数类有严重偏好。<br>
                    动机：为了解决这个问题，本文提出了一种基于专家混合的方法（MoE），该方法可以关注长尾分布的不同部分。<br>
                    方法：首先，我们提出了深度知识融合（DKF）来融合不同专家网络中浅层和深层之间的特征，使每个专家在表示上更加多样化。然后，我们进一步提出了动态知识转移（DKT），以减少最难的负类别对我们MoE框架中尾部类别的影响。<br>
                    效果：实验结果表明，SHIKE在CIFAR100-LT、ImageNet-LT、iNaturalist 2018和Places-LT等数据集上的分类准确率得到了显著提高，特别是尾部类别的准确率，达到了最先进的56.3%、60.3%、75.4%和41.9%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks have made huge progress in the last few decades. However, as the real-world data often exhibits a long-tailed distribution, vanilla deep models tend to be heavily biased toward the majority classes. To address this problem, state-of-the-art methods usually adopt a mixture of experts (MoE) to focus on different parts of the long-tailed distribution. Experts in these methods are with the same model depth, which neglects the fact that different classes may have different preferences to be fit by models with different depths. To this end, we propose a novel MoE-based method called Self-Heterogeneous Integration with Knowledge Excavation (SHIKE). We first propose Depth-wise Knowledge Fusion (DKF) to fuse features between different shallow parts and the deep part in one network for each expert, which makes experts more diverse in terms of representation. Based on DKF, we further propose Dynamic Knowledge Transfer (DKT) to reduce the influence of the hardest negative class that has a non-negligible impact on the tail classes in our MoE framework. As a result, the classification accuracy of long-tailed data can be significantly improved, especially for the tail classes. SHIKE achieves the state-of-the-art performance of 56.3%, 60.3%, 75.4%, and 41.9% on CIFAR100-LT (IF100), ImageNet-LT, iNaturalist 2018, and Places-LT, respectively. The source code is available at https://github.com/jinyan-06/SHIKE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">847.GeoNet: Benchmarking Unsupervised Adaptation Across Geographies</span><br>
                <span class="as">Kalluri, TarunandXu, WangdongandChandraker, Manmohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kalluri_GeoNet_Benchmarking_Unsupervised_Adaptation_Across_Geographies_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15368-15379.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉模型在训练期间未见过的领域的稳健性，特别是在新地理区域中部署的模型。<br>
                    动机：解决在训练数据集未充分代表的新地理区域中部署模型时面临的直接挑战，以实现公平和包容的计算机视觉。<br>
                    方法：介绍了一个大规模的地理适应数据集GeoNet，包括场景识别（GeoPlaces）、图像分类（GeoImNet）和通用适应（GeoUniDA）等任务的基准测试。调查了地理适应问题中典型的分布变化的性质，并假设地理间的主要领域变化源于场景上下文（上下文变化）、对象设计（设计变化）和标签分布（先验变化）的巨大差异。<br>
                    效果：对几种最先进的无监督领域适应算法和架构进行了广泛的评估，发现它们不足以进行地理适应，大规模预训练的大型视觉模型也不能带来地理稳健性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, several efforts have been aimed at improving the robustness of vision models to domains and environments unseen during training. An important practical problem pertains to models deployed in a new geography that is under-represented in the training dataset, posing a direct challenge to fair and inclusive computer vision. In this paper, we study the problem of geographic robustness and make three main contributions. First, we introduce a large-scale dataset GeoNet for geographic adaptation containing benchmarks across diverse tasks like scene recognition (GeoPlaces), image classification (GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the nature of distribution shifts typical to the problem of geographic adaptation and hypothesize that the major source of domain shifts arise from significant variations in scene context (context shift), object design (design shift) and label distribution (prior shift) across geographies. Third, we conduct an extensive evaluation of several state-of-the-art unsupervised domain adaptation algorithms and architectures on GeoNet, showing that they do not suffice for geographical adaptation, and that large-scale pre-training using large vision models also does not lead to geographic robustness. Our dataset is publicly available at https://tarun005.github.io/GeoNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">848.Learning Transformation-Predictive Representations for Detection and Description of Local Features</span><br>
                <span class="as">Wang, ZihaoandWu, ChunxuandYang, YifeiandLi, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Transformation-Predictive_Representations_for_Detection_and_Description_of_Local_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11464-11473.png><br>
            
            <span class="tt"><span class="t0">研究问题：关键点检测和描述的任务是估计局部特征的稳定位置和区别性表示，这对图像匹配至关重要。然而，由图像之间的一对一对应关系生成的粗糙的硬正或负标签带来了无法区分的样本，称为伪正或负样本，这在学习和匹配中使用关键点时会产生不一致的监督。这种伪标记样本阻止了深度神经网络学习用于准确匹配的区别性描述。<br>
                    动机：为了解决这个问题，我们提出了一种使用自我监督对比学习的转换预测表示学习方法。我们通过不使用任何负样本对（包括真实和伪负样本）并避免解决方案崩溃，来最大化相同3D点（地标）的对应视图之间的相似性。然后，我们设计了一个可学习的标签预测机制，将硬正标签软化为软连续目标。积极更新的软标签广泛解决了训练瓶颈（源自伪正样本的标签噪声），使模型可以在更强的增强范例下进行训练。<br>
                    方法：我们的方法是一种自我监督方法，通过最大化相同3D点的对应视图之间的相似性，并设计一个可学习的标签预测机制，将硬正标签软化为软连续目标，从而避免了伪正或负样本带来的不一致监督。<br>
                    效果：我们的自我监督方法在标准的图像匹配基准测试中优于最先进的方法，并且在不同的下游任务上表现出优秀的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of key-points detection and description is to estimate the stable location and discriminative representation of local features, which is essential for image matching. However, either the rough hard positive or negative labels generated from one-to-one correspondences among images bring indistinguishable samples, called pseudo positives or negatives, which act as inconsistent supervisions while learning key-points used for matching. Such pseudo-labeled samples prevent deep neural networks from learning discriminative descriptions for accurate matching. To tackle this challenge, we propose to learn transformation-predictive representations with self-supervised contrastive learning. We maximize the similarity between corresponded views of the same 3D point (landmark) by using none of the negative sample pairs (including true and pseudo negatives) and avoiding collapsing solutions. Then we design a learnable label prediction mechanism to soften the hard positive labels into soft continuous targets. The aggressively updated soft labels extensively deal with the training bottleneck (derived from the label noise of pseudo positives) and make the model can be trained under a stronger augmentation paradigm. Our self-supervised method outperforms the state-of-the-art on the standard image matching benchmarks by noticeable margins and shows excellent generalization capability on multiple downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">849.Two-Way Multi-Label Loss</span><br>
                <span class="as">Kobayashi, Takumi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kobayashi_Two-Way_Multi-Label_Loss_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7476-7485.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地处理自然图像中的多标签分类问题。<br>
                    动机：现有的单标签分类方法无法有效处理多标签分类问题，而现有的多标签处理方法存在类别不平衡等问题。<br>
                    方法：提出了一种基于相对比较的多标签损失函数，该函数可以同时区分类别和样本，增强了特征的判别能力。<br>
                    效果：实验结果表明，该方法在多标签分类任务上具有竞争力的性能，并且在ImageNet的单标签训练中提供了可转移的特征。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A natural image frequently contains multiple classification targets, accordingly providing multiple class labels rather than a single label per image. While the single-label classification is effectively addressed by applying a softmax cross-entropy loss, the multi-label task is tackled mainly in a binary cross-entropy (BCE) framework. In contrast to the softmax loss, the BCE loss involves issues regarding imbalance as multiple classes are decomposed into a bunch of binary classifications; recent works improve the BCE loss to cope with the issue by means of weighting. In this paper, we propose a multi-label loss by bridging a gap between the softmax loss and the multi-label scenario. The proposed loss function is formulated on the basis of relative comparison among classes which also enables us to further improve discriminative power of features by enhancing classification margin. The loss function is so flexible as to be applicable to a multi-label setting in two ways for discriminating classes as well as samples. In the experiments on multi-label classification, the proposed method exhibits competitive performance to the other multi-label losses, and it also provides transferrable features on single-label ImageNet training. Codes are available at https://github.com/tk1980/TwowayMultiLabelLoss.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">850.Dionysus: Recovering Scene Structures by Dividing Into Semantic Pieces</span><br>
                <span class="as">Wang, LikangandChen, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dionysus_Recovering_Scene_Structures_by_Dividing_Into_Semantic_Pieces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12576-12587.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数现有的3D重建方法在效率和细节保留上存在不足。<br>
                    动机：在自动驾驶和增强现实等真实世界应用中，效果和效率同样重要，而现有的方法往往在处理无价值深度样本时浪费资源。<br>
                    方法：提出一种名为Dionysus的新型基于学习的3D重建框架，通过从估计的语义图中找出最有希望的深度候选对象来解决问题。<br>
                    效果：通过对跨视图语义一致性进行检查以区分不可靠的深度候选对象，并通过在像素之间重新分配深度提名者来实现自适应采样。实验结果证实了所提出的框架的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing 3D reconstruction methods result in either detail loss or unsatisfying efficiency. However, effectiveness and efficiency are equally crucial in real-world applications, e.g., autonomous driving and augmented reality. We argue that this dilemma comes from wasted resources on valueless depth samples. This paper tackles the problem by proposing a novel learning-based 3D reconstruction framework named Dionysus. Our main contribution is to find out the most promising depth candidates from estimated semantic maps. This strategy simultaneously enables high effectiveness and efficiency by attending to the most reliable nominators. Specifically, we distinguish unreliable depth candidates by checking the cross-view semantic consistency and allow adaptive sampling by redistributing depth nominators among pixels. Experiments on the most popular datasets confirm our proposed framework's effectiveness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">851.Noisy Correspondence Learning With Meta Similarity Correction</span><br>
                <span class="as">Han, HaochenandMiao, KaiyaoandZheng, QinghuaandLuo, Minnan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Noisy_Correspondence_Learning_With_Meta_Similarity_Correction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7517-7526.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管多模态学习在跨模态检索任务中取得了成功，但其依赖于多媒体数据之间的正确对应关系。然而，收集这种理想的数据既昂贵又耗时。<br>
                    动机：在实践中，大多数广泛使用的数据集是从互联网上收集的，不可避免地包含不匹配的对。在这种噪声关联数据集上进行训练会导致性能下降，因为跨模态检索方法可能会错误地强制使不匹配的数据相似。<br>
                    方法：我们提出了一种元相似性校正网络（MSCN）来提供可靠的相似度分数。我们将二分类任务视为元过程，鼓励MSCN从正负元数据中学习区分能力。为了进一步减轻噪声的影响，我们设计了一种有效的数据净化策略，使用元数据作为先验知识来去除噪声样本。<br>
                    效果：通过在Flickr30K、MS-COCO和Conceptual Captions等合成和真实世界噪声中进行大量实验，我们的方法展示了其优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the success of multimodal learning in cross-modal retrieval task, the remarkable progress relies on the correct correspondence among multimedia data. However, collecting such ideal data is expensive and time-consuming. In practice, most widely used datasets are harvested from the Internet and inevitably contain mismatched pairs. Training on such noisy correspondence datasets causes performance degradation because the cross-modal retrieval methods can wrongly enforce the mismatched data to be similar. To tackle this problem, we propose a Meta Similarity Correction Network (MSCN) to provide reliable similarity scores. We view a binary classification task as the meta-process that encourages the MSCN to learn discrimination from positive and negative meta-data. To further alleviate the influence of noise, we design an effective data purification strategy using meta-data as prior knowledge to remove the noisy samples. Extensive experiments are conducted to demonstrate the strengths of our method in both synthetic and real-world noises, including Flickr30K, MS-COCO, and Conceptual Captions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">852.PCR: Proxy-Based Contrastive Replay for Online Class-Incremental Continual Learning</span><br>
                <span class="as">Lin, HuiweiandZhang, BaoquanandFeng, ShanshanandLi, XutaoandYe, Yunming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_PCR_Proxy-Based_Contrastive_Replay_for_Online_Class-Incremental_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24246-24255.png><br>
            
            <span class="tt"><span class="t0">研究问题：在线类别增量持续学习是一种特定的持续学习任务，旨在从数据流中持续学习新类别，但数据流的样本只能看一次，这会遭受灾难性遗忘问题的影响。<br>
                    动机：现有的重播方法通过以代理或对比为基础的重播方式保存和重播部分旧数据，有效地缓解了这个问题。然而，这两种重播方式都有其局限性，前者由于类别不平衡问题倾向于新的类别，后者由于样本数量有限而不稳定且难以收敛。<br>
                    方法：本文对这两种重播方式进行了全面分析，发现它们可以互补。受此发现的启发，我们提出了一种新的基于重播的方法，称为代理对比重播（PCR）。其关键操作是以对比的方式用相应的代理替换锚点的对比样本。<br>
                    效果：我们在三个真实世界的基准数据集上进行了广泛的实验，实验结果一致地证明了PCR优于各种最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Online class-incremental continual learning is a specific task of continual learning. It aims to continuously learn new classes from data stream and the samples of data stream are seen only once, which suffers from the catastrophic forgetting issue, i.e., forgetting historical knowledge of old classes. Existing replay-based methods effectively alleviate this issue by saving and replaying part of old data in a proxy-based or contrastive-based replay manner. Although these two replay manners are effective, the former would incline to new classes due to class imbalance issues, and the latter is unstable and hard to converge because of the limited number of samples. In this paper, we conduct a comprehensive analysis of these two replay manners and find that they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR). The key operation is to replace the contrastive samples of anchors with corresponding proxies in the contrastive-based way. It alleviates the phenomenon of catastrophic forgetting by effectively addressing the imbalance issue, as well as keeps a faster convergence of the model. We conduct extensive experiments on three real-world benchmark datasets, and empirical results consistently demonstrate the superiority of PCR over various state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">853.Multi-View Adversarial Discriminator: Mine the Non-Causal Factors for Object Detection in Unseen Domains</span><br>
                <span class="as">Xu, MingjunandQin, LingyunandChen, WeijieandPu, ShiliangandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Multi-View_Adversarial_Discriminator_Mine_the_Non-Causal_Factors_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8103-8112.png><br>
            
            <span class="tt"><span class="t0">研究问题：领域转移会降低对象检测模型在实际应用程序中的性能。<br>
                    动机：先前的领域对抗学习（DAL）方法忽略了隐藏在公共特征中的非因果无关因素，这主要是由于DAL的单视图特性。<br>
                    方法：我们提出了一种基于多视图对抗训练的领域泛化模型，通过在源域上进行多视图对抗训练来消除公共特征中的非因果无关因素。<br>
                    效果：我们在六个基准测试上进行的大量实验表明，我们的MAD模型取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain shift degrades the performance of object detection models in practical applications. To alleviate the influence of domain shift, plenty of previous work try to decouple and learn the domain-invariant (common) features from source domains via domain adversarial learning (DAL). However, inspired by causal mechanisms, we find that previous methods ignore the implicit insignificant non-causal factors hidden in the common features. This is mainly due to the single-view nature of DAL. In this work, we present an idea to remove non-causal factors from common features by multi-view adversarial training on source domains, because we observe that such insignificant non-causal factors may still be significant in other latent spaces (views) due to the multi-mode structure of data. To summarize, we propose a Multi-view Adversarial Discriminator (MAD) based domain generalization model, consisting of a Spurious Correlations Generator (SCG) that increases the diversity of source domain by random augmentation and a Multi-View Domain Classifier (MVDC) that maps features to multiple latent spaces, such that the non-causal factors are removed and the domain-invariant features are purified. Extensive experiments on six benchmarks show our MAD obtains state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">854.MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset</span><br>
                <span class="as">Feng, ChenandPatras, Ioannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_MaskCon_Masked_Contrastive_Learning_for_Coarse-Labelled_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19913-19922.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用粗粒度标签训练模型以解决细粒度标签问题。<br>
                    动机：标注大规模数据集既昂贵又困难，特别是需要精细标签的专业化领域。而粗粒度标签更容易获取，不需要专业知识。<br>
                    方法：提出一种对比学习方法，称为masked contrastive learning（MaskCon），在对比学习框架中，为每个样本生成基于粗粒度标签和其他样本以及该样本的另一种增强视图的软标签。<br>
                    效果：实验表明，该方法在CIFAR10、CIFAR100、ImageNet-1K、Stanford Online Products和Stanford Cars196等数据集上取得了显著改进，优于现有最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is often costly and difficult to accurately and efficiently annotate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this setting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a contrastive learning method, called masked contrastive learning (MaskCon) to address the under-explored problem setting, where we learn with a coarse-labelled dataset in order to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sample our method generates soft-labels with the aid of coarse labels against other samples and another augmented view of the sample in question. By contrast to self-supervised contrastive learning where only the sample's augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels. This allows us to utilize both inter-sample relations and coarse labels. We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error. Experimentally, our method achieves significant improvement over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online Products and Stanford Cars196 datasets. Code and annotations are available at https://github.com/MrChenFeng/MaskCon_CVPR2023.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">855.Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection</span><br>
                <span class="as">VS, VibashanandOza, PoojanandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3520-3530.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让预训练的语言模型更好地利用结构化知识，提升语言理解能力？<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用。<br>
                    方法：提出一种增强的语言表示模型ERNIE，通过大规模文本语料库和知识图谱进行联合训练，以充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the issue of domain shift. Specifically, UDA methods try to align the source and target representations to improve generalization on the target domain. Further, UDA methods work under the assumption that the source data is accessible during the adaptation process. However, in real-world scenarios, the labelled source data is often restricted due to privacy regulations, data transmission constraints, or proprietary data concerns. The Source-Free Domain Adaptation (SFDA) setting aims to alleviate these concerns by adapting a source-trained model for the target domain without requiring access to the source data. In this paper, we explore the SFDA setting for the task of adaptive object detection. To this end, we propose a novel training strategy for adapting a source-trained object detector to the target domain without source data. More precisely, we design a novel contrastive loss to enhance the target representations by exploiting the objects relations for a given target domain input. These object instance relations are modelled using an Instance Relation Graph (IRG) network, which are then used to guide the contrastive representation learning. In addition, we utilize a student-teacher to effectively distill knowledge from source-trained model to target domain. Extensive experiments on multiple object detection benchmark datasets show that the proposed approach is able to efficiently adapt source-trained object detectors to the target domain, outperforming state-of-the-art domain adaptive detection methods. Code and models are provided in https://viudomain.github.io/irg-sfda-web/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">856.DiGA: Distil To Generalize and Then Adapt for Domain Adaptive Semantic Segmentation</span><br>
                <span class="as">Shen, FengyiandGurram, AkhilandLiu, ZiyuanandWang, HeandKnoll, Alois</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiGA_Distil_To_Generalize_and_Then_Adapt_for_Domain_Adaptive_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15866-15877.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决领域自适应语义分割方法中各阶段的挑战，包括预训练阶段的有限性能提升和自我训练阶段的阈值选择问题。<br>
                    动机：目前的领域自适应语义分割方法在预训练阶段采用的对抗性训练由于盲目的特征对齐导致性能提升有限，而在自我训练阶段寻找合适的类别阈值非常棘手。<br>
                    方法：作者提出了一种新的对称知识蒸馏模块来替代预训练阶段的对抗性训练，使模型具有领域泛化能力。同时，还提出了一种无阈值的动态伪标签选择机制来解决自我训练阶段的阈值问题，使模型更好地适应目标领域。<br>
                    效果：实验结果表明，该方法在流行的基准测试上取得了显著且一致的性能提升，优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain adaptive semantic segmentation methods commonly utilize stage-wise training, consisting of a warm-up and a self-training stage. However, this popular approach still faces several challenges in each stage: for warm-up, the widely adopted adversarial training often results in limited performance gain, due to blind feature alignment; for self-training, finding proper categorical thresholds is very tricky. To alleviate these issues, we first propose to replace the adversarial training in the warm-up stage by a novel symmetric knowledge distillation module that only accesses the source domain data and makes the model domain generalizable. Surprisingly, this domain generalizable warm-up model brings substantial performance improvement, which can be further amplified via our proposed cross-domain mixture data augmentation technique. Then, for the self-training stage, we propose a threshold-free dynamic pseudo-label selection mechanism to ease the aforementioned threshold problem and make the model better adapted to the target domain. Extensive experiments demonstrate that our framework achieves remarkable and consistent improvements compared to the prior arts on popular benchmarks. Codes and models are available at https://github.com/fy-vision/DiGA</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">857.Crossing the Gap: Domain Generalization for Image Captioning</span><br>
                <span class="as">Ren, YuchenandMao, ZhendongandFang, ShanchengandLu, YanandHe, TongandDu, HaoandZhang, YongdongandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2871-2880.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像描述方法假设训练和测试数据来自同一领域，或者目标领域的数据（即测试数据所在的领域）是可访问的。然而，在现实世界的应用中，这个假设是无效的，因为目标领域的数据在学习过程中是不可见的。<br>
                    动机：为了解决这个问题，我们引入了一种新的设置，叫做“用于图像描述的领域泛化”（DGIC），其中目标领域的数据在训练过程中是不可见的。<br>
                    方法：我们首先为DGIC构建了一个基准数据集，帮助我们研究模型在未见过的目标领域中的领域泛化能力。在这个新基准的支持下，我们进一步提出了一个名为“语言引导的语义度量学习”（LSML）的新框架来处理DGIC设置。<br>
                    效果：我们在多个数据集上进行实验，证明了这项任务的挑战性以及我们新提出的基准和LSML框架的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing image captioning methods are under the assumption that the training and testing data are from the same domain or that the data from the target domain (i.e., the domain that testing data lie in) are accessible. However, this assumption is invalid in real-world applications where the data from the target domain is inaccessible. In this paper, we introduce a new setting called Domain Generalization for Image Captioning (DGIC), where the data from the target domain is unseen in the learning process. We first construct a benchmark dataset for DGIC, which helps us to investigate models' domain generalization (DG) ability on unseen domains. With the support of the new benchmark, we further propose a new framework called language-guided semantic metric learning (LSML) for the DGIC setting. Experiments on multiple datasets demonstrate the challenge of the task and the effectiveness of our newly proposed benchmark and LSML framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">858.Quantum Multi-Model Fitting</span><br>
                <span class="as">Farina, MatteoandMagri, LucaandMenapace, WilliandRicci, ElisaandGolyanik, VladislavandArrigoni, Federica</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Farina_Quantum_Multi-Model_Fitting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13640-13649.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中几何模型拟合的挑战，特别是多模型拟合的问题。<br>
                    动机：量子优化已被证明能提高单一模型的鲁棒拟合能力，但对于多模型拟合的问题尚未解决。<br>
                    方法：本文提出了首个量子多模型拟合（MMF）方法，将MMF问题转化为可以通过现代的绝热量子计算机有效采样的问题，并提出了迭代和分解版本的该方法以支持实际大小的问题。<br>
                    效果：实验结果表明，该方法在各种数据集上取得了良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Geometric model fitting is a challenging but fundamental computer vision problem. Recently, quantum optimization has been shown to enhance robust fitting for the case of a single model, while leaving the question of multi-model fitting open. In response to this challenge, this paper shows that the latter case can significantly benefit from quantum hardware and proposes the first quantum approach to multi-model fitting (MMF). We formulate MMF as a problem that can be efficiently sampled by modern adiabatic quantum computers without the relaxation of the objective function. We also propose an iterative and decomposed version of our method, which supports real-world-sized problems. The experimental evaluation demonstrates promising results on a variety of datasets. The source code is available at https://github.com/FarinaMatteo/qmmf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">859.Learning a Deep Color Difference Metric for Photographic Images</span><br>
                <span class="as">Chen, HaoyuandWang, ZhihuaandYang, YangandSun, QilinandMa, Kede</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_a_Deep_Color_Difference_Metric_for_Photographic_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22242-22251.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为摄影图像构建一种深度颜色差异（CD）度量，使其能够准确地计算图像间的颜色差异，同时满足视觉科学中颜色和形状紧密相连的观察结果，以及数学上的合理性、对轻微几何变形的鲁棒性等要求。<br>
                    动机：现有的颜色差异度量大多是手工制作的，并且是针对均匀着色的补丁进行校准的，这并不能很好地推广到具有自然场景复杂性的摄影图像。因此，为摄影图像构建一种深度颜色差异度量是一个活跃的研究课题。<br>
                    方法：通过学习一个多尺度自回归正则化流进行特征转换，然后使用与人类感知颜色差异成线性比例的欧几里得距离，来满足所有的属性要求。<br>
                    效果：在大规模的SPCD数据集上进行的定量和定性实验表明，所学习的CD度量具有潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most well-established and widely used color difference (CD) metrics are handcrafted and subject-calibrated against uniformly colored patches, which do not generalize well to photographic images characterized by natural scene complexities. Constructing CD formulae for photographic images is still an active research topic in imaging/illumination, vision science, and color science communities. In this paper, we aim to learn a deep CD metric for photographic images with four desirable properties. First, it well aligns with the observations in vision science that color and form are linked inextricably in visual cortical processing. Second, it is a proper metric in the mathematical sense. Third, it computes accurate CDs between photographic images, differing mainly in color appearances. Fourth, it is robust to mild geometric distortions (e.g., translation or due to parallax), which are often present in photographic images of the same scene captured by different digital cameras. We show that all these properties can be satisfied at once by learning a multi-scale autoregressive normalizing flow for feature transform, followed by the Euclidean distance which is linearly proportional to the human perceptual CD. Quantitative and qualitative experiments on the large-scale SPCD dataset demonstrate the promise of the learned CD metric.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">860.Self-Correctable and Adaptable Inference for Generalizable Human Pose Estimation</span><br>
                <span class="as">Kan, ZhehanandChen, ShuoshuoandZhang, CeandTang, YushunandHe, Zhihai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kan_Self-Correctable_and_Adaptable_Inference_for_Generalizable_Human_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5537-5546.png><br>
            
            <span class="tt"><span class="t0">研究问题：人类姿态估计以及其他机器学习和预测任务中普遍存在的泛化问题。<br>
                    动机：当前的网络预测模型无法对预测错误进行特性描述，生成反馈信息并实时修正每个测试样本的预测错误，导致泛化性能下降。<br>
                    方法：我们引入了一种自我校正和适应的推理（SCAI）方法来解决这个问题，并以人体姿态估计为例进行演示。我们学习了一个校正网络，根据健身反馈误差来修正预测结果。这个反馈误差是由一个学习到的健身反馈网络产生的，它将预测结果映射回原始输入域并与原始输入进行比较。<br>
                    效果：我们在人体姿态估计上的大量实验结果表明，提出的SCAI方法能够显著提高预测的泛化能力和性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A central challenge in human pose estimation, as well as in many other machine learning and prediction tasks, is the generalization problem. The learned network does not have the capability to characterize the prediction error, generate feedback information from the test sample, and correct the prediction error on the fly for each individual test sample, which results in degraded performance in generalization. In this work, we introduce a self-correctable and adaptable inference (SCAI) method to address the generalization challenge of network prediction and use human pose estimation as an example to demonstrate its effectiveness and performance. We learn a correction network to correct the prediction result conditioned by a fitness feedback error. This feedback error is generated by a learned fitness feedback network which maps the prediction result to the original input domain and compares it against the original input. Interestingly, we find that this self-referential feedback error is highly correlated with the actual prediction error. This strong correlation suggests that we can use this error as feedback to guide the correction process. It can be also used as a loss function to quickly adapt and optimize the correction network during the inference process. Our extensive experimental results on human pose estimation demonstrate that the proposed SCAI method is able to significantly improve the generalization capability and performance of human pose estimation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">861.Few-Shot Learning With Visual Distribution Calibration and Cross-Modal Distribution Alignment</span><br>
                <span class="as">Wang, RunqiandZheng, HaoandDuan, XiaoyueandLiu, JianzhuangandLu, YuningandWang, TianandXu, SongcenandZhang, Baochang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Few-Shot_Learning_With_Visual_Distribution_Calibration_and_Cross-Modal_Distribution_Alignment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23445-23454.png><br>
            
            <span class="tt"><span class="t0">研究问题：预训练的视觉语言模型在少量学习中存在两个关键问题：图像中的视觉研究问题：预训练的视觉语言模型在少量学习中存在两个关键问题：图像中的视觉特征分布容易受到类别无关信息的干扰，以及视觉和语言特征分布之间的对齐困难。<br>
                    动机：为了解决干扰问题，我们提出了一个选择性攻击模块，通过可训练的适配器生成图像的空间注意力图来指导对类别无关的图像区域的干扰。通过扰乱这些区域，可以捕获关键特征并校准图像特征的视觉分布。为了更好地对齐描述同一对象类的视觉和语言特征分布，我们提出了跨模态分布对齐模块，引入每个类的视觉-语言原型来对齐分布，并采用地球移动距离（EMD）优化原型。<br>
                    方法：我们的方法包括选择性攻击模块、跨模态分布对齐模块和图像文本提示的增强策略。<br>
                    效果：我们在11个数据集上进行了广泛的实验，结果表明我们的方法在少量学习中始终优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-trained vision-language models have inspired much research on few-shot learning. However, with only a few training images, there exist two crucial problems: (1) the visual feature distributions are easily distracted by class-irrelevant information in images, and (2) the alignment between the visual and language feature distributions is difficult. To deal with the distraction problem, we propose a Selective Attack module, which consists of trainable adapters that generate spatial attention maps of images to guide the attacks on class-irrelevant image areas. By messing up these areas, the critical features are captured and the visual distributions of image features are calibrated. To better align the visual and language feature distributions that describe the same object class, we propose a cross-modal distribution alignment module, in which we introduce a vision-language prototype for each class to align the distributions, and adopt the Earth Mover's Distance (EMD) to optimize the prototypes. For efficient computation, the upper bound of EMD is derived. In addition, we propose an augmentation strategy to increase the diversity of the images and the text prompts, which can reduce overfitting to the few-shot training images. Extensive experiments on 11 datasets demonstrate that our method consistently outperforms prior arts in few-shot learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">862.A Strong Baseline for Generalized Few-Shot Semantic Segmentation</span><br>
                <span class="as">Hajimiri, SinaandBoudiaf, MalikandBenAyed, IsmailandDolz, Jose</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hajimiri_A_Strong_Baseline_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11269-11278.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种泛化的小样本分割框架，具有直接的训练过程和易于优化的推理阶段。<br>
                    动机：现有的小样本分割模型训练过程复杂，推理阶段不易优化。<br>
                    方法：提出了一种基于信息最大化原则的简单有效模型，通过最大化学习到的特征表示与其对应预测之间的互信息（MI），并结合知识蒸馏项来保留基类的知识。<br>
                    效果：在流行的小样本分割基准测试PASCAL-5^i和COCO-20^i上，提出的推理模型取得了显著改进，特别是在新类别中，1-shot和5-shot场景下的改进收益分别在7%到26%（PASCAL-5^i）和3%到12%（COCO-20^i）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces a generalized few-shot segmentation framework with a straightforward training process and an easy-to-optimize inference phase. In particular, we propose a simple yet effective model based on the well-known InfoMax principle, where the Mutual Information (MI) between the learned feature representations and their corresponding predictions is maximized. In addition, the terms derived from our MI-based formulation are coupled with a knowledge distillation term to retain the knowledge on base classes. With a simple training process, our inference model can be applied on top of any segmentation network trained on base classes. The proposed inference yields substantial improvements on the popular few-shot segmentation benchmarks, PASCAL-5^i and COCO-20^i. Particularly, for novel classes, the improvement gains range from 7% to 26% (PASCAL-5^i) and from 3% to 12% (COCO-20^i) in the 1-shot and 5-shot scenarios, respectively. Furthermore, we propose a more challenging setting, where performance gaps are further exacerbated. Our code is publicly available at https://github.com/sinahmr/DIaM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">863.Bias-Eliminating Augmentation Learning for Debiased Federated Learning</span><br>
                <span class="as">Xu, Yuan-YiandLin, Ci-SiangandWang, Yu-ChiangFrank</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Bias-Eliminating_Augmentation_Learning_for_Debiased_Federated_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20442-20452.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练在有偏数据集上的模型往往会观察到类别和不良特征之间的相关性，导致性能下降。<br>
                    动机：现有的去偏学习模型主要针对集中式机器学习设计，无法直接应用于联邦学习等分布式设置。<br>
                    方法：提出一种新的联邦学习框架——消除偏见的增强学习（FedBEAL），用于生成每个客户特有的偏见冲突样本。<br>
                    效果：通过在不同类型偏见的数据集上进行图像分类实验，证实了FedBEAL的有效性和适用性，其表现优于最先进的去偏化和联邦学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning models trained on biased datasets tend to observe correlations between categorical and undesirable features, which result in degraded performances. Most existing debiased learning models are designed for centralized machine learning, which cannot be directly applied to distributed settings like federated learning (FL), which collects data at distinct clients with privacy preserved. To tackle the challenging task of debiased federated learning, we present a novel FL framework of Bias-Eliminating Augmentation Learning (FedBEAL), which learns to deploy Bias-Eliminating Augmenters (BEA) for producing client-specific bias-conflicting samples at each client. Since the bias types or attributes are not known in advance, a unique learning strategy is presented to jointly train BEA with the proposed FL framework. Extensive image classification experiments on datasets with various bias types confirm the effectiveness and applicability of our FedBEAL, which performs favorably against state-of-the-art debiasing and FL methods for debiased FL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">864.Generalist: Decoupling Natural and Robust Generalization</span><br>
                <span class="as">Wang, HongjunandWang, Yisen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Generalist_Decoupling_Natural_and_Robust_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20554-20563.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时提高模型的自然泛化能力和对抗性泛化能力，避免在防御对抗性样本时自然泛化能力的下降。<br>
                    动机：现有的对抗性训练方法虽然能提高模型的对抗性泛化能力，但会导致自然泛化能力降低。<br>
                    方法：提出一种双专家框架Generalist，将自然泛化和对抗性泛化分离，分别对两种泛化进行训练。通过收集并结合基础学习器的参数形成全局学习器，再将其作为初始化参数分发给基础学习器继续训练。<br>
                    效果：实验证明，当基础学习器训练良好时，Generalist的风险会降低。大量实验验证了Generalist在保持对抗性鲁棒性的同时，能够实现高自然泛化准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks obtained by standard training have been constantly plagued by adversarial examples. Although adversarial training demonstrates its capability to defend against adversarial examples, unfortunately, it leads to an inevitable drop in the natural generalization. To address the issue, we decouple the natural generalization and the robust generalization from joint training and formulate different training strategies for each one. Specifically, instead of minimizing a global loss on the expectation over these two generalization errors, we propose a bi-expert framework called Generalist where we simultaneously train base learners with task-aware strategies so that they can specialize in their own fields. The parameters of base learners are collected and combined to form a global learner at intervals during the training process. The global learner is then distributed to the base learners as initialized parameters for continued training. Theoretically, we prove that the risks of Generalist will get lower once the base learners are well trained. Extensive experiments verify the applicability of Generalist to achieve high accuracy on natural examples while maintaining considerable robustness to adversarial ones. Code is available at https://github.com/PKU-ML/Generalist.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">865.Learning Decorrelated Representations Efficiently Using Fast Fourier Transform</span><br>
                <span class="as">Shigeto, YutaroandShimbo, MasashiandYoshikawa, YuyaandTakeuchi, Akikazu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shigeto_Learning_Decorrelated_Representations_Efficiently_Using_Fast_Fourier_Transform_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2052-2060.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低自监督表示学习模型的训练复杂度。<br>
                    动机：Barlow Twins和VICReg等自监督表示学习模型虽然效果良好，但高维嵌入的特征相关性需要大量的计算资源。<br>
                    方法：提出一种放松的去相关正则化器，通过快速傅里叶变换在O(n d log d)时间内进行计算，并设计了一种低成本的方法来缓解可能出现的不良局部极小值。<br>
                    效果：所提出的正则化器在下游任务中的准确性与现有正则化器相当，但其训练需要更少的内存，并且对于大d值的训练速度更快。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension d of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for n samples takes O(n d^2) time. In this paper, we propose a relaxed decorrelating regularizer that can be computed in O(n d log d) time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in downstream tasks, whereas their training requires less memory and is faster for large d. The source code is available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">866.Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning</span><br>
                <span class="as">Kotovenko, DmytroandMa, PingchuanandMilbich, TimoandOmmer, Bj\&quot;orn</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kotovenko_Cross-Image-Attention_for_Conditional_Embeddings_in_Deep_Metric_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11070-11081.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练出能产生图像语义相似性并能推广到未见过测试类别的紧凑图像嵌入？<br>
                    动机：当前的深度度量学习（DML）方法在将丰富的局部化图像特征图映射到紧凑嵌入向量时面临挑战，即在计算两幅图像之间的相似性之前，会忽略一幅图像中的信息。<br>
                    方法：提出在训练过程中，将一个图像的嵌入条件化为我们希望比较的另一幅图像。使用交叉注意力，使得一幅图像可以识别另一幅图像中的相关特征，从而建立层次的条件嵌入，逐渐整合关于元组的信息来指导单个图像的表示。<br>
                    效果：实验表明，这种方法显著改善了基础的标准DML流程，并在已建立的DML基准测试上超越了最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning compact image embeddings that yield semantic similarities between images and that generalize to unseen test classes, is at the core of deep metric learning (DML). Finding a mapping from a rich, localized image feature map onto a compact embedding vector is challenging: Although similarity emerges between tuples of images, DML approaches marginalize out information in an individual image before considering another image to which similarity is to be computed. Instead, we propose during training to condition the embedding of an image on the image we want to compare it to. Rather than embedding by a simple pooling as in standard DML, we use cross-attention so that one image can identify relevant features in the other image. Consequently, the attention mechanism establishes a hierarchy of conditional embeddings that gradually incorporates information about the tuple to steer the representation of an individual image. The cross-attention layers bridge the gap between the original unconditional embedding and the final similarity and allow backpropagtion to update encodings more directly than through a lossy pooling layer. At test time we use the resulting improved unconditional embeddings, thus requiring no additional parameters or computational overhead. Experiments on established DML benchmarks show that our cross-attention conditional embedding during training improves the underlying standard DML pipeline significantly so that it outperforms the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">867.Class-Balancing Diffusion Models</span><br>
                <span class="as">Qin, YimingandZheng, HuangjieandYao, JiangchaoandZhou, MingyuanandZhang, Ya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18434-18443.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型在处理长尾数据分布时，会出现多样性和保真度显著下降的问题。<br>
                    动机：现有的扩散模型在处理类别不平衡的数据分布时，尾部类别的生成结果会失去多样性，且存在严重的模式崩溃问题。<br>
                    方法：提出一种基于类别平衡的扩散模型（CBDM），通过引入分布调整正则化器进行训练。<br>
                    效果：实验证明，CBDM生成的图像在数量和质量上都具有更高的多样性和质量，并在CIFAR100/CIFAR100LT数据集上的生成结果以及下游识别任务上都表现出优异的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such class-imbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with class-imbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. Experiments show that images generated by CBDM exhibit higher diversity and quality in both quantitative and qualitative ways. Our method benchmarked the generation results on CIFAR100/CIFAR100LT dataset and shows outstanding performance on the downstream recognition task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">868.Feature Alignment and Uniformity for Test Time Adaptation</span><br>
                <span class="as">Wang, ShuaiandZhang, DaoanandYan, ZipeiandZhang, JianguoandLi, Rui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20050-20060.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度神经网络在接收分布外测试样本时的问题。<br>
                    动机：由于源领域和目标领域之间的领域差距，我们首次将TTA视为特征修订问题。<br>
                    方法：我们提出了一种测试时间自我蒸馏策略来保证当前批次和所有先前批次的表示一致性，以及一种记忆空间局部聚类策略来对齐即将到来的批次的邻居样本的表示。同时，为了处理常见的噪声标签问题，我们提出了熵和一致性过滤器来选择和丢弃可能的噪声标签。<br>
                    效果：实验结果表明，我们的方法不仅稳定地提高了基线性能，而且优于现有的最先进的测试时间适应方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Test time adaptation (TTA) aims to adapt deep neural networks when receiving out of distribution test domain samples. In this setting, the model can only access online unlabeled test samples and pre-trained models on the training domains. We first address TTA as a feature revision problem due to the domain gap between source domains and target domains. After that, we follow the two measurements alignment and uniformity to discuss the test time feature revision. For test time feature uniformity, we propose a test time self-distillation strategy to guarantee the consistency of uniformity between representations of the current batch and all the previous batches. For test time feature alignment, we propose a memorized spatial local clustering strategy to align the representations among the neighborhood samples for the upcoming batch. To deal with the common noisy label problem, we propound the entropy and consistency filters to select and drop the possible noisy labels. To prove the scalability and efficacy of our method, we conduct experiments on four domain generalization benchmarks and four medical image segmentation tasks with various backbones. Experiment results show that our method not only improves baseline stably but also outperforms existing state-of-the-art test time adaptation methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">869.Balanced Product of Calibrated Experts for Long-Tailed Recognition</span><br>
                <span class="as">Aimar, EmanuelSanchezandJonnarth, ArviandFelsberg, MichaelandKuhlmann, Marco</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Aimar_Balanced_Product_of_Calibrated_Experts_for_Long-Tailed_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19967-19977.png><br>
            
            <span class="tt"><span class="t0">研究问题：现实世界的识别问题往往具有长尾标签分布，这对表示学习提出了挑战。<br>
                    动机：由于训练分布与测试分布（如均匀分布和长尾分布）的差异，需要解决分布偏移的问题。<br>
                    方法：提出平衡专家产品（BalPoE），通过调整专家的logit来鼓励多样性，并结合一系列不同的测试时间目标分布。<br>
                    效果：在CIFAR-100-LT、ImageNet-LT和iNaturalist-2018三个长尾数据集上，该方法取得了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ensemble is Fisher-consistent for minimizing the balanced error. Our theoretical analysis shows that our balanced ensemble requires calibrated experts, which we achieve in practice using mixup. We conduct extensive experiments and our method obtains new state-of-the-art results on three long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our code is available at https://github.com/emasa/BalPoE-CalibratedLT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">870.RMLVQA: A Margin Loss Approach for Visual Question Answering With Language Biases</span><br>
                <span class="as">Basu, AbhipsaandAddepalli, SravantiandBabu, R.Venkatesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11671-11680.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉问答模型存在语言偏见，即模型在学习问题和答案之间的关联时忽视了图像。<br>
                    动机：早期的工作试图使用仅问题模型或数据增强来减少这种偏见，但效果不佳。因此，我们提出了一种具有两个组件的自适应边距损失方法。<br>
                    方法：该方法的第一个组件考虑了训练数据中某一类型问题的答案频率，以解决类别不平衡引起的语言偏见问题。第二个组件则通过学习实例特定的边距，使模型能够区分不同复杂度的样本。我们还在模型中引入了一个偏见注入组件，并从该组件的置信度计算实例特定的边距。我们将这些与估计的边距相结合，以在训练损失中同时考虑答案频率和任务复杂性。<br>
                    效果：实验结果表明，虽然边距损失对分布外（ood）数据有效，但偏见注入组件对于泛化到分布内（id）数据至关重要。我们的RMLVQA方法在基准VQA数据集上优于无增强的方法，同时在id数据上保持了竞争力，使其成为所有可比方法中最稳健的一种。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual Question Answering models have been shown to suffer from language biases, where the model learns a correlation between the question and the answer, ignoring the image. While early works attempted to use question-only models or data augmentations to reduce this bias, we propose an adaptive margin loss approach having two components. The first component considers the frequency of answers within a question type in the training data, which addresses the concern of the class-imbalance causing the language biases. However, it does not take into account the answering difficulty of the samples, which impacts their learning. We address this through the second component, where instance-specific margins are learnt, allowing the model to distinguish between samples of varying complexity. We introduce a bias-injecting component to our model, and compute the instance-specific margins from the confidence of this component. We combine these with the estimated margins to consider both answer-frequency and task-complexity in the training loss. We show that, while the margin loss is effective for out-of-distribution (ood) data, the bias-injecting component is essential for generalising to in-distribution (id) data. Our proposed approach, Robust Margin Loss for Visual Question Answering (RMLVQA) improves upon the existing state-of-the-art results when compared to augmentation-free methods on benchmark VQA datasets suffering from language biases, while maintaining competitive performance on id data, making our method the most robust one among all comparable methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">871.Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning</span><br>
                <span class="as">Wang, HanjingandJoshi, DhirajandWang, ShiqiangandJi, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Gradient-Based_Uncertainty_Attribution_for_Explainable_Bayesian_Deep_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12044-12053.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度学习模型的预测容易受到数据扰动、对抗性攻击和分布外输入的影响，因此需要准确量化预测的不确定性。<br>
                    动机：为了构建可信赖的AI系统，识别并缓解不确定性来源对预测的影响至关重要。<br>
                    方法：我们提出了一种可解释且可操作的贝叶斯深度学习方法，不仅可以准确量化不确定性，还可以解释这些不确定性，识别其来源，并提出策略来减轻不确定性的影响。具体来说，我们引入了一种基于梯度的不确定性归因方法，以确定导致预测不确定性的最有问题的输入区域。<br>
                    效果：与现有方法相比，我们提出的方法具有竞争力的准确性、宽松的假设和高效率。此外，我们还提出了一种利用归因结果作为注意力来进一步改善模型性能的不确定性缓解策略。定性和定量评估都证明了我们提出的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predictions made by deep learning models are prone to data perturbations, adversarial attacks, and out-of-distribution inputs. To build a trusted AI system, it is therefore critical to accurately quantify the prediction uncertainties. While current efforts focus on improving uncertainty quantification accuracy and efficiency, there is a need to identify uncertainty sources and take actions to mitigate their effects on predictions. Therefore, we propose to develop explainable and actionable Bayesian deep learning methods to not only perform accurate uncertainty quantification but also explain the uncertainties, identify their sources, and propose strategies to mitigate the uncertainty impacts. Specifically, we introduce a gradient-based uncertainty attribution method to identify the most problematic regions of the input that contribute to the prediction uncertainty. Compared to existing methods, the proposed UA-Backprop has competitive accuracy, relaxed assumptions, and high efficiency. Moreover, we propose an uncertainty mitigation strategy that leverages the attribution results as attention to further improve the model performance. Both qualitative and quantitative evaluations are conducted to demonstrate the effectiveness of our proposed methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">872.Manipulating Transfer Learning for Property Inference</span><br>
                <span class="as">Tian, YulongandSuya, FnuandSuri, AnshumanandXu, FengyuanandEvans, David</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Manipulating_Transfer_Learning_for_Property_Inference_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15975-15984.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了在迁移学习中，如何利用控制上游模型的敌手对下游模型进行属性推断攻击。<br>
                    动机：目前预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transfer learning is a popular method for tuning pretrained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim's tuned downstream model. For example, to infer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to conduct highly effective and specific property inference attacks (AUC score > 0.9), without incurring significant performance loss on the main task. The main idea of the manipulation is to make the upstream model generate activations (intermediate features) with different distributions for samples with and without a target property, thus enabling the adversary to distinguish easily between downstream models trained with and without training examples that have the target property. Our code is available at https://github.com/yulongt23/Transfer-Inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">873.Class Adaptive Network Calibration</span><br>
                <span class="as">Liu, BingyuanandRony, J\&#x27;er\^omeandGaldran, AdrianandDolz, JoseandBenAyed, Ismail</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Class_Adaptive_Network_Calibration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16070-16079.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练现代深度神经网络时，除了常规的准确度外，还应考虑校准问题。<br>
                    动机：目前的方法在处理学习过程中的不校准问题时存在两个主要缺点：1）所有类别的标量平衡权重相同，阻碍了解决不同类别内部困难或类别间不平衡的能力；2）平衡权重通常固定，没有自适应策略，可能无法在准确度和校准之间达到最佳折衷，并且需要为每个应用进行超参数搜索。<br>
                    方法：我们提出了一种用于校准深度网络的类自适应标签平滑（CALS）方法，该方法允许在训练期间学习类别特定的乘数，从而成为常见的标签平滑惩罚的强大替代方案。<br>
                    效果：我们在各种基准上进行了全面评估和多次比较，包括标准的和长尾的图像分类、语义分割和文本分类，结果证明了所提出的方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies have revealed that, beyond conventional accuracy, calibration should also be considered for training modern deep neural networks. To address miscalibration during learning, some methods have explored different penalty functions as part of the learning objective, alongside a standard classification loss, with a hyper-parameter controlling the relative contribution of each term. Nevertheless, these methods share two major drawbacks: 1) the scalar balancing weight is the same for all classes, hindering the ability to address different intrinsic difficulties or imbalance among classes; and 2) the balancing weight is usually fixed without an adaptive strategy, which may prevent from reaching the best compromise between accuracy and calibration, and requires hyper-parameter search for each application. We propose Class Adaptive Label Smoothing (CALS) for calibrating deep networks, which allows to learn class-wise multipliers during training, yielding a powerful alternative to common label smoothing penalties. Our method builds on a general Augmented Lagrangian approach, a well-established technique in constrained optimization, but we introduce several modifications to tailor it for large-scale, class-adaptive training. Comprehensive evaluation and multiple comparisons on a variety of benchmarks, including standard and long-tailed image classification, semantic segmentation, and text classification, demonstrate the superiority of the proposed method. The code is available at https://github.com/by-liu/CALS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">874.TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation</span><br>
                <span class="as">Tomar, DevavratandVray, GuillaumeandBozorgtabar, BehzadandThiran, Jean-Philippe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tomar_TeSLA_Test-Time_Self-Learning_With_Automatic_Adversarial_Augmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20341-20350.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练的源模型适应到未标记的流式测试数据。<br>
                    动机：现有的测试时间适应方法主要关注分类任务，使用专门的网络架构，破坏模型校准或依赖于源领域的轻量级信息。<br>
                    方法：本文提出了一种新的测试时间自我学习方法，通过自动对抗性增强（TeSLA）来适应预训练的源模型到未标记的流式测试数据。与传统基于交叉熵的自我学习方法不同，我们引入了一个新的测试时间损失函数，通过与互信息和在线知识蒸馏的隐式紧密联系来实现。此外，我们还提出了一种可学习的高效对抗性增强模块，通过模拟高熵增强图像来进一步增强在线知识蒸馏。<br>
                    效果：该方法在几个基准和类型的领域转移上取得了最先进的分类和分割结果，特别是在具有挑战性的医学图像测量转移上。与其他竞争方法相比，TeSLA还具有一些理想的特性，包括校准、不确定性度量、对模型架构的不敏感性和源训练策略，所有这些都得到了广泛的消融实验的支持。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most recent test-time adaptation methods focus on only classification tasks, use specialized network architectures, destroy model calibration or rely on lightweight information from the source domain. To tackle these issues, this paper proposes a novel Test-time Self-Learning method with automatic Adversarial augmentation dubbed TeSLA for adapting a pre-trained source model to the unlabeled streaming test data. In contrast to conventional self-learning methods based on cross-entropy, we introduce a new test-time loss function through an implicitly tight connection with the mutual information and online knowledge distillation. Furthermore, we propose a learnable efficient adversarial augmentation module that further enhances online knowledge distillation by simulating high entropy augmented images. Our method achieves state-of-the-art classification and segmentation results on several benchmarks and types of domain shifts, particularly on challenging measurement shifts of medical images. TeSLA also benefits from several desirable properties compared to competing methods in terms of calibration, uncertainty metrics, insensitivity to model architectures, and source training strategies, all supported by extensive ablations. Our code and models are available at https://github.com/devavratTomar/TeSLA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">875.Promoting Semantic Connectivity: Dual Nearest Neighbors Contrastive Learning for Unsupervised Domain Generalization</span><br>
                <span class="as">Liu, YuchenandWang, YaomingandChen, YaboandDai, WenruiandLi, ChenglinandZou, JunniandXiong, Hongkai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Promoting_Semantic_Connectivity_Dual_Nearest_Neighbors_Contrastive_Learning_for_Unsupervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3510-3519.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前无监督领域泛化（UDG）方法严重依赖昂贵的有标签源数据，而未标记的数据更易获取。因此，本研究探讨了更具实际意义的无监督领域泛化（UDG）问题。<br>
                    动机：对比学习从不同视角学习不变的视觉表示在同域无监督学习中具有良好语义特性，但在跨域场景中表现不佳。因此，本研究旨在解决这一问题。<br>
                    方法：我们首先深入探讨了普通对比学习的失败原因，并指出语义连通性是UDG的关键。具体来说，抑制同域连通性并鼓励同类间的连通性有助于学习领域不变的语义信息。然后，我们提出了一种新的无监督领域泛化方法，即带有强增强的双重最近邻对比学习（DN^2A）。<br>
                    效果：实验结果表明，我们的DN^2A大幅超越了最先进的技术，例如，仅使用1%的标签，在PACS和DomainNet上的线性评估准确率分别提高了12.01%和13.11%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain Generalization (DG) has achieved great success in generalizing knowledge from source domains to unseen target domains. However, current DG methods rely heavily on labeled source data, which are usually costly and unavailable. Since unlabeled data are far more accessible, we study a more practical unsupervised domain generalization (UDG) problem. Learning invariant visual representation from different views, i.e., contrastive learning, promises well semantic features for in-domain unsupervised learning. However, it fails in cross-domain scenarios. In this paper, we first delve into the failure of vanilla contrastive learning and point out that semantic connectivity is the key to UDG. Specifically, suppressing the intra-domain connectivity and encouraging the intra-class connectivity help to learn the domain-invariant semantic information. Then, we propose a novel unsupervised domain generalization approach, namely Dual Nearest Neighbors contrastive learning with strong Augmentation (DN^2A). Our DN^2A leverages strong augmentations to suppress the intra-domain connectivity and proposes a novel dual nearest neighbors search strategy to find trustworthy cross domain neighbors along with in-domain neighbors to encourage the intra-class connectivity. Experimental results demonstrate that our DN^2A outperforms the state-of-the-art by a large margin, e.g., 12.01% and 13.11% accuracy gain with only 1% labels for linear evaluation on PACS and DomainNet, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">876.Exploring and Utilizing Pattern Imbalance</span><br>
                <span class="as">Mei, ShibinandZhao, ChenglongandYuan, ShengchaoandNi, Bingbing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Exploring_and_Utilizing_Pattern_Imbalance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7569-7578.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决模式不平衡问题，并开发新的训练方案来避免模式偏好和虚假相关性。<br>
                    动机：现有的方法主要关注类别或领域的粒度，忽视了数据集中可能存在的更精细的结构。<br>
                    方法：我们提出了一种新的种子类别定义，作为区分同一类别或领域中不同模式的优化单位。<br>
                    效果：在各种规模的领域泛化数据集上的大量实验证明了所提出方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we identify pattern imbalance from several aspects, and further develop a new training scheme to avert pattern preference as well as spurious correlation. In contrast to prior methods which are mostly concerned with category or domain granularity, ignoring the potential finer structure that existed in datasets, we give a new definition of seed category as an appropriate optimization unit to distinguish different patterns in the same category or domain. Extensive experiments on domain generalization datasets of diverse scales demonstrate the effectiveness of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">877.Are Data-Driven Explanations Robust Against Out-of-Distribution Data?</span><br>
                <span class="as">Li, TangandQiao, FengchunandMa, MengmengandPeng, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Are_Data-Driven_Explanations_Robust_Against_Out-of-Distribution_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3821-3831.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的数据驱动解释方法是否对分布偏移具有鲁棒性？<br>
                    动机：随着黑箱模型在高风险应用中的影响力日益增强，各种数据驱动的解释方法被引入。然而，机器学习模型不断受到分布偏移的挑战。<br>
                    方法：我们提出了一个端到端的模型无关学习框架——分布稳健解释（DRE）。其核心思想是，受自我监督学习的启发，充分利用分布间信息，为无需人工标注的解释学习提供监督信号。<br>
                    效果：我们在广泛的任务和数据类型上进行了大量实验，包括图像和科学表格数据的分类和回归。实验结果表明，该方法显著提高了模型在解释和预测鲁棒性方面对抗分布偏移的能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As black-box models increasingly power high-stakes applications, a variety of data-driven explanation methods have been introduced. Meanwhile, machine learning models are constantly challenged by distributional shifts. A question naturally arises: Are data-driven explanations robust against out-of-distribution data? Our empirical results show that even though predict correctly, the model might still yield unreliable explanations under distributional shifts. How to develop robust explanations against out-of-distribution data? To address this problem, we propose an end-to-end model-agnostic learning framework Distributionally Robust Explanations (DRE). The key idea is, inspired by self-supervised learning, to fully utilizes the inter-distribution information to provide supervisory signals for the learning of explanations without human annotation. Can robust explanations benefit the model's generalization capability? We conduct extensive experiments on a wide range of tasks and data types, including classification and regression on image and scientific tabular data. Our results demonstrate that the proposed method significantly improves the model's performance in terms of explanation and prediction robustness against distributional shifts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">878.Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification</span><br>
                <span class="as">Ma, YanbiaoandJiao, LichengandLiu, FangandYang, ShuyuanandLiu, XuandLi, Lingling</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Curvature-Balanced_Feature_Manifold_Learning_for_Long-Tailed_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15824-15835.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习模型在处理长尾分类问题时存在的模型偏见问题。<br>
                    动机：尽管已有的研究提出了一些方法来减少模型的偏见，但最近的研究表明，长尾类别并非总是难以学习，并且在样本平衡的数据集中也观察到了模型偏见，暗示存在其他影响模型偏见的因素。<br>
                    方法：本文系统地提出了一系列用于深度神经网络感知流形的几何度量，并探索了感知流形的几何特性对分类难度的影响以及学习如何塑造感知流形的几何特性。<br>
                    效果：研究发现，类准确度与感知流形的分离程度之间的相关性在训练过程中逐渐减小，而与曲率的负相关性逐渐增大，这表明曲率失衡会导致模型偏见。因此，本文提出了曲率正则化方法，以帮助模型学习曲率平衡且更平坦的感知流形。在多个长尾和非长尾数据集上的评估表明，该方法具有出色的性能和令人兴奋的通用性，特别是在基于当前最先进的技术实现显著的性能改进方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measures for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to model bias. Therefore, we propose curvature regularization to facilitate the model to learn curvature-balanced and flatter perceptual manifolds. Evaluations on multiple long-tailed and non-long-tailed datasets show the excellent performance and exciting generality of our approach, especially in achieving significant performance improvements based on current state-of-the-art techniques. Our work reminds researchers to pay attention to model bias not only on long-tailed datasets but also on non-long-tailed and even data-balanced datasets, which can improve model performance from another perspective.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>