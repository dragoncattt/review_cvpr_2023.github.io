<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic2</h2>
      <b>Topic words : &ensp;</b>view, &ensp;pose, &ensp;neural, &ensp;scene, &ensp;reconstruction, &ensp;estimation, &ensp;images, &ensp;depth</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">194.GFPose: Learning 3D Human Pose Prior With Gradient Fields</span><br>
                <span class="as">Ci, HaiandWu, MingdongandZhu, WentaoandMa, XiaoxuanandDong, HaoandZhong, FangweiandWang, Yizhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4800-4810.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地学习3D人体姿态先验，以实现以人为中心的人工智能。<br>
                    动机：3D人体姿态先验的学习对于各种应用至关重要，例如姿势估计、补全和生成等。<br>
                    方法：提出了一种名为GFPose的通用框架，该框架的核心是一个时间依赖性得分网络，用于估计每个身体关节的梯度并逐步去噪扰动的3D人体姿态以匹配给定的任务规范。在去噪过程中，GFPose隐式地将姿态先验纳入梯度中，并在一个优雅的框架中统一了各种判别性和生成性任务。<br>
                    效果：实验结果表明，1）作为多假设姿态估计器，GFPose在Human3.6M数据集上比现有最先进技术高出20%。2）作为单假设姿态估计器，即使使用基本的骨架，GFPose也能取得与确定性最先进技术相当的结果。3）在姿态去噪、补全和生成任务中，GFPose能够产生多样化和真实的样本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning 3D human pose prior is essential to human-centered AI. Here, we present GFPose, a versatile framework to model plausible 3D human poses for various applications. At the core of GFPose is a time-dependent score network, which estimates the gradient on each body joint and progressively denoises the perturbed 3D human pose to match a given task specification. During the denoising process, GFPose implicitly incorporates pose priors in gradients and unifies various discriminative and generative tasks in an elegant framework. Despite the simplicity, GFPose demonstrates great potential in several downstream tasks. Our experiments empirically show that 1) as a multi-hypothesis pose estimator, GFPose outperforms existing SOTAs by 20% on Human3.6M dataset. 2) as a single-hypothesis pose estimator, GFPose achieves comparable results to deterministic SOTAs, even with a vanilla backbone. 3) GFPose is able to produce diverse and realistic samples in pose denoising, completion and generation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">195.CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes</span><br>
                <span class="as">Bhatia, HarshilandTretschk, EdithandL\&quot;ahner, ZorahandBenkner, MarcelSeelbachandMoeller, MichaelandTheobalt, ChristianandGolyanik, Vladislav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhatia_CCuantuMM_Cycle-Consistent_Quantum-Hybrid_Matching_of_Multiple_Shapes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1296-1305.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地匹配多个非刚性变形的3D形状，并保证匹配结果的循环一致性。<br>
                    动机：现有的量子形状匹配方法无法支持多形状匹配，且无法保证循环一致性。<br>
                    方法：本文提出了首个支持多形状匹配且保证循环一致性的量子混合方法。通过将N形状问题转化为三个形状的匹配序列，降低了问题的复杂性。利用量子退火技术，为中间的NP难问题获取高质量的低能量解决方案。<br>
                    效果：在基准数据集上，该方法显著优于先前的量子混合两形状匹配方法的多形状匹配扩展，并与经典的多形状匹配方法相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Jointly matching multiple, non-rigidly deformed 3D shapes is a challenging, NP-hard problem. A perfect matching is necessarily cycle-consistent: Following the pairwise point correspondences along several shapes must end up at the starting vertex of the original shape. Unfortunately, existing quantum shape-matching methods do not support multiple shapes and even less cycle consistency. This paper addresses the open challenges and introduces the first quantum-hybrid approach for 3D shape multi-matching; in addition, it is also cycle-consistent. Its iterative formulation is admissible to modern adiabatic quantum hardware and scales linearly with the total number of input shapes. Both these characteristics are achieved by reducing the N-shape case to a sequence of three-shape matchings, the derivation of which is our main technical contribution. Thanks to quantum annealing, high-quality solutions with low energy are retrieved for the intermediate NP-hard objectives. On benchmark datasets, the proposed approach significantly outperforms extensions to multi-shape matching of a previous quantum-hybrid two-shape matching method and is on-par with classical multi-matching methods. Our source code is available at 4dqv.mpi-inf.mpg.de/CCuantuMM/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">196.Painting 3D Nature in 2D: View Synthesis of Natural Scenes From a Single Semantic Mask</span><br>
                <span class="as">Zhang, ShangzhanandPeng, SidaandChen, TianrunandMou, LinzhanandLin, HaotongandYu, KaichengandLiao, YiyiandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Painting_3D_Nature_in_2D_View_Synthesis_of_Natural_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8518-8528.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单个语义掩码合成自然场景的多视图一致颜色图像。<br>
                    动机：现有的3D感知图像合成方法需要多视图监督或学习特定类别物体的类别级先验，不适用于自然场景。<br>
                    方法：使用语义场作为中间表示，从输入的语义掩码中重建，然后借助现成的语义图像合成模型转化为辐射场。<br>
                    效果：实验表明，该方法优于基线方法，并能生成各种自然场景的逼真和多视图一致的视频。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a novel approach that takes a single semantic mask as input to synthesize multi-view consistent color images of natural scenes, trained with a collection of single images from the Internet. Prior works on 3D-aware image synthesis either require multi-view supervision or learning category-level prior for specific classes of objects, which are inapplicable to natural scenes. Our key idea to solve this challenge is to use a semantic field as the intermediate representation, which is easier to reconstruct from an input semantic mask and then translated to a radiance field with the assistance of off-the-shelf semantic image synthesis models. Experiments show that our method outperforms baseline methods and produces photorealistic and multi-view consistent videos of a variety of natural scenes. The project website is https://zju3dv.github.io/paintingnature/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">197.Shape, Pose, and Appearance From a Single Image via Bootstrapped Radiance Field Inversion</span><br>
                <span class="as">Pavllo, DarioandTan, DavidJosephandRakotosaona, Marie-JulieandTombari, Federico</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pavllo_Shape_Pose_and_Appearance_From_a_Single_Image_via_Bootstrapped_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4391-4401.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从单视图进行3D重建，特别是在没有精确地面真位姿的情况下。<br>
                    动机：尽管神经辐射场（NeRF）和生成对抗网络（GANs）在合成数据集上表现出色，但它们忽视了对位姿估计的重要性，这对于增强现实（AR）和机器人等某些下游应用至关重要。<br>
                    方法：我们提出了一种端到端的自然图像重建框架，该框架无需在训练过程中利用多视图信息，就能从物体的单张图像中恢复出SDF参数化的3D形状、姿态和外观。<br>
                    效果：我们的框架可以在10步内完成图像的反渲染，使其适用于实际场景。我们在各种真实和合成基准测试中展示了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) coupled with GANs represent a promising direction in the area of 3D reconstruction from a single view, owing to their ability to efficiently model arbitrary topologies. Recent work in this area, however, has mostly focused on synthetic datasets where exact ground-truth poses are known, and has overlooked pose estimation, which is important for certain downstream applications such as augmented reality (AR) and robotics. We introduce a principled end-to-end reconstruction framework for natural images, where accurate ground-truth poses are not available. Our approach recovers an SDF-parameterized 3D shape, pose, and appearance from a single image of an object, without exploiting multiple views during training. More specifically, we leverage an unconditional 3D-aware generator, to which we apply a hybrid inversion scheme where a model produces a first guess of the solution which is then refined via optimization. Our framework can de-render an image in as few as 10 steps, enabling its use in practical scenarios. We demonstrate state-of-the-art results on a variety of real and synthetic benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">198.NoPe-NeRF: Optimising Neural Radiance Field With No Pose Prior</span><br>
                <span class="as">Bian, WenjingandWang, ZiruiandLi, KejieandBian, Jia-WangandPrisacariu, VictorAdrian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4160-4169.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练一个无需预计算相机位姿的神经辐射场（NeRF）模型。<br>
                    动机：尽管近期的研究已经证明在面向前方的场景中，可以联合优化NeRF和相机位姿，但在剧烈的相机运动中仍面临困难。<br>
                    方法：通过引入未失真的单目深度先验来解决此难题。这些先验是通过在训练过程中修正比例和位移参数生成的，然后我们使用这些参数来约束连续帧之间的相对姿态。<br>
                    效果：在真实世界的室内和室外场景上的实验表明，我们的方法能够处理具有挑战性的相机轨迹，并在新视图渲染质量和姿态估计准确性方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">199.3D Shape Reconstruction of Semi-Transparent Worms</span><br>
                <span class="as">Ilett, ThomasP.andYuval, OmerandRanner, ThomasandCohen, NettaandHogg, DavidC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ilett_3D_Shape_Reconstruction_of_Semi-Transparent_Worms_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12565-12575.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对半透明且不断进出焦点的主题进行3D形状重建？<br>
                    动机：传统的多图像识别方法在面对半透明和动态模糊的主题时无法有效工作。<br>
                    方法：我们通过渲染具有自适应模糊和透明度的候选形状并与图像进行比较来克服这些挑战。我们还开发了一种新的可微分渲染器，用于从2D投影构建图像并与原始图像进行比较，以生成像素级的误差，并使用梯度下降法联合更新曲线、相机和渲染器参数。<br>
                    效果：该方法能够抵抗干扰（如流体中被困的气泡和污垢），在复杂的姿势序列中保持一致性，从模糊的图像中恢复可靠的估计，并在跟踪秀丽隐杆线虫的3D形状方面取得了显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D shape reconstruction typically requires identifying object features or textures in multiple images of a subject. This approach is not viable when the subject is semi-transparent and moving in and out of focus. Here we overcome these challenges by rendering a candidate shape with adaptive blurring and transparency for comparison with the images. We use the microscopic nematode Caenorhabditis elegans as a case study as it freely explores a 3D complex fluid with constantly changing optical properties. We model the slender worm as a 3D curve using an intrinsic parametrisation that naturally admits biologically-informed constraints and regularisation. To account for the changing optics we develop a novel differentiable renderer to construct images from 2D projections and compare against raw images to generate a pixel-wise error to jointly update the curve, camera and renderer parameters using gradient descent. The method is robust to interference such as bubbles and dirt trapped in the fluid, stays consistent through complex sequences of postures, recovers reliable estimates from blurry images and provides a significant improvement on previous attempts to track C. elegans in 3D. Our results demonstrate the potential of direct approaches to shape estimation in complex physical environments in the absence of ground-truth data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">200.Swept-Angle Synthetic Wavelength Interferometry</span><br>
                <span class="as">Kotwal, AlankarandLevin, AnatandGkioulekas, Ioannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kotwal_Swept-Angle_Synthetic_Wavelength_Interferometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8233-8243.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的成像技术，用于全域微米级3D传感的扫描角合成波长干涉测量。<br>
                    动机：传统的合成波长干涉测量技术使用两种窄带分隔的光学波长的光，其相位编码场景深度，但易受像差和（亚）表面散射的影响。<br>
                    方法：本文提出的新技术通过模拟空间非相干照明，使干涉测量对像差和（亚）表面散射不敏感，同时结合了扫描干涉测量设置的鲁棒性和全域干涉测量设置的速度。<br>
                    效果：实验证明，该技术可以在5Hz的帧速率下，甚至在强烈的环境光下，以5微米的横向和轴向分辨率恢复完整的深度图。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a new imaging technique, swept-angle synthetic wavelength interferometry, for full-field micron-scale 3D sensing. As in conventional synthetic wavelength interferometry, our technique uses light consisting of two narrowly-separated optical wavelengths, resulting in per-pixel interferometric measurements whose phase encodes scene depth. Our technique additionally uses a new type of light source that, by emulating spatially-incoherent illumination, makes interferometric measurements insensitive to aberrations and (sub)surface scattering, effects that corrupt phase measurements. The resulting technique combines the robustness to such corruptions of scanning interferometric setups, with the speed of full-field interferometric setups. Overall, our technique can recover full-frame depth at a lateral and axial resolution of 5 microns, at frame rates of 5 Hz, even under strong ambient light. We build an experimental prototype, and use it to demonstrate these capabilities by scanning a variety of objects, including objects representative of applications in inspection and fabrication, and objects that contain challenging light scattering effects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">201.Multi-Space Neural Radiance Fields</span><br>
                <span class="as">Yin, Ze-XinandQiu, JiaxiongandCheng, Ming-MingandRen, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Multi-Space_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12407-12416.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于NeRF的方法在处理反射物体时，常常导致渲染结果模糊或失真。<br>
                    动机：提出一种多空间神经辐射场（MS-NeRF）方法，通过并行子空间的特征场来表示场景，以更好地理解神经网络对反射和折射物体的存在。<br>
                    方法：MS-NeRF将场景表示为一组并行子空间中的特征场，而不是计算单个辐射场。这种方法是对现有NeRF方法的增强，只需要很小的额外空间输出的训练和推理计算开销。<br>
                    效果：通过三个代表性的基于NeRF的模型进行比较，实验表明，对于通过类似镜子的物体的复杂光线路径的高质渲染，MS-NeRF方法显著优于现有的单空间NeRF方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) and its variants have reached state-of-the-art performance in many novel-view-synthesis-related tasks. However, current NeRF-based methods still suffer from the existence of reflective objects, often resulting in blurry or distorted rendering. Instead of calculating a single radiance field, we propose a multispace neural radiance field (MS-NeRF) that represents the scene using a group of feature fields in parallel sub-spaces, which leads to a better understanding of the neural network toward the existence of reflective and refractive objects. Our multi-space scheme works as an enhancement to existing NeRF methods, with only small computational overheads needed for training and inferring the extra-space outputs. We demonstrate the superiority and compatibility of our approach using three representative NeRF-based models, i.e., NeRF, Mip-NeRF, and Mip-NeRF 360. Comparisons are performed on a novelly constructed dataset consisting of 25 synthetic scenes and 7 real captured scenes with complex reflection and refraction, all having 360-degree viewpoints. Extensive experiments show that our approach significantly outperforms the existing single-space NeRF methods for rendering high-quality scenes concerned with complex light paths through mirror-like objects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">202.Two-View Geometry Scoring Without Correspondences</span><br>
                <span class="as">Barroso-Laguna, AxelandBrachmann, EricandPrisacariu, VictorAdrianandBrostow, GabrielJ.andTurmukhambetov, Daniyar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Barroso-Laguna_Two-View_Geometry_Scoring_Without_Correspondences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8979-8989.png><br>
            
            <span class="tt"><span class="t0">研究问题：传统的双视图几何的相机姿态估计依赖于RANSAC，但在某些情况下，这种方法会偏向于选择不理想的模型。<br>
                    动机：为了解决这个问题，我们提出了基础评分网络（FSNet），它通过极线注意力机制预测两张重叠图像的姿态误差，而不是依赖于稀疏的对应关系。<br>
                    方法：FSNet不需要稀疏的对应关系，而是通过一个极线注意力机制来体现双视图几何模型，可以融入到传统的RANSAC循环中。<br>
                    效果：我们在室内和室外数据集上评估了FSNet在基本矩阵和本质矩阵估计上的表现，结果表明，FSNet能够成功地为具有少量或不可靠对应关系的图像对识别出好的姿态。此外，我们还表明，将FSNet与MAGSAC++评分方法相结合可以达到最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Camera pose estimation for two-view geometry traditionally relies on RANSAC. Normally, a multitude of image correspondences leads to a pool of proposed hypotheses, which are then scored to find a winning model. The inlier count is generally regarded as a reliable indicator of "consensus". We examine this scoring heuristic, and find that it favors disappointing models under certain circumstances. As a remedy, we propose the Fundamental Scoring Network (FSNet), which infers a score for a pair of overlapping images and any proposed fundamental matrix. It does not rely on sparse correspondences, but rather embodies a two-view geometry model through an epipolar attention mechanism that predicts the pose error of the two images. FSNet can be incorporated into traditional RANSAC loops. We evaluate FSNet on fundamental and essential matrix estimation on indoor and outdoor datasets, and establish that FSNet can successfully identify good poses for pairs of images with few or unreliable correspondences. Besides, we show that naively combining FSNet with MAGSAC++ scoring approach achieves state of the art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">203.Panoptic Lifting for 3D Scene Understanding With Neural Fields</span><br>
                <span class="as">Siddiqui, YawarandPorzi, LorenzoandBul\`o, SamuelRotaandM\&quot;uller, NormanandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9043-9052.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从野外场景的图像中学习全面的3D体积表示。<br>
                    动机：现有的方法需要直接或间接使用3D输入，而我们的方法只需要从预训练网络中推断出的2D全景分割蒙版。<br>
                    方法：我们提出了一种新颖的全景提升方法，该方法基于神经场表示生成场景的统一和多视图一致的3D全景表示。为了解决不同视图中2D实例标识符的不一致性，我们解决了一个基于模型当前预测和机器生成分割蒙版的成本的线性分配问题，从而以一致的方式将2D实例提升到3D。<br>
                    效果：我们在具有挑战性的Hypersim、Replica和ScanNet数据集上验证了我们的方法，与最先进的技术相比，在场景级PQ上提高了8.4%、13.8%和10.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Panoptic Lifting, a novel approach for learning panoptic 3D volumetric representations from images of in-the-wild scenes. Once trained, our model can render color images together with 3D-consistent panoptic segmentation from novel viewpoints. Unlike existing approaches which use 3D input directly or indirectly, our method requires only machine-generated 2D panoptic segmentation masks inferred from a pre-trained network. Our core contribution is a panoptic lifting scheme based on a neural field representation that generates a unified and multi-view consistent, 3D panoptic representation of the scene. To account for inconsistencies of 2D instance identifiers across views, we solve a linear assignment with a cost based on the model's current predictions and the machine-generated segmentation masks, thus enabling us to lift 2D instances to 3D in a consistent way. We further propose and ablate contributions that make our method more robust to noisy, machine-generated labels, including test-time augmentations for confidence estimates, segment consistency loss, bounded segmentation fields, and gradient stopping. Experimental results validate our approach on the challenging Hypersim, Replica, and ScanNet datasets, improving by 8.4, 13.8, and 10.6% in scene-level PQ over state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">204.Single View Scene Scale Estimation Using Scale Field</span><br>
                <span class="as">Lee, Byeong-UkandZhang, JianmingandHold-Geoffroy, YannickandKweon, InSo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Single_View_Scene_Scale_Estimation_Using_Scale_Field_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21435-21444.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种基于新型尺度场表示的单图像比例估计方法。<br>
                    动机：现有的相机参数存在模糊性，需要一种简单有效的方法来收集任意图像的比例标注。<br>
                    方法：通过在校准的全景图像数据和野外人工标注数据上训练模型，生成各种图像上的稳健尺度场。<br>
                    效果：该方法可以应用于各种3D理解和尺度感知的图像编辑应用中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose a single image scale estimation method based on a novel scale field representation. A scale field defines the local pixel-to-metric conversion ratio along the gravity direction on all the ground pixels. This representation resolves the ambiguity in camera parameters, allowing us to use a simple yet effective way to collect scale annotations on arbitrary images from human annotators. By training our model on calibrated panoramic image data and the in-the-wild human annotated data, our single image scene scale estimation network generates robust scale field on a variety of image, which can be utilized in various 3D understanding and scale-aware image editing applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">205.SCADE: NeRFs from Space Carving With Ambiguity-Aware Depth Estimates</span><br>
                <span class="as">Uy, MikaelaAngelinaandMartin-Brualla, RicardoandGuibas, LeonidasandLi, Ke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Uy_SCADE_NeRFs_from_Space_Carving_With_Ambiguity-Aware_Depth_Estimates_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16518-16527.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经辐射场（NeRFs）在少量视图下的表现不佳，因为体积渲染施加的约束不足。<br>
                    动机：为了解决这一问题，我们提出了SCADE，一种改进稀疏、无约束输入视图下的NeRF重建质量的新方法。<br>
                    方法：我们利用最先进的单目深度估计模型产生的每视图深度估计作为几何先验来约束NeRF重建。同时，我们提出一种新的方法，通过条件隐式最大似然估计（cIMLE）预测每个视图的连续多模态深度估计分布。此外，我们还引入了一种创新的空间雕刻损失，以融合来自每个视图的多个假设深度图，并从中提炼出与所有视图一致的共同几何体。<br>
                    效果：实验表明，我们的方法能够在稀疏视图下实现更高的新颖视图合成保真度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction from multiple 2D input views. However, a well-known drawback of NeRFs is the less-than-ideal performance under a small number of views, due to insufficient constraints enforced by volumetric rendering. To address this issue, we introduce SCADE, a novel technique that improves NeRF reconstruction quality on sparse, unconstrained input views for in-the-wild indoor scenes. To constrain NeRF reconstruction, we leverage geometric priors in the form of per-view depth estimates produced with state-of-the-art monocular depth estimation models, which can generalize across scenes. A key challenge is that monocular depth estimation is an ill-posed problem, with inherent ambiguities. To handle this issue, we propose a new method that learns to predict, for each view, a continuous, multimodal distribution of depth estimates using conditional Implicit Maximum Likelihood Estimation (cIMLE). In order to disambiguate exploiting multiple views, we introduce an original space carving loss that guides the NeRF representation to fuse multiple hypothesized depth maps from each view and distill from them a common geometry that is consistent with all views. Experiments show that our approach enables higher fidelity novel view synthesis from sparse views. Our project page can be found at https://scade-spacecarving-nerfs.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">206.CloSET: Modeling Clothed Humans on Continuous Surface With Explicit Template Decomposition</span><br>
                <span class="as">Zhang, HongwenandLin, SiyouandShao, RuizhiandZhang, YuxiangandZheng, ZerongandHuang, HanandGuo, YandongandLiu, Yebin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_CloSET_Modeling_Clothed_Humans_on_Continuous_Surface_With_Explicit_Template_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/501-511.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从静态扫描中创建可动画化的头像，需要在不同姿态下对服装形变进行建模。<br>
                    动机：现有的基于学习的方法通常在最小化衣物网格模板或学习到的隐式模板上添加依赖于姿态的形变，这在捕捉细节或阻碍端到端学习方面存在限制。<br>
                    方法：我们重新审视了基于点的解法，并提出了分解显式的与服装相关的模板，然后在其上添加依赖于姿态的皱纹。这样，服装形变被解耦，使得依赖于姿态的皱纹可以更好地学习和应用于未见过的姿态。此外，为了解决最近最先进的基于点的方法是存在的接缝伪影问题，我们提出在身体表面学习点特征，这建立了一个连续和紧凑的特征空间来捕捉精细和依赖于姿态的服装几何。<br>
                    效果：我们的方法是在一个现有的数据集和我们新引入的数据集上进行验证，显示出在未见过的姿态下更好的服装形变结果。项目页面、代码和数据集可以在https://www.liuyebin.com/closet找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Creating animatable avatars from static scans requires the modeling of clothing deformations in different poses. Existing learning-based methods typically add pose-dependent deformations upon a minimally-clothed mesh template or a learned implicit template, which have limitations in capturing details or hinder end-to-end learning. In this paper, we revisit point-based solutions and propose to decompose explicit garment-related templates and then add pose-dependent wrinkles to them. In this way, the clothing deformations are disentangled such that the pose-dependent wrinkles can be better learned and applied to unseen poses. Additionally, to tackle the seam artifact issues in recent state-of-the-art point-based methods, we propose to learn point features on a body surface, which establishes a continuous and compact feature space to capture the fine-grained and pose-dependent clothing geometry. To facilitate the research in this field, we also introduce a high-quality scan dataset of humans in real-world clothing. Our approach is validated on two existing datasets and our newly introduced dataset, showing better clothing deformation results in unseen poses. The project page with code and dataset can be found at https://www.liuyebin.com/closet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">207.BUOL: A Bottom-Up Framework With Occupancy-Aware Lifting for Panoptic 3D Scene Reconstruction From a Single Image</span><br>
                <span class="as">Chu, TaoandZhang, PanandLiu, QiongandWang, Jiaqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_BUOL_A_Bottom-Up_Framework_With_Occupancy-Aware_Lifting_for_Panoptic_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4937-4946.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图像理解和重建3D场景，同时进行3D重建和3D全景分割。<br>
                    动机：现有的方法只关注自上而下的方法，根据估计的深度将2D实例填充到3D体素中，存在实例-通道歧义和体素-重建歧义两个问题。<br>
                    方法：提出BUOL框架，通过占用感知提升来解决这两个问题。对于实例-通道歧义，采用自下而上的方法，基于确定的语义分配将2D信息提升到3D体素，然后根据预测的2D实例中心对3D体素进行细化和分组。对于体素-重建歧义，利用估计的多平面占用和深度来填充物体和素材的所有区域。<br>
                    效果：在合成数据集3D-Front和真实世界数据集Matterport3D上，该方法比最先进的方法具有显著的性能优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding and modeling the 3D scene from a single image is a practical problem. A recent advance proposes a panoptic 3D scene reconstruction task that performs both 3D reconstruction and 3D panoptic segmentation from a single image. Although having made substantial progress, recent works only focus on top-down approaches that fill 2D instances into 3D voxels according to estimated depth, which hinders their performance by two ambiguities. (1) instance-channel ambiguity: The variable ids of instances in each scene lead to ambiguity during filling voxel channels with 2D information, confusing the following 3D refinement. (2) voxel-reconstruction ambiguity: 2D-to-3D lifting with estimated single view depth only propagates 2D information onto the surface of 3D regions, leading to ambiguity during the reconstruction of regions behind the frontal view surface. In this paper, we propose BUOL, a Bottom-Up framework with Occupancy-aware Lifting to address the two issues for panoptic 3D scene reconstruction from a single image. For instance-channel ambiguity, a bottom-up framework lifts 2D information to 3D voxels based on deterministic semantic assignments rather than arbitrary instance id assignments. The 3D voxels are then refined and grouped into 3D instances according to the predicted 2D instance centers. For voxel-reconstruction ambiguity, the estimated multi-plane occupancy is leveraged together with depth to fill the whole regions of things and stuff. Our method shows a tremendous performance advantage over state-of-the-art methods on synthetic dataset 3D-Front and real-world dataset Matterport3D, respectively. Code and models will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">208.SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction</span><br>
                <span class="as">Zhou, ZhizhuoandTulsiani, Shubham</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12588-12597.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种稀疏视角的3D重建方法SparseFusion，以统一神经渲染和概率图像生成的最新进展。<br>
                    动机：现有的方法通常基于重新投影特征的神经渲染，但无法生成未见过的区域或处理大视角变化下的不确定性。其他方法将其视为（概率）2D合成任务，虽然可以生成合理的2D图像，但不能推断出一致的底层3D。<br>
                    方法：通过从视图条件的潜在扩散模型中提炼出3D一致的场景表示，能够恢复出一个合理且真实的3D表示，其渲染既准确又真实。<br>
                    效果：在CO3D数据集的51个类别上评估该方法，并在稀疏视角新视图合成的畸变和感知指标上都优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose SparseFusion, a sparse view 3D reconstruction approach that unifies recent advances in neural rendering and probabilistic image generation. Existing approaches typically build on neural rendering with re-projected features but fail to generate unseen regions or handle uncertainty under large viewpoint changes. Alternate methods treat this as a (probabilistic) 2D synthesis task, and while they can generate plausible 2D images, they do not infer a consistent underlying 3D. However, we find that this trade-off between 3D consistency and probabilistic image generation does not need to exist. In fact, we show that geometric consistency and generative inference can be complementary in a mode seeking behavior. By distilling a 3D consistent scene representation from a view-conditioned latent diffusion model, we are able to recover a plausible 3D representation whose renderings are both accurate and realistic. We evaluate our approach across 51 categories in the CO3D dataset and show that it outperforms existing methods, in both distortion and perception metrics, for sparse view novel view synthesis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">209.Differentiable Shadow Mapping for Efficient Inverse Graphics</span><br>
                <span class="as">Worchel, MarkusandAlexa, Marc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Worchel_Differentiable_Shadow_Mapping_for_Efficient_Inverse_Graphics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/142-153.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在三角形网格的可微渲染中生成阴影？<br>
                    动机：现有的阴影近似技术需要大量的计算资源，且效果不尽人意。<br>
                    方法：通过将预过滤阴影映射与现有的可微栅格化器结合，产生可微可见性信息。<br>
                    效果：在多个逆图形学问题上，这种方法比具有相似准确性的可微光照传输模拟快几个数量级，而没有阴影的可微栅格化通常无法收敛。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We show how shadows can be efficiently generated in differentiable rendering of triangle meshes. Our central observation is that pre-filtered shadow mapping, a technique for approximating shadows based on rendering from the perspective of a light, can be combined with existing differentiable rasterizers to yield differentiable visibility information. We demonstrate at several inverse graphics problems that differentiable shadow maps are orders of magnitude faster than differentiable light transport simulation with similar accuracy -- while differentiable rasterization without shadows often fails to converge.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">210.A Practical Stereo Depth System for Smart Glasses</span><br>
                <span class="as">Wang, JialiangandScharstein, DanielandBapat, AkashandBlackburn-Matzen, KevinandYu, MatthewandLehman, JonathanandAlsisan, SuhibandWang, YanghanandTsai, SamandFrahm, Jan-MichaelandHe, ZijianandVajda, PeterandCohen, MichaelF.andUyttendaele, Matt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_A_Practical_Stereo_Depth_System_for_Smart_Glasses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21498-21507.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种端到端的立体深度传感系统，实现预处理、在线立体矫正和立体深度估计，并在矫正不可靠时切换到单目深度估计。<br>
                    动机：在智能手机上执行所有这些步骤，需要处理各种失败情况和不理想的输入数据，同时满足内存和延迟限制，以提供流畅的用户体验。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present the design of a productionized end-to-end stereo depth sensing system that does pre-processing, online stereo rectification, and stereo depth estimation with a fallback to monocular depth estimation when rectification is unreliable. The output of our depth sensing system is then used in a novel view generation pipeline to create 3D computational photography effects using point-of-view images captured by smart glasses. All these steps are executed on-device on the stringent compute budget of a mobile phone, and because we expect the users can use a wide range of smartphones, our design needs to be general and cannot be dependent on a particular hardware or ML accelerator such as a smartphone GPU. Although each of these steps is well studied, a description of a practical system is still lacking. For such a system, all these steps need to work in tandem with one another and fallback gracefully on failures within the system or less than ideal input data. We show how we handle unforeseen changes to calibration, e.g., due to heat, robustly support depth estimation in the wild, and still abide by the memory and latency constraints required for a smooth user experience. We show that our trained models are fast, and run in less than 1s on a six-year-old Samsung Galaxy S8 phone's CPU. Our models generalize well to unseen data and achieve good results on Middlebury and in-the-wild images captured from the smart glasses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">211.Instant Volumetric Head Avatars</span><br>
                <span class="as">Zielonka, WojciechandBolkart, TimoandThies, Justus</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zielonka_Instant_Volumetric_Head_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4574-4584.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实时重建照片级真实的数字头像？<br>
                    动机：目前最先进的方法需要几天时间来训练一个数字头像，我们的目标是在现代GPU硬件上，将训练时间缩短到10分钟以内。<br>
                    方法：我们提出了一种名为INSTA的新方法，该方法基于嵌入在参数化人脸模型周围的神经图形原语的动态神经辐射场进行建模。我们的管道在一个单一的视角RGB肖像视频上进行训练，该视频观察了在不同表情和视角下的主题。<br>
                    效果：实验结果表明，INSTA在渲染质量和训练时间方面优于最先进的方法，并且能够对未见过的姿态进行推断。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Instant Volumetric Head Avatars (INSTA), a novel approach for reconstructing photo-realistic digital avatars instantaneously. INSTA models a dynamic neural radiance field based on neural graphics primitives embedded around a parametric face model. Our pipeline is trained on a single monocular RGB portrait video that observes the subject under different expressions and views. While state-of-the-art methods take up to several days to train an avatar, our method can reconstruct a digital avatar in less than 10 minutes on modern GPU hardware, which is orders of magnitude faster than previous solutions. In addition, it allows for the interactive rendering of novel poses and expressions. By leveraging the geometry prior of the underlying parametric face model, we demonstrate that INSTA extrapolates to unseen poses. In quantitative and qualitative studies on various subjects, INSTA outperforms state-of-the-art methods regarding rendering quality and training time. Project website: https://zielon.github.io/insta/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">212.HARP: Personalized Hand Reconstruction From a Monocular RGB Video</span><br>
                <span class="as">Karunratanakul, KorraweandProkudin, SergeyandHilliges, OtmarandTang, Siyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12802-12813.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目RGB视频中重建出高保真度的手部化身？<br>
                    动机：目前大多数手部化身创建方法都采用神经隐式表示，而HARP提出了一种基于网格参数化手模型、顶点位移图、法线图和漫反射的显式表示方法，无需任何神经网络组件。<br>
                    方法：HARP通过梯度下降优化，直接使用手持手机捕获的短序列进行训练，并设计了一种阴影感知的可微分渲染方案，以实现真实感渲染和实时渲染能力。<br>
                    效果：实验证明，HARP在外观重建、新视角和新姿态合成以及3D手部姿态优化等方面具有优越的性能和可扩展性，是一种适用于AR/VR应用的个性化手部表示方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present HARP (HAnd Reconstruction and Personalization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-fidelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP models a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. The explicit nature of our representation enables a truly scalable, robust, and efficient approach to hand avatar creation as validated by our experiments. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with real-time rendering capability. To enable this, we carefully design and implement a shadow-aware differentiable rendering scheme that is robust to high degree articulations and self-shadowing regularly present in hand motions, as well as challenging lighting conditions. It also generalizes to unseen poses and novel viewpoints, producing photo-realistic renderings of hand animations. Furthermore, the learned HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel view and novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready personalized hand representation that shows superior fidelity and scalability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">213.DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields</span><br>
                <span class="as">Chen, YuandLee, GimHee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24-34.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化基于复杂3D CNN或变压器架构的广义NeRFs（GeNeRFs）中的相机姿态。<br>
                    动机：尽管BARF和GARF等方法在调整NeRFs中的相机姿态方面取得了显著成果，但它们无法应用于需要图像特征提取的GeNeRFs。<br>
                    方法：我们提出了DBARF，该方法通过将成本特征图作为隐式成本函数来调整相机姿态，可以与GeNeRFs进行自监督联合训练。<br>
                    效果：实验表明，我们的DBARF在真实世界数据集上具有高效性和泛化能力，并且无需任何良好的初始初始化，即可跨场景应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent works such as BARF and GARF can bundle adjust camera poses with neural radiance fields (NeRF) which is based on coordinate-MLPs. Despite the impressive results, these methods cannot be applied to Generalizable NeRFs (GeNeRFs) which require image feature extractions that are often based on more complicated 3D CNN or transformer architectures. In this work, we first analyze the difficulties of jointly optimizing camera poses with GeNeRFs, and then further propose our DBARF to tackle these issues. Our DBARF which bundle adjusts camera poses by taking a cost feature map as an implicit cost function can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF and its follow-up works, which can only be applied to per-scene optimized NeRFs and need accurate initial camera poses with the exception of forward-facing scenes, our method can generalize across scenes and does not require any good initialization. Experiments show the effectiveness and generalization ability of our DBARF when evaluated on real-world datasets. Our code is available at https://aibluefisher.github.io/dbarf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">214.Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries</span><br>
                <span class="as">Yue, YuanwenandKontogianni, TheodoraandSchindler, KonradandEngelmann, Francis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yue_Connecting_the_Dots_Floorplan_Reconstruction_Using_Two-Level_Queries_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/845-854.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从3D扫描中重建二维平面图的问题。<br>
                    动机：现有的方法通常采用启发式设计的多阶段管道，而我们将其转化为一个单一的结构预测任务。<br>
                    方法：我们开发了一种新的Transformer架构，该架构可以并行生成多个房间的多边形，无需手工设计的中间阶段。<br>
                    效果：该方法在两个具有挑战性的数据集Structured3D和SceneCAD上取得了新的最先进的结果，并且比之前的方法有更快的推理速度。此外，它还可以方便地扩展到预测其他信息，如语义房间类型和建筑元素（如门和窗户）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address 2D floorplan reconstruction from 3D scans. Existing approaches typically employ heuristically designed multi-stage pipelines. Instead, we formulate floorplan reconstruction as a single-stage structured prediction task: find a variable-size set of polygons, which in turn are variable-length sequences of ordered vertices. To solve it we develop a novel Transformer architecture that generates polygons of multiple rooms in parallel, in a holistic manner without hand-crafted intermediate stages. The model features two-level queries for polygons and corners, and includes polygon matching to make the network end-to-end trainable. Our method achieves a new state-of-the-art for two challenging datasets, Structured3D and SceneCAD, along with significantly faster inference than previous methods. Moreover, it can readily be extended to predict additional information, i.e., semantic room types and architectural elements like doors and windows. Our code and models are available at: https://github.com/ywyue/RoomFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">215.Analyzing and Diagnosing Pose Estimation With Attributions</span><br>
                <span class="as">He, QiyuanandYang, LinlinandGu, KeruiandLin, QiuxiaandYao, Angela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Analyzing_and_Diagnosing_Pose_Estimation_With_Attributions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4821-4830.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种用于姿态估计的可解释性技术。<br>
                    动机：为了理解不同姿态框架的影响，需要一种可以生成像素级归因图的技术。<br>
                    方法：提出Pose Integrated Gradient（PoseIG），将不同的姿态输出统一到一个共同的输出空间，并使用似然近似函数进行梯度反向传播。<br>
                    效果：通过这些工具，我们系统地比较了不同的人体姿态估计框架，揭示了手部姿态估计中关节的捷径和身体姿态估计中关键点的未探索的反转误差。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Pose Integrated Gradient (PoseIG), the first interpretability technique designed for pose estimation. We extend the concept of integrated gradients for pose estimation to generate pixel-level attribution maps. To enable comparison across different pose frameworks, we unify different pose outputs into a common output space, along with a likelihood approximation function for gradient back-propagation. To complement the qualitative insight from the attribution maps, we propose three indices for quantitative analysis. With these tools, we systematically compare different pose estimation frameworks to understand the impacts of network design, backbone and auxiliary tasks. Our analysis reveals an interesting shortcut of the knuckles (MCP joints) for hand pose estimation and an under-explored inversion error for keypoints in body pose estimation. Project page: https://qy-h00.github.io/poseig/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">216.Scalable, Detailed and Mask-Free Universal Photometric Stereo</span><br>
                <span class="as">Ikehata, Satoshi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ikehata_Scalable_Detailed_and_Mask-Free_Universal_Photometric_Stereo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13198-13207.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种名为SDM-UniPS的通用光度立体网络，用于在未知、空间变化的照明条件下恢复复杂的表面法线映射。<br>
                    动机：现有的光度立体网络在非受控环境中的表现受到限制，尤其是在未知、空间变化的照明条件下。<br>
                    方法：我们扩展了之前的通用光度立体网络，以提取空间光特性，利用高分辨率输入图像中的所有可用信息，并考虑表面点之间的非局部交互。我们还创建了一个包含真实世界场景中各种形状、材料和照明情况的新合成训练数据集。<br>
                    效果：实验结果表明，我们的方法不仅在公共基准测试上超越了校准的、特定于照明的技术，而且在输入图像数量显著减少的情况下也能表现出色，甚至无需物体掩码。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we introduce SDM-UniPS, a groundbreaking Scalable, Detailed, Mask-free, and Universal Photometric Stereo network. Our approach can recover astonishingly intricate surface normal maps, rivaling the quality of 3D scanners, even when images are captured under unknown, spatially-varying lighting conditions in uncontrolled environments. We have extended previous universal photometric stereo networks to extract spatial-light features, utilizing all available information in high-resolution input images and accounting for non-local interactions among surface points. Moreover, we present a new synthetic training dataset that encompasses a diverse range of shapes, materials, and illumination scenarios found in real-world scenes. Through extensive evaluation, we demonstrate that our method not only surpasses calibrated, lighting-specific techniques on public benchmarks, but also excels with a significantly smaller number of input images even without object masks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">217.Persistent Nature: A Generative Model of Unbounded 3D Worlds</span><br>
                <span class="as">Chai, LucyandTucker, RichardandLi, ZhengqiandIsola, PhillipandSnavely, Noah</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_Persistent_Nature_A_Generative_Model_of_Unbounded_3D_Worlds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20863-20874.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决当前3D图像生成模型在固定范围和有限相机运动下的问题。<br>
                    动机：尽管最近的3D图像生成模型的图像质量越来越高，但它们通常在固定范围内的3D体积上操作，并且相机的运动受到限制。<br>
                    方法：我们提出了一种无条件合成无限自然场景的方法，可以在保持3D世界模型持续性的同时实现任意大的相机运动。我们的场景表示由可扩展的平面场景布局网格和全景天空穹顶组成，可以通过3D解码器和体积渲染从任意相机姿态进行渲染。基于这种表示，我们仅从单视图互联网照片中学习生成世界模型。<br>
                    效果：我们的方法是当前3D生成模型的固定边界之外的场景外推，同时支持与自回归3D预测模型形成对比的持续、与相机无关的世界表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency---for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in contrast to auto-regressive 3D prediction models. Our project page: https://chail.github.io/persistent-nature/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">218.Behind the Scenes: Density Fields for Single View Reconstruction</span><br>
                <span class="as">Wimbauer, FelixandYang, NanandRupprecht, ChristianandCremers, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wimbauer_Behind_the_Scenes_Density_Fields_for_Single_View_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9076-9086.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图像中推断有意义的几何场景表示。<br>
                    动机：传统的深度图预测方法只能推理图像中可见的区域，而神经辐射场（NeRFs）虽然能捕获真实的3D颜色信息，但生成过程过于复杂。<br>
                    方法：提出从单张图像中预测隐密度场的方法，将图像的锥体中的每个位置映射到体积密度，通过直接从可用视图中采样颜色而不是在密度场中存储颜色，使得场景表示比NeRFs明显简单，神经网络可以在一次前向传递中进行预测。<br>
                    效果：实验表明，该方法能够对输入图像中遮挡的区域进行有意义的几何预测，并在三个数据集上展示了深度预测和新颖视图合成的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inferring a meaningful geometric scene representation from a single image is a fundamental problem in computer vision. Approaches based on traditional depth map prediction can only reason about areas that are visible in the image. Currently, neural radiance fields (NeRFs) can capture true 3D including color, but are too complex to be generated from a single image. As an alternative, we propose to predict an implicit density field from a single image. It maps every location in the frustum of the image to volumetric density. By directly sampling color from the available views instead of storing color in the density field, our scene representation becomes significantly less complex compared to NeRFs, and a neural network can predict it in a single forward pass. The network is trained through self-supervision from only video data. Our formulation allows volume rendering to perform both depth prediction and novel view synthesis. Through experiments, we show that our method is able to predict meaningful geometry for regions that are occluded in the input image. Additionally, we demonstrate the potential of our approach on three datasets for depth prediction and novel-view synthesis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">219.Sphere-Guided Training of Neural Implicit Surfaces</span><br>
                <span class="as">Dogaru, AndreeaandArdelean, Andrei-TimoteiandIgnatyev, SavvaandZakharov, EgorandBurnaev, Evgeny</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dogaru_Sphere-Guided_Training_of_Neural_Implicit_Surfaces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20844-20853.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，通过体积光线行进训练的神经距离函数已被广泛用于多视角3D重建。然而，这些方法在整个场景体积上应用光线行进程序，导致采样效率降低，从而在高频细节区域产生较低的重建质量。<br>
                    动机：本文通过联合训练隐式函数和新的基于粗球的表面重建来解决此问题。我们使用粗糙表示来有效地从体积光线行进过程中排除场景的空体积，而无需额外的神经表面网络前向传递，从而提高与基本系统的重建保真度。<br>
                    方法：我们将这种方法纳入几种隐式表面建模方法的训练过程中，并在合成和真实世界数据集上进行评估。<br>
                    效果：实验结果表明，我们的方法在所有数据集上都取得了一致的改进。我们的代码库可以通过项目页面访问。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, neural distance functions trained via volumetric ray marching have been widely adopted for multi-view 3D reconstruction. These methods, however, apply the ray marching procedure for the entire scene volume, leading to reduced sampling efficiency and, as a result, lower reconstruction quality in the areas of high-frequency details. In this work, we address this problem via joint training of the implicit function and our new coarse sphere-based surface reconstruction. We use the coarse representation to efficiently exclude the empty volume of the scene from the volumetric ray marching procedure without additional forward passes of the neural surface network, which leads to an increased fidelity of the reconstructions compared to the base systems. We evaluate our approach by incorporating it into the training procedures of several implicit surface modeling methods and observe uniform improvements across both synthetic and real-world datasets. Our codebase can be accessed via the project page.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">220.BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields</span><br>
                <span class="as">Wang, PengandZhao, LingzheandMa, RuijieandLiu, Peidong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4170-4179.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高神经网络辐射场（NeRF）在真实世界场景中，对质量下降的图像（如低光照条件下的运动模糊图像）的处理能力。<br>
                    动机：现有的方法通常假设输入的图像质量良好，但在现实世界中，图像退化（例如低光照条件下的运动模糊）是常见的，这会影响NeRF的渲染质量。<br>
                    方法：我们提出了一种新的捆绑调整去模糊神经辐射场（BAD-NeRF），它可以处理严重运动模糊的图像和不准确的相机姿态。我们的方法模拟了运动模糊图像的实际物理成像过程，并联合学习了NeRF的参数和恢复曝光期间的相机运动轨迹。<br>
                    效果：实验表明，通过直接模拟实际物理成像过程，BAD-NeRF在合成数据集和真实数据集上都优于先前的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) have received considerable attention recently, due to its impressive capability in photo-realistic 3D reconstruction and novel view synthesis, given a set of posed camera images. Earlier work usually assumes the input images are of good quality. However, image degradation (e.g. image motion blur in low-light conditions) can easily happen in real-world scenarios, which would further affect the rendering quality of NeRF. In this paper, we present a novel bundle adjusted deblur Neural Radiance Fields (BAD-NeRF), which can be robust to severe motion blurred images and inaccurate camera poses. Our approach models the physical image formation process of a motion blurred image, and jointly learns the parameters of NeRF and recovers the camera motion trajectories during exposure time. In experiments, we show that by directly modeling the real physical image formation process, BAD-NeRF achieves superior performance over prior works on both synthetic and real datasets. Code and data are available at https://github.com/WU-CVGL/BAD-NeRF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">221.Multiscale Tensor Decomposition and Rendering Equation Encoding for View Synthesis</span><br>
                <span class="as">Han, KangandXiang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Multiscale_Tensor_Decomposition_and_Rendering_Equation_Encoding_for_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4232-4241.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高从捕获的多视图图像中渲染新视角的质量。<br>
                    动机：随着神经辐射场的出现，从捕获的多视图图像中渲染新视角已经取得了显著的进步，但质量仍有待提高。<br>
                    方法：提出了一种名为神经辐射特征场（NRFF）的新方法。首先，提出一个多尺度张量分解方案来组织可学习的特徵，以从粗到细的比例表示场景。然后，通过使用从提出的多尺度表示预测的各向异性球形高斯混合模型在特征空间中编码渲染方程，而不是编码视图方向以模拟依赖视图的效果。<br>
                    效果：实验结果表明，提出的NRFF在NeRF和NSVF合成数据集上将最先进的渲染结果提高了1 dB以上的PSNR，并且在现实世界的Tanks & Temples数据集上也观察到了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rendering novel views from captured multi-view images has made considerable progress since the emergence of the neural radiance field. This paper aims to further advance the quality of view rendering by proposing a novel approach dubbed the neural radiance feature field (NRFF). We first propose a multiscale tensor decomposition scheme to organize learnable features so as to represent scenes from coarse to fine scales. We demonstrate many benefits of the proposed multiscale representation, including more accurate scene shape and appearance reconstruction, and faster convergence compared with the single-scale representation. Instead of encoding view directions to model view-dependent effects, we further propose to encode the rendering equation in the feature space by employing the anisotropic spherical Gaussian mixture predicted from the proposed multiscale representation. The proposed NRFF improves state-of-the-art rendering results by over 1 dB in PSNR on both the NeRF and NSVF synthetic datasets. A significant improvement has also been observed on the real-world Tanks & Temples dataset. Code can be found at https://github.com/imkanghan/nrff.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">222.Learning Accurate 3D Shape Based on Stereo Polarimetric Imaging</span><br>
                <span class="as">Huang, TianyuandLi, HaoangandHe, KejingandSui, CongyingandLi, BinandLiu, Yun-Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Learning_Accurate_3D_Shape_Based_on_Stereo_Polarimetric_Imaging_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17287-17296.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的形状从极化（SfP）方法在恢复表面法线时的两个主要问题，即极化线索的模糊性导致的错误法线估计和广泛使用的正射投影假设过于理想。<br>
                    动机：为了解决这些问题，作者提出了第一个结合深度学习和立体极化信息的方法，不仅可以恢复法线，还可以恢复视差。<br>
                    方法：对于模糊性问题，设计了一个基于形状一致性的掩码预测模块，利用法线和视差的固有一致性来识别错误法线估计的区域，并用全局注意力机制提取的新特征替换这些区域中的不可靠特征。对于正射投影问题，提出了一种新的视角辅助位置编码策略，使神经网络能够处理非正射投影。<br>
                    效果：实验表明，与现有的SfP方法相比，该方法更准确，对光照变化更具鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Shape from Polarization (SfP) aims to recover surface normal using the polarization cues of light. The accuracy of existing SfP methods is affected by two main problems. First, the ambiguity of polarization cues partially results in false normal estimation. Second, the widely-used assumption about orthographic projection is too ideal. To solve these problems, we propose the first approach that combines deep learning and stereo polarization information to recover not only normal but also disparity. Specifically, for the ambiguity problem, we design a Shape Consistency-based Mask Prediction (SCMP) module. It exploits the inherent consistency between normal and disparity to identify the areas with false normal estimation. We replace the unreliable features enclosed by these areas with new features extracted by global attention mechanism. As to the orthographic projection problem, we propose a novel Viewing Direction-aided Positional Encoding (VDPE) strategy. This strategy is based on the unique pixel-viewing direction encoding, and thus enables our neural network to handle the non-orthographic projection. In addition, we establish a real-world stereo SfP dataset that contains various object categories and illumination conditions. Experiments showed that compared with existing SfP methods, our approach is more accurate. Moreover, our approach shows higher robustness to light variation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">223.GANmouflage: 3D Object Nondetection With Texture Fields</span><br>
                <span class="as">Guo, RuiandCollins, JasmineanddeLima, OscarandOwens, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_GANmouflage_3D_Object_Nondetection_With_Texture_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4702-4712.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种在场景中隐藏3D物体的方法。<br>
                    动机：为了解决在各种视角下准确复制场景纹理并处理冲突约束的问题，我们提出了基于纹理场和对抗学习的模型。<br>
                    方法：我们的模型通过学习从输入场景中的随机位置和视角隐藏各种物体形状，并首次解决了隐藏复杂物体形状的问题。<br>
                    效果：通过人类视觉搜索研究，我们发现估计的纹理比之前的方法更好地隐藏了物体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method that learns to camouflage 3D objects within scenes. Given an object's shape and a distribution of viewpoints from which it will be seen, we estimate a texture that will make it difficult to detect. Successfully solving this task requires a model that can accurately reproduce textures from the scene, while simultaneously dealing with the highly conflicting constraints imposed by each viewpoint. We address these challenges with a model based on texture fields and adversarial learning. Our model learns to camouflage a variety of object shapes from randomly sampled locations and viewpoints within the input scene, and is the first to address the problem of hiding complex object shapes. Using a human visual search study, we find that our estimated textures conceal objects significantly better than previous methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">224.OReX: Object Reconstruction From Planar Cross-Sections Using Neural Fields</span><br>
                <span class="as">Sawdayee, HaimandVaxman, AmirandBermano, AmitH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sawdayee_OReX_Object_Reconstruction_From_Planar_Cross-Sections_Using_Neural_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20854-20862.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅从平面切片重建三维形状。<br>
                    动机：解决稀疏和病态问题，改善现有方法的低质量结果或对额外先验（如目标拓扑、外观信息或输入法线方向）的依赖。<br>
                    方法：提出了OReX方法，使用神经场作为插值先验进行三维形状重建。通过在输入平面上训练一个适度的神经网络来估计给定3D坐标的内外，产生强大的先验，引入平滑性和自相似性。<br>
                    效果：通过广泛的定性和定量实验，证明该方法稳健、准确且具有良好的扩展性。与先前的方法和最近的可能解决方案相比，取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstructing 3D shapes from planar cross-sections is a challenge inspired by downstream applications like medical imaging and geographic informatics. The input is an in/out indicator function fully defined on a sparse collection of planes in space, and the output is an interpolation of the indicator function to the entire volume. Previous works addressing this sparse and ill-posed problem either produce low quality results, or rely on additional priors such as target topology, appearance information, or input normal directions. In this paper, we present OReX, a method for 3D shape reconstruction from slices alone, featuring a Neural Field as the interpolation prior. A modest neural network is trained on the input planes to return an inside/outside estimate for a given 3D coordinate, yielding a powerful prior that induces smoothness and self-similarities. The main challenge for this approach is high-frequency details, as the neural prior is overly smoothing. To alleviate this, we offer an iterative estimation architecture and a hierarchical input sampling scheme that encourage coarse-to-fine training, allowing the training process to focus on high frequencies at later stages. In addition, we identify and analyze a ripple-like effect stemming from the mesh extraction step. We mitigate it by regularizing the spatial gradients of the indicator function around input in/out boundaries during network training, tackling the problem at the root. Through extensive qualitative and quantitative experimentation, we demonstrate our method is robust, accurate, and scales well with the size of the input. We report state-of-the-art results compared to previous approaches and recent potential solutions, and demonstrate the benefit of our individual contributions through analysis and ablation studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">225.SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting With Neural Radiance Fields</span><br>
                <span class="as">Mirzaei, AshkanandAumentado-Armstrong, TristanandDerpanis, KonstantinosG.andKelly, JonathanandBrubaker, MarcusA.andGilitschenski, IgorandLevinshtein, Alex</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20669-20679.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从3D场景中移除不需要的对象，并保持视觉可信度和上下文一致性。<br>
                    动机：现有的方法在处理3D场景的编辑任务时，如移除不需要的对象，仍存在挑战。<br>
                    方法：提出一种新的3D场景修复方法，通过使用少量的图像和稀疏的标注，首先快速获取目标对象的3D分割掩码，然后利用学习到的2D图像修复器的信息，将其信息提炼到3D空间，同时确保视图一致性。<br>
                    效果：实验结果表明，该方法在多视图分割和3D场景修复任务上均取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRFs) have emerged as a popular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intuitively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both consistent across multiple views and geometrically valid. In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed images and sparse annotations in a single input image, our framework first rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimization-based approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised of challenging real-world scenes. In particular, our dataset contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We first demonstrate the superiority of our approach on multiview segmentation, comparing to NeRF-based methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-of-the-art performance against other NeRF manipulation algorithms, as well as a strong 2D image inpainter baseline.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">226.Patch-Based 3D Natural Scene Generation From a Single Example</span><br>
                <span class="as">Li, WeiyuandChen, XuelinandWang, JueandChen, Baoquan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Patch-Based_3D_Natural_Scene_Generation_From_a_Single_Example_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16762-16772.png><br>
            
            <span class="tt"><span class="t0">研究问题：我们的目标是为通常独特且复杂的自然场景开发一种3D生成模型。由于缺乏必要的训练数据量，以及在场景特征变化的情况下进行特殊设计的难度，使得现有的设置难以处理。<br>
                    动机：受到经典基于补丁的图像模型的启发，我们主张在给定单个示例的情况下，在补丁级别合成3D场景。这项工作的核心在于解决从经典的2D补丁基础框架提升到3D生成所面临的独特挑战的重要算法设计。<br>
                    方法：我们采用了一种新颖的、有效的、高效的模型，该模型能够生成具有真实几何结构和视觉外观的高质量通用自然场景，并在大量和各种类型的示范场景上进行了演示。<br>
                    效果：实验结果表明，我们的模型可以生成大量的高质量通用自然场景，这些场景既具有真实的几何结构，又具有视觉外观。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We target a 3D generative model for general natural scenes that are typically unique and intricate. Lacking the necessary volumes of training data, along with the difficulties of having ad hoc designs in presence of varying scene characteristics, renders existing setups intractable. Inspired by classical patch-based image models, we advocate for synthesizing 3D scenes at the patch level, given a single example. At the core of this work lies important algorithmic designs w.r.t the scene representation and generative patch nearest-neighbor module, that address unique challenges arising from lifting classical 2D patch-based framework to 3D generation. These design choices, on a collective level, contribute to a robust, effective, and efficient model that can generate high-quality general natural scenes with both realistic geometric structure and visual appearance, in large quantities and varieties, as demonstrated upon a variety of exemplar scenes. Data and code can be found at http://wyysf-98.github.io/Sin3DGen.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">227.Efficient View Synthesis and 3D-Based Multi-Frame Denoising With Multiplane Feature Representations</span><br>
                <span class="as">Tanay, ThomasandLeonardis, Ale\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tanay_Efficient_View_Synthesis_and_3D-Based_Multi-Frame_Denoising_With_Multiplane_Feature_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20898-20907.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While current multi-frame restoration methods combine information from multiple input images using 2D alignment techniques, recent advances in novel view synthesis are paving the way for a new paradigm relying on volumetric scene representations. In this work, we introduce the first 3D-based multi-frame denoising method that significantly outperforms its 2D-based counterparts with lower computational requirements. Our method extends the multiplane image (MPI) framework for novel view synthesis by introducing a learnable encoder-renderer pair manipulating multiplane representations in feature space. The encoder fuses information across views and operates in a depth-wise manner while the renderer fuses information across depths and operates in a view-wise manner. The two modules are trained end-to-end and learn to separate depths in an unsupervised way, giving rise to Multiplane Feature (MPF) representations. Experiments on the Spaces and Real Forward-Facing datasets as well as on raw burst data validate our approach for view synthesis, multi-frame denoising, and view synthesis under noisy conditions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">228.HairStep: Transfer Synthetic to Real Using Strand and Depth Maps for Single-View 3D Hair Modeling</span><br>
                <span class="as">Zheng, YujianandJin, ZirongandLi, MoranandHuang, HaibinandMa, ChongyangandCui, ShuguangandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_HairStep_Transfer_Synthetic_to_Real_Using_Strand_and_Depth_Maps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12726-12735.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决基于学习的单视图3D头发建模难题。<br>
                    动机：由于收集真实图像和3D头发数据的困难，使用合成数据为真实领域提供先验知识成为主要解决方案，但这引入了领域差距的挑战。<br>
                    方法：提出一种新的中间表示法——HairStep，包括一个发丝图和一个深度图。设计了一个学习框架将真实图像转化为发丝图和深度图。<br>
                    效果：实验表明，HairStep缩小了合成与真实的领域差距，并在单视图3D头发重建方面取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we tackle the challenging problem of learning-based single-view 3D hair modeling. Due to the great difficulty of collecting paired real image and 3D hair data, using synthetic data to provide prior knowledge for real domain becomes a leading solution. This unfortunately introduces the challenge of domain gap. Due to the inherent difficulty of realistic hair rendering, existing methods typically use orientation maps instead of hair images as input to bridge the gap. We firmly think an intermediate representation is essential, but we argue that orientation map using the dominant filtering-based methods is sensitive to uncertain noise and far from a competent representation. Thus, we first raise this issue up and propose a novel intermediate representation, termed as HairStep, which consists of a strand map and a depth map. It is found that HairStep not only provides sufficient information for accurate 3D hair modeling, but also is feasible to be inferred from real images. Specifically, we collect a dataset of 1,250 portrait images with two types of annotations. A learning framework is further designed to transfer real images to the strand map and depth map. It is noted that, an extra bonus of our new dataset is the first quantitative metric for 3D hair modeling. Our experiments show that HairStep narrows the domain gap between synthetic and real and achieves state-of-the-art performance on single-view 3D hair reconstruction.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">229.Complete 3D Human Reconstruction From a Single Incomplete Image</span><br>
                <span class="as">Wang, JunyingandYoon, JaeShinandWang, TuanfengY.andSingh, KrishnaKumarandNeumann, Ulrich</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8748-8758.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从只有部分身体（如躯干）可见的图像中重建完整的人体几何和纹理。<br>
                    动机：由于遮挡的存在，许多现有的单视图人体重建方法无法处理不可见部分，导致3D中存在缺失数据。<br>
                    方法：提出了一种新的从粗到细的人体重建框架。对于粗重建，通过学习体积特征生成具有3D卷积神经网络的完整人体几何，该网络由3D身体模型和可见部分的风格特征进行条件化。一个隐式网络将学习的3D特征与从多视图增强的高质表面法线相结合，以产生精细的局部细节，例如高频皱纹。最后，执行渐进式纹理修复，以一致的方式重建人的完整外观，这在没有完整几何重建的情况下是不可能的。<br>
                    效果：实验表明，该方法可以重建高质量的3D人体，对遮挡具有鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a method to reconstruct a complete human geometry and texture from an image of a person with only partial body observed, e.g., a torso. The core challenge arises from the occlusion: there exists no pixel to reconstruct where many existing single-view human reconstruction methods are not designed to handle such invisible parts, leading to missing data in 3D. To address this challenge, we introduce a novel coarse-to-fine human reconstruction framework. For coarse reconstruction, explicit volumetric features are learned to generate a complete human geometry with 3D convolutional neural networks conditioned by a 3D body model and the style features from visible parts. An implicit network combines the learned 3D features with the high-quality surface normals enhanced from multiview to produce fine local details, e.g., high-frequency wrinkles. Finally, we perform progressive texture inpainting to reconstruct a complete appearance of the person in a view-consistent way, which is not possible without the reconstruction of a complete geometry. In experiments, we demonstrate that our method can reconstruct high-quality 3D humans, which is robust to occlusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">230.Reconstructing Animatable Categories From Videos</span><br>
                <span class="as">Yang, GengshanandWang, ChaoyangandReddy, N.DineshandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Reconstructing_Animatable_Categories_From_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16995-17005.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目视频中构建类别级别的3D模型，解决实例间变化和时间运动的问题。<br>
                    动机：现有的方法需要3D扫描、繁琐的注册和手动装配，而基于可区分渲染的方法又仅限于刚性类别或单个实例。<br>
                    方法：提出了一种名为RAC的新方法，通过专门针对实例的类别级骨架、鼓励类别共享结构同时保留实例细节的潜空间正则化以及使用3D背景模型将对象与背景分离等三个关键思想来解决这个问题。<br>
                    效果：成功为人类、猫和狗构建了基于单目视频的3D模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Building animatable 3D models is challenging due to the need for 3D scans, laborious registration, and manual rigging. Recently, differentiable rendering provides a pathway to obtain high-quality 3D models from monocular videos, but these are limited to rigid categories or single instances. We present RAC, a method to build category-level 3D models from monocular videos, disentangling variations over instances and motion over time. Three key ideas are introduced to solve this problem: (1) specializing a category-level skeleton to instances, (2) a method for latent space regularization that encourages shared structure across a category while maintaining instance details, and (3) using 3D background models to disentangle objects from the background. We build 3D models for humans, cats, and dogs given monocular videos. Project page: gengshan-y.github.io/rac-www/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">231.High-Fidelity 3D Human Digitization From Single 2K Resolution Images</span><br>
                <span class="as">Han, Sang-HunandPark, Min-GyuandYoon, JuHongandKang, Ju-MiandPark, Young-JaeandJeon, Hae-Gon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_High-Fidelity_3D_Human_Digitization_From_Single_2K_Resolution_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12869-12879.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用高分辨率输入图像进行高质量的3D人体重建。<br>
                    动机：现有的3D人体重建方法需要大量的训练数据和适当的网络设计，以充分利用高分辨率的输入图像。<br>
                    方法：我们提出了一种名为2K2K的简单而有效的3D人体数字化方法，构建了一个大规模的2K人体数据集，并从2K分辨率的图像中推断出3D人体模型。该方法分别恢复人体的全局形状和细节。低分辨率深度网络从低分辨率图像中预测全局结构，部分图像到法线网络预测3D人体结构的详细部分。高分辨率深度网络将全局3D形状和详细结构合并，以推断高分辨率的前侧和后侧深度图。最后，一个现成的网格生成器重建完整的3D人体模型。<br>
                    效果：在实验中，我们在各种数据集上的表现优于最近的工作，提供了2050个包含纹理映射、3D关节和SMPL参数的3D人体模型用于研究目的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-quality 3D human body reconstruction requires high-fidelity and large-scale training data and appropriate network design that effectively exploits the high-resolution input images. To tackle these problems, we propose a simple yet effective 3D human digitization method called 2K2K, which constructs a large-scale 2K human dataset and infers 3D human models from 2K resolution images. The proposed method separately recovers the global shape of a human and its details. The low-resolution depth network predicts the global structure from a low-resolution image, and the part-wise image-to-normal network predicts the details of the 3D human body structure. The high-resolution depth network merges the global 3D shape and the detailed structures to infer the high-resolution front and back side depth maps. Finally, an off-the-shelf mesh generator reconstructs the full 3D human model, which are available at https://github.com/SangHunHan92/2K2K. In addition, we also provide 2,050 3D human models, including texture maps, 3D joints, and SMPL parameters for research purposes. In experiments, we demonstrate competitive performance over the recent works on various datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">232.NeFII: Inverse Rendering for Reflectance Decomposition With Near-Field Indirect Illumination</span><br>
                <span class="as">Wu, HaoqianandHu, ZhipengandLi, LinchengandZhang, YongqiangandFan, ChangjieandYu, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_NeFII_Inverse_Rendering_for_Reflectance_Decomposition_With_Near-Field_Indirect_Illumination_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4295-4304.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从多视角RGB图像中估计几何、材料和照明的问题。<br>
                    动机：现有的逆渲染方法通过球面高斯模型间接照明，但这种方法往往会模糊高频反射的细节。<br>
                    方法：本文提出了一种端到端的逆渲染流程，从多视角图像中分解材料和照明，同时考虑近场间接照明。具体来说，我们引入了基于蒙特卡洛采样的路径追踪，并将间接照明缓存为神经辐射度，实现了一种物理真实且易于优化的逆渲染方法。为了提高效率和实用性，我们利用SG表示平滑的环境照明，并应用了重要性采样技术。为了监督未被观察到方向的间接照明，我们开发了一种新颖的辐射度一致性约束，结合材料和照明的联合优化，从而显著提高了分解性能。<br>
                    效果：大量实验表明，我们的方法在多个合成和真实数据集上优于最先进的方法，特别是在互反射分解方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inverse rendering methods aim to estimate geometry, materials and illumination from multi-view RGB images. In order to achieve better decomposition, recent approaches attempt to model indirect illuminations reflected from different materials via Spherical Gaussians (SG), which, however, tends to blur the high-frequency reflection details. In this paper, we propose an end-to-end inverse rendering pipeline that decomposes materials and illumination from multi-view images, while considering near-field indirect illumination. In a nutshell, we introduce the Monte Carlo sampling based path tracing and cache the indirect illumination as neural radiance, enabling a physics-faithful and easy-to-optimize inverse rendering method. To enhance efficiency and practicality, we leverage SG to represent the smooth environment illuminations and apply importance sampling techniques. To supervise indirect illuminations from unobserved directions, we develop a novel radiance consistency constraint between implicit neural radiance and path tracing results of unobserved rays along with the joint optimization of materials and illuminations, thus significantly improving the decomposition performance. Extensive experiments demonstrate that our method outperforms the state-of-the-art on multiple synthetic and real datasets, especially in terms of inter-reflection decomposition.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">233.Fully Self-Supervised Depth Estimation From Defocus Clue</span><br>
                <span class="as">Si, HaozheandZhao, BinandWang, DongandGao, YunpengandChen, MulinandWang, ZhigangandLi, Xuelong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Si_Fully_Self-Supervised_Depth_Estimation_From_Defocus_Clue_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9140-9149.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅从稀疏散焦堆栈中估计深度，以克服在真实世界场景中无法获取深度和全焦点图像（AIF）的问题。<br>
                    动机：现有的基于散焦的深度估计方法需要依赖深度和全焦点图像的真实值，这在实际场景中是无法获取的。<br>
                    方法：提出一种完全自监督的框架，通过预测深度和全焦点图像来估计深度，同时利用光学模型对预测结果进行验证和优化。<br>
                    效果：在具有渲染散焦堆栈和真实散焦堆栈的三个基准数据集上进行验证，实验结果表明该方法为自监督的基于散焦的深度估计任务提供了强有力的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Depth-from-defocus (DFD), modeling the relationship between depth and defocus pattern in images, has demonstrated promising performance in depth estimation. Recently, several self-supervised works try to overcome the difficulties in acquiring accurate depth ground-truth. However, they depend on the all-in-focus (AIF) images, which cannot be captured in real-world scenarios. Such limitation discourages the applications of DFD methods. To tackle this issue, we propose a completely self-supervised framework that estimates depth purely from a sparse focal stack. We show that our framework circumvents the needs for the depth and AIF image ground-truth, and receives superior predictions, thus closing the gap between the theoretical success of DFD works and their applications in the real world. In particular, we propose (i) a more realistic setting for DFD tasks, where no depth or AIF image ground-truth is available; (ii) a novel self-supervision framework that provides reliable predictions of depth and AIF image under the the challenging setting. The proposed framework uses a neural model to predict the depth and AIF image, and utilizes an optical model to validate and refine the prediction. We verify our framework on three benchmark datasets with rendered focal stacks and real focal stacks. Qualitative and quantitative evaluations show that our method provides a strong baseline for self-supervised DFD tasks. The source code is publicly available at https://github.com/Ehzoahis/DEReD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">234.Deep Stereo Video Inpainting</span><br>
                <span class="as">Wu, ZhiliangandSun, ChangchangandXuan, HanyuandYan, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Deep_Stereo_Video_Inpainting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5693-5702.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决立体视频修复中，如何同时对左右视图的缺失区域进行合理填充的问题。<br>
                    动机：虽然单视频修复已取得显著成果，但立体视频修复尚未得到充分探索。其核心挑战在于保持左右视图的立体一致性，以减轻观众的3D疲劳感。<br>
                    方法：本文提出了一种名为SVINet的新型深度立体视频修复网络，首次尝试使用深度卷积神经网络进行立体视频修复。首先，利用自我监督的流引导变形时序对齐模块分别对左右视图分支的特征进行对齐；然后，将对齐后的特征输入到共享的自适应特征聚合模块中生成各自的缺失内容；最后，引入视差注意力模块（PAM），利用跨视图信息考虑显著的立体相关性，融合左右视图的完成特征。此外，还开发了一种立体一致性损失来规范训练参数，使模型能够产生具有更好立体一致性的高质量立体视频修复结果。<br>
                    效果：实验结果表明，我们的SVINet在性能上超过了最先进的单视频修复模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Stereo video inpainting aims to fill the missing regions on the left and right views of the stereo video with plausible content simultaneously. Compared with the single video inpainting that has achieved promising results using deep convolutional neural networks, inpainting the missing regions of stereo video has not been thoroughly explored. In essence, apart from the spatial and temporal consistency that single video inpainting needs to achieve, another key challenge for stereo video inpainting is to maintain the stereo consistency between left and right views and hence alleviate the 3D fatigue for viewers. In this paper, we propose a novel deep stereo video inpainting network named SVINet, which is the first attempt for stereo video inpainting task utilizing deep convolutional neural networks. SVINet first utilizes a self-supervised flow-guided deformable temporal alignment module to align the features on the left and right view branches, respectively. Then, the aligned features are fed into a shared adaptive feature aggregation module to generate missing contents of their respective branches. Finally, the parallax attention module (PAM) that uses the cross-view information to consider the significant stereo correlation is introduced to fuse the completed features of left and right views. Furthermore, we develop a stereo consistency loss to regularize the trained parameters, so that our model is able to yield high-quality stereo video inpainting results with better stereo consistency. Experimental results demonstrate that our SVINet outperforms state-of-the-art single video inpainting models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">235.CLOTH4D: A Dataset for Clothed Human Reconstruction</span><br>
                <span class="as">Zou, XingxingandHan, XintongandWong, Waikeung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12847-12857.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决虚拟世界中重建穿衣人类的问题，以提升虚拟化身的质量。<br>
                    动机：高质量的穿衣人类重建是创建虚拟世界的关键，其质量在很大程度上决定了Metaverse是否会成为一时的热潮。<br>
                    方法：本文介绍了CLOTH4D，这是一个包含1000个主题、1000个3D服装和超过10万个带配对未穿衣人类的穿衣网格的穿衣人类数据集，以填补大规模高质量4D服装数据的空白。<br>
                    效果：基于CLOTH4D，我们设计了一系列新颖的时间感知指标来评估生成的3D人体网格的时间稳定性，这是之前被忽视的。此外，通过评估和再训练当前最先进的穿衣人类重建方法，我们揭示了一些见解，展示了改进的性能，并提出了可能的未来研究方向，证实了我们数据集的进步。该数据集可在www.github.com/AemikaChow/AiDLab-fAshIon-Data获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Clothed human reconstruction is the cornerstone for creating the virtual world. To a great extent, the quality of recovered avatars decides whether the Metaverse is a passing fad. In this work, we introduce CLOTH4D, a clothed human dataset containing 1,000 subjects with varied appearances, 1,000 3D outfits, and over 100,000 clothed meshes with paired unclothed humans, to fill the gap in large-scale and high-quality 4D clothing data. It enjoys appealing characteristics: 1) Accurate and detailed clothing textured meshes---all clothing items are manually created and then simulated in professional software, strictly following the general standard in fashion design. 2) Separated textured clothing and under-clothing body meshes, closer to the physical world than single-layer raw scans. 3) Clothed human motion sequences simulated given a set of 289 actions, covering fundamental and complicated dynamics. Upon CLOTH4D, we novelly designed a series of temporally-aware metrics to evaluate the temporal stability of the generated 3D human meshes, which has been overlooked previously. Moreover, by assessing and retraining current state-of-the-art clothed human reconstruction methods, we reveal insights, present improved performance, and propose potential future research directions, confirming our dataset's advancement. The dataset is available at www.github.com/AemikaChow/AiDLab-fAshIon-Data</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">236.Learning To Render Novel Views From Wide-Baseline Stereo Pairs</span><br>
                <span class="as">Du, YilunandSmith, CameronandTewari, AyushandSitzmann, Vincent</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Learning_To_Render_Novel_Views_From_Wide-Baseline_Stereo_Pairs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4970-4980.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅通过单个宽基线立体图像对进行新视角合成。<br>
                    动机：现有的稀疏观测新视角合成方法由于恢复错误的3D几何和高成本的可微渲染，导致其无法扩展至大规模训练。<br>
                    方法：提出一种多视图变换编码器，设计了一种高效的图像空间极线采样方案来组装目标射线的图像特征，以及一个轻量级的交叉注意力渲染器。<br>
                    效果：在两个真实世界数据集上进行了广泛的比较，显著优于现有基于稀疏图像观测的新视角合成方法，实现了多视图一致的新视角合成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a method for novel view synthesis given only a single wide-baseline stereo image pair. In this challenging regime, 3D scene points are regularly observed only once, requiring prior-based reconstruction of scene geometry and appearance. We find that existing approaches to novel view synthesis from sparse observations fail due to recovering incorrect 3D geometry and the high cost of differentiable rendering that precludes their scaling to large-scale training. We take a step towards resolving these shortcomings by formulating a multi-view transformer encoder, proposing an efficient, image-space epipolar line sampling scheme to assemble image features for a target ray, and a lightweight cross-attention-based renderer. Our contributions enable training of our method on a large-scale real-world dataset of indoor and outdoor scenes. In several ablation studies, we demonstrate that our contributions enable learning of powerful multi-view geometry priors while reducing both rendering time and memory footprint. We conduct extensive comparisons on held-out test scenes across two real-world datasets, significantly outperforming prior work on novel view synthesis from sparse image observations and achieving multi-view-consistent novel view synthesis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">237.Multi-Object Manipulation via Object-Centric Neural Scattering Functions</span><br>
                <span class="as">Tian, StephenandCai, YanchengandYu, Hong-XingandZakharov, SergeyandLiu, KatherineandGaidon, AdrienandLi, YunzhuandWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Multi-Object_Manipulation_via_Object-Centric_Neural_Scattering_Functions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9021-9031.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地表示涉及多物体交互的场景。<br>
                    动机：现有的方法将场景分解为离散的对象，但在具有挑战性的光照条件下，它们在精确建模和操纵方面存在困难，因为它们只编码了与特定照明相关的外观。<br>
                    方法：提出使用以对象为中心的神经散射函数（OSFs）作为模型预测控制框架中的对象表示。OSFs对每个对象的光传输进行建模，使能在对象重新排列和变化的光照条件下进行组合场景的重新渲染。通过将这种方法与逆参数估计和基于图的神经动力学模型相结合，展示了在组合多物体环境中改进的模型预测控制性能和泛化能力，即使在以前未见过的场景和恶劣的光照条件下也是如此。<br>
                    效果：实验结果表明，该方法在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learned visual dynamics models have proven effective for robotic manipulation tasks. Yet, it remains unclear how best to represent scenes involving multi-object interactions. Current methods decompose a scene into discrete objects, yet they struggle with precise modeling and manipulation amid challenging lighting conditions since they only encode appearance tied with specific illuminations. In this work, we propose using object-centric neural scattering functions (OSFs) as object representations in a model-predictive control framework. OSFs model per-object light transport, enabling compositional scene re-rendering under object rearrangement and varying lighting conditions. By combining this approach with inverse parameter estimation and graph-based neural dynamics models, we demonstrate improved model-predictive control performance and generalization in compositional multi-object environments, even in previously unseen scenarios and harsh lighting conditions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">238.Invertible Neural Skinning</span><br>
                <span class="as">Kant, YashandSiarohin, AliaksandrandGuler, RizaAlpandChai, MengleiandRen, JianandTulyakov, SergeyandGilitschenski, Igor</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kant_Invertible_Neural_Skinning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8715-8725.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从原始3D扫描和姿势建立可动画化和可编辑的穿衣人类模型。<br>
                    动机：现有的重新定位方法受到线性混合蒙皮（LBS）表达性的限制，需要昂贵的网格提取来生成每个新的姿势，并且通常不会保留不同姿势之间的表面对应关系。<br>
                    方法：引入可逆神经蒙皮（INS），通过学习额外的随姿势变化的形状变形来保持对应关系。结合了可微分LBS模块，构建了一个具有表现力和端到端的可逆神经蒙皮（INS）管道。<br>
                    效果：在穿衣人类上超越了最先进的重新定位技术，同时保留了表面对应关系，速度快了一个数量级。定性结果显示，INS可以很好地纠正LBS引入的人工制品。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Building animatable and editable models of clothed humans from raw 3D scans and poses is a challenging problem. Existing reposing methods suffer from the limited expressiveness of Linear Blend Skinning (LBS), require costly mesh extraction to generate each new pose, and typically do not preserve surface correspondences across different poses. In this work, we introduce Invertible Neural Skinning (INS) to address these shortcomings. To maintain correspondences, we propose a Pose-conditioned Invertible Network (PIN) architecture, which extends the LBS process by learning additional pose-varying deformations. Next, we combine PIN with a differentiable LBS module to build an expressive and end-to-end Invertible Neural Skinning (INS) pipeline. We demonstrate the strong performance of our method by outperforming the state-of-the-art reposing techniques on clothed humans and preserving surface correspondences, while being an order of magnitude faster. We also perform an ablation study, which shows the usefulness of our pose-conditioning formulation, and our qualitative results display that INS can rectify artefacts introduced by LBS well.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">239.Seeing a Rose in Five Thousand Ways</span><br>
                <span class="as">Zhang, YunzhiandWu, ShangzheandSnavely, NoahandWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Seeing_a_Rose_in_Five_Thousand_Ways_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/962-971.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图像中学习并捕捉物体的内在属性（如几何分布、纹理和材质）。<br>
                    动机：现有的模型在处理多实例对象时，往往忽视了对象的内在属性，导致生成的图像缺乏真实感。<br>
                    方法：本文提出了一种生成模型，通过学习单张图像中多个实例对象的内在属性，实现对不同大小、形状和姿态的物体进行渲染。<br>
                    效果：实验证明，该方法能成功捕捉多种物体的内在属性，并在内在图像分解、形状和图像生成、视图合成和重光照等下游任务上取得优越结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>What is a rose, visually? A rose comprises its intrinsics, including the distribution of geometry, texture, and material specific to its object category. With knowledge of these intrinsic properties, we may render roses of different sizes and shapes, in different poses, and under different lighting conditions. In this work, we build a generative model that learns to capture such object intrinsics from a single image, such as a photo of a bouquet. Such an image includes multiple instances of an object type. These instances all share the same intrinsics, but appear different due to a combination of variance within these intrinsics and differences in extrinsic factors, such as pose and illumination. Experiments show that our model successfully learns object intrinsics (distribution of geometry, texture, and material) for a wide range of objects, each from a single Internet image. Our method achieves superior results on multiple downstream tasks, including intrinsic image decomposition, shape and image generation, view synthesis, and relighting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">240.Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos</span><br>
                <span class="as">Wang, LiaoandHu, QiangandHe, QihanandWang, ZiyuandYu, JingyiandTuytelaars, TinneandXu, LanandWu, Minye</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/76-87.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用神经网络渲染技术实现动态场景的实时自由视点视频（FVV）渲染。<br>
                    动机：目前的神经渲染技术在处理动态场景的FVV时，要么仅限于离线渲染，要么只能处理运动最小的短暂序列。<br>
                    方法：提出了一种新的技术——残差辐射场（ReRF），作为一种高度紧凑的神经表示，以实现对长时间动态场景的实时FVV渲染。ReRF显式地在时空特征空间中建模相邻时间戳之间的残差信息，使用全局坐标基础的微型MLP作为特征解码器。<br>
                    效果：实验表明，这种策略可以在不牺牲质量的情况下处理大的运动。基于ReRF，设计了一个特殊的FVV编解码器，实现了三个数量级的压缩率，并提供了一个配套的ReRF播放器，支持在线流式传输长时间动态场景的FVV。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of the Neural Radiance Fields (NeRFs) for modeling and free-view rendering static objects has inspired numerous attempts on dynamic scenes. Current techniques that utilize neural rendering for facilitating free-view videos (FVVs) are restricted to either offline rendering or are capable of processing only brief sequences with minimal motion. In this paper, we present a novel technique, Residual Radiance Field or ReRF, as a highly compact neural representation to achieve real-time FVV rendering on long-duration dynamic scenes. ReRF explicitly models the residual information between adjacent timestamps in the spatial-temporal feature space, with a global coordinate-based tiny MLP as the feature decoder. Specifically, ReRF employs a compact motion grid along with a residual feature grid to exploit inter-frame feature similarities. We show such a strategy can handle large motions without sacrificing quality. We further present a sequential training scheme to maintain the smoothness and the sparsity of the motion/residual grids. Based on ReRF, we design a special FVV codec that achieves three orders of magnitudes compression rate and provides a companion ReRF player to support online streaming of long-duration FVVs of dynamic scenes. Extensive experiments demonstrate the effectiveness of ReRF for compactly representing dynamic radiance fields, enabling an unprecedented free-viewpoint viewing experience in speed and quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">241.NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds</span><br>
                <span class="as">Yang, ChenandLi, PeihaoandZhou, ZanweiandYuan, ShanxinandLiu, BingbingandYang, XiaokangandQiu, WeichaoandShen, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16549-16558.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使NeRF在渲染与训练视图显著不同的新视图时，也能获得好的效果。<br>
                    动机：目前的NeRF方法在渲染与训练视图相似的新视图上表现优秀，但在处理与训练视图有显著差异的新视图时表现不佳。<br>
                    方法：提出一种名为NeRFVS的新方法，利用神经重建的整体先验（包括伪深度图和视图覆盖信息）来指导3D室内场景的隐式神经表示的学习。具体来说，首先利用现成的神经重建方法生成一个几何框架，然后根据整体先验提出两种改进NeRF学习的损失函数：1）鲁棒的深度损失，可以容忍伪深度图的误差，引导NeRF的几何学习；2）方差损失，用于规范隐式神经表示的方差，减少学习过程中的几何和颜色歧义。这两种损失函数会根据视图覆盖信息在NeRF优化过程中进行调整，以减少视图覆盖不平衡带来的负面影响。<br>
                    效果：大量实验表明，我们的NeRFVS在室内场景上优于最先进的视图合成方法，无论是定量还是定性，都实现了高保真的自由导航结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room. NeRF achieves impressive performance in rendering images for novel views similar to the input views while suffering for novel views that are significantly different from the training views. To address this issue, we utilize the holistic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to improve the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the geometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure. These two loss functions are modulated during NeRF optimization according to the view coverage information to reduce the negative influence brought by the view coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms state-of-the-art view synthesis methods quantitatively and qualitatively on indoor scenes, achieving high-fidelity free navigation results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">242.A Unified Spatial-Angular Structured Light for Single-View Acquisition of Shape and Reflectance</span><br>
                <span class="as">Xu, XianminandLin, YuxinandZhou, HaoyangandZeng, ChongandYu, YaxinandZhou, KunandWu, Hongzhi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_A_Unified_Spatial-Angular_Structured_Light_for_Single-View_Acquisition_of_Shape_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/206-215.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种由LED阵列和LCD掩模组成的统一结构光，以从单一视角获取高质量的形状和反射率。<br>
                    动机：为了解决传统方法在获取物体形状和反射率方面的不足，提出了一种结合结构光和深度学习的方法。<br>
                    方法：使用LED投影一组学习到的掩模模式来准确编码空间信息，然后聚合多个LED的解码结果以生成最终的深度图。对于外观，将学习到的光模式投射通过透明掩模以高效地探测角度变化的反射率。优化BRDF参数并存储在纹理图中作为最终的反射率。建立了一个可微分的管道来进行联合捕获，自动优化掩模和光模式以提高采集质量。<br>
                    效果：通过各种实物对象展示了该方法的有效性，并与最先进的技术进行了比较。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a unified structured light, consisting of an LED array and an LCD mask, for high-quality acquisition of both shape and reflectance from a single view. For geometry, one LED projects a set of learned mask patterns to accurately encode spatial information; the decoded results from multiple LEDs are then aggregated to produce a final depth map. For appearance, learned light patterns are cast through a transparent mask to efficiently probe angularly-varying reflectance. Per-point BRDF parameters are differentiably optimized with respect to corresponding measurements, and stored in texture maps as the final reflectance. We establish a differentiable pipeline for the joint capture to automatically optimize both the mask and light patterns towards optimal acquisition quality. The effectiveness of our light is demonstrated with a wide variety of physical objects. Our results compare favorably with state-of-the-art techniques.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">243.On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks</span><br>
                <span class="as">Jung, HyunJunandRuhkamp, PatrickandZhai, GuangyaoandBrasch, NikolasandLi, YitongandVerdie, YannickandSong, JifeiandZhou, YirenandArmagan, AnilandIlic, SlobodanandLeonardis, Ale\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_On_the_Importance_of_Accurate_Geometry_Data_for_Dense_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/780-791.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于学习的方法在解决密集3D视觉问题时，通常使用3D传感器数据进行训练，但并未对传感器测量的距离原理的优缺点进行比较和讨论。<br>
                    动机：由于缺乏多模态数据集，纹理较少的区域对于运动结构和立体视觉存在问题，反射材料对主动传感造成影响，半透明物体的距离难以用现有硬件准确测量。训练不准确或损坏的数据会引入模型偏差，阻碍泛化能力。<br>
                    方法：本文通过调查传感器误差对密集3D视觉任务（如深度估计和重建）的影响，严格展示了传感器特性对学习预测的重大影响，并注意到了在日常生活中各种技术引起的泛化问题。<br>
                    效果：我们引入了一个精心设计的数据集，包括来自商用传感器（如D-ToF、I-ToF、被动/主动立体视觉和单目RGB+P）的测量数据。这项研究量化了显著的传感器噪声影响，并为改进密集视觉估计和有针对性的数据融合铺平了道路。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware. Training on inaccurate or corrupt data induces model bias and hampers generalisation capabilities. These effects remain unnoticed if the sensor measurement is considered as ground truth during the evaluation. This paper investigates the effect of sensor errors for the dense 3D vision tasks of depth estimation and reconstruction. We rigorously show the significant impact of sensor characteristics on the learned predictions and notice generalisation issues arising from various technologies in everyday household environments. For evaluation, we introduce a carefully designed dataset comprising measurements from commodity sensors, namely D-ToF, I-ToF, passive/active stereo, and monocular RGB+P. Our study quantifies the considerable sensor noise impact and paves the way to improved dense vision estimates and targeted data fusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">244.K-Planes: Explicit Radiance Fields in Space, Time, and Appearance</span><br>
                <span class="as">Fridovich-Keil, SaraandMeanti, GiacomoandWarburg, FrederikRahb{\ae</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12479-12488.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地表示任意维度的辐射场？<br>
                    动机：目前的模型在处理动态场景时存在困难，需要一种能够无缝转换静态和动态场景的方法。<br>
                    方法：提出了k-planes模型，使用d-choose-2平面来表示d维场景，通过平面分解方便地添加特定维度的先验知识，如时间平滑和多分辨率空间结构，并自然地将场景的静态和动态部分进行分解。<br>
                    效果：在一系列合成和真实的静态、动态、固定和变化外观的场景中，k-planes模型具有竞争力且通常达到最先进的重建保真度，同时内存使用率低，实现了对完整4D网格的1000倍压缩，并且优化速度快，采用纯PyTorch实现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d-choose-2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color basis that yields similar performance as a nonlinear black-box MLP decoder. Across a range of synthetic and real, static and dynamic, fixed and varying appearance scenes, k-planes yields competitive and often state-of-the-art reconstruction fidelity with low memory usage, achieving 1000x compression over a full 4D grid, and fast optimization with a pure PyTorch implementation. For video results and code, please see sarafridov.github.io/K-Planes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">245.Viewpoint Equivariance for Multi-View 3D Object Detection</span><br>
                <span class="as">Chen, DianandLi, JieandGuizilini, VitorandAmbrus, RaresAndreiandGaidon, Adrien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Viewpoint_Equivariance_for_Multi-View_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9213-9222.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从视觉传感器中进行3D物体检测，这是机器人系统的关键能力。<br>
                    动机：多视角一致性在3D场景理解和几何学习中起着关键作用，我们希望通过利用这一点来改进物体定位。<br>
                    方法：我们提出了一种新的3D物体检测框架VEDet，该框架通过利用3D多视角几何来提高定位的准确性，具有视图意识和等变性。<br>
                    效果：我们在nuScenes基准测试上取得了最先进的性能，代码和模型可以在https://github.com/TRI-ML/VEDet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection from visual sensors is a cornerstone capability of robotic systems. State-of-the-art methods focus on reasoning and decoding object bounding boxes from multi-view camera input. In this work we gain intuition from the integral role of multi-view consistency in 3D scene understanding and geometric learning. To this end, we introduce VEDet, a novel 3D object detection framework that exploits 3D multi-view geometry to improve localization through viewpoint awareness and equivariance. VEDet leverages a query-based transformer architecture and encodes the 3D scene by augmenting image features with positional encodings from their 3D perspective geometry. We design view-conditioned queries at the output level, which enables the generation of multiple virtual frames during training to learn viewpoint equivariance by enforcing multi-view consistency. The multi-view geometry injected at the input level as positional encodings and regularized at the loss level provides rich geometric cues for 3D object detection, leading to state-of-the-art performance on the nuScenes benchmark. The code and model are made available at https://github.com/TRI-ML/VEDet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">246.Putting People in Their Place: Affordance-Aware Human Insertion Into Scenes</span><br>
                <span class="as">Kulal, SumithandBrooks, TimandAiken, AlexandWu, JiajunandYang, JimeiandLu, JingwanandEfros, AlexeiA.andSingh, KrishnaKumar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kulal_Putting_People_in_Their_Place_Affordance-Aware_Human_Insertion_Into_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17089-17099.png><br>
            
            <span class="tt"><span class="t0">研究问题：通过在场景中插入人物来推断场景的可利用性。<br>
                    动机：现有的方法无法真实地将人物插入到场景中，并尊重场景的可利用性。<br>
                    方法：我们提出了一种方法，该方法可以推断出给定场景上下文的一组真实的姿态，重新定位参考人物，并协调构图。<br>
                    效果：我们的模型可以在没有条件的情况下，当提示时，也可以想象出真实的人和场景，并且可以实现交互式编辑。实验结果表明，与现有工作相比，我们的方法合成了更真实的人物外观和更自然的人类-场景互动。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of inferring scene affordances by presenting a method for realistically inserting people into scenes. Given a scene image with a marked region and an image of a person, we insert the person into the scene while respecting the scene affordances. Our model can infer the set of realistic poses given the scene context, re-pose the reference person, and harmonize the composition. We set up the task in a self-supervised fashion by learning to re- pose humans in video clips. We train a large-scale diffusion model on a dataset of 2.4M video clips that produces diverse plausible poses while respecting the scene context. Given the learned human-scene composition, our model can also hallucinate realistic people and scenes when prompted without conditioning and also enables interactive editing. We conduct quantitative evaluation and show that our method synthesizes more realistic human appearance and more natural human-scene interactions when compared to prior work.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">247.3D Neural Field Generation Using Triplane Diffusion</span><br>
                <span class="as">Shue, J.RyanandChan, EricRyanandPo, RyanandAnkner, ZacharyandWu, JiajunandWetzstein, Gordon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20875-20886.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种有效的基于扩散模型的3D感知神经场生成方法。<br>
                    动机：现有的3D感知神经场生成方法效果不理想，需要寻找新的优化方案。<br>
                    方法：通过将训练数据（如ShapeNet网格）转换为连续占用场并分解为一系列轴对齐的三平面特征表示，将其转化为2D特征平面进行训练，直接在2D特征平面上训练现有的2D扩散模型以生成高质量的、多样化的3D神经场。<br>
                    效果：实验结果表明，该方法在ShapeNet的几个对象类别上的3D生成任务中取得了最先进的结果，优于其他3D感知生成方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have emerged as the state-of-the-art for image generation, among other tasks. Here, we present an efficient diffusion-based model for 3D-aware generation of neural fields. Our approach pre-processes training data, such as ShapeNet meshes, by converting them to continuous occupancy fields and factoring them into a set of axis-aligned triplane feature representations. Thus, our 3D training scenes are all represented by 2D feature planes, and we can directly train existing 2D diffusion models on these representations to generate 3D neural fields with high quality and diversity, outperforming alternative approaches to 3D-aware generation. Our approach requires essential modifications to existing triplane factorization pipelines to make the resulting features easy to learn for the diffusion model. We demonstrate state-of-the-art results on 3D generation on several object classes from ShapeNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">248.Semantic Scene Completion With Cleaner Self</span><br>
                <span class="as">Wang, FengyunandZhang, DongandZhang, HanwangandTang, JinhuiandSun, Qianru</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Semantic_Scene_Completion_With_Cleaner_Self_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/867-877.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善语义场景补全（SSC）中由于深度相机的感官缺陷导致的预测不完整和语义标签混乱的问题。<br>
                    动机：现有的基于从深度值估计的有噪声TSDF的方法存在预测不完整和语义标签混乱的问题。<br>
                    方法：使用真实3D体素生成完美的可见表面（TSDF-CAD），然后训练一个“清洁”的SSC模型，并将这个“清洁”的知识提炼到另一个输入有噪声TSDF的模型中。<br>
                    效果：实验结果表明，该方法提高了有噪声模型的IoU和mIoU，分别达到了3.1%和2.2%，并在流行的NYU数据集上取得了新的最先进的精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic Scene Completion (SSC) transforms an image of single-view depth and/or RGB 2D pixels into 3D voxels, each of whose semantic labels are predicted. SSC is a well-known ill-posed problem as the prediction model has to "imagine" what is behind the visible surface, which is usually represented by Truncated Signed Distance Function (TSDF). Due to the sensory imperfection of the depth camera, most existing methods based on the noisy TSDF estimated from depth values suffer from 1) incomplete volumetric predictions and 2) confused semantic labels. To this end, we use the ground-truth 3D voxels to generate a perfect visible surface, called TSDF-CAD, and then train a "cleaner" SSC model. As the model is noise-free, it is expected to focus more on the "imagination" of unseen voxels. Then, we propose to distill the intermediate "cleaner" knowledge into another model with noisy TSDF input. In particular, we use the 3D occupancy feature and the semantic relations of the "cleaner self" to supervise the counterparts of the "noisy self" to respectively address the above two incorrect predictions. Experimental results validate that the proposed method improves the noisy counterparts with 3.1% IoU and 2.2% mIoU for measuring scene completion and SSC, and also achieves new state-of-the-art accuracy on the popular NYU dataset. The code is available at https://github.com/fereenwong/CleanerS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">249.3D Human Mesh Estimation From Virtual Markers</span><br>
                <span class="as">Ma, XiaoxuanandSu, JiajunandWang, ChunyuandZhu, WentaoandWang, Yizhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/534-543.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从无标记的野外图像中恢复具有现实形状的完整3D人体网格。<br>
                    动机：现有的方法在提取骨架时会丢失身体形状信息，导致性能一般。高级运动捕捉系统通过在身体表面放置密集的物理标记解决了这个问题，但不能应用于没有标记的野外图像。<br>
                    方法：提出一种名为虚拟标记的中间表示方法，该方法基于大规模mocap数据以生成式风格学习身体表面的64个关键点，模仿物理标记的效果。虚拟标记可以从野外图像中准确检测出来，并通过简单的插值重建具有现实形状的完整网格。<br>
                    效果：在三个数据集上，该方法都优于最先进的方法，特别是在具有多样身体形状的SURREAL数据集上，其表现显著超过现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inspired by the success of volumetric 3D pose estimation, some recent human mesh estimators propose to estimate 3D skeletons as intermediate representations, from which, the dense 3D meshes are regressed by exploiting the mesh topology. However, body shape information is lost in extracting skeletons, leading to mediocre performance. The advanced motion capture systems solve the problem by placing dense physical markers on the body surface, which allows to extract realistic meshes from their non-rigid motions. However, they cannot be applied to wild images without markers. In this work, we present an intermediate representation, named virtual markers, which learns 64 landmark keypoints on the body surface based on the large-scale mocap data in a generative style, mimicking the effects of physical markers. The virtual markers can be accurately detected from wild images and can reconstruct the intact meshes with realistic shapes by simple interpolation. Our approach outperforms the state-of-the-art methods on three datasets. In particular, it surpasses the existing methods by a notable margin on the SURREAL dataset, which has diverse body shapes. Code is available at https://github.com/ShirleyMaxx/VirtualMarker.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">250.High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition</span><br>
                <span class="as">Luan, TianyuandZhai, YuanhaoandMeng, JingjingandLi, ZhongandChen, ZhangandXu, YiandYuan, Junsong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16795-16804.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管最新的单图像手势建模技术取得了令人印象深刻的性能，但它们缺乏捕获3D手部网格的足够细节的能力。<br>
                    动机：这种缺陷在需要高保真度手势建模时（如个性化手势建模）大大限制了其应用。<br>
                    方法：设计了一个频率分割网络，以粗到细的方式使用不同的频率带生成3D手部网格。为了捕捉高频个性化细节，我们将3D网格转换为频率域，并提出了一种新的频率分解损失来监督每个频率组件。通过利用这种粗到细的方案，可以保留对应于更高频率域的手部细节。此外，所提出的方法具有可扩展性，可以在任何分辨率级别停止推理，以适应计算能力不同的各种硬件。<br>
                    效果：大量的实验表明，我们的方法为高保真3D手部重建生成了精细的细节，而且我们的评估指标在测量网格细节方面比传统指标更有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the impressive performance obtained by recent single-image hand modeling techniques, they lack the capability to capture sufficient details of the 3D hand mesh. This deficiency greatly limits their applications when high fidelity hand modeling is required, e.g., personalized hand modeling. To address this problem, we design a frequency split network to generate 3D hand mesh using different frequency bands in a coarse-to-fine manner. To capture high-frequency personalized details, we transform the 3D mesh into the frequency domain, and propose a novel frequency decomposition loss to supervise each frequency component. By leveraging such a coarse-to-fine scheme, hand details that correspond to the higher frequency domain can be preserved. In addition, the proposed network is scalable, and can stop the inference at any resolution level to accommodate different hardwares with varying computational powers. To quantitatively evaluate the performance of our method in terms of recovering personalized shape details, we introduce a new evaluation metric named Mean Signal-to-Noise Ratio (MSNR) to measure the signal-to-noise ratio of each mesh frequency component. Extensive experiments demonstrate that our approach generates fine-grained details for high fidelity 3D hand reconstruction, and our evaluation metric is more effective for measuring mesh details compared with traditional metrics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">251.Neural Scene Chronology</span><br>
                <span class="as">Lin, HaotongandWang, QianqianandCai, RuojinandPeng, SidaandAverbuch-Elor, HadarandZhou, XiaoweiandSnavely, Noah</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Neural_Scene_Chronology_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20752-20761.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在重建一个能够从互联网大尺度地标照片中生成具有独立视角、光照和时间控制的逼真渲染的时变3D模型。<br>
                    动机：主要挑战在于图像中的不同类型时间变化（如光照和场景本身的变化）是交织在一起的，并且场景级别的时间变化通常是离散和零星的。<br>
                    方法：提出一种新的场景表示方法，配备一种新颖的时间步长函数编码方法，可以将离散的场景级内容变化建模为随时间的分段常数函数。具体来说，我们将场景表示为带有每张图像光照嵌入的空间-时间辐射场，其中随时间变化的场景通过一组学习到的步长函数进行编码。<br>
                    效果：在展示各种时间变化的四个场景的新数据集上，我们的方法表现出了最先进的视图合成结果，同时实现了独立的视角、时间和光照控制。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we aim to reconstruct a time-varying 3D model, capable of rendering photo-realistic renderings with independent control of viewpoint, illumination, and time, from Internet photos of large-scale landmarks. The core challenges are twofold. First, different types of temporal changes, such as illumination and changes to the underlying scene itself (such as replacing one graffiti artwork with another) are entangled together in the imagery. Second, scene-level temporal changes are often discrete and sporadic over time, rather than continuous. To tackle these problems, we propose a new scene representation equipped with a novel temporal step function encoding method that can model discrete scene-level content changes as piece-wise constant functions over time. Specifically, we represent the scene as a space-time radiance field with a per-image illumination embedding, where temporally-varying scene changes are encoded using a set of learned step functions. To facilitate our task of chronology reconstruction from Internet imagery, we also collect a new dataset of four scenes that exhibit various changes over time. We demonstrate that our method exhibits state-of-the-art view synthesis results on this dataset, while achieving independent control of viewpoint, time, and illumination. Code and data are available at https://zju3dv.github.io/NeuSC/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">252.Light Source Separation and Intrinsic Image Decomposition Under AC Illumination</span><br>
                <span class="as">Yoshida, YusakuandKawahara, RyoandOkabe, Takahiro</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshida_Light_Source_Separation_and_Intrinsic_Image_Decomposition_Under_AC_Illumination_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5735-5743.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用交流电照明的闪烁提取场景信息，进行固有图像分解。<br>
                    动机：交流电照明下的闪烁可以提取丰富的场景信息，有助于固有图像分解。<br>
                    方法：通过矩阵分解解决盲目光源分离和Lambert模型下的固有图像分解中的模糊性，并进行光源分离和固有图像分解。<br>
                    效果：实验证明该方法可以恢复各光源的颜色、漫反射值以及各光源下的漫反射和镜面强度，且交流电照明下的固有图像分解对自动白平衡应用有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Artificial light sources are often powered by an electric grid, and then their intensities rapidly oscillate in response to the grid's alternating current (AC). Interestingly, the flickers of scene radiance values due to AC illumination are useful for extracting rich information on a scene of interest. In this paper, we show that the flickers due to AC illumination is useful for intrinsic image decomposition (IID). Our proposed method conducts the light source separation (LSS) followed by the IID under AC illumination. In particular, we reveal the ambiguity in the blind LSS via matrix factorization and the ambiguity in the IID assuming the Lambert model, and then show why and how those ambiguities can be resolved. We experimentally confirmed that our method can recover the colors of the light sources, the diffuse reflectance values, and the diffuse and specular intensities (shadings) under each of the light sources, and that the IID under AC illumination is effective for application to auto white balancing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">253.Plateau-Reduced Differentiable Path Tracing</span><br>
                <span class="as">Fischer, MichaelandRitschel, Tobias</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fischer_Plateau-Reduced_Differentiable_Path_Tracing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4285-4294.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的可微渲染器在优化过程中可能会遇到梯度为零的区域，导致无法收敛。<br>
                    动机：为了解决这个问题，我们提出了一种通过模糊参数空间来消除梯度为零区域的新的优化方法。<br>
                    方法：我们将高维的渲染函数（将场景参数映射到图像）与另一个用于模糊参数空间的核进行卷积，从而得到无梯度平坦区域的优化方法。我们还描述了两种高效的蒙特卡洛估计器来计算无梯度平坦区域的梯度。<br>
                    效果：我们的方法是对黑盒和可微渲染器的直接扩展，能够成功优化复杂的光传输问题，如焦散或全局照明，而现有的可微路径跟踪器无法收敛。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current differentiable renderers provide light transport gradients with respect to arbitrary scene parameters. However, the mere existence of these gradients does not guarantee useful update steps in an optimization. Instead, inverse rendering might not converge due to inherent plateaus, i.e., regions of zero gradient, in the objective function. We propose to alleviate this by convolving the high-dimensional rendering function that maps scene parameters to images with an additional kernel that blurs the parameter space. We describe two Monte Carlo estimators to compute plateau-free gradients efficiently, i.e., with low variance, and show that these translate into net-gains in optimization error and runtime performance. Our approach is a straightforward extension to both black-box and differentiable renderers and enables the successful optimization of problems with intricate light transport, such as caustics or global illumination, that existing differentiable path tracers do not converge on. Our code is at github.com/mfischer-ucl/prdpt.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">254.ECON: Explicit Clothed Humans Optimized via Normal Integration</span><br>
                <span class="as">Xiu, YuliangandYang, JinlongandCao, XuandTzionas, DimitriosandBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/512-523.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从图像中创建详细、穿着衣服的3D人物模型。<br>
                    动机：现有的方法在处理新的姿势或衣物时，会出现无肢体或形状退化的问题。<br>
                    方法：提出了一种新的方法ECON，它结合了深度学习、艺术家策划的扫描和隐式函数（IF），首先推断出穿着人的详细2D贴图，然后根据这些贴图恢复出2.5D前后面，最后"inpaints"缺失的几何形状。<br>
                    效果：实验结果表明，ECON在宽松的衣服和挑战性的姿态下也能推断出高保真度的3D人物，超越了以往的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The combination of deep learning, artist-curated scans, and Implicit Functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry, but produce disembodied limbs or degenerate shapes for novel poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit representation and explicit body regularization. To this end, we make two key observations: (1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a "canvas" for stitching together detailed surface patches. Based on these, our method, ECON, has three main steps: (1) It infers detailed 2D normal maps for the front and back side of a clothed person. (2) From these, it recovers 2.5D front and back surfaces, called d-BiNI, that are equally detailed, yet incomplete, and registers these w.r.t. each other with the help of a SMPL-X body mesh recovered from the image. (3) It "inpaints" the missing geometry between d-BiNI surfaces. If the face and hands are noisy, they can optionally be replaced with the ones of SMPL-X. As a result, ECON infers high-fidelity 3D humans even in loose clothes and challenging poses. This goes beyond previous methods, according to the quantitative evaluation on the CAPE and Renderpeople datasets. Perceptual studies also show that ECON's perceived realism is better by a large margin. Code and models are available for research purposes at econ.is.tue.mpg.de</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">255.F2-NeRF: Fast Neural Radiance Field Training With Free Camera Trajectories</span><br>
                <span class="as">Wang, PengandLiu, YuanandChen, ZhaoxiandLiu, LingjieandLiu, ZiweiandKomura, TakuandTheobalt, ChristianandWang, Wenping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4150-4159.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的基于网格的NeRF模型F^2-NeRF，用于新视角合成，能够处理任意输入的相机轨迹，并且训练时间只需几分钟。<br>
                    动机：现有的快速网格基NeRF训练框架，如Instant-NGP、Plenoxels、DVGO或TensoRF，主要设计用于有限的场景，并依赖于空间扭曲来处理无限的场景。现有的两种广泛使用的空间扭曲方法只能处理向前的轨迹或360度的对象中心轨迹，不能处理任意的轨迹。<br>
                    方法：本文深入研究了处理无限场景的空间扭曲机制。基于我们的分析，我们进一步提出了一种新的空间扭曲方法，称为透视扭曲，使我们能够在网格基NeRF框架中处理任意的轨迹。<br>
                    效果：大量的实验证明，F^2-NeRF能够使用相同的透视扭曲在两个标准数据集和我们收集的新的自由轨迹数据集上渲染高质量的图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a novel grid-based NeRF called F^2-NeRF (Fast-Free-NeRF) for novel view synthesis, which enables arbitrary input camera trajectories and only costs a few minutes for training. Existing fast grid-based NeRF training frameworks, like Instant-NGP, Plenoxels, DVGO, or TensoRF, are mainly designed for bounded scenes and rely on space warping to handle unbounded scenes. Existing two widely-used space-warping methods are only designed for the forward-facing trajectory or the 360deg object-centric trajectory but cannot process arbitrary trajectories. In this paper, we delve deep into the mechanism of space warping to handle unbounded scenes. Based on our analysis, we further propose a novel space-warping method called perspective warping, which allows us to handle arbitrary trajectories in the grid-based NeRF framework. Extensive experiments demonstrate that F^2-NeRF is able to use the same perspective warping to render high-quality images on two standard datasets and a new free trajectory dataset collected by us.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">256.Balanced Spherical Grid for Egocentric View Synthesis</span><br>
                <span class="as">Choi, ChangwoonandKim, SangMinandKim, YoungMin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Balanced_Spherical_Grid_for_Egocentric_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16590-16599.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地重建大规模的真实世界环境，用于虚拟现实资产。<br>
                    动机：现有的方法在处理大规模无界场景时效率低下，且存在奇异性问题。<br>
                    方法：提出了EgoNeRF模型，采用球形参数化代替传统的笛卡尔坐标，通过组合两个平衡的网格解决了两极的不规则性和无法表示无界场景的问题，同时使用重采样技术增加了训练NeRF体积的有效样本数量。<br>
                    效果：在新的合成和真实世界的以自我为中心的360度视频数据集上进行了广泛的评估，EgoNeRF模型始终实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present EgoNeRF, a practical solution to reconstruct large-scale real-world environments for VR assets. Given a few seconds of casually captured 360 video, EgoNeRF can efficiently build neural radiance fields which enable high-quality rendering from novel viewpoints. Motivated by the recent acceleration of NeRF using feature grids, we adopt spherical coordinate instead of conventional Cartesian coordinate. Cartesian feature grid is inefficient to represent large-scale unbounded scenes because it has a spatially uniform resolution, regardless of distance from viewers. The spherical parameterization better aligns with the rays of egocentric images, and yet enables factorization for performance enhancement. However, the naive spherical grid suffers from irregularities at two poles, and also cannot represent unbounded scenes. To avoid singularities near poles, we combine two balanced grids, which results in a quasi-uniform angular grid. We also partition the radial grid exponentially and place an environment map at infinity to represent unbounded scenes. Furthermore, with our resampling technique for grid-based methods, we can increase the number of valid samples to train NeRF volume. We extensively evaluate our method in our newly introduced synthetic and real-world egocentric 360 video datasets, and it consistently achieves state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">257.Unsupervised 3D Shape Reconstruction by Part Retrieval and Assembly</span><br>
                <span class="as">Xu, XianghaoandGuerrero, PaulandFisher, MatthewandChaudhuri, SiddharthaandRitchie, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Unsupervised_3D_Shape_Reconstruction_by_Part_Retrieval_and_Assembly_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8559-8567.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地表示和分解3D形状。<br>
                    动机：现有的方法要么使用简单的参数化原始模型，要么学习部分的生成形状空间，两者都有局限性。<br>
                    方法：我们提出使用用户提供的3D部件库来分解形状，给予用户对部件选择的完全控制。这种方法通过从库中检索并优化部件的位置来实现。<br>
                    效果：该方法在重建精度和理想分解方面优于现有方法，并且可以通过使用不同的部件库来重构相同的形状来控制分解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Representing a 3D shape with a set of primitives can aid perception of structure, improve robotic object manipulation, and enable editing, stylization, and compression of 3D shapes. Existing methods either use simple parametric primitives or learn a generative shape space of parts. Both have limitations: parametric primitives lead to coarse approximations, while learned parts offer too little control over the decomposition. We instead propose to decompose shapes using a library of 3D parts provided by the user, giving full control over the choice of parts. The library can contain parts with high-quality geometry that are suitable for a given category, resulting in meaningful decom- positions with clean geometry. The type of decomposition can also be controlled through the choice of parts in the library. Our method works via a unsupervised approach that iteratively retrieves parts from the library and refines their placements. We show that this approach gives higher reconstruction accuracy and more desirable decompositions than existing approaches. Additionally, we show how the decom- position can be controlled through the part library by using different part libraries to reconstruct the same shapes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">258.Instant-NVR: Instant Neural Volumetric Rendering for Human-Object Interactions From Monocular RGBD Stream</span><br>
                <span class="as">Jiang, YuhengandYao, KaixinandSu, ZhuoandShen, ZhehaoandLuo, HaiminandXu, Lan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/595-605.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行单目RGBD相机的人体-物体交互的即时体积跟踪和渲染。<br>
                    动机：传统的非刚性跟踪与最近的即时辐射场技术相结合，可以解决复杂交互场景下的单目跟踪和渲染问题。<br>
                    方法：提出了一种名为Instant-NVR的神经方法，通过多线程跟踪-渲染机制将传统非刚性跟踪与最新的即时辐射场技术相结合。在跟踪前端，采用了一种稳健的人体-物体捕获方案以提供足够的运动先验知识。进一步引入了一种新的混合变形模块，用于处理互动场景的即时神经表示。同时，还提供了一种基于有效运动先验搜索的动态/静态辐射场的在线重建方案。此外，还引入了一种在线关键帧选择方案和一种注重渲染的优化策略，以显著提高在线新视角合成的外观细节。<br>
                    效果：大量的实验证明，该方法能够有效地实时生成人体-物体辐射场，特别是在复杂的人体-物体交互下，实现了实时照片级的新视角合成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Convenient 4D modeling of human-object interactions is essential for numerous applications. However, monocular tracking and rendering of complex interaction scenarios remain challenging. In this paper, we propose Instant-NVR, a neural approach for instant volumetric human-object tracking and rendering using a single RGBD camera. It bridges traditional non-rigid tracking with recent instant radiance field techniques via a multi-thread tracking-rendering mechanism. In the tracking front-end, we adopt a robust human-object capture scheme to provide sufficient motion priors. We further introduce a separated instant neural representation with a novel hybrid deformation module for the interacting scene. We also provide an on-the-fly reconstruction scheme of the dynamic/static radiance fields via efficient motion-prior searching. Moreover, we introduce an online key frame selection scheme and a rendering-aware refinement strategy to significantly improve the appearance details for online novel-view synthesis. Extensive experiments demonstrate the effectiveness and efficiency of our approach for the instant generation of human-object radiance fields on the fly, notably achieving real-time photo-realistic novel view synthesis under complex human-object interactions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">259.DINER: Depth-Aware Image-Based NEural Radiance Fields</span><br>
                <span class="as">Prinzler, MalteandHilliges, OtmarandThies, Justus</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Prinzler_DINER_Depth-Aware_Image-Based_NEural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12449-12459.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用稀疏的RGB输入视图预测深度和特征图，以重建体积场景表示并渲染新视角下的3D对象。<br>
                    动机：目前的技术水平在处理具有较大视差的输入视图时，合成质量较低且需要改变捕获硬件要求。<br>
                    方法：提出一种深度感知的基于图像的神经辐射场（DINER）模型，通过将深度信息融入特征融合和有效场景采样中，来预测深度和特征图。<br>
                    效果：与现有技术相比，DINER模型在合成新视角、人头和一般物体等方面，显著提高了合成质量和感知度量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Depth-aware Image-based NEural Radiance fields (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to render 3D objects under novel views. Specifically, we propose novel techniques to incorporate depth information into feature fusion and efficient scene sampling. In comparison to the previous state of the art, DINER achieves higher synthesis quality and can process input views with greater disparity. This allows us to capture scenes more completely without changing capturing hardware requirements and ultimately enables larger viewpoint changes during novel view synthesis. We evaluate our method by synthesizing novel views, both for human heads and for general objects, and observe significantly improved qualitative results and increased perceptual metrics compared to the previous state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">260.AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction</span><br>
                <span class="as">Chatziagapi, AggelinaandSamaras, Dimitris</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chatziagapi_AVFace_Towards_Detailed_Audio-Visual_4D_Face_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16878-16889.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决从单目视频中重建4D面部的问题。<br>
                    动机：由于深度的模糊性，从2D图像进行3D面部重建是一个欠约束的问题。最先进的方法试图通过利用单个图像或视频的视觉信息来解决此问题，而3D网格动画方法则更多地依赖于音频。然而，在大多数情况下（例如AR/VR应用程序），视频同时包含视觉和语音信息。我们提出AVFace，该方法结合了这两种模态，能够准确重建任何说话者的4D面部和唇部运动，无需任何3D真实数据进行训练。<br>
                    方法：首先进行粗略阶段，估计3D变形模型的每帧参数，然后进行唇部细化，最后进行精细阶段以恢复面部几何细节。由于由基于变压器的模块捕获的音频和视频信息具有时间性，因此我们的方法在任一模态不足的情况下（例如面部遮挡）都具有很强的鲁棒性。<br>
                    效果：大量的定性和定量评估表明，我们的方法优于当前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we present a multimodal solution to the problem of 4D face reconstruction from monocular videos. 3D face reconstruction from 2D images is an under-constrained problem due to the ambiguity of depth. State-of-the-art methods try to solve this problem by leveraging visual information from a single image or video, whereas 3D mesh animation approaches rely more on audio. However, in most cases (e.g. AR/VR applications), videos include both visual and speech information. We propose AVFace that incorporates both modalities and accurately reconstructs the 4D facial and lip motion of any speaker, without requiring any 3D ground truth for training. A coarse stage estimates the per-frame parameters of a 3D morphable model, followed by a lip refinement, and then a fine stage recovers facial geometric details. Due to the temporal audio and video information captured by transformer-based modules, our method is robust in cases when either modality is insufficient (e.g. face occlusions). Extensive qualitative and quantitative evaluation demonstrates the superiority of our method over the current state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">261.A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation</span><br>
                <span class="as">Qu, HaoxuanandCai, YujunandFoo, LinGengandKumar, AjayandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13009-13018.png><br>
            
            <span class="tt"><span class="t0">研究问题：在人体姿态估计任务中，如何优化热力图预测以更准确地定位不同子区域的身体关节。<br>
                    动机：现有的方法使用整体L2损失来优化热力图预测，但在包含多个身体关节的自底向上人体姿态估计中，这种方法可能不是最佳选择。<br>
                    方法：提出一种新的自底向上人体姿态估计方法，通过最小化从预测热力图和真实热力图构造的两个特征函数之间的距离来优化热力图预测。<br>
                    效果：实验表明，该方法在COCO数据集和CrowdPose数据集上均取得了良好的效果，能更准确地定位预测热力图中不同子区域的身体关节。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most recent methods formulate the task of human pose estimation as a heatmap estimation problem, and use the overall L2 loss computed from the entire heatmap to optimize the heatmap prediction. In this paper, we show that in bottom-up human pose estimation where each heatmap often contains multiple body joints, using the overall L2 loss to optimize the heatmap prediction may not be the optimal choice. This is because, minimizing the overall L2 loss cannot always lead the model to locate all the body joints across different sub-regions of the heatmap more accurately. To cope with this problem, from a novel perspective, we propose a new bottom-up human pose estimation method that optimizes the heatmap prediction via minimizing the distance between two characteristic functions respectively constructed from the predicted heatmap and the groundtruth heatmap. Our analysis presented in this paper indicates that the distance between these two characteristic functions is essentially the upper bound of the L2 losses w.r.t. sub-regions of the predicted heatmap. Therefore, via minimizing the distance between the two characteristic functions, we can optimize the model to provide a more accurate localization result for the body joints in different sub-regions of the predicted heatmap. We show the effectiveness of our proposed method through extensive experiments on the COCO dataset and the CrowdPose dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">262.RefSR-NeRF: Towards High Fidelity and Super Resolution View Synthesis</span><br>
                <span class="as">Huang, XudongandLi, WeiandHu, JieandChen, HantingandWang, Yunhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_RefSR-NeRF_Towards_High_Fidelity_and_Super_Resolution_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8244-8253.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决NeRF在高分辨率渲染时存在的模糊问题，以及如何结合高分辨率参考图像进行超分辨率重建。<br>
                    动机：尽管NeRF在神经渲染领域取得了非凡的成功，但其内在的多层感知机在高分辨率渲染时难以学习高频细节，且随着分辨率的增加，计算量会爆炸性增长。<br>
                    方法：提出一种端到端的RefSR-NeRF框架，首先学习低分辨率的NeRF表示，然后借助高分辨率参考图像重构高频细节。为了解决直接引入文献中的预训练模型会产生不满足的伪影的问题，设计了一种新颖的轻量级RefSR模型，从NeRF渲染中学习到目标HR的逆退化过程。<br>
                    效果：在多个基准测试上的大量实验表明，该方法在渲染质量、速度和内存使用方面表现出令人印象深刻的权衡，优于或与NeRF及其变体相当，同时速度提高了52倍，内存使用略有增加。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Reference-guided Super-Resolution Neural Radiance Field (RefSR-NeRF) that extends NeRF to super resolution and photorealistic novel view synthesis. Despite NeRF's extraordinary success in the neural rendering field, it suffers from blur in high resolution rendering because its inherent multilayer perceptron struggles to learn high frequency details and incurs a computational explosion as resolution increases. Therefore, we propose RefSR-NeRF, an end-to-end framework that first learns a low resolution NeRF representation, and then reconstructs the high frequency details with the help of a high resolution reference image. We observe that simply introducing the pre-trained models from the literature tends to produce unsatisfied artifacts due to the divergence in the degradation model. To this end, we design a novel lightweight RefSR model to learn the inverse degradation process from NeRF renderings to target HR ones. Extensive experiments on multiple benchmarks demonstrate that our method exhibits an impressive trade-off among rendering quality, speed, and memory usage, outperforming or on par with NeRF and its variants while being 52x speedup with minor extra memory usage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">263.Polarimetric iToF: Measuring High-Fidelity Depth Through Scattering Media</span><br>
                <span class="as">Jeon, DanielS.andMeuleman, Andr\&#x27;easandBaek, Seung-HwanandKim, MinH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Polarimetric_iToF_Measuring_High-Fidelity_Depth_Through_Scattering_Media_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12353-12362.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决间接飞行时间（iToF）成像在散射介质中受到多路径干扰（MPI）影响，导致深度精度严重下降的问题。<br>
                    动机：由于iToF成像经常在散射介质中受到MPI的影响，导致深度精度严重下降，例如在雾气中无法准确测量深度。<br>
                    方法：本文提出了一种极化iToF成像方法，通过散射介质可以稳健地捕获深度信息。通过对间接ToF成像原理和光的偏振性的观察，我们制定了一种新的计算模型，可以纠正MPI错误。<br>
                    效果：我们在一个使用定制的现成iToF相机的实验设置上验证了我们的方法。通过我们的散射模型和极化相位测量，我们的方法比基线方法有显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Indirect time-of-flight (iToF) imaging allows us to capture dense depth information at a low cost. However, iToF imaging often suffers from multipath interference (MPI) artifacts in the presence of scattering media, resulting in severe depth-accuracy degradation. For instance, iToF cameras cannot measure depth accurately through fog because ToF active illumination scatters back to the sensor before reaching the farther target surface. In this work, we propose a polarimetric iToF imaging method that can capture depth information robustly through scattering media. Our observations on the principle of indirect ToF imaging and polarization of light allow us to formulate a novel computational model of scattering-aware polarimetric phase measurements that enables us to correct MPI errors. We first devise a scattering-aware polarimetric iToF model that can estimate the phase of unpolarized backscattered light. We then combine the optical filtering of polarization and our computational modeling of unpolarized backscattered light via scattering analysis of phase and amplitude. This allows us to tackle the MPI problem by estimating the scattering energy through the participating media. We validate our method on an experimental setup using a customized off-the-shelf iToF camera. Our method outperforms baseline methods by a significant margin by means of our scattering model and polarimetric phase measurements.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">264.Self-Supervised Super-Plane for Neural 3D Reconstruction</span><br>
                <span class="as">Ye, BotaoandLiu, SifeiandLi, XuetingandYang, Ming-Hsuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Self-Supervised_Super-Plane_for_Neural_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21415-21424.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经隐式表面表示方法在处理室内场景中广泛存在的无纹理平面区域时表现不佳。<br>
                    动机：为了解决这个问题，本文提出了一种自我监督的超级平面约束方法，通过探索预测表面的自由几何线索来进一步规范平面区域的重建，而无需任何其他地面真值标注。<br>
                    方法：我们引入了一种迭代训练方案，包括（i）将像素分组以形成一个超级平面（类似于超像素），以及（ii）通过超级平面约束优化场景重建网络。<br>
                    效果：实验表明，使用超级平面训练的模型性能优于使用传统标注平面的模型，因为单个超级平面在统计上占据更大的面积，导致更稳定的训练。大量的实验还表明，我们的自我监督超级平面约束显著提高了3D重建质量，甚至优于使用地面真值平面分割的效果。此外，我们的模型生成的平面重建结果可以用于其他视觉任务的自动标注。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural implicit surface representation methods show impressive reconstruction results but struggle to handle texture-less planar regions that widely exist in indoor scenes. Existing approaches addressing this leverage image prior that requires assistive networks trained with large-scale annotated datasets. In this work, we introduce a self-supervised super-plane constraint by exploring the free geometry cues from the predicted surface, which can further regularize the reconstruction of plane regions without any other ground truth annotations. Specifically, we introduce an iterative training scheme, where (i) grouping of pixels to formulate a super-plane (analogous to super-pixels), and (ii) optimizing of the scene reconstruction network via a super-plane constraint, are progressively conducted. We demonstrate that the model trained with super-planes surprisingly outperforms the one using conventional annotated planes, as individual super-plane statistically occupies a larger area and leads to more stable training. Extensive experiments show that our self-supervised super-plane constraint significantly improves 3D reconstruction quality even better than using ground truth plane segmentation. Additionally, the plane reconstruction results from our model can be used for auto-labeling for other vision tasks. The code and models are available at https: //github.com/botaoye/S3PRecon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">265.GM-NeRF: Learning Generalizable Model-Based Neural Radiance Fields From Multi-View Images</span><br>
                <span class="as">Chen, JianchuanandYi, WentaoandMa, LiqianandJia, XuandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_GM-NeRF_Learning_Generalizable_Model-Based_Neural_Radiance_Fields_From_Multi-View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20648-20658.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从稀疏的多视角图像中为任意人体表演者合成高保真度的新视角图像。<br>
                    动机：由于人体姿态变化大且存在严重的自我遮挡，这是一个具有挑战性的任务。<br>
                    方法：提出了一种有效的、可泛化的框架——基于模型的神经辐射场（GM-NeRF），用于合成自由视点图像。具体来说，我们提出了一种几何引导的注意力机制，将来自多视角2D图像的外观代码注册到几何代理上，以缓解不准确几何先验和像素空间之间的错位。此外，我们还进行了神经渲染和部分梯度反向传播，以进行高效的感知监督并提高合成的感知质量。<br>
                    效果：在THuman2.0和Multi-garment等合成数据集以及Genebody和ZJUMocap等真实世界数据集上进行的实验表明，我们的方法在新颖视图合成和几何重建方面优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we focus on synthesizing high-fidelity novel view images for arbitrary human performers, given a set of sparse multi-view images. It is a challenging task due to the large variation among articulated body poses and heavy self-occlusions. To alleviate this, we introduce an effective generalizable framework Generalizable Model-based Neural Radiance Fields (GM-NeRF) to synthesize free-viewpoint images. Specifically, we propose a geometry-guided attention mechanism to register the appearance code from multi-view 2D images to a geometry proxy which can alleviate the misalignment between inaccurate geometry prior and pixel space. On top of that, we further conduct neural rendering and partial gradient backpropagation for efficient perceptual supervision and improvement of the perceptual quality of synthesis. To evaluate our method, we conduct experiments on synthesized datasets THuman2.0 and Multi-garment, and real-world datasets Genebody and ZJUMocap. The results demonstrate that our approach outperforms state-of-the-art methods in terms of novel view synthesis and geometric reconstruction.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">266.VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence Normalization</span><br>
                <span class="as">Zhu, BingfanandYang, YanchaoandWang, XulongandZheng, YouyiandGuibas, Leonidas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_VDN-NeRF_Resolving_Shape-Radiance_Ambiguity_via_View-Dependence_Normalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/35-45.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练神经辐射场（NeRFs）以在非朗伯表面和动态光照条件下获得更好的几何形状？<br>
                    动机：现有的方法在处理非朗伯表面和动态光照条件时，由于光线从不同角度照射到某一点时，其辐射量会有很大变化，导致几何形状表现不佳。<br>
                    方法：提出VDN-NeRF方法，通过提取已学习到的NeRFs中的不变信息进行归一化，消除了视图依赖性，然后联合训练用于视图合成的NeRFs以获得高质量的几何形状。<br>
                    效果：实验表明，尽管形状-辐射度歧义不可避免，但提出的归一化方法可以最小化其对几何形状的影响，从而有效地解释了视图依赖性的变化。该方法适用于各种基线，并在不改变体绘制流程的情况下显著提高了几何形状的质量，即使在移动光源下捕获数据也能取得良好效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for better geometry under non-Lambertian surface and dynamic lighting conditions that cause significant variation in the radiance of a point when viewed from different angles. Instead of explicitly modeling the underlying factors that result in the view-dependent phenomenon, which could be complex yet not inclusive, we develop a simple and effective technique that normalizes the view-dependence by distilling invariant information already encoded in the learned NeRFs. We then jointly train NeRFs for view synthesis with view-dependence normalization to attain quality geometry. Our experiments show that even though shape-radiance ambiguity is inevitable, the proposed normalization can minimize its effect on geometry, which essentially aligns the optimal capacity needed for explaining view-dependent variations. Our method applies to various baselines and significantly improves geometry without changing the volume rendering pipeline, even if the data is captured under a moving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">267.Perspective Fields for Single Image Camera Calibration</span><br>
                <span class="as">Jin, LinyiandZhang, JianmingandHold-Geoffroy, YannickandWang, OliverandBlackburn-Matzen, KevinandSticha, MatthewandFouhey, DavidF.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Perspective_Fields_for_Single_Image_Camera_Calibration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17307-17316.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过模型捕捉图像的局部视角特性，以进行更准确的相机标定。<br>
                    动机：传统的相机标定方法需要对相机模型做出许多假设，且对常见的图像编辑操作如裁剪、扭曲和旋转等不具有不变性或等变性。<br>
                    方法：提出使用透视场作为图像的视角特性表示，该表示包含每个像素关于相机视图的信息，并参数化为一个上向量和一个纬度值。训练神经网络预测透视场，并将预测的透视场转换为易于校准的参数。<br>
                    效果：在各种场景下，该方法比基于相机标定的方法更稳健，并在图像合成等应用中表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Geometric camera calibration is often required for applications that understand the perspective of the image. We propose perspective fields as a representation that models the local perspective properties of an image. Perspective Fields contain per-pixel information about the camera view, parameterized as an up vector and a latitude value. This representation has a number of advantages as it makes minimal assumptions about the camera model and is invariant or equivariant to common image editing operations like cropping, warping, and rotation. It is also more interpretable and aligned with human perception. We train a neural network to predict Perspective Fields and the predicted Perspective Fields can be converted to calibration parameters easily. We demonstrate the robustness of our approach under various scenarios compared with camera calibration-based methods and show example applications in image compositing. Project page: https://jinlinyi.github.io/PerspectiveFields/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">268.Iterative Geometry Encoding Volume for Stereo Matching</span><br>
                <span class="as">Xu, GangweiandWang, XianqiandDing, XiaohuanandYang, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Iterative_Geometry_Encoding_Volume_for_Stereo_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21919-21928.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的匹配任务中，全对关系缺乏非局部几何知识，在病态区域处理局部歧义性时存在困难。<br>
                    动机：为了解决这些问题，提出了一种新的深度网络架构——迭代几何编码体积（IGEV-Stereo），用于立体匹配。<br>
                    方法：通过构建一个组合的几何编码体积，将几何和上下文信息以及局部匹配细节进行编码，并迭代索引以更新视差图。同时利用GEV为ConvGRUs的迭代过程回归一个准确的起始点，以提高收敛速度。<br>
                    效果：实验结果表明，IGEV-Stereo在KITTI 2015和2012（反射）上在所有已发布的技术中排名第一，并且在前10名中速度最快。此外，IGEV-Stereo具有强大的跨数据集泛化能力和高效的推理效率。将其扩展到多视图立体（MVS），即IGEV-MVS，在DTU基准测试上取得了有竞争力的准确性。代码可在https://github.com/gangweiX/IGEV获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recurrent All-Pairs Field Transforms (RAFT) has shown great potentials in matching tasks. However, all-pairs correlations lack non-local geometry knowledge and have difficulties tackling local ambiguities in ill-posed regions. In this paper, we propose Iterative Geometry Encoding Volume (IGEV-Stereo), a new deep network architecture for stereo matching. The proposed IGEV-Stereo builds a combined geometry encoding volume that encodes geometry and context information as well as local matching details, and iteratively indexes it to update the disparity map. To speed up the convergence, we exploit GEV to regress an accurate starting point for ConvGRUs iterations. Our IGEV-Stereo ranks first on KITTI 2015 and 2012 (Reflective) among all published methods and is the fastest among the top 10 methods. In addition, IGEV-Stereo has strong cross-dataset generalization as well as high inference efficiency. We also extend our IGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which achieves competitive accuracy on DTU benchmark. Code is available at https://github.com/gangweiX/IGEV.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">269.Enhanced Stable View Synthesis</span><br>
                <span class="as">Jain, NishantandKumar, SuryanshandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_Enhanced_Stable_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13208-13217.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高自由移动相机拍摄的图像中新颖视图合成的效果。<br>
                    动机：现有的稳定视图合成（SVS）方法在户外场景中恢复准确的几何骨架和相机姿态时面临挑战，导致效果不佳。<br>
                    方法：提出一种从多视图几何基本原理出发的方法来增强新颖视图合成解决方案。通过利用MVS和单目深度的互补性，分别对近点和远点获得更好的场景深度。此外，该方法通过多次旋转平均图优化联合优化图像渲染的相机姿态。恢复的场景深度和相机姿态有助于整个场景的依赖视图的表面特征聚合。<br>
                    效果：在流行的基准数据集（如坦克和寺庙）上进行广泛评估，与现有技术相比，该方法在视图合成结果方面取得了显著改进。例如，在坦克和寺庙上，该方法显示出1.5 dB的PSNR改善。在其他基准数据集（如FVS、Mip-NeRF 360和DTU）上观察到类似的统计数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce an approach to enhance the novel view synthesis from images taken from a freely moving camera. The introduced approach focuses on outdoor scenes where recovering accurate geometric scaffold and camera pose is challenging, leading to inferior results using the state-of-the-art stable view synthesis (SVS) method. SVS and related methods fail for outdoor scenes primarily due to (i) over-relying on the multiview stereo (MVS) for geometric scaffold recovery and (ii) assuming COLMAP computed camera poses as the best possible estimates, despite it being well-studied that MVS 3D reconstruction accuracy is limited to scene disparity and camera-pose accuracy is sensitive to key-point correspondence selection. This work proposes a principled way to enhance novel view synthesis solutions drawing inspiration from the basics of multiple view geometry. By leveraging the complementary behavior of MVS and monocular depth, we arrive at a better scene depth per view for nearby and far points, respectively. Moreover, our approach jointly refines camera poses with image-based rendering via multiple rotation averaging graph optimization. The recovered scene depth and the camera-pose help better view-dependent on-surface feature aggregation of the entire scene. Extensive evaluation of our approach on the popular benchmark dataset, such as Tanks and Temples, shows substantial improvement in view synthesis results compared to the prior art. For instance, our method shows 1.5 dB of PSNR improvement on the Tank and Temples. Similar statistics are observed when tested on other benchmark datasets such as FVS, Mip-NeRF 360, and DTU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">270.Biomechanics-Guided Facial Action Unit Detection Through Force Modeling</span><br>
                <span class="as">Cui, ZijunandKuang, ChenyiandGao, TianandTalamadupula, KartikandJi, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Biomechanics-Guided_Facial_Action_Unit_Detection_Through_Force_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8694-8703.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的情感表达识别（AU）检测算法主要基于从二维图像中提取的外观信息，而很少考虑支配三维面部皮肤变形的成熟面部生物力学。<br>
                    动机：提出一种生物力学引导的AU检测方法，模拟面部肌肉激活力，用于预测AU激活。<br>
                    方法：模型由两个分支组成：3D物理分支和2D图像分支。在3D物理分支中，首先推导支配面部变形的欧拉-拉格朗日方程，并将其表示为常微分方程（ODE），嵌入可微分ODE求解器中。首先回归肌肉激活力和其他物理参数，然后通过解ODE来模拟3D变形。2D图像分支利用额外的二维图像外观信息补偿3D物理分支。使用估计的力量和外观特征进行AU检测。<br>
                    效果：该方法在两个基准数据集上实现了有竞争力的AU检测性能。此外，通过利用生物力学，该方法在减少训练数据的情况下取得了出色的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing AU detection algorithms are mainly based on appearance information extracted from 2D images, and well-established facial biomechanics that governs 3D facial skin deformation is rarely considered. In this paper, we propose a biomechanics-guided AU detection approach, where facial muscle activation forces are modelled, and are employed to predict AU activation. Specifically, our model consists of two branches: 3D physics branch and 2D image branch. In 3D physics branch, we first derive the Euler-Lagrange equation governing facial deformation. The Euler-Lagrange equation represented as an ordinary differential equation (ODE) is embedded into a differentiable ODE solver. Muscle activation forces together with other physics parameters are firstly regressed, and then are utilized to simulate 3D deformation by solving the ODE. By leveraging facial biomechanics, we obtain physically plausible facial muscle activation forces. 2D image branch compensates 3D physics branch by employing additional appearance information from 2D images. Both estimated forces and appearance features are employed for AU detection. The proposed approach achieves competitive AU detection performance on two benchmark datasets. Furthermore, by leveraging biomechanics, our approach achieves outstanding performance with reduced training data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">271.Clothed Human Performance Capture With a Double-Layer Neural Radiance Fields</span><br>
                <span class="as">Wang, KangkanandZhang, GuofengandCong, SuxuandYang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21098-21107.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从稀疏视图或单目视频中捕捉穿衣人的表演。<br>
                    动机：现有的方法要么使用个性化模板捕捉全身的表演，要么从静态人体姿势的单帧中恢复服装，但这些方法在提取服装语义和捕捉服装运动方面存在不便，而基于单帧的方法可能会在跨视频中出现不稳定的跟踪。<br>
                    方法：提出一种新的方法，通过分别追踪服装和人体运动来捕捉人类表演，该方法使用双层神经辐射场（NeRFs）。具体来说，我们为身体和服装提出了一个双层NeRFs，并通过联合优化变形场和规范的双层NeRFs来追踪密集变形的服装和身体的模板。<br>
                    效果：与现有方法相比，我们的方法是完全可微分的，能够从动态视频中稳健地捕捉到身体和服装的运动。此外，我们的方法用独立的NeRFs表示服装，使我们能够合理地模拟一般衣服的隐含场。实验评估证实了它在真实多视角或单目视频上的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper addresses the challenge of capturing performance for the clothed humans from sparse-view or monocular videos. Previous methods capture the performance of full humans with a personalized template or recover the garments from a single frame with static human poses. However, it is inconvenient to extract cloth semantics and capture clothing motion with one-piece template, while single frame-based methods may suffer from instable tracking across videos. To address these problems, we propose a novel method for human performance capture by tracking clothing and human body motion separately with a double-layer neural radiance fields (NeRFs). Specifically, we propose a double-layer NeRFs for the body and garments, and track the densely deforming template of the clothing and body by jointly optimizing the deformation fields and the canonical double-layer NeRFs. In the optimization, we introduce a physics-aware cloth simulation network which can help generate physically plausible cloth dynamics and body-cloth interactions. Compared with existing methods, our method is fully differentiable and can capture both the body and clothing motion robustly from dynamic videos. Also, our method represents the clothing with an independent NeRFs, allowing us to model implicit fields of general clothes feasibly. The experimental evaluations validate its effectiveness on real multi-view or monocular videos.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">272.NeuFace: Realistic 3D Neural Face Rendering From Multi-View Images</span><br>
                <span class="as">Zheng, MingwuandZhang, HaiyuandYang, HongyuandHuang, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_NeuFace_Realistic_3D_Neural_Face_Rendering_From_Multi-View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16868-16877.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从多视角图像中恢复真实且高效的三维人脸表示。<br>
                    动机：由于面部复杂的空间变化反射特性和几何特性，当前的研究在恢复三维人脸表示上仍然具有挑战性。<br>
                    方法：本文提出了一种新的3D人脸渲染模型NeuFace，通过神经渲染技术学习准确且物理上有意义的底层3D表示。它自然地将神经BRDFs融入基于物理的渲染中，以协作的方式捕捉复杂的面部几何和外观线索。<br>
                    效果：大量实验证明，NeuFace在人脸渲染方面具有优越性，并对常见物体具有良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Realistic face rendering from multi-view images is beneficial to various computer vision and graphics applications. Due to the complex spatially-varying reflectance properties and geometry characteristics of faces, however, it remains challenging to recover 3D facial representations both faithfully and efficiently in the current studies. This paper presents a novel 3D face rendering model, namely NeuFace, to learn accurate and physically-meaningful underlying 3D representations by neural rendering techniques. It naturally incorporates the neural BRDFs into physically based rendering, capturing sophisticated facial geometry and appearance clues in a collaborative manner. Specifically, we introduce an approximated BRDF integration and a simple yet new low-rank prior, which effectively lower the ambiguities and boost the performance of the facial BRDFs. Extensive experiments demonstrate the superiority of NeuFace in human face rendering, along with a decent generalization ability to common objects. Code is released at https://github.com/aejion/NeuFace.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">273.Cross-Guided Optimization of Radiance Fields With Multi-View Image Super-Resolution for High-Resolution Novel View Synthesis</span><br>
                <span class="as">Yoon, YounghoandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yoon_Cross-Guided_Optimization_of_Radiance_Fields_With_Multi-View_Image_Super-Resolution_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12428-12438.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有知识图谱预训练语言模型无法充分利用结构化知识的问题，以及在研究问题：本文旨在解决现有知识图谱预训练语言模型无法充分利用结构化知识的问题，以及在高分辨率新视角合成（HRNVS）中，基于坐标的网络的光谱特性对光线场性能提升的限制。<br>
                    动机：现有的预训练语言模型和高分辨率新视角合成方法存在一些局限性，如缺乏对结构化知识的利用，以及对高分辨率图像的处理能力有限。<br>
                    方法：本文提出了一种联合优化单图像超分辨率（SISR）和光线场的新框架。在光线场优化过程中，对训练视图图像进行多视图图像超分辨率处理。通过融合从SISR获取的特征图和由训练视图图像集成误差生成的体素基不确定性场，得到更新的超分辨率结果。<br>
                    效果：实验结果表明，该方法在各种基准数据集上的HRNVS和MVSR性能显著优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel View Synthesis (NVS) aims at synthesizing an image from an arbitrary viewpoint using multi-view images and camera poses. Among the methods for NVS, Neural Radiance Fields (NeRF) is capable of NVS for an arbitrary resolution as it learns a continuous volumetric representation. However, radiance fields rely heavily on the spectral characteristics of coordinate-based networks. Thus, there is a limit to improving the performance of high-resolution novel view synthesis (HRNVS). To solve this problem, we propose a novel framework using cross-guided optimization of the single-image super-resolution (SISR) and radiance fields. We perform multi-view image super-resolution (MVSR) on train-view images during the radiance fields optimization process. It derives the updated SR result by fusing the feature map obtained from SISR and voxel-based uncertainty fields generated by integrated errors of train-view images. By repeating the updates during radiance fields optimization, train-view images for radiance fields optimization have multi-view consistency and high-frequency details simultaneously, ultimately improving the performance of HRNVS. Experiments of HRNVS and MVSR on various benchmark datasets show that the proposed method significantly surpasses existing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">274.SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object Pose Estimation</span><br>
                <span class="as">Tan, TaoandDong, Qiulei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_SMOC-Net_Leveraging_Camera_Pose_for_Self-Supervised_Monocular_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21307-21316.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用未标注的真实图像进行自监督的单目物体姿态估计，并解决训练阶段由于渲染图像与真实图像的差距以及计算成本高的问题。<br>
                    动机：现有的方法在训练阶段使用耗时的可微渲染器进行物体姿态预测，导致其在真实图像上的性能受限，且训练过程计算成本高。<br>
                    方法：提出一种名为SMOC-Net的新型网络，通过利用未标注的真实图像中预测的相机姿态进行自监督的单目物体姿态估计。该网络在知识蒸馏框架下进行探索，包括一个教师模型和一个学生模型。教师模型包含一个用于初始物体姿态估计的主干估计模块和一个用于使用从相对相机姿态派生的几何约束（称为相对姿态约束）细化初始物体姿态的对象姿态细化器。学生模型通过施加相对姿态约束从教师模型获取物体姿态估计的知识。<br>
                    效果：实验结果表明，SMOC-Net在两个公共数据集上都优于几种最先进的方法，同时比基于可微渲染器的方法需要更少的训练时间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, self-supervised 6D object pose estimation, where synthetic images with object poses (sometimes jointly with un-annotated real images) are used for training, has attracted much attention in computer vision. Some typical works in literature employ a time-consuming differentiable renderer for object pose prediction at the training stage, so that (i) their performances on real images are generally limited due to the gap between their rendered images and real images and (ii) their training process is computationally expensive. To address the two problems, we propose a novel Network for Self-supervised Monocular Object pose estimation by utilizing the predicted Camera poses from un-annotated real images, called SMOC-Net. The proposed network is explored under a knowledge distillation framework, consisting of a teacher model and a student model. The teacher model contains a backbone estimation module for initial object pose estimation, and an object pose refiner for refining the initial object poses using a geometric constraint (called relative-pose constraint) derived from relative camera poses. The student model gains knowledge for object pose estimation from the teacher model by imposing the relative-pose constraint. Thanks to the relative-pose constraint, SMOC-Net could not only narrow the domain gap between synthetic and real data but also reduce the training cost. Experimental results on two public datasets demonstrate that SMOC-Net outperforms several state-of-the-art methods by a large margin while requiring much less training time than the differentiable-renderer-based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">275.Learning Human Mesh Recovery in 3D Scenes</span><br>
                <span class="as">Shen, ZehongandCen, ZhiandPeng, SidaandShuai, QingandBao, HujunandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Learning_Human_Mesh_Recovery_in_3D_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17038-17047.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在预扫描场景中，仅通过单张图像恢复人的绝对姿态和形状。<br>
                    动机：与以往执行场景感知网格优化的方法不同，我们提出首先使用稀疏的3D CNN估计绝对位置和密集的场景接触，然后通过交叉注意力增强预先训练的人网格恢复网络。<br>
                    方法：我们在图像和场景几何上进行联合学习，以减少深度和遮挡引起的模糊性，从而产生更合理的全局姿势和接触。在网络中编码场景感知线索还使得该方法无需优化，为实时应用开辟了机会。<br>
                    效果：实验表明，该方法能够通过一次前向传递恢复准确且物理上可信的网格，并在准确性和速度方面优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel method for recovering the absolute pose and shape of a human in a pre-scanned scene given a single image. Unlike previous methods that perform sceneaware mesh optimization, we propose to first estimate absolute position and dense scene contacts with a sparse 3D CNN, and later enhance a pretrained human mesh recovery network by cross-attention with the derived 3D scene cues. Joint learning on images and scene geometry enables our method to reduce the ambiguity caused by depth and occlusion, resulting in more reasonable global postures and contacts. Encoding scene-aware cues in the network also allows the proposed method to be optimization-free, and opens up the opportunity for real-time applications. The experiments show that the proposed network is capable of recovering accurate and physically-plausible meshes by a single forward pass and outperforms state-of-the-art methods in terms of both accuracy and speed. Code is available on our project page: https://zju3dv.github.io/sahmr/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">276.Learning Locally Editable Virtual Humans</span><br>
                <span class="as">Ho, Hsuan-IandXue, LixinandSong, JieandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ho_Learning_Locally_Editable_Virtual_Humans_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21024-21035.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的混合表示和可端到端训练的网络架构，以实现完全可编辑和可定制的神经化身。<br>
                    动机：现有的神经化身模型存在使用复杂、缺乏一致性等问题，因此需要一种结合了神经场的强大建模能力和基于蒙皮网格的易用性和固有3D一致性的新型表示方法。<br>
                    方法：构建了一个可训练的特征码本来存储可变形身体模型顶点的局部几何和纹理特征，并利用其关节下的一致拓扑结构。然后，将这种表示应用于生成自动解码器架构，使其能够适应未见过的扫描并生成具有不同外观和几何形状的现实化身。此外，我们的表示还允许通过在3D资产之间交换局部特征进行局部编辑。<br>
                    效果：通过定量和定性实验证明，我们的方法可以生成多样化的详细化身，并在模型拟合性能上优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent topology under articulation. This representation is then employed in a generative auto-decoder architecture that admits fitting to unseen scans and sampling of realistic avatars with varied appearances and geometries. Furthermore, our representation allows local editing by swapping local features between 3D assets. To verify our method for avatar creation and editing, we contribute a new high-quality dataset, dubbed CustomHumans, for training and evaluation. Our experiments quantitatively and qualitatively show that our method generates diverse detailed avatars and achieves better model fitting performance compared to state-of-the-art methods. Our code and dataset are available at https://ait.ethz.ch/custom-humans.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">277.Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural Real-Time SLAM</span><br>
                <span class="as">Wang, HengyiandWang, JingwenandAgapito, Lourdes</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13293-13302.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种基于混合表示的神经RGB-D SLAM系统，实现实时的鲁棒相机跟踪和高保真表面重建。<br>
                    动机：目前的神经网络SLAM系统在处理高频局部特征、保持表面连贯性和完成未观测区域方面存在挑战。<br>
                    方法：Co-SLAM将场景表示为多分辨率哈希网格，利用其高速收敛能力和高频率局部特征表示能力。同时，引入一blob编码以增强表面连贯性和完成未观测区域。此外，采用射线采样策略进行全局关键帧捆绑调整，无需像其他竞争性神经网络SLAM方法那样选择关键帧以维持少量活动关键帧。<br>
                    效果：实验结果表明，Co-SLAM运行频率为10-17Hz，并在各种数据集和基准测试（ScanNet、TUM、Replica、Synthetic RGBD）中实现了最先进的场景重建结果和具有竞争力的跟踪性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Co-SLAM, a neural RGB-D SLAM system based on a hybrid representation, that performs robust camera tracking and high-fidelity surface reconstruction in real time. Co-SLAM represents the scene as a multi-resolution hash-grid to exploit its high convergence speed and ability to represent high-frequency local features. In addition, Co-SLAM incorporates one-blob encoding, to encourage surface coherence and completion in unobserved areas. This joint parametric-coordinate encoding enables real-time and robust performance by bringing the best of both worlds: fast convergence and surface hole filling. Moreover, our ray sampling strategy allows Co-SLAM to perform global bundle adjustment over all keyframes instead of requiring keyframe selection to maintain a small number of active keyframes as competing neural SLAM approaches do. Experimental results show that Co-SLAM runs at 10-17Hz and achieves state-of-the-art scene reconstruction results, and competitive tracking performance in various datasets and benchmarks (ScanNet, TUM, Replica, Synthetic RGBD). Project page: https://hengyiwang.github.io/projects/CoSLAM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">278.Incremental 3D Semantic Scene Graph Prediction From RGB Sequences</span><br>
                <span class="as">Wu, Shun-ChengandTateno, KeisukeandNavab, NassirandTombari, Federico</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Incremental_3D_Semantic_Scene_Graph_Prediction_From_RGB_Sequences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5064-5074.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D semantic scene graphs are a powerful holistic representation as they describe the individual objects and depict the relation between them. They are compact high-level graphs that enable many tasks requiring scene reasoning. In real-world settings, existing 3D estimation methods produce robust predictions that mostly rely on dense inputs. In this work, we propose a real-time framework that incrementally builds a consistent 3D semantic scene graph of a scene given an RGB image sequence. Our method consists of a novel incremental entity estimation pipeline and a scene graph prediction network. The proposed pipeline simultaneously reconstructs a sparse point map and fuses entity estimation from the input images. The proposed network estimates 3D semantic scene graphs with iterative message passing using multi-view and geometric features extracted from the scene entities. Extensive experiments on the 3RScan dataset show the effectiveness of the proposed method in this challenging task, outperforming state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">279.TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation</span><br>
                <span class="as">Chen, HanzhiandManhardt, FabianandNavab, NassirandBusam, Benjamin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_TexPose_Neural_Texture_Learning_for_Self-Supervised_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4841-4852.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从合成数据和少量未标记的真实图像中进行6D物体姿态估计的神经纹理学习问题。<br>
                    动机：现有的方法存在对共模态或额外细化的强烈依赖性，这是为了提供收敛的训练信号。<br>
                    方法：提出了一种新的学习方案，将纹理学习和姿态学习分为两个子优化问题。分别从真实图像集合中预测物体的真实纹理，并从像素完美的合成数据中学习姿态估计。结合这两种能力，可以合成逼真的新视图，以准确几何形状监督姿态估计器。同时，提出基于surfel的对抗训练损失和来自合成数据的纹理正则化，以减轻在纹理学习阶段出现的姿态噪声和分割不完美问题。<br>
                    效果：实验结果表明，该方法在无需地面真值姿态注释的情况下显著优于最新的最先进技术，并在面对未见场景时表现出显著的泛化改进。即使使用性能较差的姿态估计器初始化，该方案也能显著提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we introduce neural texture learning for 6D object pose estimation from synthetic data and a few unlabelled real images. Our major contribution is a novel learning scheme which removes the drawbacks of previous works, namely the strong dependency on co-modalities or additional refinement. These have been previously necessary to provide training signals for convergence. We formulate such a scheme as two sub-optimisation problems on texture learning and pose learning. We separately learn to predict realistic texture of objects from real image collections and learn pose estimation from pixel-perfect synthetic data. Combining these two capabilities allows then to synthesise photorealistic novel views to supervise the pose estimator with accurate geometry. To alleviate pose noise and segmentation imperfection present during the texture learning phase, we propose a surfel-based adversarial training loss together with texture regularisation from synthetic data. We demonstrate that the proposed approach significantly outperforms the recent state-of-the-art methods without ground-truth pose annotations and demonstrates substantial generalisation improvements towards unseen scenes. Remarkably, our scheme improves the adopted pose estimators substantially even when initialised with much inferior performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">280.DynIBaR: Neural Dynamic Image-Based Rendering</span><br>
                <span class="as">Li, ZhengqiandWang, QianqianandCole, ForresterandTucker, RichardandSnavely, Noah</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4273-4284.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目视频中合成复杂动态场景的新视角。<br>
                    动机：现有的基于时变神经辐射场（即动态NeRF）的方法在这项任务上表现出色，但对于具有复杂物体运动和不受控制的摄像机轨迹的长视频，这些方法可能会产生模糊或不准确的渲染，限制了其在现实世界应用中的使用。<br>
                    方法：我们提出了一种新的方法，采用体积图像渲染框架，通过以场景运动感知的方式聚合场景中附近视图的特征来合成新的视角，而不是将整个动态场景编码到MLP的权重中。<br>
                    效果：我们的系统在模拟复杂场景和视点相关效应方面保持了现有方法的优势，同时也能够从具有复杂场景动态和不受约束的摄像机轨迹的长视频中合成逼真的新视角。我们在动态场景数据集上取得了显著优于现有方法的改进，并将该方法应用于具有挑战性的摄像机和物体运动的野外视频，在这些情况下，现有方法无法生成高质量的渲染。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of synthesizing novel views from a monocular video depicting a complex dynamic scene. State-of-the-art methods based on temporally varying Neural Radiance Fields (aka dynamic NeRFs) have shown impressive results on this task. However, for long videos with complex object motions and uncontrolled camera trajectories,these methods can produce blurry or inaccurate renderings, hampering their use in real-world applications. Instead of encoding the entire dynamic scene within the weights of MLPs, we present a new approach that addresses these limitations by adopting a volumetric image-based rendering framework that synthesizes new viewpoints by aggregating features from nearby views in a scene motion-aware manner.Our system retains the advantages of prior methods in its ability to model complex scenes and view-dependent effects,but also enables synthesizing photo-realistic novel views from long videos featuring complex scene dynamics with unconstrained camera trajectories. We demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets, and also apply our approach to in-the-wild videos with challenging camera and object motion, where prior methods fail to produce high-quality renderings</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">281.Efficient Second-Order Plane Adjustment</span><br>
                <span class="as">Zhou, Lipu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Efficient_Second-Order_Plane_Adjustment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13113-13121.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决三维重建中，如何优化平面和传感器姿态以最小化点到平面的距离。<br>
                    动机：在3D重建中，通常使用深度传感器如RGB-D相机和LiDARs，但需要估计最优的平面和传感器姿态以最小化点到平面的距离，这是视觉重建中的一个问题。<br>
                    方法：本文采用牛顿法来有效解决平面调整（PA）问题。具体来说，给定姿态后，最优平面有闭型解，因此可以从成本函数中消除平面，大大减少变量的数量。此外，由于最优平面是姿态的函数，这种方法实际上确保了在每次迭代时都能获得当前估计姿态的最优平面，有利于收敛。难点在于如何高效地计算海森矩阵和结果成本的梯度，本文提供了一种有效的解决方案。<br>
                    效果：实验结果表明，我们的算法优于最先进的算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Planes are generally used in 3D reconstruction for depth sensors, such as RGB-D cameras and LiDARs. This paper focuses on the problem of estimating the optimal planes and sensor poses to minimize the point-to-plane distance. The resulting least-squares problem is referred to as plane adjustment (PA) in the literature, which is the counterpart of bundle adjustment (BA) in visual reconstruction. Iterative methods are adopted to solve these least-squares problems. Typically, Newton's method is rarely used for a large-scale least-squares problem, due to the high computational complexity of the Hessian matrix. Instead, methods using an approximation of the Hessian matrix, such as the Levenberg-Marquardt (LM) method, are generally adopted. This paper adopts the Newton's method to efficiently solve the PA problem. Specifically, given poses, the optimal plane have a close-form solution. Thus we can eliminate planes from the cost function, which significantly reduces the number of variables. Furthermore, as the optimal planes are functions of poses, this method actually ensures that the optimal planes for the current estimated poses can be obtained at each iteration, which benefits the convergence. The difficulty lies in how to efficiently compute the Hessian matrix and the gradient of the resulting cost. This paper provides an efficient solution. Empirical evaluation shows that our algorithm outperforms the state-of-the-art algorithms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">282.Fresnel Microfacet BRDF: Unification of Polari-Radiometric Surface-Body Reflection</span><br>
                <span class="as">Ichikawa, TomokiandFukao, YoshikiandNobuhara, ShoheiandNishino, Ko</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ichikawa_Fresnel_Microfacet_BRDF_Unification_of_Polari-Radiometric_Surface-Body_Reflection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16489-16497.png><br>
            
            <span class="tt"><span class="t0">研究问题：计算机视觉应用中，现有的反射模型由于物理不兼容和适用性有限，无法准确表示反射辐射。<br>
                    动机：为了解决现有模型的问题，本文提出了一种新的反射模型——菲涅尔微面BRDF模型。<br>
                    方法：通过使用一组定向镜面微面来模拟表面微观几何的菲涅尔反射和透射，同时考虑了体反射和表面反射。<br>
                    效果：实验结果表明，该模型在准确性、表达能力、基于图像的估计和几何恢复等方面都表现出了有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Computer vision applications have heavily relied on the linear combination of Lambertian diffuse and microfacet specular reflection models for representing reflected radiance, which turns out to be physically incompatible and limited in applicability. In this paper, we derive a novel analytical reflectance model, which we refer to as Fresnel Microfacet BRDF model, that is physically accurate and generalizes to various real-world surfaces. Our key idea is to model the Fresnel reflection and transmission of the surface microgeometry with a collection of oriented mirror facets, both for body and surface reflections. We carefully derive the Fresnel reflection and transmission for each microfacet as well as the light transport between them in the subsurface. This physically-grounded modeling also allows us to express the polarimetric behavior of reflected light in addition to its radiometric behavior. That is, FMBRDF unifies not only body and surface reflections but also light reflection in radiometry and polarization and represents them in a single model. Experimental results demonstrate its effectiveness in accuracy, expressive power, image-based estimation, and geometry recovery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">283.DiffusioNeRF: Regularizing Neural Radiance Fields With Denoising Diffusion Models</span><br>
                <span class="as">Wynn, JamieandTurmukhambetov, Daniyar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wynn_DiffusioNeRF_Regularizing_Neural_Radiance_Fields_With_Denoising_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4180-4189.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的Neural Radiance Fields（NeRFs）在训练视图不足的情况下，其场景几何和颜色场受到严重限制，可能导致伪影。<br>
                    动机：为了解决这个问题，我们使用去噪扩散模型（DDM）来学习场景几何和颜色的先验知识。<br>
                    方法：我们在合成的Hypersim数据集上训练DDM，使其能够预测颜色和深度补丁的联合概率分布的对数梯度。然后，在NeRF训练过程中，渲染随机RGBD补丁，并将估计的对数似然梯度反向传播到颜色和密度字段。<br>
                    效果：实验结果表明，我们的学习先验在重建的几何形状和对新视图的泛化上都取得了改进。在最相关的LLFF数据集上以及DTU上的评估都显示出比其他NeRF方法更好的重建质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Under good conditions, Neural Radiance Fields (NeRFs) have shown impressive results on novel view synthesis tasks. NeRFs learn a scene's color and density fields by minimizing the photometric discrepancy between training views and differentiable renderings of the scene. Once trained from a sufficient set of views, NeRFs can generate novel views from arbitrary camera positions. However, the scene geometry and color fields are severely under-constrained, which can lead to artifacts, especially when trained with few input views. To alleviate this problem we learn a prior over scene geometry and color, using a denoising diffusion model (DDM). Our DDM is trained on RGBD patches of the synthetic Hypersim dataset and can be used to predict the gradient of the logarithm of a joint probability distribution of color and depth patches. We show that, these gradients of logarithms of RGBD patch priors serve to regularize geometry and color of a scene. During NeRF training, random RGBD patches are rendered and the estimated gradient of the log-likelihood is backpropagated to the color and density fields. Evaluations on LLFF, the most relevant dataset, show that our learned prior achieves improved quality in the reconstructed geometry and improved generalization to novel views. Evaluations on DTU show improved reconstruction quality among NeRF methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">284.Learning Neural Parametric Head Models</span><br>
                <span class="as">Giebenhain, SimonandKirschstein, TobiasandGeorgopoulos, MarkosandR\&quot;unz, MartinandAgapito, LourdesandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Giebenhain_Learning_Neural_Parametric_Head_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21003-21012.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种基于混合神经网络场的全新3D可变形人脸模型。<br>
                    动机：现有的模型无法在分离的潜在空间中区分身份和表情，我们的目标是通过新的模型解决这个问题。<br>
                    方法：我们的模型的核心是一种神经参数表示，将一个人的身份和表情分别表示在不同的潜在空间中。我们使用一个符号距离场（SDF）来捕捉一个人的身份，并使用神经形变场来模拟面部表情。此外，我们还引入了以面部锚点为中心的局部场的集合，以实现高保真的局部细节。<br>
                    效果：我们在一个新的扫描数据集上训练我们的模型，该数据集包含超过3700个头部扫描，来自203个不同的个体。我们的数据集在质量和几何完整性方面都大大超过了现有的数据集。实验结果表明，我们的方法在拟合误差和重建质量上都优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel 3D morphable model for complete human heads based on hybrid neural fields. At the core of our model lies a neural parametric representation that disentangles identity and expressions in disjoint latent spaces. To this end, we capture a person's identity in a canonical space as a signed distance field (SDF), and model facial expressions with a neural deformation field. In addition, our representation achieves high-fidelity local detail by introducing an ensemble of local fields centered around facial anchor points. To facilitate generalization, we train our model on a newly-captured dataset of over 3700 head scans from 203 different identities using a custom high-end 3D scanning setup. Our dataset significantly exceeds comparable existing datasets, both with respect to quality and completeness of geometry, averaging around 3.5M mesh faces per scan. Finally, we demonstrate that our approach outperforms state-of-the-art methods in terms of fitting error and reconstruction quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">285.Removing Objects From Neural Radiance Fields</span><br>
                <span class="as">Weder, SilvanandGarcia-Hernando, GuillermoandMonszpart, \&#x27;AronandPollefeys, MarcandBrostow, GabrielJ.andFirman, MichaelandVicente, Sara</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weder_Removing_Objects_From_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16528-16538.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从NeRF场景表示中移除个人或不雅观的对象。<br>
                    动机：在共享NeRF之前，可能需要删除其中包含的个人信息或不雅观的对象，但目前的编辑框架难以实现这一目标。<br>
                    方法：提出一种基于用户提供的遮罩的NeRF图像修复方法，该方法利用了二维图像修复的最新研究成果，并通过基于置信度的视角选择过程来确保修复后的NeRF在三维空间中的一致性。<br>
                    效果：通过提出一个新的、具有挑战性的数据集进行验证，结果表明该方法能有效生成多视角一致且合理的NeRF修复结果，优于其他竞争方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRFs) are emerging as a ubiquitous scene representation that allows for novel view synthesis. Increasingly, NeRFs will be shareable with other people. Before sharing a NeRF, though, it might be desirable to remove personal information or unsightly objects. Such removal is not easily achieved with the current NeRF editing frameworks. We propose a framework to remove objects from a NeRF representation created from an RGB-D sequence. Our NeRF inpainting method leverages recent work in 2D image inpainting and is guided by a user-provided mask. Our algorithm is underpinned by a confidence based view selection procedure. It chooses which of the individual 2D inpainted images to use in the creation of the NeRF, so that the resulting inpainted NeRF is 3D consistent. We show that our method for NeRF editing is effective for synthesizing plausible inpaintings in a multi-view coherent manner, outperforming competing methods. We validate our approach by proposing a new and still-challenging dataset for the task of NeRF inpainting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">286.Structural Multiplane Image: Bridging Neural View Synthesis and 3D Reconstruction</span><br>
                <span class="as">Zhang, MingfangandWang, JingluandLi, XiaoandHuang, YifeiandSato, YoichiandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Structural_Multiplane_Image_Bridging_Neural_View_Synthesis_and_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16707-16716.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从稀疏输入中进行视图合成，特别是在斜角表面成像时的性能限制。<br>
                    动机：现有的多平面图像（MPI）虽然是一种有效且高效的表示方法，但其固定结构限制了性能，特别是对于倾斜角度的表面成像。<br>
                    方法：提出了一种结构性MPI（S-MPI），其平面结构简洁地近似3D场景。通过几何上忠实的结构传递RGBA上下文，S-MPI直接连接了视图合成和3D重建。<br>
                    效果：尽管应用S-MPI具有直观性和需求，但引入了巨大的挑战，如高保真度的RGBA层和平面位姿近似、多视图一致性、非平面区域建模以及带有相交平面的高效渲染等。因此，我们提出了一种基于分割模型的转换器网络，能够预测紧凑且表达性强的S-MPI层及其相应的掩码、位姿和RGBA上下文。我们的统一框架将非平面区域作为特殊情况进行处理，并通过共享全局代理嵌入来确保多视图一致性。大量实验表明，我们的方法优于先前最先进的基于MPI的视图合成方法和平面重建方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The Multiplane Image (MPI), containing a set of fronto-parallel RGBA layers, is an effective and efficient representation for view synthesis from sparse inputs. Yet, its fixed structure limits the performance, especially for surfaces imaged at oblique angles. We introduce the Structural MPI (S-MPI), where the plane structure approximates 3D scenes concisely. Conveying RGBA contexts with geometrically-faithful structures, the S-MPI directly bridges view synthesis and 3D reconstruction. It can not only overcome the critical limitations of MPI, i.e., discretization artifacts from sloped surfaces and abuse of redundant layers, and can also acquire planar 3D reconstruction. Despite the intuition and demand of applying S-MPI, great challenges are introduced, e.g., high-fidelity approximation for both RGBA layers and plane poses, multi-view consistency, non-planar regions modeling, and efficient rendering with intersected planes. Accordingly, we propose a transformer-based network based on a segmentation model. It predicts compact and expressive S-MPI layers with their corresponding masks, poses, and RGBA contexts. Non-planar regions are inclusively handled as a special case in our unified framework. Multi-view consistency is ensured by sharing global proxy embeddings, which encode plane-level features covering the complete 3D scenes with aligned coordinates. Intensive experiments show that our method outperforms both previous state-of-the-art MPI-based view synthesis methods and planar reconstruction methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">287.3D Human Pose Estimation via Intuitive Physics</span><br>
                <span class="as">Tripathi, ShashankandM\&quot;uller, LeaandHuang, Chun-HaoP.andTaheri, OmidandBlack, MichaelJ.andTzionas, Dimitrios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4713-4725.png><br>
            
            <span class="tt"><span class="t0">研究问题：从图像中估计3D人体时，常常会产生不合理的倾斜、浮动或穿透地板的身体。<br>
                    动机：目前的方法忽视了身体通常由场景支撑的事实。虽然可以使用物理引擎来强制实现物理合理性，但这些方法不可微分，依赖于不切实际的代理身体，并且难以集成到现有的优化和学习框架中。<br>
                    方法：我们利用可以从与场景交互的3D SMPL身体中推断出的直观物理（IP）术语。受生物力学启发，我们推断出身体上的压力热图、压力中心（CoP）以及SMPL身体的质心（CoM）。通过这些，我们开发了IPMAN，通过鼓励合理的地面接触和重叠的CoP和CoM，从彩色图像中估计一个“稳定”配置的3D身体。<br>
                    效果：我们在标准数据集和MoYo（一个新的具有同步多视图图像、复杂姿势的地面真值3D身体、身体-地面接触、CoM和压力的数据集）上评估了IPMAN。IPMAN产生了比现有技术更合理的结果，提高了静态姿势的准确性，同时不影响动态姿势。代码和数据可在https://ipman.is.tue.mpg.de/获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Estimating 3D humans from images often produces implausible bodies that lean, float, or penetrate the floor. Such methods ignore the fact that bodies are typically supported by the scene. A physics engine can be used to enforce physical plausibility, but these are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Inspired by biomechanics, we infer the pressure heatmap on the body, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With these, we develop IPMAN, to estimate a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, differentiable, and can be integrated into existing optimization and regression methods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with synchronized multi-view images, ground-truth 3D bodies with complex poses, body-floor contact, CoM and pressure. IPMAN produces more plausible results than the state of the art, improving accuracy for static poses, while not hurting dynamic ones. Code and data are available for research at https://ipman.is.tue.mpg.de/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">288.Learning To Predict Scene-Level Implicit 3D From Posed RGBD Data</span><br>
                <span class="as">Kulkarni, NileshandJin, LinyiandJohnson, JustinandFouhey, DavidF.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kulkarni_Learning_To_Predict_Scene-Level_Implicit_3D_From_Posed_RGBD_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17256-17265.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从已定位的RGBD数据中学习预测场景级的隐式函数用于3D重建？<br>
                    动机：现有的3D重建隐式函数通常与网格相关，但作者提出可以通过仅使用一组已定位的RGBD图像进行训练。<br>
                    方法：通过将先前未见过的RGB图像映射到场景的3D重建，实现了一种从已定位的RGBD数据中学习预测场景级隐式函数的方法。<br>
                    效果：该方法可以匹配并有时超越当前使用网格监督的方法，并在稀疏数据上表现出更好的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a method that can learn to predict scene-level implicit functions for 3D reconstruction from posed RGBD data. At test time, our system maps a previously unseen RGB image to a 3D reconstruction of a scene via implicit functions. While implicit functions for 3D reconstruction have often been tied to meshes, we show that we can train one using only a set of posed RGBD images. This setting may help 3D reconstruction unlock the sea of accelerometer+RGBD data that is coming with new phones. Our system, D2-DRDF, can match and sometimes outperform current methods that use mesh supervision and shows better robustness to sparse data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">289.Level-S\${\textasciicircum</span><br>
                <span class="as">Xiao, YuxiandXue, NanandWu, TianfuandXia, Gui-Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Level-S2fM_Structure_From_Motion_on_Neural_Level_Set_of_Implicit_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17205-17214.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从一组未校准的图像中估计相机姿态和场景几何。<br>
                    动机：现有的增量结构运动（SfM）方法在处理两视图和少视图配置时存在困难，优化体积神经渲染的坐标多层感知器（MLPs）具有挑战性。<br>
                    方法：提出一种名为Level-S2fM的新方法，通过学习已建立的关键点对应关系的隐式表面和辐射场的坐标MLPs来估计相机姿态和场景几何。<br>
                    效果：实验结果表明，Level-S2fM不仅在相机姿态估计和场景几何重建方面取得了良好的结果，而且为在不知道相机外参的情况下进行神经隐式渲染提供了一种有前景的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a neural incremental Structure-from-Motion (SfM) approach, Level-S2fM, which estimates the camera poses and scene geometry from a set of uncalibrated images by learning coordinate MLPs for the implicit surfaces and the radiance fields from the established keypoint correspondences. Our novel formulation poses some new challenges due to inevitable two-view and few-view configurations in the incremental SfM pipeline, which complicates the optimization of coordinate MLPs for volumetric neural rendering with unknown camera poses. Nevertheless, we demonstrate that the strong inductive basis conveying in the 2D correspondences is promising to tackle those challenges by exploiting the relationship between the ray sampling schemes. Based on this, we revisit the pipeline of incremental SfM and renew the key components, including two-view geometry initialization, the camera poses registration, the 3D points triangulation, and Bundle Adjustment, with a fresh perspective based on neural implicit surfaces. By unifying the scene geometry in small MLP networks through coordinate MLPs, our Level-S2fM treats the zero-level set of the implicit surface as an informative top-down regularization to manage the reconstructed 3D points, reject the outliers in correspondences via querying SDF, and refine the estimated geometries by NBA (Neural BA). Not only does our Level-S2fM lead to promising results on camera pose estimation and scene geometry reconstruction, but it also shows a promising way for neural implicit rendering without knowing camera extrinsic beforehand.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">290.MEGANE: Morphable Eyeglass and Avatar Network</span><br>
                <span class="as">Li, JunxuanandSaito, ShunsukeandSimon, TomasandLombardi, StephenandLi, HongdongandSaragih, Jason</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MEGANE_Morphable_Eyeglass_and_Avatar_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12769-12779.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地捕捉眼镜与面部的几何和外观交互作用，以在虚拟人脸表示中准确建模眼镜？<br>
                    动机：目前的模型大多独立地对眼镜和面部进行建模，无法准确捕捉它们之间的物理交互。同时，现有的尝试将交互处理为2D图像合成问题，但存在视图和时间不一致的问题。<br>
                    方法：提出一种3D组合变形眼镜模型，结合表面几何和体积表示，有效支持眼镜拓扑的大变化。该模型自然保留了眼镜间的对应关系，简化了透镜插入和框架变形等明确的几何修改操作。此外，该模型在点光源和自然光照下可重光照，支持各种镜框材料的高保真渲染。<br>
                    效果：通过与最先进的方法进行比较，实验结果表明该方法显著提高了质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Eyeglasses play an important role in the perception of identity. Authentic virtual representations of faces can benefit greatly from their inclusion. However, modeling the geometric and appearance interactions of glasses and the face of virtual representations of humans is challenging. Glasses and faces affect each other's geometry at their contact points, and also induce appearance changes due to light transport. Most existing approaches do not capture these physical interactions since they model eyeglasses and faces independently. Others attempt to resolve interactions as a 2D image synthesis problem and suffer from view and temporal inconsistencies. In this work, we propose a 3D compositional morphable model of eyeglasses that accurately incorporates high-fidelity geometric and photometric interaction effects. To support the large variation in eyeglass topology efficiently, we employ a hybrid representation that combines surface geometry and a volumetric representation. Unlike volumetric approaches, our model naturally retains correspondences across glasses, and hence explicit modification of geometry, such as lens insertion and frame deformation, is greatly simplified. In addition, our model is relightable under point lights and natural illumination, supporting high-fidelity rendering of various frame materials, including translucent plastic and metal within a single morphable model. Importantly, our approach models global light transport effects, such as casting shadows between faces and glasses. Our morphable model for eyeglasses can also be fit to novel glasses via inverse rendering. We compare our approach to state-of-the-art methods and demonstrate significant quality improvements.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">291.Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud Normal Estimation</span><br>
                <span class="as">Du, HangandYan, XuejunandWang, JingjingandXie, DiandPu, Shiliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Rethinking_the_Approximation_Error_in_3D_Surface_Fitting_for_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9486-9495.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的点云法线估计方法主要通过局部拟合几何表面来计算法线，但这种方法忽视了拟合问题的近似误差，导致拟合表面的准确性较低。<br>
                    动机：为了解决这一问题，我们提出了一种新的方法，该方法通过对表面拟合问题的近似误差进行深入分析，并设计了两个基本的原则来提高法线估计的准确性。<br>
                    方法：我们的方法包括两个主要步骤。首先，我们应用Z方向变换旋转局部补丁以获得更低近似误差的更好表面拟合。其次，我们将法线估计的误差建模为一个可学习的项。这两个原则都通过深度神经网络实现，并与最先进的法线估计方法无缝集成。<br>
                    效果：大量的实验验证了我们的方法在点云法线估计上的优势，并在合成和真实世界的数据集上都推动了最先进性能的发展。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing approaches for point cloud normal estimation aim to locally fit a geometric surface and calculate the normal from the fitted surface. Recently, learning-based methods have adopted a routine of predicting point-wise weights to solve the weighted least-squares surface fitting problem. Despite achieving remarkable progress, these methods overlook the approximation error of the fitting problem, resulting in a less accurate fitted surface. In this paper, we first carry out in-depth analysis of the approximation error in the surface fitting problem. Then, in order to bridge the gap between estimated and precise surface normals, we present two basic design principles: 1) applies the Z-direction Transform to rotate local patches for a better surface fitting with a lower approximation error; 2) models the error of the normal estimation as a learnable term. We implement these two principles using deep neural networks, and integrate them with the state-of-the-art (SOTA) normal estimation methods in a plug-and-play manner. Extensive experiments verify our approaches bring benefits to point cloud normal estimation and push the frontier of state-of-the-art performance on both synthetic and real-world datasets. The code is available at https://github.com/hikvision-research/3DVision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">292.VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction</span><br>
                <span class="as">Ren, YufanandWang, FangjinhuaandZhang, TongandPollefeys, MarcandS\&quot;usstrunk, Sabine</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_VolRecon_Volume_Rendering_of_Signed_Ray_Distance_Functions_for_Generalizable_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16685-16695.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经隐式重建方法缺乏对新场景的泛化能力。<br>
                    动机：为了解决这一问题，研究者提出了一种新的具有Signed Ray Distance Function（SRDF）的通用隐式重建方法VolRecon。<br>
                    方法：VolRecon通过结合多视图特征的投影特征和从粗全局特征体积插值得到的体积特征来重建具有精细细节和少量噪声的场景。使用射线变换器计算采样点上的SRDF值，然后渲染颜色和深度。<br>
                    效果：在DTU数据集上，VolRecon在稀疏视图重建方面比SparseNeuS提高了约30%，并在全视图重建方面实现了与MVSNet相当的准确性。此外，该方法在大规模的ETH3D基准测试中表现出良好的泛化性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of the Neural Radiance Fields (NeRF) in novel view synthesis has inspired researchers to propose neural implicit scene reconstruction. However, most existing neural implicit reconstruction methods optimize per-scene parameters and therefore lack generalizability to new scenes. We introduce VolRecon, a novel generalizable implicit reconstruction method with Signed Ray Distance Function (SRDF). To reconstruct the scene with fine details and little noise, VolRecon combines projection features aggregated from multi-view features, and volume features interpolated from a coarse global feature volume. Using a ray transformer, we compute SRDF values of sampled points on a ray and then render color and depth. On DTU dataset, VolRecon outperforms SparseNeuS by about 30% in sparse view reconstruction and achieves comparable accuracy as MVSNet in full view reconstruction. Furthermore, our approach exhibits good generalization performance on the large-scale ETH3D benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">293.CutMIB: Boosting Light Field Super-Resolution via Multi-View Image Blending</span><br>
                <span class="as">Xiao, ZeyuandLiu, YutongandGao, RuishengandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_CutMIB_Boosting_Light_Field_Super-Resolution_via_Multi-View_Image_Blending_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1672-1682.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络的性能？<br>
                    动机：现有的数据增强策略在单图像超分辨率上表现出了效用，但在需要利用多视图信息的光场超分辨率上的研究却很少。<br>
                    方法：首次在光场超分辨率中提出了一种有效的数据增强策略——CutMIB，该策略通过剪切低分辨率的补丁并混合生成混合补丁，然后将混合补丁粘贴到高分辨率光场视图的相应区域，从而提高现有光场超分辨率网络的性能。<br>
                    效果：实验结果表明，CutMIB可以改善现有光场超分辨率网络的重建性能和角度一致性，并在真实世界的光场超分辨率和光场去噪上验证了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Data augmentation (DA) is an efficient strategy for improving the performance of deep neural networks. Recent DA strategies have demonstrated utility in single image super-resolution (SR). Little research has, however, focused on the DA strategy for light field SR, in which multi-view information utilization is required. For the first time in light field SR, we propose a potent DA strategy called CutMIB to improve the performance of existing light field SR networks while keeping their structures unchanged. Specifically, CutMIB first cuts low-resolution (LR) patches from each view at the same location. Then CutMIB blends all LR patches to generate the blended patch and finally pastes the blended patch to the corresponding regions of high-resolution light field views, and vice versa. By doing so, CutMIB enables light field SR networks to learn from implicit geometric information during the training stage. Experimental results demonstrate that CutMIB can improve the reconstruction performance and the angular consistency of existing light field SR networks. We further verify the effectiveness of CutMIB on real-world light field SR and light field denoising. The implementation code is available at https://github.com/zeyuxiao1997/CutMIB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">294.Energy-Efficient Adaptive 3D Sensing</span><br>
                <span class="as">Tilmon, BrevinandSun, ZhanghaoandKoppal, SanjeevJ.andWu, YichengandEvangelidis, GeorgiosandZahreddine, RamziandKrishnan, GurunandanandMa, SizhuoandWang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tilmon_Energy-Efficient_Adaptive_3D_Sensing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5054-5063.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现深度感应的优化，同时解决其探测范围受限和人眼安全问题。<br>
                    动机：现有的主动深度感应虽然能实现稳健的深度估计，但通常受到感知范围的限制。简单地增加光学功率可以提高感知范围，但对包括自主机器人和增强现实在内的许多应用来说，可能会引发人眼安全的问题。<br>
                    方法：提出一种自适应主动深度传感器，该传感器联合优化了探测范围、功耗和人眼安全。主要观察发现，我们不需要将光模式投射到整个场景，而只需要投射到对应用来说深度必要的小区域，以及被动立体深度估计失败的地方。<br>
                    效果：实验结果表明，与其他感知策略（如全帧投影、线扫描和点扫描）相比，为实现相同的最大感知距离，该方法消耗的功率最少，且具有最短（最佳）的人眼安全距离。通过相位空间光调制器（SLM）和一个微机电系统（MEMS）镜和衍射光学元件（DOE）两个硬件原型实现了这种自适应感知方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Active depth sensing achieves robust depth estimation but is usually limited by the sensing range. Naively increasing the optical power can improve sensing range but induces eye-safety concerns for many applications, including autonomous robots and augmented reality. In this paper, we propose an adaptive active depth sensor that jointly optimizes range, power consumption, and eye-safety. The main observation is that we need not project light patterns to the entire scene but only to small regions of interest where depth is necessary for the application and passive stereo depth estimation fails. We theoretically compare this adaptive sensing scheme with other sensing strategies, such as full-frame projection, line scanning, and point scanning. We show that, to achieve the same maximum sensing distance, the proposed method consumes the least power while having the shortest (best) eye-safety distance. We implement this adaptive sensing scheme with two hardware prototypes, one with a phase-only spatial light modulator (SLM) and the other with a micro-electro-mechanical (MEMS) mirror and diffractive optical elements (DOE). Experimental results validate the advantage of our method and demonstrate its capability of acquiring higher quality geometry adaptively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">295.Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene Flow, Optical Flow and Stereo</span><br>
                <span class="as">Mehl, LukasandSchmalfuss, JennyandJahedi, AzinandNalivayko, YaroslavaandBruhn, Andr\&#x27;es</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4981-4991.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基准测试和评估方法无法充分反映高度详细的运动和立体估计结构。<br>
                    动机：为了解决这个问题，我们引入了Spring，这是一个大规模的、高分辨率的、高细节的计算机生成的场景流、光流和立体基准。<br>
                    方法：基于开源软件Blender电影"Spring"的渲染场景，我们提供了具有最先进的视觉特效和真实训练数据的逼真高清数据集。我们还提供了一个网站来上传、分析和比较结果。<br>
                    效果：我们的Spring基准可以评估精细结构的质量，并提供不同图像区域的详细性能统计数据。初步结果显示，估计精细细节确实具有挑战性，其准确性还有很大的提升空间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While recent methods for motion and stereo estimation recover an unprecedented amount of details, such highly detailed structures are neither adequately reflected in the data of existing benchmarks nor their evaluation methodology. Hence, we introduce Spring -- a large, high-resolution, high-detail, computer-generated benchmark for scene flow, optical flow, and stereo. Based on rendered scenes from the open-source Blender movie "Spring", it provides photo-realistic HD datasets with state-of-the-art visual effects and ground truth training data. Furthermore, we provide a website to upload, analyze and compare results. Using a novel evaluation methodology based on a super-resolved UHD ground truth, our Spring benchmark can assess the quality of fine structures and provides further detailed performance statistics on different image regions. Regarding the number of ground truth frames, Spring is 60x larger than the only scene flow benchmark, KITTI 2015, and 15x larger than the well-established MPI Sintel optical flow benchmark. Initial results for recent methods on our benchmark show that estimating fine details is indeed challenging, as their accuracy leaves significant room for improvement. The Spring benchmark and the corresponding datasets are available at http://spring-benchmark.org.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">296.BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</span><br>
                <span class="as">Black, MichaelJ.andPatel, PriyankaandTesch, JoachimandYang, Jinlong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8726-8737.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练仅在合成数据上表现出色的神经网络，实现从真实图像中估计3D人体姿态和形状（HPS）的问题。<br>
                    动机：现有的合成数据集规模小、不现实或缺乏真实的衣物，而达到足够的现实性是具有挑战性的。<br>
                    方法：创建了一个名为BEDLAM的数据集，包含单眼RGB视频和SMPL-X格式的真实3D身体。该数据集包括各种体型、运动、肤色、头发和服装，并使用商业服装物理模拟在移动的身体上逼真地模拟服装。<br>
                    效果：使用BEDLAM训练了各种HPS回归器，并在真实图像基准测试上实现了最先进的准确性，尽管使用的是合成数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We show, for the first time, that neural networks trained only on synthetic data achieve state-of-the-art accuracy on the problem of 3D human pose and shape (HPS) estimation from real images. Previous synthetic datasets have been small, unrealistic, or lacked realistic clothing. Achieving sufficient realism is non-trivial and we show how to do this for full bodies in motion. Specifically, our BEDLAM dataset contains monocular RGB videos with ground-truth 3D bodies in SMPL-X format. It includes a diversity of body shapes, motions, skin tones, hair, and clothing. The clothing is realistically simulated on the moving bodies using commercial clothing physics simulation. We render varying numbers of people in realistic scenes with varied lighting and camera motions. We then train various HPS regressors using BEDLAM and achieve state-of-the-art accuracy on real-image benchmarks despite training with synthetic data. We use BEDLAM to gain insights into what model design choices are important for accuracy. With good synthetic training data, we find that a basic method like HMR approaches the accuracy of the current SOTA method (CLIFF). BEDLAM is useful for a variety of tasks and all images, ground truth bodies, 3D clothing, support code, and more are available for research purposes. Additionally, we provide detailed information about our synthetic data generation pipeline, enabling others to generate their own datasets. See the project page: https://bedlam.is.tue.mpg.de/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">297.Accidental Light Probes</span><br>
                <span class="as">Yu, Hong-XingandAgarwala, SamirandHerrmann, CharlesandSzeliski, RichardandSnavely, NoahandWu, JiajunandSun, Deqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Accidental_Light_Probes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12521-12530.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图像中恢复场景的光照。<br>
                    动机：虽然镜子球光探头可以捕获全向照明，但在日常图像中通常无法获得光探头。<br>
                    方法：提出一种基于物理的方法来模拟偶然光探头（ALPs），并估计其外观中的光照。主要思想是通过摄影测量原理的着色来模拟ALPs的外观，并通过可微渲染进行逆向处理以恢复入射照明。<br>
                    效果：通过将ALP放入场景中，可以实现高保真度的光照估计。该方法还可以恢复包含ALP的现有图像的光照。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recovering lighting in a scene from a single image is a fundamental problem in computer vision. While a mirror ball light probe can capture omnidirectional lighting, light probes are generally unavailable in everyday images. In this work, we study recovering lighting from accidental light probes (ALPs)---common, shiny objects like Coke cans, which often accidentally appear in daily scenes. We propose a physically-based approach to model ALPs and estimate lighting from their appearances in single images. The main idea is to model the appearance of ALPs by photogrammetrically principled shading and to invert this process via differentiable rendering to recover incidental illumination. We demonstrate that we can put an ALP into a scene to allow high-fidelity lighting estimation. Our model can also recover lighting for existing images that happen to contain an ALP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">298.HexPlane: A Fast Representation for Dynamic Scenes</span><br>
                <span class="as">Cao, AngandJohnson, Justin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/130-141.png><br>
            
            <span class="tt"><span class="t0">研究问题：动态3D场景的建模和重渲染是3D视觉中的一个挑战。<br>
                    动机：现有的方法依赖于神经辐射场（NeRF）和隐式表示，这在实际应用中速度较慢，因为需要多次MLP评估。<br>
                    方法：我们提出了一种名为HexPlane的方法，通过学习的特征平面对空间时间中的点进行特征计算，实现了高效的动态3D场景建模。<br>
                    效果：实验结果表明，HexPlane在动态场景的新颖视图合成方面取得了令人印象深刻的结果，图像质量与现有工作相当，但训练时间减少了100倍以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modeling and re-rendering dynamic 3D scenes is a challenging task in 3D vision. Prior approaches build on NeRF and rely on implicit representations. This is slow since it requires many MLP evaluations, constraining real-world applications. We show that dynamic 3D scenes can be explicitly represented by six planes of learned features, leading to an elegant solution we call HexPlane. A HexPlane computes features for points in spacetime by fusing vectors extracted from each plane, which is highly efficient. Pairing a HexPlane with a tiny MLP to regress output colors and training via volume rendering gives impressive results for novel view synthesis on dynamic scenes, matching the image quality of prior work but reducing training time by more than 100x. Extensive ablations confirm our HexPlane design and show that it is robust to different feature fusion mechanisms, coordinate systems, and decoding mechanisms. HexPlane is a simple and effective solution for representing 4D volumes, and we hope they can broadly contribute to modeling spacetime for dynamic 3D scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">299.Novel-View Acoustic Synthesis</span><br>
                <span class="as">Chen, ChanganandRichard, AlexanderandShapovalov, RomanandIthapu, VamsiKrishnaandNeverova, NataliaandGrauman, KristenandVedaldi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Novel-View_Acoustic_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6409-6419.png><br>
            
            <span class="tt"><span class="t0">研究问题：我们提出了一种新的声音合成任务——新视角声学合成（NVAS），即给定研究问题：我们提出了一种新的声音合成任务——新视角声学合成（NVAS），即给定一个源视角的视觉和听觉观察，能否从未见过的目标视角合成该场景的声音？<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：我们提出了一种名为视觉引导声学合成（ViGAS）的神经网络渲染方法，通过分析输入的音视频线索，学习合成空间中任意点的 sound。<br>
                    效果：我们收集了两个首创的大型多视角音视频数据集进行基准测试，结果显示我们的模型能够成功推理空间线索并在两个数据集上合成忠实的声音。我们认为这项工作是解决新视角声学合成任务的第一个形式、数据集和方法，具有从AR/VR到艺术和设计的激动人心的潜在应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce the novel-view acoustic synthesis (NVAS) task: given the sight and sound observed at a source viewpoint, can we synthesize the sound of that scene from an unseen target viewpoint? We propose a neural rendering approach: Visually-Guided Acoustic Synthesis (ViGAS) network that learns to synthesize the sound of an arbitrary point in space by analyzing the input audio-visual cues. To benchmark this task, we collect two first-of-their-kind large-scale multi-view audio-visual datasets, one synthetic and one real. We show that our model successfully reasons about the spatial cues and synthesizes faithful audio on both datasets. To our knowledge, this work represents the very first formulation, dataset, and approach to solve the novel-view acoustic synthesis task, which has exciting potential applications ranging from AR/VR to art and design. Unlocked by this work, we believe that the future of novel-view synthesis is in multi-modal learning from videos.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">300.ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects</span><br>
                <span class="as">Toschi, MarcoandDeMatteo, RiccardoandSpezialetti, RiccardoandDeGregorio, DanieleandDiStefano, LuigiandSalti, Samuele</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Toschi_ReLight_My_NeRF_A_Dataset_for_Novel_View_Synthesis_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20762-20772.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在未被观察的光条件下，从神经辐射场（NeRF）中生成新的视角。<br>
                    动机：为了解决这一问题，我们引入了一个名为ReNe的新数据集，通过单光源照射条件来模拟真实世界的物体，并带有精确的相机和光源姿态的注释。<br>
                    方法：我们的采集管道利用两个机械臂分别持有相机和全向点光源。我们发布了总共20个场景，描绘了具有复杂几何形状和挑战性材料的多种物体。每个场景包括2000张图像，从50个不同的视角在40个不同的单光源照射条件下获取。<br>
                    效果：通过利用该数据集，我们对原始NeRF架构的各种变体进行了重光照能力的消融研究，并确定了一种轻量级架构，可以在新的光照条件下渲染物体的新视角，我们将其用于为该数据集建立非平凡的基线。数据集和基准可在https://eyecan-ai.github.io/rene 获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we focus on the problem of rendering novel views from a Neural Radiance Field (NeRF) under unobserved light conditions. To this end, we introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world objects under one-light-at-time (OLAT) conditions, annotated with accurate ground-truth camera and light poses. Our acquisition pipeline leverages two robotic arms holding, respectively, a camera and an omni-directional point-wise light source. We release a total of 20 scenes depicting a variety of objects with complex geometry and challenging materials. Each scene includes 2000 images, acquired from 50 different points of views under 40 different OLAT conditions. By leveraging the dataset, we perform an ablation study on the relighting capability of variants of the vanilla NeRF architecture and identify a lightweight architecture that can render novel views of an object under novel light conditions, which we use to establish a non-trivial baseline for the dataset. Dataset and benchmark are available at https://eyecan-ai.github.io/rene.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">301.ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</span><br>
                <span class="as">Fan, ZicongandTaheri, OmidandTzionas, DimitriosandKocabas, MuhammedandKaufmann, ManuelandBlack, MichaelJ.andHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12943-12954.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让机器理解物体的运动是由人类操作引起的，而非物体自身移动。<br>
                    动机：目前的机器对物体运动的理解尚不完善，缺乏具有精确3D标注的数据集来研究手和关节对象物理一致且同步的运动。<br>
                    方法：介绍了一个名为ARCTIC的数据集，其中包含210万个视频帧，配以精确的3D手部和物体网格以及详细的动态接触信息。同时提出了两个新的关节手-物体交互任务：一致运动重建和交互场估计。<br>
                    效果：通过在ARCTIC数据集上进行评估，证明了提出的方法可以有效地帮助机器理解物体的运动是由人类操作引起的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. Our code and data are available at https://arctic.is.tue.mpg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">302.Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision</span><br>
                <span class="as">Ding, FangqiangandPalffy, AndrasandGavrila, DariuM.andLu, ChrisXiaoxuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Hidden_Gems_4D_Radar_Scene_Flow_Learning_Using_Cross-Modal_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9340-9349.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过跨模态学习进行4D雷达场景流估计。<br>
                    动机：现代自动驾驶车辆的同位传感器冗余提供了各种形式的雷达场景流估计监督线索。<br>
                    方法：提出了一种多任务模型架构，用于识别的跨模态学习问题，并设计了损失函数，以利用多种跨模态约束进行有效的场景流估计模型训练。<br>
                    效果：大量实验表明，该方法在性能上达到了最先进的水平，证明了跨模态监督学习推断更准确的4D雷达场景流的有效性。同时，也展示了其在运动分割和自我运动估计两个子任务上的实用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">303.Omnimatte3D: Associating Objects and Their Effects in Unconstrained Monocular Video</span><br>
                <span class="as">Suhail, MohammedandLu, ErikaandLi, ZhengqiandSnavely, NoahandSigal, LeonidandCole, Forrester</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Suhail_Omnimatte3D_Associating_Objects_and_Their_Effects_in_Unconstrained_Monocular_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/630-639.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将视频分解为背景和一组前景层，以捕捉静态元素和移动对象及其相关效果。<br>
                    动机：目前的方法限制了摄像机运动的可能性，而我们的方法则利用单目摄像机姿态和深度估计的最新进展来创建完整的RGBD视频背景层和每个前景对象的视频层。<br>
                    方法：我们的方法将视频分解为背景和一组前景层，其中背景层捕获静态元素，前景层捕获移动对象及其相关效果。为了解决这个欠约束的分解问题，我们提出了一种新的基于多视图一致性的损失函数。<br>
                    效果：我们在具有复杂摄像机运动的具有挑战性的视频上测试了我们的方法，并显示出比当前方法显著的定性改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method to decompose a video into a background and a set of foreground layers, where the background captures stationary elements while the foreground layers capture moving objects along with their associated effects (e.g. shadows and reflections). Our approach is designed for unconstrained monocular videos, with arbitrary camera and object motion. Prior work that tackles this problem assumes that the video can be mapped onto a fixed 2D canvas, severely limiting the possible space of camera motion. Instead, our method applies recent progress in monocular camera pose and depth estimation to create a full, RGBD video layer for the background, along with a video layer for each foreground object. To solve the underconstrained decomposition problem, we propose a new loss formulation based on multi-view consistency. We test our method on challenging videos with complex camera motion and show significant qualitative improvement over current approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">304.High-Fidelity Clothed Avatar Reconstruction From a Single Image</span><br>
                <span class="as">Liao, TingtingandZhang, XiaomeiandXiu, YuliangandYi, HongweiandLiu, XudongandQi, Guo-JunandZhang, YongandWang, XuanandZhu, XiangyuandLei, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_High-Fidelity_Clothed_Avatar_Reconstruction_From_a_Single_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8662-8672.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种有效的三维穿衣虚拟人物重建框架。<br>
                    动机：结合优化方法和学习方法的优点，实现从单张图片中高精度的穿衣虚拟人物重建。<br>
                    方法：首先使用隐式模型在标准空间中以学习的方式获取人物的大致形状，然后通过估计姿态空间中的非刚性变形来优化表面细节。利用超网络生成良好的初始值，大大加速了优化过程的收敛。<br>
                    效果：实验证明，该方法能成功重建真实场景中任意穿着的人类高保真虚拟人物。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence of the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes. The codes will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">305.CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis</span><br>
                <span class="as">Zheng, JuntianandZheng, QingyuanandFang, LixingandLiu, YunandYi, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CAMS_CAnonicalized_Manipulation_Spaces_for_Category-Level_Functional_Hand-Object_Manipulation_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/585-594.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决一种新的类别级功能性手-物体操作合成任务，包括刚体和铰接物体类别。<br>
                    动机：给定一个物体几何形状、初始人手姿态以及稀疏的物体姿态控制序列，目标是生成一段像人类一样物理合理的手-物体操作序列。<br>
                    方法：首先设计了CAMS（Canonicalized Manipulation Spaces），这是一个两层的空间层次结构，以对象为中心和接触中心的观点对手中的位姿进行规范化。然后提出了一个两阶段框架来合成类人的操作动画。<br>
                    效果：该框架在刚体和铰接类别上都取得了最先进的性能，并产生了令人印象深刻的视觉效果。代码和视频结果可以在项目主页上找到：https://cams-hoi.github.io/。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we focus on a novel task of category-level functional hand-object manipulation synthesis covering both rigid and articulated object categories. Given an object geometry, an initial human hand pose as well as a sparse control sequence of object poses, our goal is to generate a physically reasonable hand-object manipulation sequence that performs like human beings. To address such a challenge, we first design CAnonicalized Manipulation Spaces (CAMS), a two-level space hierarchy that canonicalizes the hand poses in an object-centric and contact-centric view. Benefiting from the representation capability of CAMS, we then present a two-stage framework for synthesizing human-like manipulation animations. Our framework achieves state-of-the-art performance for both rigid and articulated categories with impressive visual effects. Codes and video results can be found at our project homepage: https://cams-hoi.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">306.Neural Lens Modeling</span><br>
                <span class="as">Xian, WenqiandBo\v{z</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xian_Neural_Lens_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8435-8445.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的3D重建和渲染方法越来越依赖整个图像形成过程的端到端优化，但这种方法受限于难以统一建模光学硬件堆栈和镜头的影响。<br>
                    动机：为了提高相机校准的质量以及3D重建结果的准确性，本文提出了一种神经透镜模型——NeuroLens，用于畸变和渐晕处理。<br>
                    方法：NeuroLens可以用于点投影和光线投射，并通过这两种操作进行优化。这意味着它可以（可选）使用传统的校准目标进行预捕获校准，也可以在3D重建过程中进行校准或优化，例如在优化辐射场时。<br>
                    效果：通过从Lensfun数据库中收集的大量镜头创建的综合数据集和其他真实世界数据集，我们证明了提出的透镜模型的性能优于标准包和最近的其他方法，同时更易于使用和扩展。该模型适用于多种镜头类型，并可轻松集成到现有的3D重建和渲染系统中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent methods for 3D reconstruction and rendering increasingly benefit from end-to-end optimization of the entire image formation process. However, this approach is currently limited: effects of the optical hardware stack and in particular lenses are hard to model in a unified way. This limits the quality that can be achieved for camera calibration and the fidelity of the results of 3D reconstruction. In this paper, we propose NeuroLens, a neural lens model for distortion and vignetting that can be used for point projection and ray casting and can be optimized through both operations. This means that it can (optionally) be used to perform pre-capture calibration using classical calibration targets, and can later be used to perform calibration or refinement during 3D reconstruction, e.g., while optimizing a radiance field. To evaluate the performance of our proposed model, we create a comprehensive dataset assembled from the Lensfun database with a multitude of lenses. Using this and other real-world datasets, we show that the quality of our proposed lens model outperforms standard packages as well as recent approaches while being much easier to use and extend. The model generalizes across many lens types and is trivial to integrate into existing 3D reconstruction and rendering systems. Visit our project website at: https://neural-lens.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">307.Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification</span><br>
                <span class="as">Feng, JiaweiandWu, AncongandZheng, Wei-Shi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Shape-Erased_Feature_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22752-22761.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于可见光和红外图像的模态差异以及高度视觉模糊性，学习多样化研究问题：由于可见光和红外图像的模态差异以及高度视觉模糊性，学习多样化的模态共享语义概念对于可见光-红外人体再识别（VI-ReID）仍然是一个挑战。<br>
                    动机：身体形状是VI-ReID的重要模态共享线索之一。为了挖掘更多多样化的模态共享线索，我们期望在已学习的特征中擦除与身体形状相关的语义概念，以迫使ReID模型提取更多其他模态共享特征进行识别。<br>
                    方法：我们提出了一种形状擦除特征学习范式，该范式将模态共享特征解耦到两个正交子空间中。在一个子空间中联合学习与形状相关的特征，在另一个正交补空间中学习形状擦除特征，从而实现形状擦除特征与身份之间的条件互信息最大化，从而显式增强学习的表示的多样性。<br>
                    效果：我们在SYSU-MM01、RegDB和HITSZ-VCM数据集上进行了大量实验，证明了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the modality gap between visible and infrared images with high visual ambiguity, learning diverse modality-shared semantic concepts for visible-infrared person re-identification (VI-ReID) remains a challenging problem. Body shape is one of the significant modality-shared cues for VI-ReID. To dig more diverse modality-shared cues, we expect that erasing body-shape-related semantic concepts in the learned features can force the ReID model to extract more and other modality-shared features for identification. To this end, we propose shape-erased feature learning paradigm that decorrelates modality-shared features in two orthogonal subspaces. Jointly learning shape-related feature in one subspace and shape-erased features in the orthogonal complement achieves a conditional mutual information maximization between shape-erased feature and identity discarding body shape information, thus enhancing the diversity of the learned representation explicitly. Extensive experiments on SYSU-MM01, RegDB, and HITSZ-VCM datasets demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">308.Neural Part Priors: Learning To Optimize Part-Based Object Completion in RGB-D Scans</span><br>
                <span class="as">Bokhovkin, AlekseiandDai, Angela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bokhovkin_Neural_Part_Priors_Learning_To_Optimize_Part-Based_Object_Completion_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9032-9042.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D场景理解中，独立的对象预测方法无法实现全局一致性的问题。<br>
                    动机：目前的3D场景理解主要关注独立的对象预测，缺乏全局一致性的考虑。<br>
                    方法：提出学习神经部分先验（NPPs）的方法，通过参数化对象及其部分的空间，优化以适应新的输入3D扫描几何形状，同时满足全局场景一致性约束。<br>
                    效果：实验结果表明，NPPs在ScanNet数据集上显著优于现有技术，实现了更准确的场景重建和对象补全。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D scene understanding has seen significant advances in recent years, but has largely focused on object understanding in 3D scenes with independent per-object predictions. We thus propose to learn Neural Part Priors (NPPs), parametric spaces of objects and their parts, that enable optimizing to fit to a new input 3D scan geometry with global scene consistency constraints. The rich structure of our NPPs enables accurate, holistic scene reconstruction across similar objects in the scene. Both objects and their part geometries are characterized by coordinate field MLPs, facilitating optimization at test time to fit to input geometric observations as well as similar objects in the input scan. This enables more accurate reconstructions than independent per-object predictions as a single forward pass, while establishing global consistency within a scene. Experiments on the ScanNet dataset demonstrate that NPPs significantly outperforms the state-of-the-art in part decomposition and object completion in real-world scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">309.NeuralUDF: Learning Unsigned Distance Fields for Multi-View Reconstruction of Surfaces With Arbitrary Topologies</span><br>
                <span class="as">Long, XiaoxiaoandLin, ChengandLiu, LingjieandLiu, YuanandWang, PengandTheobalt, ChristianandKomura, TakuandWang, Wenping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_NeuralUDF_Learning_Unsigned_Distance_Fields_for_Multi-View_Reconstruction_of_Surfaces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20834-20843.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从2D图像重建具有任意拓扑结构的表面？<br>
                    动机：现有的基于神经渲染的重建方法仅限于封闭表面，因为它们采用需要将目标形状分为内外部分的符号距离函数（SDF）作为表面表示。<br>
                    方法：提出将表面表示为无符号距离函数（UDF），并开发新的体积渲染方案来学习神经UDF表示。具体来说，引入了一种新的密度函数，该函数将UDF的属性与体积渲染方案相关联，以稳健优化UDF场。<br>
                    效果：在DTU和DeepFashion3D数据集上的实验表明，该方法不仅能够高质量重建具有复杂类型的非封闭形状，而且在封闭表面的重建上也能与基于SDF的方法相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel method, called NeuralUDF, for reconstructing surfaces with arbitrary topologies from 2D images via volume rendering. Recent advances in neural rendering based reconstruction have achieved compelling results. However, these methods are limited to objects with closed surfaces since they adopt Signed Distance Function (SDF) as surface representation which requires the target shape to be divided into inside and outside. In this paper, we propose to represent surfaces as the Unsigned Distance Function (UDF) and develop a new volume rendering scheme to learn the neural UDF representation. Specifically, a new density function that correlates the property of UDF with the volume rendering scheme is introduced for robust optimization of the UDF fields. Experiments on the DTU and DeepFashion3D datasets show that our method not only enables high-quality reconstruction of non-closed shapes with complex typologies, but also achieves comparable performance to the SDF based methods on the reconstruction of closed surfaces. Visit our project page at https://www.xxlong.site/NeuralUDF/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">310.Shape-Constraint Recurrent Flow for 6D Object Pose Estimation</span><br>
                <span class="as">Hai, YangandSong, RuiandLi, JiaojiaoandHu, Yinlin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Shape-Constraint_Recurrent_Flow_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4831-4840.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的6D物体姿态估计方法主要依赖2D光流网络来优化结果，但这些研究问题：现有的6D物体姿态估计方法主要依赖2D光流网络来优化结果，但这些方法在匹配过程中通常不考虑目标的3D形状信息，导致其在6D物体姿态估计上表现不佳。<br>
                    动机：为了解决这个问题，我们提出了一种形状约束循环流网络用于6D物体姿态估计，该方法将目标的3D形状信息嵌入到匹配过程中。<br>
                    方法：我们首先引入了一个从当前流估计中学习中间姿态的流到姿态组件，然后从当前姿态对4D关联体积的查找空间施加形状约束，大大减少了匹配空间，使其更易于学习。最后，我们在一个循环的方式下同时优化流和姿态，直到收敛。<br>
                    效果：我们在三个具有挑战性的6D物体姿态数据集上评估了我们的方法，结果显示它在准确性和效率上都优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most recent 6D object pose estimation methods rely on 2D optical flow networks to refine their results. However, these optical flow methods typically do not consider any 3D shape information of the targets during matching, making them suffer in 6D object pose estimation. In this work, we propose a shape-constraint recurrent flow network for 6D object pose estimation, which embeds the 3D shape information of the targets into the matching procedure. We first introduce a flow-to-pose component to learn an intermediate pose from the current flow estimation, then impose a shape constraint from the current pose on the lookup space of the 4D correlation volume for flow estimation, which reduces the matching space significantly and is much easier to learn. Finally, we optimize the flow and pose simultaneously in a recurrent manner until convergence. We evaluate our method on three challenging 6D object pose datasets and show that it outperforms the state of the art in both accuracy and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">311.High-Fidelity Event-Radiance Recovery via Transient Event Frequency</span><br>
                <span class="as">Han, JinandAsano, YutaandShi, BoxinandZheng, YinqiangandSato, Imari</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_High-Fidelity_Event-Radiance_Recovery_via_Transient_Event_Frequency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20616-20625.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过事件相机和生物启发的硅传感器恢复精确的辐射度值，以改善场景信息重建和理解。<br>
                    动机：传统的摄像机在动态范围、位深度和光谱响应等方面存在限制，而事件相机和生物启发的硅传感器对辐射度变化敏感，可以用于精确恢复辐射度值。<br>
                    方法：本文提出了一种创新的方法，将事件信号的高时间分辨率转换为精确的辐射度值。在有源照明条件下，触发线性的事件信号瞬时频率反映了辐射度值。<br>
                    效果：实验证明，仅通过瞬时事件频率（TEF）就可以恢复辐射度值，这种方法在图像分析方面具有多种能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-fidelity radiance recovery plays a crucial role in scene information reconstruction and understanding. Conventional cameras suffer from limited sensitivity in dynamic range, bit depth, and spectral response, etc. In this paper, we propose to use event cameras with bio-inspired silicon sensors, which are sensitive to radiance changes, to recover precise radiance values. We reveal that, under active lighting conditions, the transient frequency of event signals triggering linearly reflects the radiance value. We propose an innovative method to convert the high temporal resolution of event signals into precise radiance values. The precise radiance values yields several capabilities in image analysis. We demonstrate the feasibility of recovering radiance values solely from the transient event frequency (TEF) through multiple experiments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">312.NeMo: Learning 3D Neural Motion Fields From Multiple Video Instances of the Same Action</span><br>
                <span class="as">Wang, Kuan-ChiehandWeng, ZhenzhenandXenochristou, MariaandAra\&#x27;ujo, Jo\~aoPedroandGu, JeffreyandLiu, KarenandYeung, Serena</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_NeMo_Learning_3D_Neural_Motion_Fields_From_Multiple_Video_Instances_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22129-22138.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过利用同一动作的多个视频实例的信息，弥合单眼人体网格恢复（HMR）和多视图运动捕捉（MoCap）系统之间的差距。<br>
                    动机：现有的HMR方法在包含挑战性和动态运动的视频中的表现会下降，限制了其在3D运动恢复等应用中的吸引力。<br>
                    方法：引入神经运动（NeMo）领域，优化表示同一动作的一系列视频中的底层3D运动。<br>
                    效果：实验证明，NeMo可以在体育比赛中使用来自宾夕法尼亚行动数据集的视频恢复3D运动，并在2D关键点检测方面优于现有的HMR方法。进一步收集模仿宾夕法尼亚行动中的动作的小MoCap数据集，NeMo在各种基线上实现了更好的3D重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of reconstructing 3D human motion has wide-ranging applications. The gold standard Motion capture (MoCap) systems are accurate but inaccessible to the general public due to their cost, hardware, and space constraints. In contrast, monocular human mesh recovery (HMR) methods are much more accessible than MoCap as they take single-view videos as inputs. Replacing the multi-view MoCap systems with a monocular HMR method would break the current barriers to collecting accurate 3D motion thus making exciting applications like motion analysis and motion-driven animation accessible to the general public. However, the performance of existing HMR methods degrades when the video contains challenging and dynamic motion that is not in existing MoCap datasets used for training. This reduces its appeal as dynamic motion is frequently the target in 3D motion recovery in the aforementioned applications. Our study aims to bridge the gap between monocular HMR and multi-view MoCap systems by leveraging information shared across multiple video instances of the same action. We introduce the Neural Motion (NeMo) field. It is optimized to represent the underlying 3D motions across a set of videos of the same action. Empirically, we show that NeMo can recover 3D motion in sports using videos from the Penn Action dataset, where NeMo outperforms existing HMR methods in terms of 2D keypoint detection. To further validate NeMo using 3D metrics, we collected a small MoCap dataset mimicking actions in Penn Action, and show that NeMo achieves better 3D reconstruction compared to various baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">313.Distilling Neural Fields for Real-Time Articulated Shape Reconstruction</span><br>
                <span class="as">Tan, JeffandYang, GengshanandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Distilling_Neural_Fields_for_Real-Time_Articulated_Shape_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4692-4701.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在实时视频中重建有关节的3D模型，同时无需在训练时进行测试优化或手动3D监督。<br>
                    动机：目前的方法往往依赖于预构建的可变形模型（如SMAL/SMPL），或者通过不同的可微渲染进行慢速的场景优化（如动态NeRFs）。这些方法无法支持任意对象类别，或不适合实时应用。<br>
                    方法：我们使用现成的基于视频的动态NeRFs作为3D监督来训练一个快速的前馈网络，将3D形状和运动预测转化为有监督的蒸馏任务。我们的时态感知网络使用有关节的骨骼和混合蒙皮表示任意形变，并在视频数据集上自我监督，无需输入3D形状或视点。<br>
                    效果：通过蒸馏，我们的网络能够在交互式帧率下学习重建未见过的有关节物体，其3D重建的逼真度高于现有实时方法，并能在新的视点和姿态下渲染出真实的图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a method for reconstructing articulated 3D models from videos in real-time, without test-time optimization or manual 3D supervision at training time. Prior work often relies on pre-built deformable models (e.g. SMAL/SMPL), or slow per-scene optimization through differentiable rendering (e.g. dynamic NeRFs). Such methods fail to support arbitrary object categories, or are unsuitable for real-time applications. To address the challenge of collecting large-scale 3D training data for arbitrary deformable object categories, our key insight is to use off-the-shelf video-based dynamic NeRFs as 3D supervision to train a fast feed-forward network, turning 3D shape and motion prediction into a supervised distillation task. Our temporal-aware network uses articulated bones and blend skinning to represent arbitrary deformations, and is self-supervised on video datasets without requiring 3D shapes or viewpoints as input. Through distillation, our network learns to 3D-reconstruct unseen articulated objects at interactive frame rates. Our method yields higher-fidelity 3D reconstructions than prior real-time methods for animals, with the ability to render realistic images at novel viewpoints and poses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">314.AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation</span><br>
                <span class="as">Ohkawa, TakehikoandHe, KunandSener, FadimeandHodan, TomasandTran, LuanandKeskin, Cem</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12999-13008.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一个大规模的基准数据集AssemblyHands，以促进具有挑战性的手-物体交互的自我中心活动的研究。<br>
                    动机：现有的自我中心3D手部姿态估计数据集规模较小，质量不高，限制了相关研究的发展。<br>
                    方法：从近期的Assembly101数据集中采样同步的自我中心和外在中心图像，创建AssemblyHands数据集。为了获取高质量的自我中心图像的3D手部姿态注释，开发了一个高效的管道，使用初始的手动注释集训练模型自动注释更大的数据集。<br>
                    效果：AssemblyHands提供了300万个标注的图像，包括49万个自我中心图像，使其成为现有最大的自我中心3D手部姿态估计基准数据集。使用此数据，开发了一个强大的单视图自我中心图像3D手部姿态估计基线。此外，设计了一个新颖的动作分类任务来评估预测的3D手部姿态。研究显示，拥有更高质量的手部姿态可以直接提高动作识别的能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of egocentric activities with challenging hand-object interactions. The dataset includes synchronized egocentric and exocentric images sampled from the recent Assembly101 dataset, in which participants assemble and disassemble take-apart toys. To obtain high-quality 3D hand pose annotations for the egocentric images, we develop an efficient pipeline, where we use an initial set of manual annotations to train a model to automatically annotate a much larger dataset. Our annotation model uses multi-view feature fusion and an iterative refinement scheme, and achieves an average keypoint error of 4.20 mm, which is 85 % lower than the error of the original annotations in Assembly101. AssemblyHands provides 3.0M annotated images, including 490K egocentric images, making it the largest existing benchmark dataset for egocentric 3D hand pose estimation. Using this data, we develop a strong single-view baseline of 3D hand pose estimation from egocentric images. Furthermore, we design a novel action classification task to evaluate predicted 3D hand poses. Our study shows that having higher-quality hand poses directly improves the ability to recognize actions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">315.Scene-Aware Egocentric 3D Human Pose Estimation</span><br>
                <span class="as">Wang, JianandLuvizon, DiogoandXu, WeipengandLiu, LingjieandSarkar, KripasindhuandTheobalt, Christian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13031-13040.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单目鱼眼镜头进行自我中心的三维人体姿态估计，特别是在人体被高度遮挡或与场景紧密互动的情况下。<br>
                    动机：现有的方法在挑战性的姿态下仍然挣扎，例如当人体被高度遮挡或与场景紧密互动时。<br>
                    方法：提出了一种场景感知的自我中心姿态估计方法，通过场景约束来指导自我中心姿态的预测。具体包括：提出一个自我中心深度估计网络，从广角自我中心鱼眼镜头预测场景深度图，同时使用深度修复网络减轻人体遮挡的影响；提出一个场景感知的姿态估计网络，将2D图像特征和场景的估计深度图投影到体素空间，并使用V2V网络回归3D姿态。<br>
                    效果：实验结果表明，预测的3D自我中心姿态准确且在人-场景交互方面物理上可信，表明该方法在数量和质量上都优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Egocentric 3D human pose estimation with a single head-mounted fisheye camera has recently attracted attention due to its numerous applications in virtual and augmented reality. Existing methods still struggle in challenging poses where the human body is highly occluded or is closely interacting with the scene. To address this issue, we propose a scene-aware egocentric pose estimation method that guides the prediction of the egocentric pose with scene constraints. To this end, we propose an egocentric depth estimation network to predict the scene depth map from a wide-view egocentric fisheye camera while mitigating the occlusion of the human body with a depth-inpainting network. Next, we propose a scene-aware pose estimation network that projects the 2D image features and estimated depth map of the scene into a voxel space and regresses the 3D pose with a V2V network. The voxel-based feature representation provides the direct geometric connection between 2D image features and scene geometry, and further facilitates the V2V network to constrain the predicted pose based on the estimated scene geometry. To enable the training of the aforementioned networks, we also generated a synthetic dataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, called EgoPW-Scene. The experimental results of our new evaluation sequences show that the predicted 3D egocentric poses are accurate and physically plausible in terms of human-scene interaction, demonstrating that our method outperforms the state-of-the-art methods both quantitatively and qualitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">316.Learning Geometry-Aware Representations by Sketching</span><br>
                <span class="as">Lee, HyundoandHwang, InwooandGo, HyunsungandChoi, Won-SeokandKim, KibeomandZhang, Byoung-Tak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Learning_Geometry-Aware_Representations_by_Sketching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23315-23326.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将几何概念（如距离和形状）融入场景的视觉表示中。<br>
                    动机：理解几何概念对于理解现实世界和许多视觉任务至关重要。<br>
                    方法：提出通过学习素描来表示场景，该方法在单个推理步骤中明确地结合了场景的几何信息，而无需素描数据集。<br>
                    效果：实验结果表明，LBS显著提高了CLEVR数据集上的对象属性分类性能，以及CLEVR和STL-10数据集之间的领域转移，并证实了LBS提供了丰富的几何信息。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding geometric concepts, such as distance and shape, is essential for understanding the real world and also for many vision tasks. To incorporate such information into a visual representation of a scene, we propose learning to represent the scene by sketching, inspired by human behavior. Our method, coined Learning by Sketching (LBS), learns to convert an image into a set of colored strokes that explicitly incorporate the geometric information of the scene in a single inference step without requiring a sketch dataset. A sketch is then generated from the strokes where CLIP-based perceptual loss maintains a semantic similarity between the sketch and the image. We show theoretically that sketching is equivariant with respect to arbitrary affine transformations and thus provably preserves geometric information. Experimental results show that LBS substantially improves the performance of object attribute classification on the unlabeled CLEVR dataset, domain transfer between CLEVR and STL-10 datasets, and for diverse downstream tasks, confirming that LBS provides rich geometric information.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">317.X-Avatar: Expressive Human Avatars</span><br>
                <span class="as">Shen, KaiyueandGuo, ChenandKaufmann, ManuelandZarate, JuanJoseandValentin, JulienandSong, JieandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16911-16921.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种全新的数字化人像模型X-Avatar，以实现远程呈现、AR/VR等场景中逼真的体验。<br>
                    动机：目前的数字化人像模型无法充分捕捉人类表达的丰富性，无法带来逼真的体验。<br>
                    方法：我们的方法全面地对身体、手部、面部表情和外观进行建模，并可以从完整的3D扫描或RGB-D数据中学习。为实现这一目标，我们提出了一个部分感知的学习前皮肤绑定模块，该模块可以由SMPL-X的参数空间驱动，从而实现X-Avatar的富有表现力的动画效果。<br>
                    效果：通过新颖的部分感知采样和初始化策略，我们的方法在保持有效训练的同时，实现了更高的保真度结果，尤其是在较小的身体部位上。我们还通过将纹理网络扩展到几何和变形字段，使其受到姿势、面部表情、几何形状和变形表面的法线条件的影响，从而捕捉到具有高频细节的头像外观。实验证明，我们的方法在动画任务上无论是定量还是定性都优于强大的基线。为了促进未来关于表现力头像的研究，我们贡献了一个名为X-Humans的新数据集，其中包含20名参与者的233个高质量纹理扫描序列，总共有35,500个数据帧。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present X-Avatar, a novel avatar model that captures the full expressiveness of digital humans to bring about life-like experiences in telepresence, AR/VR and beyond. Our method models bodies, hands, facial expressions and appearance in a holistic fashion and can be learned from either full 3D scans or RGB-D data. To achieve this, we propose a part-aware learned forward skinning module that can be driven by the parameter space of SMPL-X, allowing for expressive animation of X-Avatars. To efficiently learn the neural shape and deformation fields, we propose novel part-aware sampling and initialization strategies. This leads to higher fidelity results, especially for smaller body parts while maintaining efficient training despite increased number of articulated bones. To capture the appearance of the avatar with high-frequency details, we extend the geometry and deformation fields with a texture network that is conditioned on pose, facial expression, geometry and the normals of the deformed surface. We show experimentally that our method outperforms strong baselines both quantitatively and qualitatively on the animation task. To facilitate future research on expressive avatars we contribute a new dataset, called X-Humans, containing 233 sequences of high-quality textured scans from 20 participants, totalling 35,500 data frames.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">318.Recovering 3D Hand Mesh Sequence From a Single Blurry Image: A New Dataset and Temporal Unfolding</span><br>
                <span class="as">Oh, YeongukandPark, JoonKyuandKim, JaehaandMoon, GyeongsikandLee, KyoungMu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Oh_Recovering_3D_Hand_Mesh_Sequence_From_a_Single_Blurry_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/554-563.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D手部网格恢复方法主要关注清晰手部图像，而忽视了由于缺乏提供模糊手部图像的数据集而导致的模糊问题。<br>
                    动机：我们首次提出了一个包含模糊手部图像和3D真实值的新数据集BlurHand，以解决这一问题。<br>
                    方法：我们构建了BlurHandNet，这是一个从模糊手部图像准确恢复3D手部网格的基线网络。与以往将模糊输入图像展开为静态单个手部网格的工作不同，我们的网络将其展开为3D手部网格序列，以利用模糊输入图像中的时间信息。<br>
                    效果：实验证明，BlurHand对于从模糊图像中恢复3D手部网格非常有用。我们提出的BlurHandNet在模糊图像上产生了更稳健的结果，同时在自然环境下的图像上也有很好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Hands, one of the most dynamic parts of our body, suffer from blur due to their active movements. However, previous 3D hand mesh recovery methods have mainly focused on sharp hand images rather than considering blur due to the absence of datasets providing blurry hand images. We first present a novel dataset BlurHand, which contains blurry hand images with 3D groundtruths. The BlurHand is constructed by synthesizing motion blur from sequential sharp hand images, imitating realistic and natural motion blurs. In addition to the new dataset, we propose BlurHandNet, a baseline network for accurate 3D hand mesh recovery from a blurry hand image. Our BlurHandNet unfolds a blurry input image to a 3D hand mesh sequence to utilize temporal information in the blurry input image, while previous works output a static single hand mesh. We demonstrate the usefulness of BlurHand for the 3D hand mesh recovery from blurry images in our experiments. The proposed BlurHandNet produces much more robust results on blurry images while generalizing well to in-the-wild images. The training codes and BlurHand dataset are available at https://github.com/JaehaKim97/BlurHand_RELEASE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">319.Fast Monocular Scene Reconstruction With Global-Sparse Local-Dense Grids</span><br>
                <span class="as">Dong, WeiandChoy, ChristopherandLoop, CharlesandLitany, OrandZhu, YukeandAnandkumar, Anima</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Fast_Monocular_Scene_Reconstruction_With_Global-Sparse_Local-Dense_Grids_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4263-4272.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目图像中重建室内场景，同时提高训练和渲染的速度。<br>
                    动机：目前的方法主要依赖于多层感知机（MLP），这在训练和渲染速度上存在明显限制。<br>
                    方法：我们提出直接使用稀疏体素块网格中的有符号距离函数（SDF）进行快速准确的场景重建，而不使用MLP。我们的全局稀疏和局部密集的数据结构利用了表面的空间稀疏性，实现了对缓存友好的查询，并允许直接扩展到多模态数据，如颜色和语义标签。<br>
                    效果：实验表明，我们的方法在训练上快10倍，渲染上快100倍，同时达到了与最先进的神经隐式方法相当的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Indoor scene reconstruction from monocular images has long been sought after by augmented reality and robotics developers. Recent advances in neural field representations and monocular priors have led to remarkable results in scene-level surface reconstructions. The reliance on Multilayer Perceptrons (MLP), however, significantly limits speed in training and rendering. In this work, we propose to directly use signed distance function (SDF) in sparse voxel block grids for fast and accurate scene reconstruction without MLPs. Our globally sparse and locally dense data structure exploits surfaces' spatial sparsity, enables cache-friendly queries, and allows direct extensions to multi-modal data such as color and semantic labels. To apply this representation to monocular scene reconstruction, we develop a scale calibration algorithm for fast geometric initialization from monocular depth priors. We apply differentiable volume rendering from this initialization to refine details with fast convergence. We also introduce efficient high-dimensional Continuous Random Fields (CRFs) to further exploit the semantic-geometry consistency between scene objects. Experiments show that our approach is 10x faster in training and 100x faster in rendering while achieving comparable accuracy to state-of-the-art neural implicit methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">320.Thermal Spread Functions (TSF): Physics-Guided Material Classification</span><br>
                <span class="as">Dashpute, AniketandSaragadam, VishwanathandAlexander, EmmaandWillomitzer, FlorianandKatsaggelos, AggelosandVeeraraghavan, AshokandCossairt, Oliver</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dashpute_Thermal_Spread_Functions_TSF_Physics-Guided_Material_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1641-1650.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何进行鲁棒且非破坏性的物质分类，这是许多视觉应用中的关键第一步。<br>
                    动机：物体的加热和冷却速率取决于材料的独特内在属性，即发射率和扩散率。我们利用这一观察结果，通过用低功率激光轻轻加热场景中的物体一段时间，然后关闭它，同时热像仪在加热和冷却过程中进行测量。<br>
                    方法：我们采用有限差分法解决逆向热方程，得到空间变化的扩散率和发射率估计值。然后使用这些元组训练一个分类器，为每个空间像素产生精细的材料标签。<br>
                    效果：我们的方法非常简单，只需要一个小光源（低功率激光器）和一个热像仪，就可以产生鲁棒的分类结果，16个类别的准确率达到86%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust and non-destructive material classification is a challenging but crucial first-step in numerous vision applications. We propose a physics-guided material classification framework that relies on thermal properties of the object. Our key observation is that the rate of heating and cooling of an object depends on the unique intrinsic properties of the material, namely the emissivity and diffusivity. We leverage this observation by gently heating the objects in the scene with a low-power laser for a fixed duration and then turning it off, while a thermal camera captures measurements during the heating and cooling process. We then take this spatial and temporal "thermal spread function" (TSF) to solve an inverse heat equation using the finite-differences approach, resulting in a spatially varying estimate of diffusivity and emissivity. These tuples are then used to train a classifier that produces a fine-grained material label at each spatial pixel. Our approach is extremely simple requiring only a small light source (low power laser) and a thermal camera, and produces robust classification results with 86% accuracy over 16 classes</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">321.ESLAM: Efficient Dense SLAM System Based on Hybrid Representation of Signed Distance Fields</span><br>
                <span class="as">Johari, MohammadMahdiandCarta, CamillaandFleuret, Fran\c{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17408-17419.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地实现同时定位与地图构建（SLAM）的神经表征方法。<br>
                    动机：现有的SLAM系统需要大量的预训练，并且效率低下。<br>
                    方法：提出ESLAM模型，通过连续读取未知相机位姿的RGB-D帧并逐步重建场景表示，同时估计当前场景中的相机位置。将最新的Neural Radiance Fields（NeRF）技术整合到SLAM系统中，形成一种高效准确的密集视觉SLAM方法。<br>
                    效果：在Replica、ScanNet和TUM RGB-D三个标准数据集上的大量实验表明，ESLAM比最先进的密集视觉SLAM方法提高了3D重建和相机定位的准确性超过50%，并且运行速度提高了10倍，无需任何预训练。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ESLAM, an efficient implicit neural representation method for Simultaneous Localization and Mapping (SLAM). ESLAM reads RGB-D frames with unknown camera poses in a sequential manner and incrementally reconstructs the scene representation while estimating the current camera position in the scene. We incorporate the latest advances in Neural Radiance Fields (NeRF) into a SLAM system, resulting in an efficient and accurate dense visual SLAM method. Our scene representation consists of multi-scale axis-aligned perpendicular feature planes and shallow decoders that, for each point in the continuous space, decode the interpolated features into Truncated Signed Distance Field (TSDF) and RGB values. Our extensive experiments on three standard datasets, Replica, ScanNet, and TUM RGB-D show that ESLAM improves the accuracy of 3D reconstruction and camera localization of state-of-the-art dense visual SLAM methods by more than 50%, while it runs up to 10 times faster and does not require any pre-training. Project page: https://www.idiap.ch/paper/eslam</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">322.iDisc: Internal Discretization for Monocular Depth Estimation</span><br>
                <span class="as">Piccinelli, LuigiandSakaridis, ChristosandYu, Fisher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Piccinelli_iDisc_Internal_Discretization_for_Monocular_Depth_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21477-21487.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目深度估计是3D场景理解和下游应用的基础，但在有监督的设置下，由于缺乏几何约束，仍然具有挑战性和不适定。<br>
                    动机：尽管一个场景可能由数百万个像素组成，但高层次的模式要少得多。我们观察到这一点，并提出iDisc来学习这些模式的内部离散表示。<br>
                    方法：我们提出了一种新的模块，内部离散化（ID），实现了连续-离散-连续的瓶颈，以在没有监督的情况下学习这些概念。与最先进的方法相比，我们的方法不对深度输出施加任何显式的约束或先验知识。<br>
                    效果：我们的模型在NYU-Depth v2和KITTI上取得了显著的改进，并在官方KITTI基准测试上超越了所有已发布的方法。iDisc还可以在表面法线估计上取得最先进的结果。此外，我们还通过零射测试探索了模型的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular depth estimation is fundamental for 3D scene understanding and downstream applications. However, even under the supervised setup, it is still challenging and ill-posed due to the lack of geometric constraints. We observe that although a scene can consist of millions of pixels, there are much fewer high-level patterns. We propose iDisc to learn those patterns with internal discretized representations. The method implicitly partitions the scene into a set of high-level concepts. In particular, our new module, Internal Discretization (ID), implements a continuous-discrete-continuous bottleneck to learn those concepts without supervision. In contrast to state-of-the-art methods, the proposed model does not enforce any explicit constraints or priors on the depth output. The whole network with the ID module can be trained in an end-to-end fashion thanks to the bottleneck module based on attention. Our method sets the new state of the art with significant improvements on NYU-Depth v2 and KITTI, outperforming all published methods on the official KITTI benchmark. iDisc can also achieve state-of-the-art results on surface normal estimation. Further, we explore the model generalization capability via zero-shot testing. From there, we observe the compelling need to promote diversification in the outdoor scenario and we introduce splits of two autonomous driving datasets, DDAD and Argoverse. Code is available at http://vis.xyz/pub/idisc/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">323.Sampling Is Matter: Point-Guided 3D Human Mesh Reconstruction</span><br>
                <span class="as">Kim, JeonghwanandGwon, Mi-GyeongandPark, HyunwooandKwon, HyukminandUm, Gi-MunandKim, Wonjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Sampling_Is_Matter_Point-Guided_3D_Human_Mesh_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12880-12889.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中重建3D人体网格？<br>
                    动机：尽管现有的方法在估计整体网格顶点的非局部交互和处理身体部位关系方面取得了显著进展，但直接推断从2D输入图像编码的特征与每个顶点的3D坐标之间的关系仍然困难。<br>
                    方法：提出一种简单的特征采样方案，通过按照3D网格顶点（即，真实值）的投影结果来指导2D空间中的特征采样，使模型更关注顶点相关特征，从而实现自然人体姿态的重建。<br>
                    效果：实验结果表明，该方法有效提高了3D人体网格重建的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a simple yet powerful method for 3D human mesh reconstruction from a single RGB image. Most recently, the non-local interactions of the whole mesh vertices have been effectively estimated in the transformer while the relationship between body parts also has begun to be handled via the graph model. Even though those approaches have shown the remarkable progress in 3D human mesh reconstruction, it is still difficult to directly infer the relationship between features, which are encoded from the 2D input image, and 3D coordinates of each vertex. To resolve this problem, we propose to design a simple feature sampling scheme. The key idea is to sample features in the embedded space by following the guide of points, which are estimated as projection results of 3D mesh vertices (i.e., ground truth). This helps the model to concentrate more on vertex-relevant features in the 2D space, thus leading to the reconstruction of the natural human pose. Furthermore, we apply progressive attention masking to precisely estimate local interactions between vertices even under severe occlusions. Experimental results on benchmark datasets show that the proposed method efficiently improves the performance of 3D human mesh reconstruction. The code and model are publicly available at: https://github.com/DCVL-3D/PointHMR_release.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">324.Depth Estimation From Indoor Panoramas With Neural Scene Representation</span><br>
                <span class="as">Chang, WenjieandZhang, YueyiandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Depth_Estimation_From_Indoor_Panoramas_With_Neural_Scene_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/899-908.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高室内全景图像深度估计的准确性和效率。<br>
                    动机：由于全景图像的等距畸变和不准确的匹配，从室内全景图像进行深度估计具有挑战性。<br>
                    方法：提出一种实用的框架，利用神经辐射场技术改进多视角室内全景图像的深度估计。开发两个网络来隐式学习用于深度测量的有符号距离函数和全景图的辐射场。引入一种新的球形位置嵌入方案以实现高精度。为了更好的收敛，提出了一种基于曼哈顿世界假设的网络权重初始化方法。此外，设计了一种几何一致性损失，利用表面法线进一步优化深度估计。<br>
                    效果：实验结果表明，所提出的方法在定量和定性评估中均大幅超越现有最先进技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Depth estimation from indoor panoramas is challenging due to the equirectangular distortions of panoramas and inaccurate matching. In this paper, we propose a practical framework to improve the accuracy and efficiency of depth estimation from multi-view indoor panoramic images with the Neural Radiance Field technology. Specifically, we develop two networks to implicitly learn the Signed Distance Function for depth measurements and the radiance field from panoramas. We also introduce a novel spherical position embedding scheme to achieve high accuracy. For better convergence, we propose an initialization method for the network weights based on the Manhattan World Assumption. Furthermore, we devise a geometric consistency loss, leveraging the surface normal, to further refine the depth estimation. The experimental results demonstrate that our proposed method outperforms state-of-the-art works by a large margin in both quantitative and qualitative evaluations. Our source code is available at https://github.com/WJ-Chang-42/IndoorPanoDepth.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">325.Single Image Depth Prediction Made Better: A Multivariate Gaussian Take</span><br>
                <span class="as">Liu, CeandKumar, SuryanshandGu, ShuhangandTimofte, RaduandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Single_Image_Depth_Prediction_Made_Better_A_Multivariate_Gaussian_Take_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17346-17356.png><br>
            
            <span class="tt"><span class="t0">研究问题：单图像深度预测（SIDP）是一个挑战性的任务，目标是在测试时预测场景的每个像素深度。<br>
                    动机：由于该问题本质上是病态的，因此基本目标是提出一种能够从一组训练样本可靠地对场景深度进行建模的方法。<br>
                    方法：我们引入了一种连续地对每个像素深度进行建模的方法，我们可以预测和推理每个像素深度及其分布。为此，我们使用多变量高斯分布来对每个像素的场景深度进行建模。此外，与现有的不确定性建模方法相反，我们引入了每个像素协方差建模，它编码了其相对于所有场景点的深度依赖性。<br>
                    效果：当我们在KITTI、NYU和SUN-RGB-D等基准数据集上进行测试时，通过优化我们的损失函数获得的SIDP模型显示出最先进的结果。我们的方法的准确性（命名为MG）在KITTI深度预测基准测试排行榜上名列前茅。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural-network-based single image depth prediction (SIDP) is a challenging task where the goal is to predict the scene's per-pixel depth at test time. Since the problem, by definition, is ill-posed, the fundamental goal is to come up with an approach that can reliably model the scene depth from a set of training examples. In the pursuit of perfect depth estimation, most existing state-of-the-art learning techniques predict a single scalar depth value per-pixel. Yet, it is well-known that the trained model has accuracy limits and can predict imprecise depth. Therefore, an SIDP approach must be mindful of the expected depth variations in the model's prediction at test time. Accordingly, we introduce an approach that performs continuous modeling of per-pixel depth, where we can predict and reason about the per-pixel depth and its distribution. To this end, we model per-pixel scene depth using a multivariate Gaussian distribution. Moreover, contrary to the existing uncertainty modeling methods---in the same spirit, where per-pixel depth is assumed to be independent, we introduce per-pixel covariance modeling that encodes its depth dependency w.r.t. all the scene points. Unfortunately, per-pixel depth covariance modeling leads to a computationally expensive continuous loss function, which we solve efficiently using the learned low-rank approximation of the overall covariance matrix. Notably, when tested on benchmark datasets such as KITTI, NYU, and SUN-RGB-D, the SIDP model obtained by optimizing our loss function shows state-of-the-art results. Our method's accuracy (named MG) is among the top on the KITTI depth-prediction benchmark leaderboard.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">326.UMat: Uncertainty-Aware Single Image High Resolution Material Capture</span><br>
                <span class="as">Rodriguez-Pardo, CarlosandDom{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rodriguez-Pardo_UMat_Uncertainty-Aware_Single_Image_High_Resolution_Material_Capture_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5764-5774.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单一漫反射图像中恢复材料的法线、镜面反射和粗糙度。<br>
                    动机：现有的方法在处理单张图像时，常常会产生过度平滑的输出，或者分辨率有限，或者每个类别都需要训练一个模型，泛化能力有限。<br>
                    方法：我们提出了一种基于学习的方法，利用微几何外观作为主要线索，从单一漫反射图像中恢复材料的法线、镜面反射和粗糙度。我们使用了一个带有注意力的生成网络和一个U-Net判别器，以降低计算复杂度并整合全局信息。<br>
                    效果：我们在一个真实的数字化纺织材料数据集上展示了我们方法的性能，证明普通的平板扫描仪就可以产生我们需要的输入类型。此外，由于问题可能是病态的——可能需要多张漫反射图像才能消除镜面反射的歧义——或者因为训练数据集对真实分布的代表程度不够，我们还提出了一种新的框架来量化模型对其预测的信心。我们的方法首次解决了材料数字化过程中的不确定性建模问题，提高了过程的可信度，并通过主动学习实验展示了更智能的数据集创建策略。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a learning-based method to recover normals, specularity, and roughness from a single diffuse image of a material, using microgeometry appearance as our primary cue. Previous methods that work on single images tend to produce over-smooth outputs with artifacts, operate at limited resolution, or train one model per class with little room for generalization. In contrast, in this work, we propose a novel capture approach that leverages a generative network with attention and a U-Net discriminator, which shows outstanding performance integrating global information at reduced computational complexity. We showcase the performance of our method with a real dataset of digitized textile materials and show that a commodity flatbed scanner can produce the type of diffuse illumination required as input to our method. Additionally, because the problem might be ill-posed --more than a single diffuse image might be needed to disambiguate the specular reflection-- or because the training dataset is not representative enough of the real distribution, we propose a novel framework to quantify the model's confidence about its prediction at test time. Our method is the first one to deal with the problem of modeling uncertainty in material digitization, increasing the trustworthiness of the process and enabling more intelligent strategies for dataset creation, as we demonstrate with an active learning experiment.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">327.SCOOP: Self-Supervised Correspondence and Optimization-Based Scene Flow</span><br>
                <span class="as">Lang, ItaiandAiger, DrorandCole, ForresterandAvidan, ShaiandRubinstein, Michael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lang_SCOOP_Self-Supervised_Correspondence_and_Optimization-Based_Scene_Flow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5281-5290.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的场景流估计问题，即从连续观测中找到场景的3D运动。<br>
                    动机：现有的方法通常需要大量的标注数据进行训练，而本文提出了一种新的方法，可以在少量数据上进行学习，无需使用真实的流监督。<br>
                    方法：本文提出的方法首先训练一个纯对应模型来学习点特征表示，并将流初始化为源点和其软对应目标点之间的差值。然后在运行时阶段，直接优化流精化组件，使用自我监督的目标，从而在点云之间产生连贯且准确的流场。<br>
                    效果：实验结果表明，与现有的领先技术相比，该方法在使用一小部分训练数据的情况下取得了性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scene flow estimation is a long-standing problem in computer vision, where the goal is to find the 3D motion of a scene from its consecutive observations. Recently, there have been efforts to compute the scene flow from 3D point clouds. A common approach is to train a regression model that consumes source and target point clouds and outputs the per-point translation vector. An alternative is to learn point matches between the point clouds concurrently with regressing a refinement of the initial correspondence flow. In both cases, the learning task is very challenging since the flow regression is done in the free 3D space, and a typical solution is to resort to a large annotated synthetic dataset. We introduce SCOOP, a new method for scene flow estimation that can be learned on a small amount of data without employing ground-truth flow supervision. In contrast to previous work, we train a pure correspondence model focused on learning point feature representation and initialize the flow as the difference between a source point and its softly corresponding target point. Then, in the run-time phase, we directly optimize a flow refinement component with a self-supervised objective, which leads to a coherent and accurate flow field between the point clouds. Experiments on widespread datasets demonstrate the performance gains achieved by our method compared to existing leading techniques while using a fraction of the training data. Our code is publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">328.RobustNeRF: Ignoring Distractors With Robust Losses</span><br>
                <span class="as">Sabour, SaraandVora, SuhaniandDuckworth, DanielandKrasin, IvanandFleet, DavidJ.andTagliasacchi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sabour_RobustNeRF_Ignoring_Distractors_With_Robust_Losses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20626-20636.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何消除静态场景图像中的干扰物，如移动物体、光照变化和阴影，以提高研究问题：如何消除静态场景图像中的干扰物，如移动物体、光照变化和阴影，以提高Neural radiance fields（NeRF）在新视图合成中的表现。<br>
                    动机：现有的NeRF方法在处理包含干扰物的静态场景时会出现"floaters"等伪影。<br>
                    方法：提出一种稳健的NeRF训练估计方法，将训练数据中的干扰物建模为优化问题的异常值，从而从场景中移除这些异常值。<br>
                    效果：该方法成功地从合成和真实世界的静态场景中移除了异常值，并在性能上超越了基线方法。此外，该方法易于集成到现代NeRF框架中，且无需预先了解干扰物的类型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields (NeRF) excel at synthesizing new views given multi-view, calibrated images of a static scene. When scenes include distractors, which are not persistent during image capture (moving objects, lighting variations, shadows), artifacts appear as view-dependent effects or 'floaters'. To cope with distractors, we advocate a form of robust estimation for NeRF training, modeling distractors in training data as outliers of an optimization problem. Our method successfully removes outliers from a scene and improves upon our baselines, on synthetic and real-world scenes. Our technique is simple to incorporate in modern NeRF frameworks, with few hyper-parameters. It does not assume a priori knowledge of the types of distractors, and is instead focused on the optimization problem rather than pre-processing or modeling transient objects. More results on our page https://robustnerf.github.io/public.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">329.SCOTCH and SODA: A Transformer Video Shadow Detection Framework</span><br>
                <span class="as">Liu, LihaoandProst, JeanandZhu, LeiandPapadakis, NicolasandLi\`o, PietroandSch\&quot;onlieb, Carola-BibianeandAviles-Rivero, AngelicaI.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SCOTCH_and_SODA_A_Transformer_Video_Shadow_Detection_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10449-10458.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频中的影子检测由于帧间大的影子变形而变得困难。<br>
                    动机：设计视频影子检测方法时，考虑影子变形是必要的。<br>
                    方法：提出了一种新的视频自注意力模块——阴影变形注意力轨迹（SODA），专门用于处理视频中大的影子变形。同时，还提出了一种新的阴影对比学习机制（SCOTCH），旨在引导网络从不同视频中的大量正影子对中学习统一的阴影表示。<br>
                    效果：实验证明，这两种方法在视频影子检测任务上的效果明显优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Shadows in videos are difficult to detect because of the large shadow deformation between frames. In this work, we argue that accounting for shadow deformation is essential when designing a video shadow detection method. To this end, we introduce the shadow deformation attention trajectory (SODA), a new type of video self-attention module, specially designed to handle the large shadow deformations in videos. Moreover, we present a new shadow contrastive learning mechanism (SCOTCH) which aims at guiding the network to learn a unified shadow representation from massive positive shadow pairs across different videos. We demonstrate empirically the effectiveness of our two contributions in an ablation study. Furthermore, we show that SCOTCH and SODA significantly outperforms existing techniques for video shadow detection. Code is available at the project page: https://lihaoliu-cambridge.github.io/scotch_and_soda/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">330.Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting</span><br>
                <span class="as">Zheng, RuichenandLi, PengandWang, HaoqianandYu, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/216-226.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行数字人类的详细3D重建和照片级真实感重光照？<br>
                    动机：为了解决多视图特征聚合中的遮挡歧义问题，同时评估自我阴影的光线衰减，我们提出了一种新的稀疏视图3D人体重建框架。<br>
                    方法：我们将占用场和反照率场与额外的可见性场紧密结合，将可见性离散化为一组固定的样本方向，并为其提供耦合的几何3D深度特征和局部2D图像特征。我们还提出了一种新的渲染感知损失，即TransferLoss，以隐式地强制可见性和占用场之间的对齐，实现端到端的联合训练。<br>
                    效果：实验结果和大量实验证明，该方法在重建精度上超过了最先进的技术，同时在光线追踪的真值对比下实现了相当准确的重光照。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detailed 3D reconstruction and photo-realistic relighting of digital humans are essential for various applications. To this end, we propose a novel sparse-view 3d human reconstruction framework that closely incorporates the occupancy field and albedo field with an additional visibility field--it not only resolves occlusion ambiguity in multiview feature aggregation, but can also be used to evaluate light attenuation for self-shadowed relighting. To enhance its training viability and efficiency, we discretize visibility onto a fixed set of sample directions and supply it with coupled geometric 3D depth feature and local 2D image feature. We further propose a novel rendering-inspired loss, namely TransferLoss, to implicitly enforce the alignment between visibility and occupancy field, enabling end-to-end joint training. Results and extensive experiments demonstrate the effectiveness of the proposed method, as it surpasses state-of-the-art in terms of reconstruction accuracy while achieving comparably accurate relighting to ray-traced ground truth.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">331.Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting</span><br>
                <span class="as">Yang, SiqiandCui, XuanningandZhu, YongjieandTang, JiajunandLi, SiandYu, ZhaofeiandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Complementary_Intrinsics_From_Neural_Radiance_Fields_and_CNNs_for_Outdoor_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16600-16609.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过多视角立体视觉的弱监督标签，利用神经辐射场（NeRFs）进行户外场景的光照编辑，以解决户外场景重光照的挑战。<br>
                    动机：由于户外场景的多样化照明和显著的投射阴影，使得户外场景的重光照变得具有挑战性。通过使用神经辐射场（NeRFs）对户外照片集进行固有图像分解，可以部分解决这个问题。<br>
                    方法：本论文提出了一种互补的方法，该方法结合了体积渲染的固有估计和使用卷积神经网络（CNNs）反演光度图像形成模型。前者为训练后者产生更丰富、更可靠的伪标签（投射阴影和天空外观以及反照率和法线），后者通过单图像预测管道预测可解释和可编辑的照明参数。<br>
                    效果：我们的方法在多种真实户外场景的固有图像分解和重光照方面都显示出优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Relighting an outdoor scene is challenging due to the diverse illuminations and salient cast shadows. Intrinsic image decomposition on outdoor photo collections could partly solve this problem by weakly supervised labels with albedo and normal consistency from multi-view stereo. With neural radiance fields (NeRFs), editing the appearance code could produce more realistic results without explicitly interpreting the outdoor scene image formation. This paper proposes to complement the intrinsic estimation from volume rendering using NeRFs and from inversing the photometric image formation model using convolutional neural networks (CNNs). The former produces richer and more reliable pseudo labels (cast shadows and sky appearances in addition to albedo and normal) for training the latter to predict interpretable and editable lighting parameters via a single-image prediction pipeline. We demonstrate the advantages of our method for both intrinsic image decomposition and relighting for various real outdoor scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">332.High-Res Facial Appearance Capture From Polarized Smartphone Images</span><br>
                <span class="as">Azinovi\&#x27;c, DejanandMaury, OlivierandHery, ChristopheandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Azinovic_High-Res_Facial_Appearance_Capture_From_Polarized_Smartphone_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16836-16846.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从RGB图像中高质量重建面部纹理？<br>
                    动机：使用智能手机和便宜的偏振箔，通过新颖的捕捉方式进行面部纹理重建。<br>
                    方法：将手机闪光灯转为偏振光源，并在相机上添加偏振滤镜。在黑暗环境下，用修改后的智能手机以不同的光极化捕捉对象的面部，并基于这些观察结果，使用结构运动学重建面部显式表面网格。然后利用相机和光源在同一位置的可微渲染器，通过分析合成法优化面部纹理。<br>
                    效果：优化后的纹理可用于标准渲染管道，在新环境中合成高质量的照片级真实3D数字人。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach. Our method optimizes for high-resolution normal textures, diffuse albedo, and specular albedo using a coarse-to-fine optimization scheme. We show that the optimized textures can be used in a standard rendering pipeline to synthesize high-quality photo-realistic 3D digital humans in novel environments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">333.JAWS: Just a Wild Shot for Cinematic Transfer in Neural Radiance Fields</span><br>
                <span class="as">Wang, XiandCourant, RobinandShi, JingleiandMarchand, EricandChristie, Marc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_JAWS_Just_a_Wild_Shot_for_Cinematic_Transfer_in_Neural_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16933-16942.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现从参考野外视频片段到新生成片段的视觉电影特征的稳健转移。<br>
                    动机：为了提高新生成视频片段的电影特征与参考片段的相似度，提出一种优化驱动的方法。<br>
                    方法：采用隐式神经表示（INR）计算共享相同电影特征的片段，并提出了在INR中计算相机参数和时间的通用相机优化问题公式。通过利用神经表示的可微性，将设计的电影损失通过NeRF网络反向传播到提出的电影参数。<br>
                    效果：实验结果表明，该系统能够很好地复制电影中的知名相机序列，调整生成视频片段的构图、相机参数和时间，使其与参考片段的相似度最大化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents JAWS, an optimzation-driven approach that achieves the robust transfer of visual cinematic features from a reference in-the-wild video clip to a newly generated clip. To this end, we rely on an implicit-neural-representation (INR) in a way to compute a clip that shares the same cinematic features as the reference clip. We propose a general formulation of a camera optimization problem in an INR that computes extrinsic and intrinsic camera parameters as well as timing. By leveraging the differentiability of neural representations, we can back-propagate our designed cinematic losses measured on proxy estimators through a NeRF network to the proposed cinematic parameters directly. We also introduce specific enhancements such as guidance maps to improve the overall quality and efficiency. Results display the capacity of our system to replicate well known camera sequences from movies, adapting the framing, camera parameters and timing of the generated video clip to maximize the similarity with the reference clip.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">334.Temporally Consistent Online Depth Estimation Using Point-Based Fusion</span><br>
                <span class="as">Khan, NumairandPenner, EricandLanman, DouglasandXiao, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Temporally_Consistent_Online_Depth_Estimation_Using_Point-Based_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9119-9129.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频深度估计中的时间一致性问题，即如何在没有未来帧的情况下在线实时地估计出具有时间一致性的视频深度图。<br>
                    动机：现有的视频深度估计方法在应用于视频时，其结果缺乏时间一致性，会出现闪烁和游走的伪影。同时，由于未来帧不可用，以及动态物体的存在，使得这个问题变得更加复杂。<br>
                    方法：本文提出了一种全局点云的方法来解决这个问题，每帧都会动态更新这个点云。同时，还采用了在图像空间中学习融合的方法。这种方法既鼓励了一致性，又允许对错误和动态物体进行更新。<br>
                    效果：定性和定量的实验结果表明，该方法在视频深度估计的时间一致性方面取得了最先进的质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Depth estimation is an important step in many computer vision problems such as 3D reconstruction, novel view synthesis, and computational photography. Most existing work focuses on depth estimation from single frames. When applied to videos, the result lacks temporal consistency, showing flickering and swimming artifacts. In this paper we aim to estimate temporally consistent depth maps of video streams in an online setting. This is a difficult problem as future frames are not available and the method must choose between enforcing consistency and correcting errors from previous estimations. The presence of dynamic objects further complicates the problem. We propose to address these challenges by using a global point cloud that is dynamically updated each frame, along with a learned fusion approach in image space. Our approach encourages consistency while simultaneously allowing updates to handle errors and dynamic objects. Qualitative and quantitative results show that our method achieves state-of-the-art quality for consistent video depth estimation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">335.Passive Micron-Scale Time-of-Flight With Sunlight Interferometry</span><br>
                <span class="as">Kotwal, AlankarandLevin, AnatandGkioulekas, Ioannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kotwal_Passive_Micron-Scale_Time-of-Flight_With_Sunlight_Interferometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4139-4149.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在介绍一种用于被动飞行时间成像和微米级轴向分辨率深度传感的干涉测量技术。<br>
                    动机：现有的技术需要使用复杂的光源和扫描设备，而我们的方法利用了太阳光作为唯一的光源，并通过简单的轴向扫描操作获取微米级分辨率的时间响应。<br>
                    方法：我们使用全视迈克尔逊干涉仪进行修改，以利用太阳光作为唯一的光源。太阳光的大光谱带宽使得我们能够通过简单的轴向扫描操作获取微米级分辨率的时间响应。此外，太阳光的角度带宽使得我们能够捕获对间接照明效应（如反射和次表面散射）不敏感的飞行时间测量结果。<br>
                    效果：我们在户外、直接阳光下以及在机器振动和车辆交通等不利环境条件下操作实验原型，并首次展示了对间接照明具有鲁棒性的微米级深度传感、直接成像和通过扩散器成像等被动成像能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce an interferometric technique for passive time-of-flight imaging and depth sensing at micrometer axial resolutions. Our technique uses a full-field Michelson interferometer, modified to use sunlight as the only light source. The large spectral bandwidth of sunlight makes it possible to acquire micrometer-resolution time-resolved scene responses, through a simple axial scanning operation. Additionally, the angular bandwidth of sunlight makes it possible to capture time-of-flight measurements insensitive to indirect illumination effects, such as interreflections and subsurface scattering. We build an experimental prototype that we operate outdoors, under direct sunlight, and in adverse environment conditions such as machine vibrations and vehicle traffic. We use this prototype to demonstrate, for the first time, passive imaging capabilities such as micrometer-scale depth sensing robust to indirect illumination, direct-only imaging, and imaging through diffusers.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">336.Unsupervised Volumetric Animation</span><br>
                <span class="as">Siarohin, AliaksandrandMenapace, WilliandSkorokhodov, IvanandOlszewski, KyleandRen, JianandLee, Hsin-YingandChai, MengleiandTulyakov, Sergey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4658-4669.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的无监督方法，用于对非刚性可变形物体进行3D动画制作。<br>
                    动机：目前的3D动画制作方法需要大量的标注数据和复杂的计算过程，而本文的方法可以从单视角RGB视频中学习物体的3D结构和动态，并将其分解为具有语义意义的部分进行跟踪和动画制作。<br>
                    方法：本文采用了一个3D自动解码器框架，结合了通过可微PnP算法估计关键点的方法，完全无监督地学习了物体的基本几何形状和部分分解。这使得该方法能够进行3D分割、3D关键点估计、新视图合成和动画制作。<br>
                    效果：在VoxCeleb 256^2和TEDXPeople 256^2两个视频数据集上进行了主要评估。此外，在Cats 256^2数据集上，本文的方法甚至能够从原始图像数据中学习出引人注目的3D几何形状。最后，本文的方法可以从单个或几张图像中获得可动画化的3D物体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel approach for unsupervised 3D animation of non-rigid deformable objects. Our method learns the 3D structure and dynamics of objects solely from single-view RGB videos, and can decompose them into semantically meaningful parts that can be tracked and animated. Using a 3D autodecoder framework, paired with a keypoint estimator via a differentiable PnP algorithm, our model learns the underlying object geometry and parts decomposition in an entirely unsupervised manner. This allows it to perform 3D segmentation, 3D keypoint estimation, novel view synthesis, and animation. We primarily evaluate the framework on two video datasets: VoxCeleb 256^2 and TEDXPeople 256^2. In addition, on the Cats 256^2 dataset, we show that it learns compelling 3D geometry even from raw image data. Finally, we show that our model can obtain animatable 3D objects from a singe or a few images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">337.PlaneDepth: Self-Supervised Depth Estimation via Orthogonal Planes</span><br>
                <span class="as">Wang, RuoyuandYu, ZehaoandGao, Shenghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PlaneDepth_Self-Supervised_Depth_Estimation_via_Orthogonal_Planes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21425-21434.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于近前平行平面的深度表示在自监督单目深度估计（MDE）中取得了显著成果，但这种表示方法会导致地面不连续，对自动驾驶中的可行驶空间识别造成损害。<br>
                    动机：为了解决上述问题，本文提出了一种新颖的基于正交平面的表示方法——PlaneDepth，包括垂直平面和地面平面。<br>
                    方法：PlaneDepth使用拉普拉斯混合模型基于输入图像的正交平面来估计深度分布。这些平面用于合成参考视图以提供自我监督信号。此外，作者发现常用的调整大小和裁剪数据增强会破坏正交性假设，导致平面预测效果不佳。为解决这个问题，作者通过显式构造调整大小裁剪变换来纠正预定义的平面和预测的相机姿态。此外，作者还提出了一种增强的自我蒸馏损失，使用双边遮挡掩码进行监督，以提高正交平面表示对遮挡的鲁棒性。<br>
                    效果：由于我们采用了正交平面表示法，我们可以在无监督的方式下提取地面平面，这对于自动驾驶至关重要。在KITTI数据集上的大量实验证明了我们方法的有效性和效率。代码可在https://github.com/svip-lab/PlaneDepth获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multiple near frontal-parallel planes based depth representation demonstrated impressive results in self-supervised monocular depth estimation (MDE). Whereas, such a representation would cause the discontinuity of the ground as it is perpendicular to the frontal-parallel planes, which is detrimental to the identification of drivable space in autonomous driving. In this paper, we propose the PlaneDepth, a novel orthogonal planes based presentation, including vertical planes and ground planes. PlaneDepth estimates the depth distribution using a Laplacian Mixture Model based on orthogonal planes for an input image. These planes are used to synthesize a reference view to provide the self-supervision signal. Further, we find that the widely used resizing and cropping data augmentation breaks the orthogonality assumptions, leading to inferior plane predictions. We address this problem by explicitly constructing the resizing cropping transformation to rectify the predefined planes and predicted camera pose. Moreover, we propose an augmented self-distillation loss supervised with a bilateral occlusion mask to boost the robustness of orthogonal planes representation for occlusions. Thanks to our orthogonal planes representation, we can extract the ground plane in an unsupervised manner, which is important for autonomous driving. Extensive experiments on the KITTI dataset demonstrate the effectiveness and efficiency of our method. The code is available at https://github.com/svip-lab/PlaneDepth.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">338.MobileBrick: Building LEGO for 3D Reconstruction on Mobile Devices</span><br>
                <span class="as">Li, KejieandBian, Jia-WangandCastle, RobertandTorr, PhilipH.S.andPrisacariu, VictorAdrian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MobileBrick_Building_LEGO_for_3D_Reconstruction_on_Mobile_Devices_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4892-4901.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何获取高质量的3D真实形状，以进行3D物体重建评估。<br>
                    动机：现实中难以复制物体，即使使用3D扫描仪生成的3D重建也存在偏差，因此需要一种更精确的方法。<br>
                    方法：利用已知几何形状的乐高模型作为图像捕获的3D结构，通过移动设备获取高分辨率RGB图像和低分辨率深度图，从而得到精确的3D真实形状。<br>
                    效果：提出了一种新的多视图RGBD数据集，包括153个具有不同3D结构的物体模型的高精度3D真实注释。该数据集为未来关于高保真度3D重建的研究提供了独特的机会。同时，对一系列3D重建算法进行了评估。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-quality 3D ground-truth shapes are critical for 3D object reconstruction evaluation. However, it is difficult to create a replica of an object in reality, and even 3D reconstructions generated by 3D scanners have artefacts that cause biases in evaluation. To address this issue, we introduce a novel multi-view RGBD dataset captured using a mobile device, which includes highly precise 3D ground-truth annotations for 153 object models featuring a diverse set of 3D structures. We obtain precise 3D ground-truth shape without relying on high-end 3D scanners by utilising LEGO models with known geometry as the 3D structures for image capture. The distinct data modality offered by high- resolution RGB images and low-resolution depth maps captured on a mobile device, when combined with precise 3D geometry annotations, presents a unique opportunity for future research on high-fidelity 3D reconstruction. Furthermore, we evaluate a range of 3D reconstruction algorithms on the proposed dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">339.SteerNeRF: Accelerating NeRF Rendering via Smooth Viewpoint Trajectory</span><br>
                <span class="as">Li, SichengandLi, HaoandWang, YueandLiao, YiyiandYu, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SteerNeRF_Accelerating_NeRF_Rendering_via_Smooth_Viewpoint_Trajectory_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20701-20711.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高Neural Radiance Fields（NeRF）的渲染速度，同时降低内存消耗。<br>
                    动机：现有的加速方法虽然可以加快NeRF的渲染速度，但会消耗大量的内存。<br>
                    方法：利用交互式视点控制中视点变化通常是平滑和连续的这一特性，通过利用前一个视点的信息来减少需要渲染的像素数量以及剩余像素光线上的采样点数量。<br>
                    效果：该方法可以在保持较高渲染质量的同时，显著减少渲染时间和内存开销，实现在1080P图像分辨率下30FPS的渲染速度和低内存占用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) have demonstrated superior novel view synthesis performance but are slow at rendering. To speed up the volume rendering process, many acceleration methods have been proposed at the cost of large memory consumption. To push the frontier of the efficiency-memory trade-off, we explore a new perspective to accelerate NeRF rendering, leveraging a key fact that the viewpoint change is usually smooth and continuous in interactive viewpoint control. This allows us to leverage the information of preceding viewpoints to reduce the number of rendered pixels as well as the number of sampled points along the ray of the remaining pixels. In our pipeline, a low-resolution feature map is rendered first by volume rendering, then a lightweight 2D neural renderer is applied to generate the output image at target resolution leveraging the features of preceding and current frames. We show that the proposed method can achieve competitive rendering quality while reducing the rendering time with little memory overhead, enabling 30FPS at 1080P image resolution with a low memory footprint.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">340.MonoHuman: Animatable Human Neural Field From Monocular Video</span><br>
                <span class="as">Yu, ZhengmingandCheng, WeiandLiu, XianandWu, WayneandLin, Kwan-Yee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16943-16953.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用神经辐射场（NeRF）的表示能力，从单目视频中重建人体，并实现虚拟化身的自由视点控制。<br>
                    动机：目前的虚拟化身动画技术要么依赖于姿态相关的表示，要么由于帧间独立优化而缺乏运动连贯性，难以真实地推广到未见过的序列姿态。<br>
                    方法：提出一种新的框架MonoHuman，通过双向约束对形变场进行建模，并显式利用现成的关键帧信息来推理特征相关性，以获得连贯的结果。<br>
                    效果：实验结果表明，MonoHuman在各种挑战性新姿态设置下，都能生成具有高度保真度和多视图一致性的渲染结果，优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Animating virtual avatars with free-view control is crucial for various applications like virtual reality and digital entertainment. Previous studies have attempted to utilize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation network into the NeRF to further model the dynamics of the human neural field for animating vivid human motions. However, such pipelines either rely on pose-dependent representations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically. In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses. Our key insight is to model the deformation field with bi-directional constraints and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation module, which creates a pose-independent generalizable deformation field by disentangling backward and forward deformation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward Correspondence Search module, which queries the correspondence feature of keyframes to guide the rendering network. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose settings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">341.NeRFLix: High-Quality Neural View Synthesis by Learning a Degradation-Driven Inter-Viewpoint MiXer</span><br>
                <span class="as">Zhou, KunandLi, WenboandWang, YiandHu, TaoandJiang, NianjuanandHan, XiaoguangandLu, Jiangbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_NeRFLix_High-Quality_Neural_View_Synthesis_by_Learning_a_Degradation-Driven_Inter-Viewpoint_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12363-12374.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于NeRF的新颖视图合成方法在真实世界场景中，由于可能的不完美校准信息和场景表示不准确，从源图像恢复高质量细节仍然具有挑战性。<br>
                    动机：尽管有高质量的训练帧，但NeRF模型产生的合成新视图帧仍然存在明显的渲染伪影，如噪声、模糊等。为了提高基于NeRF的方法的合成质量，我们提出了NeRFLiX，一种通用的NeRF无关的恢复器范式，通过学习一个退化驱动的视点间混合器。<br>
                    方法：我们设计了一个NeRF风格的退化建模方法并构建了大规模的训练数据，使得有可能有效地去除现有深度神经网络中的那些NeRF原生渲染伪影。此外，除了退化消除，我们还提出了一个视点间聚合框架，能够融合高度相关的高质量训练图像，将最先进的NeRF模型的性能提升到全新的水平，生成高度逼真的合成图像。<br>
                    效果：实验结果表明，NeRFLiX能够在各种真实世界场景中显著改善基于NeRF的方法的合成质量，同时提高了渲染图像的细节质量和视觉真实性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields(NeRF) show great success in novel-view synthesis. However, in real-world scenes, recovering high-quality details from the source images is still challenging for the existing NeRF-based approaches, due to the potential imperfect calibration information and scene representation inaccuracy. Even with high-quality training frames, the synthetic novel-view frames produced by NeRF models still suffer from notable rendering artifacts, such as noise, blur, etc. Towards to improve the synthesis quality of NeRF-based approaches, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm by learning a degradation-driven inter-viewpoint mixer. Specially, we design a NeRF-style degradation modeling approach and construct large-scale training data, enabling the possibility of effectively removing those NeRF-native rendering artifacts for existing deep neural networks. Moreover, beyond the degradation removal, we propose an inter-viewpoint aggregation framework that is able to fuse highly related high-quality training images, pushing the performance of cutting-edge NeRF models to entirely new levels and producing highly photo-realistic synthetic images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">342.3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels</span><br>
                <span class="as">Weng, ZhenzhenandGorban, AlexanderS.andJi, JingweiandNajibi, MahyarandZhou, YinandAnguelov, Dragomir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1158-1167.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从点云中训练一个3D人体关键点检测器，而无需大量的高质量标签。<br>
                    动机：虽然获取大量的人体点云相对容易，但标注3D关键点既昂贵又主观，且易出错，特别是对于长尾情况（罕见姿势的行人、滑板车手等）。<br>
                    方法：我们提出了GC-KPL——一种受几何一致性启发的关键点学习法，通过创新的无监督损失公式来学习点云中的3D人体关节位置，无需人工标注。<br>
                    效果：我们在Waymo开放数据集上进行大规模训练，即使没有人工标注的关键点，也能取得与全监督方法相当的性能。此外，该方法对下游的关键点少样本学习也有帮助，只需在10%的标注训练数据上进行微调，就能达到与在整个数据集上微调相当的性能。实验证明，当在完整数据集上训练时，GC-KPL比现有技术有大幅度的提升，并能有效地利用大量未标注的数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training a 3D human keypoint detector from point clouds in a supervised manner requires large volumes of high quality labels. While it is relatively easy to capture large amounts of human point clouds, annotating 3D keypoints is expensive, subjective, error prone and especially difficult for long-tail cases (pedestrians with rare poses, scooterists, etc.). In this work, we propose GC-KPL - Geometry Consistency inspired Key Point Leaning, an approach for learning 3D human joint locations from point clouds without human labels. We achieve this by our novel unsupervised loss formulations that account for the structure and movement of the human body. We show that by training on a large training set from Waymo Open Dataset without any human annotated keypoints, we are able to achieve reasonable performance as compared to the fully supervised approach. Further, the backbone benefits from the unsupervised training and is useful in downstream fewshot learning of keypoints, where fine-tuning on only 10 percent of the labeled training data gives comparable performance to fine-tuning on the entire set. We demonstrated that GC-KPL outperforms by a large margin over SoTA when trained on entire dataset and efficiently leverages large volumes of unlabeled data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">343.FLEX: Full-Body Grasping Without Full-Body Grasps</span><br>
                <span class="as">Tendulkar, PurvaandSur{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tendulkar_FLEX_Full-Body_Grasping_Without_Full-Body_Grasps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21179-21189.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成与场景互动的逼真的3D人类化身，特别是在抓取日常物品时。<br>
                    动机：现有的方法需要收集大量的3D人体与物体互动数据集进行训练，但存在无法适应不同物体位置、方向和场景中家具的问题，且生成的全身姿态多样性有限。<br>
                    方法：本文提出了一种新的方法，利用全身姿态和手部抓取的先验知识，通过3D几何约束来合成全身抓取姿态。<br>
                    效果：实验证明，这种方法可以生成各种可行的人体抓取姿态，无论是在数量还是质量上，都优于基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesizing 3D human avatars interacting realistically with a scene is an important problem with applications in AR/VR, video games, and robotics. Towards this goal, we address the task of generating a virtual human -- hands and full body -- grasping everyday objects. Existing methods approach this problem by collecting a 3D dataset of humans interacting with objects and training on this data. However, 1) these methods do not generalize to different object positions and orientations or to the presence of furniture in the scene, and 2) the diversity of their generated full-body poses is very limited. In this work, we address all the above challenges to generate realistic, diverse full-body grasps in everyday scenes without requiring any 3D full-body grasping data. Our key insight is to leverage the existence of both full-body pose and hand-grasping priors, composing them using 3D geometrical constraints to obtain full-body grasps. We empirically validate that these constraints can generate a variety of feasible human grasps that are superior to baselines both quantitatively and qualitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">344.IMP: Iterative Matching and Pose Estimation With Adaptive Pooling</span><br>
                <span class="as">Xue, FeiandBudvytis, IgnasandCipolla, Roberto</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_IMP_Iterative_Matching_and_Pose_Estimation_With_Adaptive_Pooling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21317-21326.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的特征匹配和姿态估计方法效率低下，准确性有限。<br>
                    动机：我们提出了一个迭代匹配和姿态估计框架（IMP），利用两个任务之间的几何关系，通过良好的匹配进行粗略准确的姿态估计，再通过粗略准确的姿态提供几何约束来指导匹配。<br>
                    方法：我们实现了一个具有变换器的几何感知循环模块，该模块联合输出稀疏的匹配和相机姿态。在每个迭代中，我们首先通过姿态一致性损失将几何信息隐式嵌入到模块中，使其能够逐步预测几何感知的匹配。其次，我们引入了有效的IMP（EIMP）来动态地丢弃没有潜在匹配的关键点，避免冗余更新并显著减少变换器中注意力计算的二次时间复杂度。<br>
                    效果：在YFCC100m、Scannet和Aachen Day-Night数据集上的实验表明，所提出的方法在准确性和效率方面优于先前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous methods solve feature matching and pose estimation using a two-stage process by first finding matches and then estimating the pose. As they ignore the geometric relationships between the two tasks, they focus on either improving the quality of matches or filtering potential outliers, leading to limited efficiency or accuracy. In contrast, we propose an iterative matching and pose estimation framework (IMP) leveraging the geometric connections between the two tasks: a few good matches are enough for a roughly accurate pose estimation; a roughly accurate pose can be used to guide the matching by providing geometric constraints. To this end, we implement a geometry-aware recurrent module with transformers which jointly outputs sparse matches and camera poses. Specifically, for each iteration, we first implicitly embed geometric information into the module via a pose-consistency loss, allowing it to predict geometry-aware matches progressively. Second, we introduce an efficient IMP (EIMP) to dynamically discard keypoints without potential matches, avoiding redundant updating and significantly reducing the quadratic time complexity of attention computation in transformers. Experiments on YFCC100m, Scannet, and Aachen Day-Night datasets demonstrate that the proposed method outperforms previous approaches in terms of accuracy and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">345.Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution</span><br>
                <span class="as">Liao, BangyanandQu, DelinandXue, YifeiandZhang, HuiqingandLao, Yizhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4863-4871.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种基于滚动快门相机测量的鲁棒快速束调整解决方案，以估计相机的6自由度位姿和环境的几何形状。<br>
                    动机：解决现有方法依赖额外传感器、高帧率视频输入、对相机运动、读取方向的限制性假设以及效率低下等问题。<br>
                    方法：首先研究了图像点归一化对RSBA性能的影响，并展示了其在模拟真实6自由度相机运动中的更好近似。然后提出了一种新的视觉残差协方差分析模型，用于优化过程中重投影误差的标准，从而提高整体精度。更重要的是，结合归一化和协方差标准化权重的RSBA（NW-RSBA）可以避免常见的平面退化，而无需限制拍摄方式。此外，还提出了一种基于其雅可比矩阵和舒尔补的NW-RSBA加速策略。<br>
                    效果：大量的合成和真实数据实验验证了所提出解决方案在现有技术中的效果和效率。同时证明该方法可以很容易地实现，并可作为著名的GSSfM和GSSLAM系统的补充RSSfM和RSSLAM解决方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a robust and fast bundle adjustment solution that estimates the 6-DoF pose of the camera and the geometry of the environment based on measurements from a rolling shutter (RS) camera. This tackles the challenges in the existing works, namely relying on additional sensors, high frame rate video as input, restrictive assumptions on camera motion, readout direction, and poor efficiency. To this end, we first investigate the influence of normalization to the image point on RSBA performance and show its better approximation in modelling the real 6-DoF camera motion. Then we present a novel analytical model for the visual residual covariance, which can be used to standardize the reprojection error during the optimization, consequently improving the overall accuracy. More importantly, the combination of normalization and covariance standardization weighting in RSBA (NW-RSBA) can avoid common planar degeneracy without needing to constrain the filming manner. Besides, we propose an acceleration strategy for NW-RSBA based on the sparsity of its Jacobian matrix and Schur complement. The extensive synthetic and real data experiments verify the effectiveness and efficiency of the proposed solution over the state-of-the-art works. We also demonstrate the proposed method can be easily implemented and plug-in famous GSSfM and GSSLAM systems as completed RSSfM and RSSLAM solutions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">346.Role of Transients in Two-Bounce Non-Line-of-Sight Imaging</span><br>
                <span class="as">Somasundaram, SiddharthandDave, AkshatandHenley, ConnorandVeeraraghavan, AshokandRaskar, Ramesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Somasundaram_Role_of_Transients_in_Two-Bounce_Non-Line-of-Sight_Imaging_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9192-9201.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用多次散射光对摄像机视野外的被遮挡物体进行成像？<br>
                    动机：最近的研究表明，通过扫描激光并测量场景中两个中继面的被遮挡物体的投影阴影，可以实现两次反弹（2B）NLOS成像。<br>
                    方法：本研究探讨了在多路照明条件下，飞行时间（ToF）测量（即瞬变）在2B-NLOS中的作用。具体来说，我们研究了ToF信息如何减少形状重建所需的测量次数和空间分辨率。<br>
                    效果：我们的研究结果表明，在（1）时间分辨率、（2）空间分辨率和（3）图像捕获次数方面，系统参数的优化可以带来信噪比和可恢复性的最佳平衡。这为未来NLOS成像系统的设计提供了形式化的数学约束条件，特别是在ToF传感器变得越来越普遍的情况下。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of non-line-of-sight (NLOS) imaging is to image objects occluded from the camera's field of view using multiply scattered light. Recent works have demonstrated the feasibility of two-bounce (2B) NLOS imaging by scanning a laser and measuring cast shadows of occluded objects in scenes with two relay surfaces. In this work, we study the role of time-of-flight (ToF) measurements, i.e. transients, in 2B-NLOS under multiplexed illumination. Specifically, we study how ToF information can reduce the number of measurements and spatial resolution needed for shape reconstruction. We present our findings with respect to tradeoffs in (1) temporal resolution, (2) spatial resolution, and (3) number of image captures by studying SNR and recoverability as functions of system parameters. This leads to a formal definition of the mathematical constraints for 2B lidar. We believe that our work lays an analytical groundwork for design of future NLOS imaging systems, especially as ToF sensors become increasingly ubiquitous.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">347.ObjectMatch: Robust Registration Using Canonical Object Correspondences</span><br>
                <span class="as">G\&quot;umeli, CanandDai, AngelaandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gumeli_ObjectMatch_Robust_Registration_Using_Canonical_Object_Correspondences_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13082-13091.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高RGB-D SLAM管道中语义和对象中心的相机位姿估计器的性能。<br>
                    动机：现有的相机位姿估计器依赖于帧之间的重叠区域的直接对应关系，但在重叠区域较小或没有重叠的情况下无法对齐相机帧。<br>
                    方法：通过识别语义对象获取间接对应关系，例如，当一个物体在一帧中从前看到，而在另一帧中从后看到时，可以通过标准物体对应关系提供额外的位姿约束。首先提出一种神经网络在像素级别预测这种对应关系，然后将其与用联合高斯-牛顿优化求解的顶级关键点匹配相结合。<br>
                    效果：在成对设置中，该方法提高了最先进的特征匹配的注册召回率，包括在10%或更少的帧间重叠对中从24%提高到45%。在注册RGB-D序列时，该方法在具有挑战性的低帧率场景中优于尖端的SLAM基线，在多个场景中实现了超过35%的轨迹误差减少。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ObjectMatch, a semantic and object-centric camera pose estimator for RGB-D SLAM pipelines. Modern camera pose estimators rely on direct correspondences of overlapping regions between frames; however, they cannot align camera frames with little or no overlap. In this work, we propose to leverage indirect correspondences obtained via semantic object identification. For instance, when an object is seen from the front in one frame and from the back in another frame, we can provide additional pose constraints through canonical object correspondences. We first propose a neural network to predict such correspondences on a per-pixel level, which we then combine in our energy formulation with state-of-the-art keypoint matching solved with a joint Gauss-Newton optimization. In a pairwise setting, our method improves registration recall of state-of-the-art feature matching, including from 24% to 45% in pairs with 10% or less inter-frame overlap. In registering RGB-D sequences, our method outperforms cutting-edge SLAM baselines in challenging, low-frame-rate scenarios, achieving more than 35% reduction in trajectory error in multiple scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">348.Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo</span><br>
                <span class="as">Wang, YuesongandZeng, ZhaojieandGuan, TaoandYang, WeiandChen, ZhuoandLiu, WenkaiandXu, LuoyuanandLuo, Yawei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adaptive_Patch_Deformation_for_Textureless-Resilient_Multi-View_Stereo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1621-1630.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度学习在大规模无纹理区域多视角立体视觉任务中的性能，同时减少内存消耗。<br>
                    动机：大多数基于学习的多视角立体视觉方法需要构建成本体积并显著增加感受野以获得满意的结果，这在处理大规模无纹理区域时会导致巨大的内存消耗。<br>
                    方法：将深度学习中的变形卷积思想创新地移植到传统的PatchMatch方法中。对于每个具有匹配歧义的像素（称为不可靠的像素），我们自适应地变形其上的补丁，以扩大感受野，直到覆盖足够的相关可靠像素（没有匹配歧义）作为锚点。执行PatchMatch时，受锚像素的限制，不可靠像素的匹配成本保证在正确的深度处达到全局最小值，从而显著提高了多视角立体视觉的鲁棒性。为了检测更多的锚像素以确保更好的自适应补丁变形，我们建议通过检查优化过程中估计深度的收敛来评估某个像素的匹配歧义。<br>
                    效果：该方法在ETH3D和Tanks and Temples上取得了最先进的性能，同时保持了低内存消耗。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, deep learning-based approaches have shown great strength in multi-view stereo because of their outstanding ability to extract robust visual features. However, most learning-based methods need to build the cost volume and increase the receptive field enormously to get a satisfactory result when dealing with large-scale textureless regions, consequently leading to prohibitive memory consumption. To ensure both memory-friendly and textureless-resilient, we innovatively transplant the spirit of deformable convolution from deep learning into the traditional PatchMatch-based method. Specifically, for each pixel with matching ambiguity (termed unreliable pixel), we adaptively deform the patch centered on it to extend the receptive field until covering enough correlative reliable pixels (without matching ambiguity) that serve as anchors. When performing PatchMatch, constrained by the anchor pixels, the matching cost of an unreliable pixel is guaranteed to reach the global minimum at the correct depth and therefore increases the robustness of multi-view stereo significantly. To detect more anchor pixels to ensure better adaptive patch deformation, we propose to evaluate the matching ambiguity of a certain pixel by checking the convergence of the estimated depth as optimization proceeds. As a result, our method achieves state-of-the-art performance on ETH3D and Tanks and Temples while preserving low memory consumption.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">349.SeaThru-NeRF: Neural Radiance Fields in Scattering Media</span><br>
                <span class="as">Levy, DeborahandPeleg, AmitandPearl, NaamaandRosenbaum, DanandAkkaynak, DeryaandKorman, SimonandTreibitz, Tali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/56-65.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经辐射场（NeRF）模型在处理水下或雾天场景时，由于介质对物体外观的强烈影响，其表现效果不佳。<br>
                    动机：基于体积渲染的NeRF框架具有考虑介质效应的内在能力，但目前尚未有模型能够妥善处理这种情况。<br>
                    方法：我们开发了一种新的散射介质中的NeRF渲染模型，该模型基于SeaThru图像形成模型，并提出了学习场景信息和介质参数的合适架构。<br>
                    效果：通过模拟和真实世界的场景，我们证明了该方法的有效性，能够在水下正确渲染出新颖的真实感视图。更令人兴奋的是，我们可以清晰地渲染这些场景，消除了摄像机和场景之间的介质，重建了被介质严重遮挡的远处物体的外观和深度。我们的代码和独特的数据集可以在项目的网站上找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Research on neural radiance fields (NeRFs) for novel view generation is exploding with new models and extensions. However, a question that remains unanswered is what happens in underwater or foggy scenes where the medium strongly influences the appearance of objects. Thus far, NeRF and its variants have ignored these cases. However, since the NeRF framework is based on volumetric rendering, it has inherent capability to account for the medium's effects, once modeled appropriately. We develop a new rendering model for NeRFs in scattering media, which is based on the SeaThru image formation model, and suggest a suitable architecture for learning both scene information and medium parameters. We demonstrate the strength of our method using simulated and real-world scenes, correctly rendering novel photorealistic views underwater. Even more excitingly, we can render clear views of these scenes, removing the medium between the camera and the scene and reconstructing the appearance and depth of far objects, which are severely occluded by the medium. Our code and unique datasets are available on the project's website.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">350.Human Pose Estimation in Extremely Low-Light Conditions</span><br>
                <span class="as">Lee, SohyunandRim, JaesungandJeong, BoseungandKim, GeonuandWoo, ByungjuandLee, HaechanandCho, SunghyunandKwak, Suha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Human_Pose_Estimation_in_Extremely_Low-Light_Conditions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/704-714.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究在极低光照条件下的人体姿态估计。<br>
                    动机：由于收集具有准确标签的真实低光照图像困难，以及严重损坏的输入会显著降低预测质量，因此这个任务具有挑战性。<br>
                    方法：我们开发了一个专用的相机系统，并建立了一个新的真实低光照图像数据集，其中包含准确的人体姿态标签。此外，我们还提出了一种新的模型和训练策略，充分利用这些特权信息来学习对光照条件不敏感的表示。<br>
                    效果：我们的方法是在真实的极低光照图像上表现出色，广泛的分析验证了我们的模型和数据集对此成功的贡献。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study human pose estimation in extremely low-light images. This task is challenging due to the difficulty of collecting real low-light images with accurate labels, and severely corrupted inputs that degrade prediction quality significantly. To address the first issue, we develop a dedicated camera system and build a new dataset of real low-light images with accurate pose labels. Thanks to our camera system, each low-light image in our dataset is coupled with an aligned well-lit image, which enables accurate pose labeling and is used as privileged information during training. We also propose a new model and a new training strategy that fully exploit the privileged information to learn representation insensitive to lighting conditions. Our method demonstrates outstanding performance on real extremely low-light images, and extensive analyses validate that both of our model and dataset contribute to the success.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">351.EventNeRF: Neural Radiance Fields From a Single Colour Event Camera</span><br>
                <span class="as">Rudnev, ViktorandElgharib, MohamedandTheobalt, ChristianandGolyanik, Vladislav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rudnev_EventNeRF_Neural_Radiance_Fields_From_a_Single_Colour_Event_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4992-5002.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于事件的3D重建方法只能恢复稀疏的点云，这对于许多计算机视觉和图形应用来说是一个限制。<br>
                    动机：为了解决这个问题，本文提出了一种使用单一颜色事件流作为输入进行3D一致、密集和照片真实感的新视图合成的方法。<br>
                    方法：该方法的核心是一种完全在自监督方式下从事件中训练的神经辐射场，同时保留了彩色事件通道的原始分辨率。此外，我们的光线采样策略也是针对事件的，可以实现数据高效的训练。<br>
                    效果：在测试中，我们的方法在RGB空间中产生了前所未有的高质量结果。我们在几个具有挑战性的合成场景和真实场景上进行了定性和定量评估，结果显示，我们的方法产生的渲染结果比现有方法更密集，视觉效果更好。我们还展示了在快速运动和低光照条件下的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Asynchronously operating event cameras find many applications due to their high dynamic range, vanishingly low motion blur, low latency and low data bandwidth. The field saw remarkable progress during the last few years, and existing event-based 3D reconstruction approaches recover sparse point clouds of the scene. However, such sparsity is a limiting factor in many cases, especially in computer vision and graphics, that has not been addressed satisfactorily so far. Accordingly, this paper proposes the first approach for 3D-consistent, dense and photorealistic novel view synthesis using just a single colour event stream as input. At its core is a neural radiance field trained entirely in a self-supervised manner from events while preserving the original resolution of the colour event channels. Next, our ray sampling strategy is tailored to events and allows for data-efficient training. At test, our method produces results in the RGB space at unprecedented quality. We evaluate our method qualitatively and numerically on several challenging synthetic and real scenes and show that it produces significantly denser and more visually appealing renderings than the existing methods. We also demonstrate robustness in challenging scenarios with fast motion and under low lighting conditions. We release the newly recorded dataset and our source code to facilitate the research field, see https://4dqv.mpi-inf.mpg.de/EventNeRF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">352.Representing Volumetric Videos As Dynamic MLP Maps</span><br>
                <span class="as">Peng, SidaandYan, YunzhiandShuai, QingandBao, HujunandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4252-4262.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地实时合成动态场景的体积视频。<br>
                    动机：现有的神经场景表示方法在模拟和渲染复杂静态场景方面表现出色，但将其扩展到表示动态场景并不简单，因为其渲染速度慢或存储成本高。<br>
                    方法：将每帧的辐射场表示为一组浅层MLP网络，这些网络的参数存储在称为MLP图的二维网格中，并由所有帧共享的二维CNN解码器动态预测。<br>
                    效果：实验表明，该方法在NHR和ZJU-MoCap数据集上实现了最先进的渲染质量，同时具有高效的实时渲染能力，在RTX 3090 GPU上对512 x 512图像的渲染速度达到41.7 fps。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces a novel representation of volumetric videos for real-time view synthesis of dynamic scenes. Recent advances in neural scene representations demonstrate their remarkable capability to model and render complex static scenes, but extending them to represent dynamic scenes is not straightforward due to their slow rendering speed or high storage cost. To solve this problem, our key idea is to represent the radiance field of each frame as a set of shallow MLP networks whose parameters are stored in 2D grids, called MLP maps, and dynamically predicted by a 2D CNN decoder shared by all frames. Representing 3D scenes with shallow MLPs significantly improves the rendering speed, while dynamically predicting MLP parameters with a shared 2D CNN instead of explicitly storing them leads to low storage cost. Experiments show that the proposed approach achieves state-of-the-art rendering quality on the NHR and ZJU-MoCap datasets, while being efficient for real-time rendering with a speed of 41.7 fps for 512 x 512 images on an RTX 3090 GPU. The code is available at https://zju3dv.github.io/mlp_maps/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">353.ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction</span><br>
                <span class="as">Yu, ZhengdiandHuang, ShaoliandFang, ChenandBreckon, TobyP.andWang, Jue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_ACR_Attention_Collaboration-Based_Regressor_for_Arbitrary_Two-Hand_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12955-12964.png><br>
            
            <span class="tt"><span class="t0">研究问题：从单目RGB图像重建两只手是具有挑战性的，因为经常会发生遮挡和相互混淆。<br>
                    动机：现有的方法主要学习一种纠缠的表示来编码两只交互的手，这种方法对于受损的交互（如截断的手、分开的手或外部遮挡）非常脆弱。<br>
                    方法：本文提出了ACR（基于注意力协作的回归器），这是首次尝试在任意场景中重建手。为了实现这一目标，ACR通过利用中心和部分关注进行特征提取，显式地减轻了手与手之间以及部分之间的相互依赖性。<br>
                    效果：我们在各种类型的手重建数据集上评估我们的方法。我们的方法在InterHand2.6M数据集上显著优于最好的交互手方法，同时在FreiHand数据集上与最先进的单手方法产生相当的性能。在野外和手-物体交互数据集以及网络图像/视频上的更多定性结果进一步证明了我们的方法在任意手重建中的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">354.Bringing Inputs to Shared Domains for 3D Interacting Hands Recovery in the Wild</span><br>
                <span class="as">Moon, Gyeongsik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Moon_Bringing_Inputs_to_Shared_Domains_for_3D_Interacting_Hands_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17028-17037.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D交互手部恢复方法主要在运动捕捉（MoCap）环境中表现良好，但在自然环境（ITW）中的表现较差。<br>
                    动机：收集自然环境下的3D交互手部数据非常具有挑战性，即使对于2D数据也是如此。<br>
                    方法：我们提出了InterWild，它将MoCap和ITW样本带到共享领域，以有限的ITW 2D/3D交互手部数据实现在自然环境中的鲁棒3D交互手部恢复。<br>
                    效果：实验结果表明，InterWild在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite recent achievements, existing 3D interacting hands recovery methods have shown results mainly on motion capture (MoCap) environments, not on in-the-wild (ITW) ones. This is because collecting 3D interacting hands data in the wild is extremely challenging, even for the 2D data. We present InterWild, which brings MoCap and ITW samples to shared domains for robust 3D interacting hands recovery in the wild with a limited amount of ITW 2D/3D interacting hands data. 3D interacting hands recovery consists of two sub-problems: 1) 3D recovery of each hand and 2) 3D relative translation recovery between two hands. For the first sub-problem, we bring MoCap and ITW samples to a shared 2D scale space. Although ITW datasets provide a limited amount of 2D/3D interacting hands, they contain large-scale 2D single hand data. Motivated by this, we use a single hand image as an input for the first sub-problem regardless of whether two hands are interacting. Hence, interacting hands of MoCap datasets are brought to the 2D scale space of single hands of ITW datasets. For the second sub-problem, we bring MoCap and ITW samples to a shared appearance-invariant space. Unlike the first sub-problem, 2D labels of ITW datasets are not helpful for the second sub-problem due to the 3D translation's ambiguity. Hence, instead of relying on ITW samples, we amplify the generalizability of MoCap samples by taking only a geometric feature without an image as an input for the second sub-problem. As the geometric feature is invariant to appearances, MoCap and ITW samples do not suffer from a huge appearance gap between the two datasets. The code is available in https://github.com/facebookresearch/InterWild.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">355.NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors</span><br>
                <span class="as">Deng, CongyueandJiang, Chiyu{\textquotedblleft</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_NeRDi_Single-View_NeRF_Synthesis_With_Language-Guided_Diffusion_As_General_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20637-20647.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>2D-to-3D reconstruction is an ill-posed problem, yet humans are good at solving this problem due to their prior knowledge of the 3D world developed over years. Driven by this observation, we propose NeRDi, a single-view NeRF synthesis framework with general image priors from 2D diffusion models. Formulating single-view reconstruction as an image-conditioned 3D generation problem, we optimize the NeRF representations by minimizing a diffusion loss on its arbitrary view renderings with a pretrained image diffusion model under the input-view constraint. We leverage off-the-shelf vision-language models and introduce a two-section language guidance as conditioning inputs to the diffusion model. This is essentially helpful for improving multiview content coherence as it narrows down the general image prior conditioned on the semantic and visual features of the single-view input image. Additionally, we introduce a geometric loss based on estimated depth maps to regularize the underlying 3D geometry of the NeRF. Experimental results on the DTU MVS dataset show that our method can synthesize novel views with higher quality even compared to existing methods trained on this dataset. We also demonstrate our generalizability in zero-shot NeRF synthesis for in-the-wild images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">356.TRACE: 5D Temporal Regression of Avatars With Dynamic Cameras in 3D Environments</span><br>
                <span class="as">Sun, YuandBao, QianandLiu, WuandMei, TaoandBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8856-8866.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前的方法无法可靠地估计移动中的人类在全局坐标中的姿态和形状，特别是在相机也在移动时。<br>
                    动机：为了解决这些问题，我们采用了一种新的5D表示（空间、时间和身份），使场景中的人能够进行端到端推理。<br>
                    方法：我们的方法名为TRACE，引入了几个新的架构组件。最重要的是，它使用两个新的“地图”来推理人在相机和世界坐标中随时间变化的3D轨迹。此外，额外的记忆单元可以在长时间的遮挡期间持续跟踪人。<br>
                    效果：TRACE是第一个从动态相机中联合恢复和跟踪3D人类在全局坐标中的方法。通过端到端的训练和使用完整的图像信息，TRACE在跟踪和HPS基准测试上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several novel architectural components. Most importantly, it uses two new "maps" to reason about the 3D trajectory of people over time in camera, and world, coordinates. An additional memory unit enables persistent tracking of people even during long occlusions. TRACE is the first one-stage method to jointly recover and track 3D humans in global coordinates from dynamic cameras. By training it end-to-end, and using full image information, TRACE achieves state-of-the-art performance on tracking and HPS benchmarks. The code and dataset are released for research purposes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">357.Neural Kernel Surface Reconstruction</span><br>
                <span class="as">Huang, JiahuiandGojcic, ZanandAtzmon, MatanandLitany, OrandFidler, SanjaandWilliams, Francis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4369-4379.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从大规模、稀疏和嘈杂的点云中重建3D隐式表面？<br>
                    动机：现有的方法（神经核场）在处理大规模场景、噪声以及训练需求上存在限制。<br>
                    方法：提出一种新方法，通过使用紧凑支持的核函数来扩展大型场景的处理能力，通过梯度拟合解法提高对噪声的鲁棒性，并通过最小化训练需求，使得该方法能够学习任何稠密有向点的数据集。<br>
                    效果：该方法能够在几秒钟内重建数百万个点，并能以超出核心的方式处理非常大的场景。在单物体（ShapeNet, ABC）、室内场景（ScanNet, Matterport3D）和室外场景（CARLA, Waymo）的重建基准测试中取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel method for reconstructing a 3D implicit surface from a large-scale, sparse, and noisy point cloud. Our approach builds upon the recently introduced Neural Kernel Fields (NKF) representation. It enjoys similar generalization capabilities to NKF, while simultaneously addressing its main limitations: (a) We can scale to large scenes through compactly supported kernel functions, which enable the use of memory-efficient sparse linear solvers. (b) We are robust to noise, through a gradient fitting solve. (c) We minimize training requirements, enabling us to learn from any dataset of dense oriented points, and even mix training data consisting of objects and scenes at different scales. Our method is capable of reconstructing millions of points in a few seconds, and handling very large scenes in an out-of-core fashion. We achieve state-of-the-art results on reconstruction benchmarks consisting of single objects (ShapeNet, ABC), indoor scenes (ScanNet, Matterport3D), and outdoor scenes (CARLA, Waymo).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">358.Learning 3D-Aware Image Synthesis With Unknown Pose Distribution</span><br>
                <span class="as">Shi, ZifanandShen, YujunandXu, YinghaoandPeng, SidaandLiao, YiyiandGuo, ShengandChen, QifengandYeung, Dit-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Learning_3D-Aware_Image_Synthesis_With_Unknown_Pose_Distribution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13062-13071.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D感知图像合成方法大多依赖于预先在训练集上估计的3D姿态分布，不准确的估计可能会误导模型学习错误的几何形状。<br>
                    动机：本文提出了一种名为PoF3D的新方法，该方法可以使生成的辐射场摆脱对3D姿态先验的需求。<br>
                    方法：首先，我们在生成器中配备了一个高效的姿位学习器，该学习器能够从潜在代码中推断出姿态，从而自动近似真实的基本姿态分布。然后，我们为判别器分配了一个任务，即在生成器的监督下学习姿态分布，并利用预测的姿态作为条件区分真实和合成的图像。最后，我们将无姿态的生成器和有姿态意识的判别器以对抗的方式联合训练。<br>
                    效果：在几个数据集上的大量实验结果证实，无论是在图像质量还是几何质量方面，我们的方法都与最先进的技术相媲美。据我们所知，PoF3D首次证明了无需使用3D姿态先验就能学习高质量的3D感知图像合成的可行性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods for 3D-aware image synthesis largely depend on the 3D pose distribution pre-estimated on the training set. An inaccurate estimation may mislead the model into learning faulty geometry. This work proposes PoF3D that frees generative radiance fields from the requirements of 3D pose priors. We first equip the generator with an efficient pose learner, which is able to infer a pose from a latent code, to approximate the underlying true pose distribution automatically. We then assign the discriminator a task to learn pose distribution under the supervision of the generator and to differentiate real and synthesized images with the predicted pose as the condition. The pose-free generator and the pose-aware discriminator are jointly trained in an adversarial manner. Extensive results on a couple of datasets confirm that the performance of our approach, regarding both image quality and geometry quality, is on par with state of the art. To our best knowledge, PoF3D demonstrates the feasibility of learning high-quality 3D-aware image synthesis without using 3D pose priors for the first time. Project page can be found at https://vivianszf.github.io/pof3d/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">359.CAPE: Camera View Position Embedding for Multi-View 3D Object Detection</span><br>
                <span class="as">Xiong, KaixinandGong, ShiandYe, XiaoqingandTan, XiaoandWan, JiandDing, ErruiandWang, JingdongandBai, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_CAPE_Camera_View_Position_Embedding_for_Multi-View_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21570-21579.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从多视角图像中检测3D物体的问题。<br>
                    动机：目前的基于查询的方法依赖于全局3D位置嵌入来学习图像和3D空间之间的几何对应关系，但这种方法存在学习视图变换困难的问题。<br>
                    方法：提出一种基于相机视图位置嵌入（CAPE）的新方法，通过在局部相机视图坐标系统中形成3D位置嵌入，避免了编码相机外参数的困扰。同时，通过利用前几帧的目标查询和编码自我运动，将CAPE扩展到时间建模，以提高3D物体检测的性能。<br>
                    效果：实验结果表明，CAPE在无LiDAR的标准nuScenes数据集上达到了最先进的性能（61.0% NDS和52.5% mAP）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we address the problem of detecting 3D objects from multi-view images. Current query-based methods rely on global 3D position embeddings (PE) to learn the geometric correspondence between images and 3D space. We claim that directly interacting 2D image features with global 3D PE could increase the difficulty of learning view transformation due to the variation of camera extrinsics. Thus we propose a novel method based on CAmera view Position Embedding, called CAPE. We form the 3D position embeddings under the local camera-view coordinate system instead of the global coordinate system, such that 3D position embedding is free of encoding camera extrinsic parameters. Furthermore, we extend our CAPE to temporal modeling by exploiting the object queries of previous frames and encoding the ego motion for boosting 3D object detection. CAPE achieves the state-of-the-art performance (61.0% NDS and 52.5% mAP) among all LiDAR-free methods on standard nuScenes dataset. Codes and models are available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">360.FlexNeRF: Photorealistic Free-Viewpoint Rendering of Moving Humans From Sparse Views</span><br>
                <span class="as">Jayasundara, VinojandAgrawal, AmitandHeron, NicolasandShrivastava, AbhinavandDavis, LarryS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jayasundara_FlexNeRF_Photorealistic_Free-Viewpoint_Rendering_of_Moving_Humans_From_Sparse_Views_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21118-21127.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目视频中实现运动人物的逼真自由视点渲染。<br>
                    动机：在目标展示快速/复杂运动时，稀疏视图是一个具有挑战性的场景。<br>
                    方法：我们提出了一种新颖的方法，联合优化一个标准的时间和姿势配置，以及一个与姿势相关的动作场和与姿势无关的时间变形，相互补充。<br>
                    效果：由于我们新颖的时间和周期性一致性约束以及在中间表示（如分割）上的额外损失，我们的方法在观察到的视图变得稀疏时提供了高质量的输出。我们在公共基准数据集以及一个自我捕捉的时尚数据集上进行实证测试，证明该方法明显优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present FlexNeRF, a method for photorealistic free-viewpoint rendering of humans in motion from monocular videos. Our approach works well with sparse views, which is a challenging scenario when the subject is exhibiting fast/complex motions. We propose a novel approach which jointly optimizes a canonical time and pose configuration, with a pose-dependent motion field and pose-independent temporal deformations complementing each other. Thanks to our novel temporal and cyclic consistency constraints along with additional losses on intermediate representation such as segmentation, our approach provides high quality outputs as the observed views become sparser. We empirically demonstrate that our method significantly outperforms the state-of-the-art on public benchmark datasets as well as a self-captured fashion dataset. The project page is available at: https://flex-nerf.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">361.DIFu: Depth-Guided Implicit Function for Clothed Human Reconstruction</span><br>
                <span class="as">Song, Dae-YoungandLee, HeeKyungandSeo, JeongilandCho, Donghyeon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_DIFu_Depth-Guided_Implicit_Function_for_Clothed_Human_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8738-8747.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单个图像进行穿衣人类重建的隐式函数（IF）方法。<br>
                    动机：现有的大多数方法依赖于使用体积的3D嵌入分支，如SMPL模型，以补偿单个图像中信息的不足。<br>
                    方法：本文提出了一种新的基于IF的方法DIFu，该方法利用包含纹理和非参数化人类3D信息的项目深度先验。具体来说，DIFu由生成器、占用预测网络和纹理预测网络组成。生成器接收人的正面RGB图像作为输入，并产生人的背面图像。然后，估计并投影到3D体积空间的前/后图像的深度图。最后，占用预测网络通过2D编码器和3D编码器分别提取像素对齐特征和体素对齐特征，并使用这些特征估计占用情况。<br>
                    效果：通过与最近的基于IF的模型进行定量和定性比较，证明了DIFu的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, implicit function (IF)-based methods for clothed human reconstruction using a single image have received a lot of attention. Most existing methods rely on a 3D embedding branch using volume such as the skinned multi-person linear (SMPL) model, to compensate for the lack of information in a single image. Beyond the SMPL, which provides skinned parametric human 3D information, in this paper, we propose a new IF-based method, DIFu, that utilizes a projected depth prior containing textured and non-parametric human 3D information. In particular, DIFu consists of a generator, an occupancy prediction network, and a texture prediction network. The generator takes an RGB image of the human front-side as input, and hallucinates the human back-side image. After that, depth maps for front/back images are estimated and projected into 3D volume space. Finally, the occupancy prediction network extracts a pixel-aligned feature and a voxel-aligned feature through a 2D encoder and a 3D encoder, respectively, and estimates occupancy using these features. Note that voxel-aligned features are obtained from the projected depth maps, thus it can contain detailed 3D information such as hair and cloths. Also, colors of each 3D point are also estimated with the texture inference branch. The effectiveness of DIFu is demonstrated by comparing to recent IF-based models quantitatively and qualitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">362.Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment</span><br>
                <span class="as">Ma, BaoruiandZhou, JunshengandLiu, Yu-ShenandHan, Zhizhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Towards_Better_Gradient_Consistency_for_Neural_Signed_Distance_Functions_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17724-17734.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地从点云或多视图图像中推断出神经符号距离函数（SDFs）。<br>
                    动机：尽管神经符号距离函数在表示几何细节方面表现出了显著的能力，但在没有符号距离监督的情况下，使用神经网络从点云或多视图图像中推断出SDFs仍然是一个挑战。<br>
                    方法：我们提出了一种水平集对齐损失来评估水平集的平行性，通过最小化这个损失可以实现更好的梯度一致性。我们的创新之处在于，我们可以通过自适应地约束查询点的梯度及其在零水平集上的投影，将所有的水平集对齐到零水平集。<br>
                    效果：我们的数值和视觉比较表明，我们的损失可以在各种基准下显著提高从点云或多视图图像中推断出的SDFs的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural signed distance functions (SDFs) have shown remarkable capability in representing geometry with details. However, without signed distance supervision, it is still a challenge to infer SDFs from point clouds or multi-view images using neural networks. In this paper, we claim that gradient consistency in the field, indicated by the parallelism of level sets, is the key factor affecting the inference accuracy. Hence, we propose a level set alignment loss to evaluate the parallelism of level sets, which can be minimized to achieve better gradient consistency. Our novelty lies in that we can align all level sets to the zero level set by constraining gradients at queries and their projections on the zero level set in an adaptive way. Our insight is to propagate the zero level set to everywhere in the field through consistent gradients to eliminate uncertainty in the field that is caused by the discreteness of 3D point clouds or the lack of observations from multi-view images. Our proposed loss is a general term which can be used upon different methods to infer SDFs from 3D point clouds and multi-view images. Our numerical and visual comparisons demonstrate that our loss can significantly improve the accuracy of SDFs inferred from point clouds or multi-view images under various benchmarks. Code and data are available at https://github.com/mabaorui/TowardsBetterGradient.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">363.Vid2Avatar: 3D Avatar Reconstruction From Videos in the Wild via Self-Supervised Scene Decomposition</span><br>
                <span class="as">Guo, ChenandJiang, TianjianandChen, XuandSong, JieandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12858-12868.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目野外视频中学习人类化身。<br>
                    动机：重建自然移动的人类的挑战性很大，需要准确地将人类与任意背景分离，并从短的视频序列中重建详细的3D表面。<br>
                    方法：提出了Vid2Avatar方法，通过在场景中共同对人和背景进行建模，直接在3D中解决场景分解和表面重建的任务，无需任何地面真值监督或从大量穿衣人类扫描数据集提取的先验知识，也不依赖任何外部分割模块。<br>
                    效果：评估结果显示，该方法在公开可用的数据集上优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Vid2Avatar, a method to learn human avatars from monocular in-the-wild videos. Reconstructing humans that move naturally from monocular in-the-wild videos is difficult. Solving it requires accurately separating humans from arbitrary backgrounds. Moreover, it requires reconstructing detailed 3D surface from short video sequences, making it even more challenging. Despite these challenges, our method does not require any groundtruth supervision or priors extracted from large datasets of clothed human scans, nor do we rely on any external segmentation modules. Instead, it solves the tasks of scene decomposition and surface reconstruction directly in 3D by modeling both the human and the background in the scene jointly, parameterized via two separate neural fields. Specifically, we define a temporally consistent human representation in canonical space and formulate a global optimization over the background model, the canonical human shape and texture, and per-frame human pose parameters. A coarse-to-fine sampling strategy for volume rendering and novel objectives are introduced for a clean separation of dynamic human and static background, yielding detailed and robust 3D human reconstructions. The evaluation of our method shows improvements over prior art on publicly available datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">364.PixHt-Lab: Pixel Height Based Light Effect Generation for Image Compositing</span><br>
                <span class="as">Sheng, YichenandZhang, JianmingandPhilip, JulienandHold-Geoffroy, YannickandSun, XinandZhang, HeandLing, LuandBenes, Bedrich</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_PixHt-Lab_Pixel_Height_Based_Light_Effect_Generation_for_Image_Compositing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16643-16653.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成更真实的合成图像中的光影效果，如阴影和反射？<br>
                    动机：传统的计算机图形学使用基于物理的渲染器和3D几何来生成光影效果，但在2D图像合成中，由于缺乏几何信息，导致生成的软阴影和反射效果有限。<br>
                    方法：提出PixHt-Lab系统，通过将像素高度表示映射到3D空间，重建剪切和背景几何，为图像合成渲染真实、多样的光影效果。对于具有基于物理的材料的表面，可以渲染不同光泽度的反射。为了生成更真实的软阴影，进一步提出了使用3D感知缓冲通道来指导神经渲染器的方法。<br>
                    效果：实验结果表明，PixHt-Lab显著提高了软阴影的生成质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Lighting effects such as shadows or reflections are key in making synthetic images realistic and visually appealing. To generate such effects, traditional computer graphics uses a physically-based renderer along with 3D geometry. To compensate for the lack of geometry in 2D Image compositing, recent deep learning-based approaches introduced a pixel height representation to generate soft shadows and reflections. However, the lack of geometry limits the quality of the generated soft shadows and constrains reflections to pure specular ones. We introduce PixHt-Lab, a system leveraging an explicit mapping from pixel height representation to 3D space. Using this mapping, PixHt-Lab reconstructs both the cutout and background geometry and renders realistic, diverse, lighting effects for image compositing. Given a surface with physically-based materials, we can render reflections with varying glossiness. To generate more realistic soft shadows, we further propose to use 3D-aware buffer channels to guide a neural renderer. Both quantitative and qualitative evaluations demonstrate that PixHt-Lab significantly improves soft shadow generation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">365.vMAP: Vectorised Object Mapping for Neural Field SLAM</span><br>
                <span class="as">Kong, XinandLiu, ShikunandTaher, MarwanandDavison, AndrewJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/952-961.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行对象级的稠密SLAM系统。<br>
                    动机：现有的稠密SLAM系统需要3D先验信息，而我们的目标是在没有这些信息的情况下进行高效的建模。<br>
                    方法：提出了vMAP，一个使用神经场表示的对象级稠密SLAM系统。每个对象由一个小的多层感知器（MLP）表示，无需3D先验信息即可实现高效、严密的对象建模。<br>
                    效果：实验证明，与先前的神经场SLAM系统相比，vMAP在场景级和对象级重建质量上都有显著提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present vMAP, an object-level dense SLAM system using neural field representations. Each object is represented by a small MLP, enabling efficient, watertight object modelling without the need for 3D priors. As an RGB-D camera browses a scene with no prior information, vMAP detects object instances on-the-fly, and dynamically adds them to its map. Specifically, thanks to the power of vectorised training, vMAP can optimise as many as 50 individual objects in a single scene, with an extremely efficient training speed of 5Hz map update. We experimentally demonstrate significantly improved scene-level and object-level reconstruction quality compared to prior neural field SLAM systems. Project page: https://kxhit.github.io/vMAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">366.Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</span><br>
                <span class="as">Chen, YueandChen, XingyuandWang, XuanandZhang, QiandGuo, YuandShan, YingandWang, Fei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8264-8273.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的神经辐射场（NeRF）模型在实现照片级新视角合成时，需要准确的相机姿态，这限制了其应用。<br>
                    动机：尽管已有的联合学习神经3D表示和注册相机帧的分析-合成扩展方法存在，但如果初始化不良，它们容易产生次优解。<br>
                    方法：我们提出了L2G-NeRF，一种用于束调整神经辐射场的局部到全局配准方法：首先进行像素级的灵活对齐，然后进行帧级的约束参数对齐。像素级的局部对齐通过优化光度重建误差的深度网络进行无监督学习。帧级的全局对齐通过对像素级对应关系使用可微参数估计求解器来找到全局变换。<br>
                    效果：我们在合成数据和真实世界数据上的实验表明，我们的方法在高保真重建和解决大相机姿态错位方面优于当前最先进的方法。我们的模块是一个易于使用的插件，可以应用于NeRF变体和其他神经场应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) have achieved photorealistic novel views synthesis; however, the requirement of accurate camera poses limits its application. Despite analysis-by-synthesis extensions for jointly learning neural 3D representations and registering camera frames exist, they are susceptible to suboptimal solutions if poorly initialized. We propose L2G-NeRF, a Local-to-Global registration method for bundle-adjusting Neural Radiance Fields: first, a pixel-wise flexible alignment, followed by a frame-wise constrained parametric alignment. Pixel-wise local alignment is learned in an unsupervised way via a deep network which optimizes photometric reconstruction errors. Frame-wise global alignment is performed using differentiable parameter estimation solvers on the pixel-wise correspondences to find a global transformation. Experiments on synthetic and real-world data show that our method outperforms the current state-of-the-art in terms of high-fidelity reconstruction and resolving large camera pose misalignment. Our module is an easy-to-use plugin that can be applied to NeRF variants and other neural field applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">367.DC2: Dual-Camera Defocus Control by Learning To Refocus</span><br>
                <span class="as">Alzayer, HadiandAbuolaim, AbdullahandChan, LeungChunandYang, YangandLou, YingChenandHuang, Jia-BinandKar, Abhishek</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Alzayer_DC2_Dual-Camera_Defocus_Control_by_Learning_To_Refocus_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21488-21497.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过软硬件的进步，使智能手机相机的多功能性和质量越来越接近专业相机。<br>
                    动机：虽然现在的手机相机已经非常先进，但固定的光圈仍然是一个重要的限制，它阻止了用户控制拍摄图像的景深（DoF）。同时，许多智能手机现在都有多个具有不同固定光圈的摄像头。<br>
                    方法：我们提出了DC^2系统，这是一个用于合成改变相机光圈、焦距和任意散焦效果的系统，通过融合来自这种双摄像头系统的信息来实现。<br>
                    效果：我们在真实世界的数据集上进行了定量和定性的评估，结果显示我们的系统在散焦去模糊、虚化渲染和图像重聚焦方面优于最先进的技术。最后，我们还展示了由我们的方法实现的创新后捕获散焦控制，包括倾斜移位和基于内容散焦效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Smartphone cameras today are increasingly approaching the versatility and quality of professional cameras through a combination of hardware and software advancements. However, fixed aperture remains a key limitation, preventing users from controlling the depth of field (DoF) of captured images. At the same time, many smartphones now have multiple cameras with different fixed apertures - specifically, an ultra-wide camera with wider field of view and deeper DoF and a higher resolution primary camera with shallower DoF. In this work, we propose DC^2, a system for defocus control for synthetically varying camera aperture, focus distance and arbitrary defocus effects by fusing information from such a dual-camera system. Our key insight is to leverage real-world smartphone camera dataset by using image refocus as a proxy task for learning to control defocus. Quantitative and qualitative evaluations on real-world data demonstrate our system's efficacy where we outperform state-of-the-art on defocus deblurring, bokeh rendering, and image refocus. Finally, we demonstrate creative post-capture defocus control enabled by our method, including tilt-shift and content-based defocus effects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">368.Looking Through the Glass: Neural Surface Reconstruction Against High Specular Reflections</span><br>
                <span class="as">Qiu, JiaxiongandJiang, Peng-TaoandZhu, YifanandYin, Ze-XinandCheng, Ming-MingandRen, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_Looking_Through_the_Glass_Neural_Surface_Reconstruction_Against_High_Specular_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20823-20833.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经隐式方法在轻微高光下的3D物体表面重建质量较高，但在通过眼镜捕捉目标物体时，会出现复杂的高光反射（HSR）现象，导致重建结果不准确。<br>
                    动机：为了解决这一问题，我们提出了一种新的基于隐式神经渲染的表面重建框架NeuS-HSR。<br>
                    方法：在NeuS-HSR中，我们将物体表面参数化为隐式符号距离函数（SDF）。为了减少HSR的干扰，我们将渲染图像分解为目标物体和辅助平面两种外观。我们设计了一个新的辅助平面模块，结合物理假设和神经网络生成辅助平面外观。<br>
                    效果：我们在合成和真实世界的数据集上进行了广泛的实验，结果表明，NeuS-HSR在对抗HSR的情况下，对目标表面的准确和鲁棒重建优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural implicit methods have achieved high-quality 3D object surfaces under slight specular highlights. However, high specular reflections (HSR) often appear in front of target objects when we capture them through glasses. The complex ambiguity in these scenes violates the multi-view consistency, then makes it challenging for recent methods to reconstruct target objects correctly. To remedy this issue, we present a novel surface reconstruction framework, NeuS-HSR, based on implicit neural rendering. In NeuS-HSR, the object surface is parameterized as an implicit signed distance function (SDF). To reduce the interference of HSR, we propose decomposing the rendered image into two appearances: the target object and the auxiliary plane. We design a novel auxiliary plane module by combining physical assumptions and neural networks to generate the auxiliary plane appearance. Extensive experiments on synthetic and real-world datasets demonstrate that NeuS-HSR outperforms state-of-the-art approaches for accurate and robust target surface reconstruction against HSR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">369.Decoupling Human and Camera Motion From Videos in the Wild</span><br>
                <span class="as">Ye, VickieandPavlakos, GeorgiosandMalik, JitendraandKanazawa, Angjoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21222-21232.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从野外视频中重建全球人类轨迹？<br>
                    动机：大多数现有方法没有模型化相机运动，依赖背景像素推断3D人体运动通常需要完整的场景重建，这在野外视频中往往是不可能的。即使现有的SLAM系统无法恢复准确的场景重建，背景像素的运动仍然提供了足够的信号来约束相机运动。<br>
                    方法：我们提出了一种方法，通过优化分离相机和人体运动，将人放置在同一世界坐标框架中。我们的方法是利用相对相机估计和数据驱动的人体运动先验来解决场景尺度模糊性并恢复全局人类轨迹。<br>
                    效果：我们的方法在具有挑战性的野外视频（如PoseTrack）中稳健地恢复了人们的全局3D轨迹。我们在Egobody三维人体数据集上量化了相对于现有方法的改进。我们还证明，我们恢复的相机尺度允许我们在共享坐标框架中推理多人的运动，从而提高了PoseTrack下游跟踪的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method to reconstruct global human trajectories from videos in the wild. Our optimization method decouples the camera and human motion, which allows us to place people in the same world coordinate frame. Most existing methods do not model the camera motion; methods that rely on the background pixels to infer 3D human motion usually require a full scene reconstruction, which is often not possible for in-the-wild videos. However, even when existing SLAM systems cannot recover accurate scene reconstructions, the background pixel motion still provides enough signal to constrain the camera motion. We show that relative camera estimates along with data-driven human motion priors can resolve the scene scale ambiguity and recover global human trajectories. Our method robustly recovers the global 3D trajectories of people in challenging in-the-wild videos, such as PoseTrack. We quantify our improvement over existing methods on 3D human dataset Egobody. We further demonstrate that our recovered camera scale allows us to reason about motion of multiple people in a shared coordinate frame, which improves performance of downstream tracking in PoseTrack. Code and additional results can be found at https://vye16.github.io/slahmr/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">370.LightedDepth: Video Depth Estimation in Light of Limited Inference View Angles</span><br>
                <span class="as">Zhu, ShengjieandLiu, Xiaoming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_LightedDepth_Video_Depth_Estimation_in_Light_of_Limited_Inference_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5003-5012.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频深度估计问题，即从相邻的视频帧中推断出密集的场景深度。<br>
                    动机：虽然最近的一些工作将视频深度估计视为简化的结构从运动（SfM）问题，但它与SfM的主要区别在于推理过程中可用的视角要少得多。这种设置适合单深度和光流估计。<br>
                    方法：本文将视频深度估计分解为两个部分，一个是在流图上的归一化姿态估计，另一个是在单深度图上的记录残差深度估计。两部分通过一个高效的现成的比例对齐算法进行统一。此外，通过添加额外的投影约束和确保足够的相机平移，稳定了室内的双视图姿态估计。<br>
                    效果：尽管是一个双视图算法，但通过与多视图迭代先前作品在室内和室外数据集上的性能显著提高，验证了分解的益处。代码和模型可在https://github.com/ShngJZ/LightedDepth获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video depth estimation infers the dense scene depth from immediate neighboring video frames. While recent works consider it a simplified structure-from-motion (SfM) problem, it still differs from the SfM in that significantly fewer view angels are available in inference. This setting, however, suits the mono-depth and optical flow estimation. This observation motivates us to decouple the video depth estimation into two components, a normalized pose estimation over a flowmap and a logged residual depth estimation over a mono-depth map. The two parts are unified with an efficient off-the-shelf scale alignment algorithm. Additionally, we stabilize the indoor two-view pose estimation by including additional projection constraints and ensuring sufficient camera translation. Though a two-view algorithm, we validate the benefit of the decoupling with the substantial performance improvement over multi-view iterative prior works on indoor and outdoor datasets. Codes and models are available at https://github.com/ShngJZ/LightedDepth.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">371.SparsePose: Sparse-View Camera Pose Regression and Refinement</span><br>
                <span class="as">Sinha, SamarthandZhang, JasonY.andTagliasacchi, AndreaandGilitschenski, IgorandLindell, DavidB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21349-21359.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用稀疏的图像集合进行准确的相机位姿估计。<br>
                    动机：现有的位姿估计方法在只有少量图像的情况下往往无法准确工作，因为其依赖于在图像对之间稳健地识别和匹配视觉特征。<br>
                    方法：提出Sparse-View Camera Pose Regression and Refinement（SparsePose）方法，通过训练大规模物体数据集（Co3D），学习从稀疏的宽基线图像中恢复准确的相机位姿。<br>
                    效果：实验表明，SparsePose在恢复准确的相机旋转和平移方面显著优于传统的和基于学习的基线方法，并且可以仅使用5-9张图像实现高保真度的3D重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Camera pose estimation is a key step in standard 3D reconstruction pipelines that operates on a dense set of images of a single object or scene. However, methods for pose estimation often fail when there are only a few images available because they rely on the ability to robustly identify and match visual features between pairs of images. While these methods can work robustly with dense camera views, capturing a large set of images can be time consuming or impractical. Here, we propose Sparse-View Camera Pose Regression and Refinement (SparsePose) for recovering accurate camera poses given a sparse set of wide-baseline images (fewer than 10). The method learns to regress initial camera poses and then iteratively refine them after training on a large-scale dataset of objects (Co3D: Common Objects in 3D). SparsePose significantly outperforms conventional and learning-based baselines in recovering accurate camera rotations and translations. We also demonstrate our pipeline for high-fidelity 3D reconstruction using only 5-9 images of an object.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">372.Flow Supervision for Deformable NeRF</span><br>
                <span class="as">Wang, ChaoyangandMacDonald, LachlanEwenandJeni, L\&#x27;aszl\&#x27;oA.andLucey, Simon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Flow_Supervision_for_Deformable_NeRF_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21128-21137.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法，直接使用光流作为监督，解决变形NeRF计算效率低下的问题。<br>
                    动机：变形NeRF在计算场景流时需要对后向形变场施加流动约束，这在计算上是低效的。<br>
                    方法：我们提出了一种新方法，不需要反转后向形变函数来计算帧之间的场景流。这种方法大大简化了问题，不再受限于可以解析反转的形变函数。<br>
                    效果：我们在单目新视角合成和快速物体运动方面进行了实验，结果显示，该方法在无需流监督的情况下，比基线方法有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper we present a new method for deformable NeRF that can directly use optical flow as supervision. We overcome the major challenge with respect to the computationally inefficiency of enforcing the flow constraints to the backward deformation field, used by deformable NeRFs. Specifically, we show that inverting the backward deformation function is actually not needed for computing scene flows between frames. This insight dramatically simplifies the problem, as one is no longer constrained to deformation functions that can be analytically inverted. Instead, thanks to the weak assumptions required by our derivation based on the inverse function theorem, our approach can be extended to a broad class of commonly used backward deformation field. We present results on monocular novel view synthesis with rapid object motion, and demonstrate significant improvements over baselines without flow supervision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">373.MOVES: Manipulated Objects in Video Enable Segmentation</span><br>
                <span class="as">Higgins, RichardE.L.andFouhey, DavidF.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Higgins_MOVES_Manipulated_Objects_in_Video_Enable_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6334-6343.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过操纵学习理解人们持有的物体以及手与物体的接触。<br>
                    动机：传统的图像分割方法需要手动标注，过程繁琐。我们希望通过观察真实的视频数据，训练一个系统来自动理解物体的分组和手与物体的接触。<br>
                    方法：我们训练了一个系统，该系统接受单张RGB图像并生成可用于回答分组问题（这两个像素是否在一起）和手关联问题（这只手是否持有那个像素）的像素嵌入。我们没有费力地标注分割掩码，而是观察了真实的视频数据。我们将极线几何与现代光流相结合，为分组生成简单而有效的伪标签。给定人物分割，我们可以进一步将像素与手关联以理解接触。<br>
                    效果：我们的系统在手部和手持物体任务上取得了具有竞争力的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a method that uses manipulation to learn to understand the objects people hold and as well as hand-object contact. We train a system that takes a single RGB image and produces a pixel-embedding that can be used to answer grouping questions (do these two pixels go together) as well as hand-association questions (is this hand holding that pixel). Rather painstakingly annotate segmentation masks, we observe people in realistic video data. We show that pairing epipolar geometry with modern optical flow produces simple and effective pseudo-labels for grouping. Given people segmentations, we can further associate pixels with hands to understand contact. Our system achieves competitive results on hand and hand-held object tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">374.ShadowNeuS: Neural SDF Reconstruction by Shadow Ray Supervision</span><br>
                <span class="as">Ling, JingwangandWang, ZhiboandXu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_ShadowNeuS_Neural_SDF_Reconstruction_by_Shadow_Ray_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/175-185.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过监督光线和阴影射线，从单视图图像中重建场景的神经表示。<br>
                    动机：目前的NeRF模型仅考虑了摄像机光线，而未考虑光源和场景之间的阴影射线，这限制了其在复杂光照条件下的性能。<br>
                    方法：提出一种新的阴影射线监督方案，优化光线样本和射线位置。通过监督阴影射线，成功地从单视图图像重建了场景的神经表面距离场（SDF）。<br>
                    效果：在从单视图二进制阴影或RGB图像重建形状的具有挑战性的任务上，与先前的工作相比，该方法表现出显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>By supervising camera rays between a scene and multi-view image planes, NeRF reconstructs a neural scene representation for the task of novel view synthesis. On the other hand, shadow rays between the light source and the scene have yet to be considered. Therefore, we propose a novel shadow ray supervision scheme that optimizes both the samples along the ray and the ray location. By supervising shadow rays, we successfully reconstruct a neural SDF of the scene from single-view images under multiple lighting conditions. Given single-view binary shadows, we train a neural network to reconstruct a complete scene not limited by the camera's line of sight. By further modeling the correlation between the image colors and the shadow rays, our technique can also be effectively extended to RGB inputs. We compare our method with previous works on challenging tasks of shape reconstruction from single-view binary shadow or RGB images and observe significant improvements. The code and data are available at https://github.com/gerwang/ShadowNeuS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">375.A Light Touch Approach to Teaching Transformers Multi-View Geometry</span><br>
                <span class="as">Bhalgat, YashandHenriques, Jo\~aoF.andZisserman, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhalgat_A_Light_Touch_Approach_to_Teaching_Transformers_Multi-View_Geometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4958-4969.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让视觉Transformer在处理多视图几何任务时，既能保持足够的灵活性，又能遵守严格的投影几何规则。<br>
                    动机：现有的视觉Transformer由于缺乏人工指定的先验知识，虽然具有强大的学习能力，但在处理涉及3D形状和视点的多变性以及投影几何的精确性的任务时，可能会出现问题。<br>
                    方法：提出一种"轻触"策略，通过使用极线来指导Transformer的交叉注意力图，对注意力值进行惩罚，鼓励更高的注意力沿着这些线，因为这些线上包含几何上合理的匹配。<br>
                    效果：实验证明，该方法在无需测试时间相机姿态信息的情况下，优于现有方法，并在物体检索任务中取得了显著的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformers are powerful visual learners, in large part due to their conspicuous lack of manually-specified priors. This flexibility can be problematic in tasks that involve multiple-view geometry, due to the near-infinite possible variations in 3D shapes and viewpoints (requiring flexibility), and the precise nature of projective geometry (obeying rigid laws). To resolve this conundrum, we propose a "light touch" approach, guiding visual Transformers to learn multiple-view geometry but allowing them to break free when needed. We achieve this by using epipolar lines to guide the Transformer's cross-attention maps, penalizing attention values outside the epipolar lines and encouraging higher attention along these lines since they contain geometrically plausible matches. Unlike previous methods, our proposal does not require any camera pose information at test-time. We focus on pose-invariant object instance retrieval, where standard Transformer networks struggle, due to the large differences in viewpoint between query and retrieved images. Experimentally, our method outperforms state-of-the-art approaches at object retrieval, without needing pose information at test-time.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">376.NeRF-Supervised Deep Stereo</span><br>
                <span class="as">Tosi, FabioandTonioni, AlessioandDeGregorio, DanieleandPoggi, Matteo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/855-866.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种无需任何真实地面训练的深度立体网络新框架。<br>
                    动机：现有的自我监督方法在挑战性的Middlebury数据集上存在30-40%的性能差距，需要填补到监督模型的差距，并在零样本泛化中表现优越。<br>
                    方法：利用最先进的神经渲染解决方案从单手持摄像机收集的图像序列生成立体训练数据，然后进行NeRF监督训练过程，利用渲染的立体三元组补偿遮挡和深度图作为代理标签。<br>
                    效果：实验结果表明，在这种机制下训练的模型在Middlebury数据集上比现有的自我监督方法提高了30-40%，填补了监督模型的差距，并在大多数情况下实现了零样本泛化的优越性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a novel framework for training deep stereo networks effortlessly and without any ground-truth. By leveraging state-of-the-art neural rendering solutions, we generate stereo training data from image sequences collected with a single handheld camera. On top of them, a NeRF-supervised training procedure is carried out, from which we exploit rendered stereo triplets to compensate for occlusions and depth maps as proxy labels. This results in stereo networks capable of predicting sharp and detailed disparity maps. Experimental results show that models trained under this regime yield a 30-40% improvement over existing self-supervised methods on the challenging Middlebury dataset, filling the gap to supervised models and, most times, outperforming them at zero-shot generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">377.DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium</span><br>
                <span class="as">Bangunharcana, AntyantaandMagd, AhmedandKim, Kyung-Soo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bangunharcana_DualRefine_Self-Supervised_Depth_and_Pose_Estimation_Through_Iterative_Epipolar_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/726-738.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练大规模文本语料库和知识图谱来提高语言表示模型的性能？<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，本文提出利用知识图谱中的有信息量的实体来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates. Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines. The code is available at https://github.com/antabangun/DualRefine.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">378.Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes</span><br>
                <span class="as">Lee, JihyunandSung, MinhyukandChoi, HonggyuandKim, Tae-Kyun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Im2Hands_Learning_Attentive_Implicit_Representation_of_Interacting_Two-Hand_Shapes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21169-21178.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地重建两只互动的手的神经隐式表示。<br>
                    动机：现有的两手重建方法依赖于参数化手模型和/或低分辨率网格，而Im2Hands可以生成两只手的精细几何形状，具有高的手-手和手-图像一致性。<br>
                    方法：Im2Hands通过两个新型的注意力模块来建模两只手的占用体积，这两个模块分别负责（1）初始占用估计和（2）上下文感知的占用细化。首先，Im2Hands在为每只手设计的正则空间中学习每只手的神经关节占用，然后使用查询-图像注意力在定位空间中细化初始的两只手的占用，以提高两只手形状之间的一致性。<br>
                    效果：实验结果表明，Im2Hands在两手重建方面的效果优于相关方法，实现了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands -- conditioned on an RGB image and coarse 3D keypoints -- by two novel attention-based modules responsible for (1) initial occupancy estimation and (2) context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results. Our code is publicly available at https://github.com/jyunlee/Im2Hands.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">379.Long-Term Visual Localization With Mobile Sensors</span><br>
                <span class="as">Yan, ShenandLiu, YuandWang, LongandShen, ZehongandPeng, ZhenandLiu, HaominandZhang, MaojunandZhang, GuofengandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Long-Term_Visual_Localization_With_Mobile_Sensors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17245-17255.png><br>
            
            <span class="tt"><span class="t0">研究问题：在变化多端的户外环境中，由于光照、季节和结构研究问题：在变化多端的户外环境中，由于光照、季节和结构变化引起的查询图像和参考图像之间的外观差异巨大，因此基于图像的相机定位仍然是一个挑战。<br>
                    动机：尽管图像匹配和姿态估计取得了显著的进步，但在变化的户外环境中，由于光照、季节和结构变化引起的查询图像和参考图像之间的外观差异巨大，因此基于图像的相机定位仍然是一个挑战。<br>
                    方法：我们提出利用手机中的附加传感器（主要是GPS、罗盘和重力传感器）来解决这个具有挑战性的问题。这些移动传感器提供了适当的初始姿态和有效的约束，以减少图像匹配和最终姿态估计的搜索空间。<br>
                    效果：我们收集了一个新的数据集，该数据集提供了各种移动传感器数据和显著的场景外观变化，并开发了一个系统来获取查询图像的地面真实姿态。我们的方法和几种最先进的基线进行了基准测试，证明了所提出方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable advances in image matching and pose estimation, image-based localization of a camera in a temporally-varying outdoor environment is still a challenging problem due to huge appearance disparity between query and reference images caused by illumination, seasonal and structural changes. In this work, we propose to leverage additional sensors on a mobile phone, mainly GPS, compass, and gravity sensor, to solve this challenging problem. We show that these mobile sensors provide decent initial poses and effective constraints to reduce the searching space in image matching and final pose estimation. With the initial pose, we are also able to devise a direct 2D-3D matching network to efficiently establish 2D-3D correspondences instead of tedious 2D-2D matching in existing systems. As no public dataset exists for the studied problem, we collect a new dataset that provides a variety of mobile sensor data and significant scene appearance variations, and develop a system to acquire ground-truth poses for query images. We benchmark our method as well as several state-of-the-art baselines and demonstrate the effectiveness of the proposed approach. Our code and dataset are available on the project page: https://zju3dv.github.io/sensloc/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">380.Relightable Neural Human Assets From Multi-View Gradient Illuminations</span><br>
                <span class="as">Zhou, TaotaoandHe, KaiandWu, DiandXu, TengandZhang, QixuanandShao, KuixiangandChen, WenzhengandXu, LanandYu, Jingyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Relightable_Neural_Human_Assets_From_Multi-View_Gradient_Illuminations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4315-4327.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的人类数据集大多只提供同一光照条件下的多视角人像，对于光照变化情况下的人体建模和重照明问题帮助有限。<br>
                    动机：为了推动人体建模和重照明领域的研究，本文提出了一个新的3D人体数据集UltraStage，包含2000多个高质量的人体模型，这些模型在不同视角和多种光照条件下拍摄。<br>
                    方法：为每个样本提供32个环绕视图，分别用一盏白光和两种渐变光照进行照亮。除了常规的多视角图像外，渐变光照有助于恢复详细的表面法线和空间变化的材质映射，支持各种重照明应用。受神经表示最新进展的启发，我们进一步将每个样本解释为一个神经人体资产，使其能在任意光照条件下合成新的视角。<br>
                    效果：我们的神经人体资产能够实现极高的捕获性能，并能表示面部皱纹和衣物褶皱等细节。在单图像重照明任务中验证UltraStage，通过使用来自神经资产的虚拟重照明数据训练神经网络，实现了比现有技术更真实的渲染效果。UltraStage将公开给社区，以刺激人体建模和渲染等各种任务的未来发展。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human modeling and relighting are two fundamental problems in computer vision and graphics, where high-quality datasets can largely facilitate related research. However, most existing human datasets only provide multi-view human images captured under the same illumination. Although valuable for modeling tasks, they are not readily used in relighting problems. To promote research in both fields, in this paper, we present UltraStage, a new 3D human dataset that contains more than 2,000 high-quality human assets captured under both multi-view and multi-illumination settings. Specifically, for each example, we provide 32 surrounding views illuminated with one white light and two gradient illuminations. In addition to regular multi-view images, gradient illuminations help recover detailed surface normal and spatially-varying material maps, enabling various relighting applications. Inspired by recent advances in neural representation, we further interpret each example into a neural human asset which allows novel view synthesis under arbitrary lighting conditions. We show our neural human assets can achieve extremely high capture performance and are capable of representing fine details such as facial wrinkles and cloth folds. We also validate UltraStage in single image relighting tasks, training neural networks with virtual relighted data from neural assets and demonstrating realistic rendering improvements over prior arts. UltraStage will be publicly available to the community to stimulate significant future developments in various human modeling and rendering tasks. The dataset is available at https://miaoing.github.io/RNHA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">381.DyLiN: Making Light Field Networks Dynamic</span><br>
                <span class="as">Yu, HengandJulin, JoelandMilacski, Zolt\&#x27;an\&#x27;A.andNiinuma, KoichiroandJeni, L\&#x27;aszl\&#x27;oA.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_DyLiN_Making_Light_Field_Networks_Dynamic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12397-12406.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高光场网络在处理动态非刚性变形场景时的性能和效率？<br>
                    动机：目前的光场网络虽然能高效地从二维观察中表示三维结构，但仅限于整体静态场景，无法处理动态非刚性变形。<br>
                    方法：提出动态光场网络（DyLiN）方法，通过学习输入光线到标准光线的形变场并提升到高维空间来处理不连续性。进一步引入可控属性输入的CoDyLiN。通过知识蒸馏从预训练动态辐射场进行训练。<br>
                    效果：在包含各种非刚性变形的合成和真实世界数据集上评估，DyLiN在视觉保真度上优于并匹配了最先进的方法，同时计算速度快25-71倍。CoDyLiN在属性注释数据上也超越了其教师模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Light Field Networks, the re-formulations of radiance fields to oriented rays, are magnitudes faster than their coordinate network counterparts, and provide higher fidelity with respect to representing 3D structures from 2D observations. They would be well suited for generic scene representation and manipulation, but suffer from one problem: they are limited to holistic and static scenes. In this paper, we propose the Dynamic Light Field Network (DyLiN) method that can handle non-rigid deformations, including topological changes. We learn a deformation field from input rays to canonical rays, and lift them into a higher dimensional space to handle discontinuities. We further introduce CoDyLiN, which augments DyLiN with controllable attribute inputs. We train both models via knowledge distillation from pretrained dynamic radiance fields. We evaluated DyLiN using both synthetic and real world datasets that include various non-rigid deformations. DyLiN qualitatively outperformed and quantitatively matched state-of-the-art methods in terms of visual fidelity, while being 25 - 71x computationally faster. We also tested CoDyLiN on attribute annotated data and it surpassed its teacher model. Project page: https://dylin2023.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">382.Neuralangelo: High-Fidelity Neural Surface Reconstruction</span><br>
                <span class="as">Li, ZhaoshuoandM\&quot;uller, ThomasandEvans, AlexandTaylor, RussellH.andUnberath, MathiasandLiu, Ming-YuandLin, Chen-Hsuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8456-8465.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过图像神经网络渲染恢复密集的3D表面。<br>
                    动机：当前的方法在恢复真实世界场景的详细结构上存在困难。<br>
                    方法：提出了Neuralangelo，该方法将多分辨率3D哈希网格的表示能力与神经表面渲染相结合，通过数值梯度计算高阶导数作为平滑操作，并在哈希网格上进行从粗到精的优化以控制不同级别的细节。<br>
                    效果：即使没有深度等辅助输入，Neuralangelo也能有效地从多视图图像中恢复密集的3D表面结构，其保真度显著超过以前的方法，能够从RGB视频捕捉中进行详细的大规模场景重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural surface reconstruction has been shown to be powerful for recovering dense 3D surfaces via image-based neural rendering. However, current methods struggle to recover detailed structures of real-world scenes. To address the issue, we present Neuralangelo, which combines the representation power of multi-resolution 3D hash grids with neural surface rendering. Two key ingredients enable our approach: (1) numerical gradients for computing higher-order derivatives as a smoothing operation and (2) coarse-to-fine optimization on the hash grids controlling different levels of details. Even without auxiliary inputs such as depth, Neuralangelo can effectively recover dense 3D surface structures from multi-view images with fidelity significantly surpassing previous methods, enabling detailed large-scale scene reconstruction from RGB video captures.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">383.Neural Vector Fields: Implicit Representation by Explicit Learning</span><br>
                <span class="as">Yang, XianghuiandLin, GuoshengandChen, ZhenghaoandZhou, Luping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Neural_Vector_Fields_Implicit_Representation_by_Explicit_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16727-16738.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行3D表面重建？<br>
                    动机：现有的方法在分辨率和拓扑性上存在限制，需要一种新方法来提高重建效果。<br>
                    方法：提出一种新的3D表示方法——神经矢量场（NVF），结合显式学习和隐式函数的强大表示能力，直接操作网格并打破分辨率和拓扑性的障碍。<br>
                    效果：实验结果表明，该方法在各种评估场景中优于现有方法，包括水密与非水密形状、特定类别与非特定类别重建、未见过的类别重建以及跨领域重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) are widely applied for nowadays 3D surface reconstruction tasks and such methods can be further divided into two categories, which respectively warp templates explicitly by moving vertices or represent 3D surfaces implicitly as signed or unsigned distance functions. Taking advantage of both advanced explicit learning process and powerful representation ability of implicit functions, we propose a novel 3D representation method, Neural Vector Fields (NVF). It not only adopts the explicit learning process to manipulate meshes directly, but also leverages the implicit representation of unsigned distance functions (UDFs) to break the barriers in resolution and topology. Specifically, our method first predicts the displacements from queries towards the surface and models the shapes as Vector Fields. Rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods, the produced vector fields encode the distance and direction fields both and mitigate the ambiguity at "ridge" points, such that the calculation of direction fields is straightforward and differentiation-free. The differentiation-free characteristic enables us to further learn a shape codebook via Vector Quantization, which encodes the cross-object priors, accelerates the training procedure, and boosts model generalization on cross-category reconstruction. The extensive experiments on surface reconstruction benchmarks indicate that our method outperforms those state-of-the-art methods in different evaluation scenarios including watertight vs non-watertight shapes, category-specific vs category-agnostic reconstruction, category-unseen reconstruction, and cross-domain reconstruction. Our code is released at https://github.com/Wi-sc/NVF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">384.Overcoming the Trade-Off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction</span><br>
                <span class="as">Yu, ZiweiandLi, ChenandYang, LinlinandZheng, XiaoxuandMi, MichaelBiandLee, GimHeeandYao, Angela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Overcoming_the_Trade-Off_Between_Accuracy_and_Plausibility_in_3D_Hand_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/544-553.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确重建3D手部形状，同时保证重建结果的合理性。<br>
                    动机：直接网格拟合方法可以高度准确地重建3D手部形状，但容易产生伪影，结果不够合理；而参数化模型如MANO能保证手部形状的合理性，但准确性不如非参数化方法。<br>
                    方法：本文提出了一种新的弱监督手部形状估计框架，将非参数化网格拟合与MANO模型进行端到端集成。<br>
                    效果：该联合模型克服了准确性和合理性之间的权衡，特别是在具有挑战性的双手和手-物体交互场景中，能够生成对齐良好、高质量的3D网格。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Direct mesh fitting for 3D hand shape reconstruction estimates highly accurate meshes. However, the resulting meshes are prone to artifacts and do not appear as plausible hand shapes. Conversely, parametric models like MANO ensure plausible hand shapes but are not as accurate as the non-parametric methods. In this work, we introduce a novel weakly-supervised hand shape estimation framework that integrates non-parametric mesh fitting with MANO models in an end-to-end fashion. Our joint model overcomes the tradeoff in accuracy and plausibility to yield well-aligned and high-quality 3D meshes, especially in challenging two-hand and hand-object interaction scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">385.EditableNeRF: Editing Topologically Varying Neural Radiance Fields by Key Points</span><br>
                <span class="as">Zheng, ChengweiandLin, WenbinandXu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_EditableNeRF_Editing_Topologically_Varying_Neural_Radiance_Fields_by_Key_Points_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8317-8327.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何编辑由NeRF-based方法建模的动态场景。<br>
                    动机：现有的NeRF模型虽然能实现高度逼真的新视角合成，但对于动态场景的编辑却是一个挑战。<br>
                    方法：提出可编辑的神经辐射场，通过图像序列自动训练网络，使用表面关键点来模型化拓扑变化。用户可以通过拖动关键点到新位置来编辑场景。<br>
                    效果：实验证明，该方法在各种动态场景上都能实现高质量的编辑，并优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields (NeRF) achieve highly photo-realistic novel-view synthesis, but it's a challenging problem to edit the scenes modeled by NeRF-based methods, especially for dynamic scenes. We propose editable neural radiance fields that enable end-users to easily edit dynamic scenes and even support topological changes. Input with an image sequence from a single camera, our network is trained fully automatically and models topologically varying dynamics using our picked-out surface key points. Then end-users can edit the scene by easily dragging the key points to desired new positions. To achieve this, we propose a scene analysis method to detect and initialize key points by considering the dynamics in the scene, and a weighted key points strategy to model topologically varying dynamics by joint key points and weights optimization. Our method supports intuitive multi-dimensional (up to 3D) editing and can generate novel scenes that are unseen in the input sequence. Experiments demonstrate that our method achieves high-quality editing on various dynamic scenes and outperforms the state-of-the-art. Our code and captured data are available at https://chengwei-zheng.github.io/EditableNeRF/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">386.NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds</span><br>
                <span class="as">Chen, Jun-KunandLyu, JipengandWang, Yu-Xiong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_NeuralEditor_Editing_Neural_Radiance_Fields_via_Manipulating_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12439-12448.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对神经辐射场（NeRFs）进行形状编辑。<br>
                    动机：尽管NeRFs在新颖视图合成方面取得了令人印象深刻的结果，但对其场景形状的编辑仍是一个基本挑战。<br>
                    方法：本文提出了一种名为NeuralEditor的方法，该方法利用显式点云表示作为构建NeRFs的基础结构，并引入了一种基于K-D树引导的密度自适应体素的新型渲染方案，通过优化产生高质量的渲染结果和精确的点云。然后，NeuralEditor通过映射相关点之间的点云来进行形状编辑。<br>
                    效果：广泛的评估表明，NeuralEditor在形状变形和场景变形任务中都达到了最先进的性能。值得注意的是，NeuralEditor支持零样本推理以及进一步对编辑后的场景进行微调。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes NeuralEditor that enables neural radiance fields (NeRFs) natively editable for general shape editing tasks. Despite their impressive results on novel-view synthesis, it remains a fundamental challenge for NeRFs to edit the shape of the scene. Our key insight is to exploit the explicit point cloud representation as the underlying structure to construct NeRFs, inspired by the intuitive interpretation of NeRF rendering as a process that projects or "plots" the associated 3D point cloud to a 2D image plane. To this end, NeuralEditor introduces a novel rendering scheme based on deterministic integration within K-D tree-guided density-adaptive voxels, which produces both high-quality rendering results and precise point clouds through optimization. NeuralEditor then performs shape editing via mapping associated points between point clouds. Extensive evaluation shows that NeuralEditor achieves state-of-the-art performance in both shape deformation and scene morphing tasks. Notably, NeuralEditor supports both zero-shot inference and further fine-tuning over the edited scene. Our code, benchmark, and demo video are available at https://immortalco.github.io/NeuralEditor.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">387.NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation</span><br>
                <span class="as">Li, JiefengandBian, SiyuanandLiu, QiandTang, JiashengandWang, FanandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12933-12942.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的顶级3D人体姿态和形状估计方法要么对遮挡具有鲁棒性，要么在非遮挡情况下获得像素对齐的准确性，但无法同时实现两者。<br>
                    动机：开发一种新方法，既能对遮挡有鲁棒性，又能获得像素对齐的精度。<br>
                    方法：提出NIKI（带有可逆神经网络的神经逆向运动学），通过双向误差建模来提高对遮挡的鲁棒性和获取像素对齐的精度。NIKI能从正向和逆向过程中学习，并使用可逆网络进行模型构建。<br>
                    效果：实验证明，NIKI在标准和特定遮挡基准测试上都表现出了有效性，实现了鲁棒性和良好对齐结果的同时存在。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the progress of 3D human pose and shape estimation, state-of-the-art methods can either be robust to occlusions or obtain pixel-aligned accuracy in non-occlusion cases. However, they cannot obtain robustness and mesh-image alignment at the same time. In this work, we present NIKI (Neural Inverse Kinematics with Invertible Neural Network), which models bi-directional errors to improve the robustness to occlusions and obtain pixel-aligned accuracy. NIKI can learn from both the forward and inverse processes with invertible networks. In the inverse process, the model separates the error from the plausible 3D pose manifold for a robust 3D human pose estimation. In the forward process, we enforce the zero-error boundary conditions to improve the sensitivity to reliable joint positions for better mesh-image alignment. Furthermore, NIKI emulates the analytical inverse kinematics algorithms with the twist-and-swing decomposition for better interpretability. Experiments on standard and occlusion-specific benchmarks demonstrate the effectiveness of NIKI, where we exhibit robust and well-aligned results simultaneously. Code is available at https://github.com/Jeff-sjtu/NIKI</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">388.Transfer4D: A Framework for Frugal Motion Capture and Deformation Transfer</span><br>
                <span class="as">Maheshwari, ShubhandNarain, RahulandHebbalaguppe, Ramya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Maheshwari_Transfer4D_A_Framework_for_Frugal_Motion_Capture_and_Deformation_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12836-12846.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用低成本深度传感器和自动化流程，将真实演员的表演转化为虚拟角色动画。<br>
                    动机：现有的动作捕捉技术需要昂贵的设备和专家操作，限制了其普及性。我们的目标是开发一种名为“Transfer4D”的替代方案，降低制作成本并简化操作流程。<br>
                    方法：我们使用单目深度序列提取骨骼作为动作捕捉和转换的中间表示，结合额外的几何信息进行运动重建和转换。通过非刚性重建从深度序列中跟踪运动，然后使用皮肤分解对源对象进行绑定。最后，将骨架嵌入目标对象进行运动重定向。<br>
                    效果：实验结果表明，我们的Transfer4D方法在运动重建和转换方面优于现有方法，实现了以低成本深度传感器为基础的虚拟角色动画制作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Animating a virtual character based on a real performance of an actor is a challenging task that currently requires expensive motion capture setups and additional effort by expert animators, rendering it accessible only to large production houses. The goal of our work is to democratize this task by developing a frugal alternative termed "Transfer4D" that uses only commodity depth sensors and further reduces animators' effort by automating the rigging and animation transfer process. To handle sparse, incomplete videos from depth video inputs and large variations between source and target objects, we propose to use skeletons as an intermediary representation between motion capture and transfer. We propose a novel skeleton extraction pipeline from single-view depth sequence that incorporates additional geometric information, resulting in superior performance in motion reconstruction and transfer in comparison to the contemporary methods. We use non-rigid reconstruction to track motion from the depth sequence, and then we rig the source object using skinning decomposition. Finally, the rig is embedded into the target object for motion retargeting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">389.Handy: Towards a High Fidelity 3D Hand Shape and Appearance Model</span><br>
                <span class="as">Potamias, RolandosAlexandrosandPloumpis, StylianosandMoschoglou, StylianosandTriantafyllou, VasileiosandZafeiriou, Stefanos</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4670-4680.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地重建和估计人类手部的形状、姿态和外观？<br>
                    动机：当前最先进的手部重建和姿态估计方法主要依赖于低多边形的MANO模型，但该模型存在表达力有限、训练样本数量不足以及对手部外观关注不足等问题。<br>
                    方法：我们提出了"Handy"模型，这是一个大规模的人类手部模型，包含超过1200个样本，覆盖了不同的年龄、性别和种族。同时，我们还训练了一个强大的生成对抗网络来生成高分辨率的手部纹理。<br>
                    效果：实验证明，我们的模型在形状、姿态和纹理重建方面都优于现有的最先进技术，即使在恶劣的“野外”条件下也能准确地重建出分布外样本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Over the last few years, with the advent of virtual and augmented reality, an enormous amount of research has been focused on modeling, tracking and reconstructing human hands. Given their power to express human behavior, hands have been a very important, but challenging component of the human body. Currently, most of the state-of-the-art reconstruction and pose estimation methods rely on the low polygon MANO model. Apart from its low polygon count, MANO model was trained with only 31 adult subjects, which not only limits its expressive power but also imposes unnecessary shape reconstruction constraints on pose estimation methods. Moreover, hand appearance remains almost unexplored and neglected from the majority of hand reconstruction methods. In this work, we propose "Handy", a large-scale model of the human hand, modeling both shape and appearance composed of over 1200 subjects which we make publicly available for the benefit of the research community. In contrast to current models, our proposed hand model was trained on a dataset with large diversity in age, gender, and ethnicity, which tackles the limitations of MANO and accurately reconstructs out-of-distribution samples. In order to create a high quality texture model, we trained a powerful GAN, which preserves high frequency details and is able to generate high resolution hand textures. To showcase the capabilities of the proposed model, we built a synthetic dataset of textured hands and trained a hand pose estimation network to reconstruct both the shape and appearance from single images. As it is demonstrated in an extensive series of quantitative as well as qualitative experiments, our model proves to be robust against the state-of-the-art and realistically captures the 3D hand shape and pose along with a high frequency detailed texture even in adverse "in-the-wild" conditions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">390.Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination</span><br>
                <span class="as">Zhao, ZimengandZuo, BinghuiandLong, ZhiyuandWang, Yangang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_Adversarial_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12125-12136.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从标记式运动捕捉（MoCap）收集的大量手部图像中恢复可靠的外观。<br>
                    动机：由于标记引起的退化限制了这些图像在手部外观重建中的应用。<br>
                    方法：首先，通过使用半监督学习范式，将退化的手部结构与裸露的手部结构分离；然后，利用双重对抗性鉴别（DAD）方案将外观包裹到该结构上。<br>
                    效果：实验证明，该方法可以稳健地从包含不同标记甚至被物体遮挡的数据集恢复照片级真实的手部外观，为其他下游学习问题提供了一种新的获取裸露手部外观数据的路径。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Enormous hand images with reliable annotations are collected through marker-based MoCap. Unfortunately, degradations caused by markers limit their application in hand appearance reconstruction. A clear appearance recovery insight is an image-to-image translation trained with unpaired data. However, most frameworks fail because there exists structure inconsistency from a degraded hand to a bare one. The core of our approach is to first disentangle the bare hand structure from those degraded images and then wrap the appearance to this structure with a dual adversarial discrimination (DAD) scheme. Both modules take full advantage of the semi-supervised learning paradigm: The structure disentanglement benefits from the modeling ability of ViT, and the translator is enhanced by the dual discrimination on both translation processes and translation results. Comprehensive evaluations have been conducted to prove that our framework can robustly recover photo-realistic hand appearance from diverse marker-contained and even object-occluded datasets. It provides a novel avenue to acquire bare hand appearance data for other downstream learning problems.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">391.Markerless Camera-to-Robot Pose Estimation via Self-Supervised Sim-to-Real Transfer</span><br>
                <span class="as">Lu, JingpeiandRichter, FlorianandYip, MichaelC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Markerless_Camera-to-Robot_Pose_Estimation_via_Self-Supervised_Sim-to-Real_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21296-21306.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决基于视觉的机器人控制中相机到机器人位姿的问题，需要准确且细致的操作。<br>
                    动机：传统的解决方法需要通过标记器修改机器人，而深度学习方法可以实现无标记的特征提取。主流的深度学习方法仅使用合成数据并依赖领域随机化来填补模拟与现实的鸿沟，因为获取3D注释是劳动密集型的。<br>
                    方法：我们提出了一种端到端的位姿估计框架，能够进行在线相机到机器人的校准，并提出了一种自我监督的训练方法，以扩展到未标记的真实世界数据。我们的框架结合了深度学习和几何视觉来解决机器人位姿问题，并且整个流程是完全可微分的。为了训练相机到机器人位姿估计网络（CtRNet），我们利用前景分割和可微分渲染进行图像级别的自我监督。通过渲染器可视化位姿预测，并将图像损失与输入图像进行反向传播以训练神经网络。<br>
                    效果：我们在两个公共真实数据集上的实验结果证实了我们的方法优于现有工作。我们还将我们的框架集成到一个视觉伺服系统中，展示了实时精确机器人位姿估计在自动化任务中的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Solving the camera-to-robot pose is a fundamental requirement for vision-based robot control, and is a process that takes considerable effort and cares to make accurate. Traditional approaches require modification of the robot via markers, and subsequent deep learning approaches enabled markerless feature extraction. Mainstream deep learning methods only use synthetic data and rely on Domain Randomization to fill the sim-to-real gap, because acquiring the 3D annotation is labor-intensive. In this work, we go beyond the limitation of 3D annotations for real-world data. We propose an end-to-end pose estimation framework that is capable of online camera-to-robot calibration and a self-supervised training method to scale the training to unlabeled real-world data. Our framework combines deep learning and geometric vision for solving the robot pose, and the pipeline is fully differentiable. To train the Camera-to-Robot Pose Estimation Network (CtRNet), we leverage foreground segmentation and differentiable rendering for image-level self-supervision. The pose prediction is visualized through a renderer and the image loss with the input image is back-propagated to train the neural network. Our experimental results on two public real datasets confirm the effectiveness of our approach over existing works. We also integrate our framework into a visual servoing system to demonstrate the promise of real-time precise robot pose estimation for automation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">392.CARTO: Category and Joint Agnostic Reconstruction of ARTiculated Objects</span><br>
                <span class="as">Heppert, NickandIrshad, MuhammadZubairandZakharov, SergeyandLiu, KatherineandAmbrus, RaresAndreiandBohg, JeannetteandValada, AbhinavandKollar, Thomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Heppert_CARTO_Category_and_Joint_Agnostic_Reconstruction_of_ARTiculated_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21201-21210.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目立体RGB观测中重建多个铰接对象。<br>
                    动机：目前的重建方法需要为每个类别单独训练解码器，效率低下。<br>
                    方法：提出CARTO方法，使用隐式的对象中心表示和学习的单一几何和关节解码器来重建多个对象类别。<br>
                    效果：在与分别对每个类别进行训练的专用解码器相比，CARTO方法在重建精度上具有可比性。结合立体图像编码器，可以在单个前向传递中推断出多个未知对象的3D形状、6D位姿、大小、关节类型和关节状态。该方法在新的实例上实现了20.4%的mAP 3D IOU50绝对改进，且推理速度快，可在NVIDIA TITAN XP GPU上以1 HZ的速度运行八个或更少的对象。虽然仅在模拟数据上进行训练，但CARTO可以转移到真实世界的物体实例。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present CARTO, a novel approach for reconstructing multiple articulated objects from a single stereo RGB observation. We use implicit object-centric representations and learn a single geometry and articulation decoder for multiple object categories. Despite training on multiple categories, our decoder achieves a comparable reconstruction accuracy to methods that train bespoke decoders separately for each category. Combined with our stereo image encoder we infer the 3D shape, 6D pose, size, joint type, and the joint state of multiple unknown objects in a single forward pass. Our method achieves a 20.4% absolute improvement in mAP 3D IOU50 for novel instances when compared to a two-stage pipeline. Inference time is fast and can run on a NVIDIA TITAN XP GPU at 1 HZ for eight or less objects present. While only trained on simulated data, CARTO transfers to real-world object instances. Code and evaluation data is available at: http://carto.cs.uni-freiburg.de</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">393.RGBD2: Generative Scene Synthesis via Incremental View Inpainting Using RGBD Diffusion Models</span><br>
                <span class="as">Lei, JiabaoandTang, JiapengandJia, Kui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_RGBD2_Generative_Scene_Synthesis_via_Incremental_View_Inpainting_Using_RGBD_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8422-8434.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从稀疏的RGBD视图中恢复底层场景几何和颜色。<br>
                    动机：现有的方法在处理多视图不一致性问题上存在困难。<br>
                    方法：提出一种新方法RGBD2，通过生成新的RGBD视图并融合，直接得到场景几何。同时，使用中间表面网格和相机投影解决多视图不一致性问题。<br>
                    效果：在ScanNet数据集上进行的实验表明，该方法在3D场景合成任务上优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the challenge of recovering an underlying scene geometry and colors from a sparse set of RGBD view observations. In this work, we present a new solution termed RGBD2 that sequentially generates novel RGBD views along a camera trajectory, and the scene geometry is simply the fusion result of these views. More specifically, we maintain an intermediate surface mesh used for rendering new RGBD views, which subsequently becomes complete by an inpainting network; each rendered RGBD view is later back-projected as a partial surface and is supplemented into the intermediate mesh. The use of intermediate mesh and camera projection helps solve the tough problem of multi-view inconsistency. We practically implement the RGBD inpainting network as a versatile RGBD diffusion model, which is previously used for 2D generative modeling; we make a modification to its reverse diffusion process to enable our use. We evaluate our approach on the task of 3D scene synthesis from sparse RGBD inputs; extensive experiments on the ScanNet dataset demonstrate the superiority of our approach over existing ones. Project page: https://jblei.site/proj/rgbd-diffusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">394.Deep Polarization Reconstruction With PDAVIS Events</span><br>
                <span class="as">Mei, HaiyangandWang, ZuowenandYang, XinandWei, XiaopengandDelbruck, Tobi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Deep_Polarization_Reconstruction_With_PDAVIS_Events_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22149-22158.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从输入的原始极化事件中直接输出极化信息，以提高极化重建性能。<br>
                    动机：目前的极化重建方法忽视了通道间的相关性，导致四个重建帧之间存在内容不一致，影响了极化重建的性能。<br>
                    方法：构建了首个大规模的事件到极化数据集，用于训练事件到极化网络E2P。E2P通过跨模态上下文整合提取输入极化事件中的丰富极化模式并增强特征。<br>
                    效果：实验结果表明，E2P在没有额外计算成本的情况下，显著优于Polarization FireNet。在快速和高动态范围的场景中，E2P产生的极化测量也比PDAVIS帧更准确。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The polarization event camera PDAVIS is a novel bio-inspired neuromorphic vision sensor that reports both conventional polarization frames and asynchronous, continuously per-pixel polarization brightness changes (polarization events) with fast temporal resolution and large dynamic range. A deep neural network method (Polarization FireNet) was previously developed to reconstruct the polarization angle and degree from polarization events for bridging the gap between the polarization event camera and mainstream computer vision. However, Polarization FireNet applies a network pre-trained for normal event-based frame reconstruction independently on each of four channels of polarization events from four linear polarization angles, which ignores the correlations between channels and inevitably introduces content inconsistency between the four reconstructed frames, resulting in unsatisfactory polarization reconstruction performance. In this work, we strive to train an effective, yet efficient, DNN model that directly outputs polarization from the input raw polarization events. To this end, we constructed the first large-scale event-to-polarization dataset, which we subsequently employed to train our events-to-polarization network E2P. E2P extracts rich polarization patterns from input polarization events and enhances features through cross-modality context integration. We demonstrate that E2P outperforms Polarization FireNet by a significant margin with no additional computing cost. Experimental results also show that E2P produces more accurate measurement of polarization than the PDAVIS frames in challenging fast and high dynamic range scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">395.Semidefinite Relaxations for Robust Multiview Triangulation</span><br>
                <span class="as">H\&quot;arenstam-Nielsen, LinusandZeller, NiclasandCremers, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Harenstam-Nielsen_Semidefinite_Relaxations_for_Robust_Multiview_Triangulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/749-757.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种基于凸松弛的方法，用于确定性最优鲁棒多视角三角化。<br>
                    动机：为了解决非鲁棒多视角三角化问题，通过引入最小二乘成本函数扩展现有的松弛方法。<br>
                    方法：提出了两种公式，一种基于极线约束，另一种基于分数重投影约束。第一种公式维度较低，在适度的噪声和异常值水平下仍然紧密；第二种公式维度较高，因此速度较慢，但在极端噪声和异常值水平下仍然紧密。<br>
                    效果：通过大量实验证明，所提出的方法即使在显著的噪声和大部分异常值存在的情况下，也能计算出确定的最优重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose an approach based on convex relaxations for certifiably optimal robust multiview triangulation. To this end, we extend existing relaxation approaches to non-robust multiview triangulation by incorporating a least squares cost function. We propose two formulations, one based on epipolar constraints and one based on fractional reprojection constraints. The first is lower dimensional and remains tight under moderate noise and outlier levels, while the second is higher dimensional and therefore slower but remains tight even under extreme noise and outlier levels. We demonstrate through extensive experiments that the proposed approaches allow us to compute provably optimal reconstructions even under significant noise and a large percentage of outliers.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">396.ContraNeRF: Generalizable Neural Radiance Fields for Synthetic-to-Real Novel View Synthesis via Contrastive Learning</span><br>
                <span class="as">Yang, HaoandHong, LanqingandLi, AoxueandHu, TianyangandLi, ZhenguoandLee, GimHeeandWang, Liwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ContraNeRF_Generalizable_Neural_Radiance_Fields_for_Synthetic-to-Real_Novel_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16508-16517.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管许多近期的研究已经探索了基于NeRF的未见场景的新型视图合成，但他们很少考虑在许多实际应用中所需的合成到真实的泛化。<br>
                    动机：本研究首先探讨了合成数据在合成到真实新型视图合成中的影响，并惊讶地发现，使用合成数据训练的模型倾向于产生更锐利但不太准确的体积密度。<br>
                    方法：为了保持使用合成数据的优点同时避免其负面影响，我们提出了引入几何感知对比学习来学习具有几何约束的多视图一致特征。同时，我们采用跨视图注意力进一步通过查询输入视图的特征增强特征的几何感知。<br>
                    效果：实验表明，在合成到真实的设置下，我们的方法可以渲染出质量更高、细节更好的图像，并在PSNR、SSIM和LPIPS方面优于现有的可泛化新型视图合成方法。当我们在真实数据上进行训练时，我们的方法也取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although many recent works have investigated generalizable NeRF-based novel view synthesis for unseen scenes, they seldom consider the synthetic-to-real generalization, which is desired in many practical applications. In this work, we first investigate the effects of synthetic data in synthetic-to-real novel view synthesis and surprisingly observe that models trained with synthetic data tend to produce sharper but less accurate volume densities. For pixels where the volume densities are correct, fine-grained details will be obtained. Otherwise, severe artifacts will be produced. To maintain the advantages of using synthetic data while avoiding its negative effects, we propose to introduce geometry-aware contrastive learning to learn multi-view consistent features with geometric constraints. Meanwhile, we adopt cross-view attention to further enhance the geometry perception of features by querying features across input views. Experiments demonstrate that under the synthetic-to-real setting, our method can render images with higher quality and better fine-grained details, outperforming existing generalizable novel view synthesis methods in terms of PSNR, SSIM, and LPIPS. When trained on real data, our method also achieves state-of-the-art results. https://haoy945.github.io/contranerf/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">397.PaletteNeRF: Palette-Based Appearance Editing of Neural Radiance Fields</span><br>
                <span class="as">Kuang, ZhengfeiandLuan, FujunandBi, SaiandShu, ZhixinandWetzstein, GordonandSunkavalli, Kalyan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20691-20700.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地编辑神经辐射场的外观，同时保持照片的真实性。<br>
                    动机：尽管神经辐射场能够实现复杂场景的高保真度3D重建和新颖视图合成，但其外观的编辑效率和照片真实性仍有待探索。<br>
                    方法：提出了一种基于3D颜色分解的照片真实感外观编辑的新方法——PaletteNeRF。该方法将每个3D点的外观分解为场景共享的调色板基（即由一组NeRF类型函数定义的3D分割）的线性组合。<br>
                    效果：通过修改颜色调色板，用户能有效地编辑3D场景的外观。在定量和定性上都优于基线方法，特别是在编辑复杂真实世界场景的外观上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in neural radiance fields have enabled the high-fidelity 3D reconstruction of complex scenes for novel view synthesis. However, it remains underexplored how the appearance of such representations can be efficiently edited while maintaining photorealism. In this work, we present PaletteNeRF, a novel method for photorealistic appearance editing of neural radiance fields (NeRF) based on 3D color decomposition. Our method decomposes the appearance of each 3D point into a linear combination of palette-based bases (i.e., 3D segmentations defined by a group of NeRF-type functions) that are shared across the scene. While our palette-based bases are view-independent, we also predict a view-dependent function to capture the color residual (e.g., specular shading). During training, we jointly optimize the basis functions and the color palettes, and we also introduce novel regularizers to encourage the spatial coherence of the decomposition. Our method allows users to efficiently edit the appearance of the 3D scene by modifying the color palettes. We also extend our framework with compressed semantic features for semantic-aware appearance editing. We demonstrate that our technique is superior to baseline methods both quantitatively and qualitatively for appearance editing of complex real-world scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">398.NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects</span><br>
                <span class="as">Yan, ZhiwenandLi, ChenandLee, GimHee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_NeRF-DS_Neural_Radiance_Fields_for_Dynamic_Specular_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8285-8295.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的动态NeRF算法在渲染动态场景的单目RGB视频时，无法准确捕捉到镜面物体反射颜色的变化，导致渲染效果不佳。<br>
                    动机：为了解决这个问题，我们提出了一种新的方法，通过将神经辐射场函数的条件改为观察空间中的表面位置和方向，使镜面物体在不同姿态下映射到公共标准空间时能保持不同的反射颜色。<br>
                    方法：我们还添加了运动物体的掩码来引导变形场。由于镜面物体在运动过程中颜色会发生变化，掩码可以缓解仅依赖RGB监督时找不到时间对应关系的问题。<br>
                    效果：我们在包含不同真实环境中移动镜面物体的自收集数据集上评估了我们的模型。实验结果表明，与现有的NeRF模型相比，我们的方法显著提高了从单目RGB视频中重建移动镜面物体的质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dynamic Neural Radiance Field (NeRF) is a powerful algorithm capable of rendering photo-realistic novel view images from a monocular RGB video of a dynamic scene. Although it warps moving points across frames from the observation spaces to a common canonical space for rendering, dynamic NeRF does not model the change of the reflected color during the warping. As a result, this approach often fails drastically on challenging specular objects in motion. We address this limitation by reformulating the neural radiance field function to be conditioned on surface position and orientation in the observation space. This allows the specular surface at different poses to keep the different reflected colors when mapped to the common canonical space. Additionally, we add the mask of moving objects to guide the deformation field. As the specular surface changes color during motion, the mask mitigates the problem of failure to find temporal correspondences with only RGB supervision. We evaluate our model based on the novel view synthesis quality with a self-collected dataset of different moving specular objects in realistic environments. The experimental results demonstrate that our method significantly improves the reconstruction quality of moving specular objects from monocular RGB videos compared to the existing NeRF models. Our code and data are available at the project website https://github.com/JokerYan/NeRF-DS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">399.RealFusion: 360deg Reconstruction of Any Object From a Single Image</span><br>
                <span class="as">Melas-Kyriazi, LukeandLaina, IroandRupprecht, ChristianandVedaldi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_RealFusion_360deg_Reconstruction_of_Any_Object_From_a_Single_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8446-8455.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单一图像中重建物体的完整360度照片模型。<br>
                    动机：目前的问题是，从单一图像重建物体的3D模型是严重不适定的。<br>
                    方法：我们采用基于扩散的条件图像生成器，并设计一个提示，鼓励它"想象"物体的新视图。使用最新的DreamFusion方法，我们将给定的输入视图、条件先验和其他正则化器融合在一起，得到最终一致的重建结果。<br>
                    效果：与先前的方法相比，我们在基准图像上展示了最先进的重建结果。定性地说，我们的重建结果忠实地匹配了输入视图，并对其外观和3D形状进行了合理的推断，包括物体不可见的一侧。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the problem of reconstructing a full 360deg photographic model of an object from a single image of it. We do so by fitting a neural radiance field to the image, but find this problem to be severely ill-posed. We thus take an off-the-self conditional image generator based on diffusion and engineer a prompt that encourages it to "dream up" novel views of the object. Using the recent DreamFusion method, we fuse the given input view, the conditional prior, and other regularizers in a final, consistent reconstruction. We demonstrate state-of-the-art reconstruction results on benchmark images when compared to prior methods for monocular 3D reconstruction of objects. Qualitatively, our reconstructions provide a faithful match of the input view and a plausible extrapolation of its appearance and 3D shape, including to the side of the object not visible in the image.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">400.TensoIR: Tensorial Inverse Rendering</span><br>
                <span class="as">Jin, HaianandLiu, IsabellaandXu, PeijiaandZhang, XiaoshuaiandHan, SongfangandBi, SaiandZhou, XiaoweiandXu, ZexiangandSu, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_TensoIR_Tensorial_Inverse_Rendering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/165-174.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的基于张量分解和神经场的逆渲染方法，以解决以往仅使用多层感知器（MLP）的神经场容量低、计算成本高的问题。<br>
                    动机：扩展现有的最先进技术TensoRF，用于估计未知光照条件下多视角图像的场景几何、表面反射和环境照明，实现辐射场重建和物理模型估计，从而生成逼真的新视图合成和重照明。<br>
                    方法：将TensoRF扩展到联合实现辐射场重建和物理模型估计，利用其效率和可扩展性表示，准确模拟二级阴影效果（如阴影和间接照明），并支持在单个或多个未知光照条件下捕获的输入图像。<br>
                    效果：通过定性和定量地比较各种具有挑战性的合成和真实世界场景，证明该方法优于基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose TensoIR, a novel inverse rendering approach based on tensor factorization and neural fields. Unlike previous works that use purely MLP-based neural fields, thus suffering from low capacity and high computation costs, we extend TensoRF, a state-of-the-art approach for radiance field modeling, to estimate scene geometry, surface reflectance, and environment illumination from multi-view images captured under unknown lighting conditions. Our approach jointly achieves radiance field reconstruction and physically-based model estimation, leading to photo-realistic novel view synthesis and relighting. Benefiting from the efficiency and extensibility of the TensoRF-based representation, our method can accurately model secondary shading effects (like shadows and indirect lighting) and generally support input images captured under a single or multiple unknown lighting conditions. The low-rank tensor representation allows us to not only achieve fast and compact reconstruction but also better exploit shared information under an arbitrary number of capturing lighting conditions. We demonstrate the superiority of our method to baseline methods qualitatively and quantitatively on various challenging synthetic and real-world scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">401.DSFNet: Dual Space Fusion Network for Occlusion-Robust 3D Dense Face Alignment</span><br>
                <span class="as">Li, HeyuanandWang, BoandCheng, YuandKankanhalli, MohanandTan, RobbyT.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DSFNet_Dual_Space_Fusion_Network_for_Occlusion-Robust_3D_Dense_Face_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4531-4540.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的单目3D稠密人脸对齐方法对严重遮挡和大视角敏感，限制了其应用场景。<br>
                    动机：现有基于3DMM的方法直接回归模型系数，没有充分利用2D空间和语义的低级信息，这些信息实际上可以为面部形状和方向提供线索。<br>
                    方法：我们展示了如何共同在图像空间和模型空间中建模3D面部几何，以解决遮挡和视角问题。我们不是直接预测整个面部，而是首先通过密集预测在可见面部区域中回归图像空间特征。然后，我们根据可见区域的回归特征预测模型的系数，利用来自变形模型的整个面部几何的先验知识来完成不可见区域。我们还提出了一个融合网络，结合图像空间和模型空间预测的优点，以实现在无约束场景中的高鲁棒性和准确性。<br>
                    效果：由于提出的融合模块，我们的方法不仅对遮挡和大俯仰角具有鲁棒性（这是我们的图像空间方法的优点），而且对噪声和大偏航角也具有鲁棒性（这是我们的模型空间方法的优点）。综合评估表明，我们的方法优于最先进的方法。在3D稠密人脸对齐任务上，我们在AFLW2000-3D数据集上实现了3.80%的NME，比最先进的方法高出5.5%。代码可在https://github.com/lhyfst/DSFNet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sensitivity to severe occlusion and large view angles limits the usage scenarios of the existing monocular 3D dense face alignment methods. The state-of-the-art 3DMM-based method, directly regresses the model's coefficients, underutilizing the low-level 2D spatial and semantic information, which can actually offer cues for face shape and orientation. In this work, we demonstrate how modeling 3D facial geometry in image and model space jointly can solve the occlusion and view angle problems. Instead of predicting the whole face directly, we regress image space features in the visible facial region by dense prediction first. Subsequently, we predict our model's coefficients based on the regressed feature of the visible regions, leveraging the prior knowledge of whole face geometry from the morphable models to complete the invisible regions. We further propose a fusion network that combines the advantages of both the image and model space predictions to achieve high robustness and accuracy in unconstrained scenarios. Thanks to the proposed fusion module, our method is robust not only to occlusion and large pitch and roll view angles, which is the benefit of our image space approach, but also to noise and large yaw angles, which is the benefit of our model space method. Comprehensive evaluations demonstrate the superior performance of our method compared with the state-of-the-art methods. On the 3D dense face alignment task, we achieve 3.80% NME on the AFLW2000-3D dataset, which outperforms the state-of-the-art method by 5.5%. Code is available at https://github.com/lhyfst/DSFNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">402.MAIR: Multi-View Attention Inverse Rendering With 3D Spatially-Varying Lighting Estimation</span><br>
                <span class="as">Choi, JunYongandLee, SeokYeongandPark, HaesolandJung, Seung-WonandKim, Ig-JaeandCho, Junghyun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_MAIR_Multi-View_Attention_Inverse_Rendering_With_3D_Spatially-Varying_Lighting_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8392-8401.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种基于多视角图像的场景级逆渲染框架，以实现场景的几何、SVBRDF和3D空间变化的照明分解。<br>
                    动机：由于缺乏多视角HDR合成数据集，以往场景级逆渲染主要使用单视角图像进行研究。然而，多视角图像提供了丰富的场景信息，因此我们试图通过扩展OpenRooms数据集和设计处理多视角图像的有效管道，以及分割空间变化的照明，来成功执行基于多视角图像的场景级逆渲染。<br>
                    方法：我们的方法包括利用多视角图像对场景进行分解，设计处理多视角图像的高效管道，以及分割空间变化的照明。我们还开发了一个复杂的3D空间变化的照明体积，可以在任何3D位置插入照片真实的物体。<br>
                    效果：实验表明，我们的方法不仅在性能上优于基于单视角的方法，而且在未见过的真实世界场景上也表现出稳健的性能。此外，我们的复杂3D空间变化的照明体积允许在任何3D位置插入照片真实的物体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a scene-level inverse rendering framework that uses multi-view images to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying lighting. Because multi-view images provide a variety of information about the scene, multi-view images in object-level inverse rendering have been taken for granted. However, owing to the absence of multi-view HDR synthetic dataset, scene-level inverse rendering has mainly been studied using single-view image. We were able to successfully perform scene-level inverse rendering using multi-view images by expanding OpenRooms dataset and designing efficient pipelines to handle multi-view images, and splitting spatially-varying lighting. Our experiments show that the proposed method not only achieves better performance than single-view-based methods, but also achieves robust performance on unseen real-world scene. Also, our sophisticated 3D spatially-varying lighting volume allows for photorealistic object insertion in any 3D location.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">403.Human Pose As Compositional Tokens</span><br>
                <span class="as">Geng, ZigangandWang, ChunyuandWei, YixuanandLiu, ZeandLi, HouqiangandHu, Han</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/660-671.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地表示和预测人体姿势？<br>
                    动机：现有的人体姿势表示方法（如关节坐标向量或热图嵌入）虽然便于数据处理，但由于缺乏身体关节之间的依赖关系建模，可能会导致不真实的姿势估计。<br>
                    方法：提出了一种结构化表示方法——"Pose as Compositional Tokens"（PCT），将人体姿势表示为M个离散的、描述子结构及其相互依赖关节的标记。通过将姿势估计转化为分类任务，并使用预训练的解码器网络从标记中恢复姿势，无需进一步的后处理。<br>
                    效果：在一般场景下，该方法实现了与现有方法相当甚至更好的姿势估计结果，并在遮挡普遍存在的情况下仍能保持良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human pose is typically represented by a coordinate vector of body joints or their heatmap embeddings. While easy for data processing, unrealistic pose estimates are admitted due to the lack of dependency modeling between the body joints. In this paper, we present a structured representation, named Pose as Compositional Tokens (PCT), to explore the joint dependency. It represents a pose by M discrete tokens with each characterizing a sub-structure with several interdependent joints. The compositional design enables it to achieve a small reconstruction error at a low cost. Then we cast pose estimation as a classification task. In particular, we learn a classifier to predict the categories of the M tokens from an image. A pre-learned decoder network is used to recover the pose from the tokens without further post-processing. We show that it achieves better or comparable pose estimation results as the existing methods in general scenarios, yet continues to work well when occlusion occurs, which is ubiquitous in practice. The code and models are publicly available at https://github.com/Gengzigang/PCT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">404.HuManiFlow: Ancestor-Conditioned Normalising Flows on SO(3) Manifolds for Human Pose and Shape Distribution Estimation</span><br>
                <span class="as">Sengupta, AkashandBudvytis, IgnasandCipolla, Roberto</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sengupta_HuManiFlow_Ancestor-Conditioned_Normalising_Flows_on_SO3_Manifolds_for_Human_Pose_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4779-4789.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目3D人体姿态和形状估计是一个病态问题，因为多个3D解决方案可以解释一个主题的2D图像。<br>
                    动机：最近的方法是预测在给定图像条件下可能的3D姿态和形状参数的概率分布，但这些方法在三个关键属性之间存在权衡：准确性、样本输入一致性和样本多样性。<br>
                    方法：我们的方法HuManiFlow同时预测准确、一致和多样的分布。我们使用人体运动树将全身姿态分解为祖先条件的身体各部分的姿态分布，并以自回归的方式实现。身体各部分的分布使用正则化流实现，尊重SO(3)的流形结构，即身体各部分姿态的李群。<br>
                    效果：实验结果表明，病态但普遍存在的3D点估计损失会降低样本多样性，只采用概率训练损失。HuManiFlow在3DPW和SSP-3D数据集上优于最先进的概率方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular 3D human pose and shape estimation is an ill-posed problem since multiple 3D solutions can explain a 2D image of a subject. Recent approaches predict a probability distribution over plausible 3D pose and shape parameters conditioned on the image. We show that these approaches exhibit a trade-off between three key properties: (i) accuracy - the likelihood of the ground-truth 3D solution under the predicted distribution, (ii) sample-input consistency - the extent to which 3D samples from the predicted distribution match the visible 2D image evidence, and (iii) sample diversity - the range of plausible 3D solutions modelled by the predicted distribution. Our method, HuManiFlow, predicts simultaneously accurate, consistent and diverse distributions. We use the human kinematic tree to factorise full body pose into ancestor-conditioned per-body-part pose distributions in an autoregressive manner. Per-body-part distributions are implemented using normalising flows that respect the manifold structure of SO(3), the Lie group of per-body-part poses. We show that ill-posed, but ubiquitous, 3D point estimate losses reduce sample diversity, and employ only probabilistic training losses. HuManiFlow outperforms state-of-the-art probabilistic approaches on the 3DPW and SSP-3D datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">405.Semantic Ray: Learning a Generalizable Semantic Field With Cross-Reprojection Attention</span><br>
                <span class="as">Liu, FangfuandZhang, ChubinandZheng, YuandDuan, Yueqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Semantic_Ray_Learning_a_Generalizable_Semantic_Field_With_Cross-Reprojection_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17386-17396.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从多个场景中学习准确、高效且可泛化的语义辐射场。<br>
                    动机：现有的NeRFs主要关注神经场景渲染、图像合成和多视图重建任务，而Semantic-NeRF等少数尝试探索使用NeRF结构进行高级别语义理解。然而，Semantic-NeRF通过单个射线的多个头部同时学习颜色和语义标签，但单个射线无法提供丰富的语义信息。因此，Semantic NeRF依赖于位置编码，并且需要为每个场景训练一个特定的模型。<br>
                    方法：我们提出Semantic Ray (S-Ray)来充分利用沿射线方向的多视图重投影的语义信息。为了解决直接在多视图重投影的射线上执行密集注意力会导致计算成本高昂的问题，我们设计了一个跨重投影注意力模块，该模块具有连续的 intra-view radial 和 cross-view sparse attentions，它沿着重投影的射线分解上下文信息并跨越多个视图，然后通过堆叠模块收集密集连接。<br>
                    效果：实验表明，我们的S-Ray能够从多个场景中学习，并且具有很强的泛化能力，可以适应未见过的场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we aim to learn a semantic radiance field from multiple scenes that is accurate, efficient and generalizable. While most existing NeRFs target at the tasks of neural scene rendering, image synthesis and multi-view reconstruction, there are a few attempts such as Semantic-NeRF that explore to learn high-level semantic understanding with the NeRF structure. However, Semantic-NeRF simultaneously learns color and semantic label from a single ray with multiple heads, where the single ray fails to provide rich semantic information. As a result, Semantic NeRF relies on positional encoding and needs to train one specific model for each scene. To address this, we propose Semantic Ray (S-Ray) to fully exploit semantic information along the ray direction from its multi-view reprojections. As directly performing dense attention over multi-view reprojected rays would suffer from heavy computational cost, we design a Cross-Reprojection Attention module with consecutive intra-view radial and cross-view sparse attentions, which decomposes contextual information along reprojected rays and cross multiple views and then collects dense connections by stacking the modules. Experiments show that our S-Ray is able to learn from multiple scenes, and it presents strong generalization ability to adapt to unseen scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">406.ORCa: Glossy Objects As Radiance-Field Cameras</span><br>
                <span class="as">Tiwary, KushagraandDave, AkshatandBehari, NikhilandKlinghoffer, TzofiandVeeraraghavan, AshokandRaskar, Ramesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tiwary_ORCa_Glossy_Objects_As_Radiance-Field_Cameras_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20773-20782.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用高光物体的反射信息，将其转化为相机，实现超越视场和看似不可能的视角的成像。<br>
                    动机：高光物体的反射包含了周围环境的重要和隐藏信息，通过将这些物体转化为相机，可以解锁令人兴奋的应用，如超越相机视场和从看似不可能的角度（如人眼的反射）进行成像。<br>
                    方法：将未知几何形状的高光物体转化为辐射场相机，从物体的视角对世界进行成像。主要思路是将物体表面转化为虚拟传感器，捕捉投射的反射作为5D环境辐射场的2D投影，该辐射场对物体及其周围可见。<br>
                    效果：通过恢复环境辐射场，不仅可以实现从物体到其周围环境的深度和辐射估计，还可以实现超越视场的新视图合成，即渲染仅直接可见于场景中存在高光物体但观察者不可见的新视图。此外，使用辐射场可以在场景中由靠近物体引起的遮挡物周围进行成像。该方法在多视角物体图像上进行端到端训练，并联合估计物体几何、漫反射和5D环境辐射场。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reflections on glossy objects contain valuable and hidden information about the surrounding environment. By converting these objects into cameras, we can unlock exciting applications, including imaging beyond the camera's field-of-view and from seemingly impossible vantage points, e.g. from reflections on the human eye. However, this task is challenging because reflections depend jointly on object geometry, material properties, the 3D environment, and the observer's viewing direction. Our approach converts glossy objects with unknown geometry into radiance-field cameras to image the world from the object's perspective. Our key insight is to convert the object surface into a virtual sensor that captures cast reflections as a 2D projection of the 5D environment radiance field visible to and surrounding the object. We show that recovering the environment radiance fields enables depth and radiance estimation from the object to its surroundings in addition to beyond field-of-view novel-view synthesis, i.e. rendering of novel views that are only directly visible to the glossy object present in the scene, but not the observer. Moreover, using the radiance field we can image around occluders caused by close-by objects in the scene. Our method is trained end-to-end on multi-view images of the object and jointly estimates object geometry, diffuse radiance, and the 5D environment radiance field.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">407.SECAD-Net: Self-Supervised CAD Reconstruction by Learning Sketch-Extrude Operations</span><br>
                <span class="as">Li, PuandGuo, JianweiandZhang, XiaopengandYan, Dong-Ming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SECAD-Net_Self-Supervised_CAD_Reconstruction_by_Learning_Sketch-Extrude_Operations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16816-16826.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从原始几何形状中反向工程CAD模型，这是一个经典但艰巨的研究问题。<br>
                    动机：现有的基于学习的方法严重依赖标签，或者重建的CAD形状不易编辑。<br>
                    方法：我们提出了SECAD-Net，一种端到端的神经网络，旨在以自监督的方式重建简洁且易于编辑的CAD模型。我们从现代CAD软件最常用的建模语言中得到启发，学习2D草图和3D拉伸参数，通过这些参数可以从原始形状生成一组拉伸圆柱体。<br>
                    效果：我们在ABC和Fusion 360数据集上进行了大量实验，证明了我们方法的有效性，并在包括密切相关的有监督CAD重建方法在内的最先进技术中脱颖而出。我们还将此方法应用于CAD编辑和单视图CAD重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reverse engineering CAD models from raw geometry is a classic but strenuous research problem. Previous learning-based methods rely heavily on labels due to the supervised design patterns or reconstruct CAD shapes that are not easily editable. In this work, we introduce SECAD-Net, an end-to-end neural network aimed at reconstructing compact and easy-to-edit CAD models in a self-supervised manner. Drawing inspiration from the modeling language that is most commonly used in modern CAD software, we propose to learn 2D sketches and 3D extrusion parameters from raw shapes, from which a set of extrusion cylinders can be generated by extruding each sketch from a 2D plane into a 3D body. By incorporating the Boolean operation (i.e., union), these cylinders can be combined to closely approximate the target geometry. We advocate the use of implicit fields for sketch representation, which allows for creating CAD variations by interpolating latent codes in the sketch latent space. Extensive experiments on both ABC and Fusion 360 datasets demonstrate the effectiveness of our method, and show superiority over state-of-the-art alternatives including the closely related method for supervised CAD reconstruction. We further apply our approach to CAD editing and single-view CAD reconstruction. The code is released at https://github.com/BunnySoCrazy/SECAD-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">408.Common Pets in 3D: Dynamic New-View Synthesis of Real-Life Deformable Categories</span><br>
                <span class="as">Sinha, SamarthandShapovalov, RomanandReizenstein, JeremyandRocco, IgnacioandNeverova, NataliaandVedaldi, AndreaandNovotny, David</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sinha_Common_Pets_in_3D_Dynamic_New-View_Synthesis_of_Real-Life_Deformable_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4881-4891.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从稀疏视图中获取物体的逼真重建？<br>
                    动机：早期的稀疏刚性物体重建方法可以从CO3D等大型数据集学习合适的重建先验，但这种方法无法应用于动态物体。<br>
                    方法：我们使用猫和狗作为代表示例，引入了CoP3D，这是一个包含约4200个独特宠物的众包视频集合。我们还提出了Tracker-NeRF，一种从我们的数据集学习4D重建的方法。在测试时，给定未见过序列的少量视频帧，Tracker-NeRF预测3D点的轨迹和动力学并生成新视图，插值视点和时间。<br>
                    效果：在CoP3D上的结果比现有的基线在非刚性新视图合成性能上有显著改善。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Obtaining photorealistic reconstructions of objects from sparse views is inherently ambiguous and can only be achieved by learning suitable reconstruction priors. Earlier works on sparse rigid object reconstruction successfully learned such priors from large datasets such as CO3D. In this paper, we extend this approach to dynamic objects. We use cats and dogs as a representative example and introduce Common Pets in 3D (CoP3D), a collection of crowd-sourced videos showing around 4,200 distinct pets. CoP3D is one of the first large-scale datasets for benchmarking non-rigid 3D reconstruction "in the wild". We also propose Tracker-NeRF, a method for learning 4D reconstruction from our dataset. At test time, given a small number of video frames of an unseen sequence, Tracker-NeRF predicts the trajectories and dynamics of the 3D points and generates new views, interpolating viewpoint and time. Results on CoP3D reveal significantly better non-rigid new-view synthesis performance than existing baselines. The data is available on the project webpage: https://cop3d.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">409.Normal-Guided Garment UV Prediction for Human Re-Texturing</span><br>
                <span class="as">Jafarian, YasaminandWang, TuanfengY.andCeylan, DuyguandYang, JimeiandCarr, NathanandZhou, YiandPark, HyunSoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jafarian_Normal-Guided_Garment_UV_Prediction_for_Human_Re-Texturing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4627-4636.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何以物理上可信的方式编辑人类视频，考虑到衣物的复杂几何变形和外观变化。<br>
                    动机：传统的3D重建方法在处理动态衣物时面临挑战，需要一种新的方法来估计衣物的几何感知纹理地图（UV map）。<br>
                    方法：提出一种无需3D重建即可编辑穿衣人类图像和视频的方法。通过利用从图像中预测的3D表面法线，设计一个保留底层3D表面等距性的UV映射。该方法以自监督的方式捕捉衣物的底层几何结构，无需真实UV图的标注，并可扩展到预测时间连贯的UV映射。<br>
                    效果：在真实和合成数据上，该方法优于最先进的人类UV图估计方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Clothes undergo complex geometric deformations, which lead to appearance changes. To edit human videos in a physically plausible way, a texture map must take into account not only the garment transformation induced by the body movements and clothes fitting, but also its 3D fine-grained surface geometry. This poses, however, a new challenge of 3D reconstruction of dynamic clothes from an image or a video. In this paper, we show that it is possible to edit dressed human images and videos without 3D reconstruction. We estimate a geometry aware texture map between the garment region in an image and the texture space, a.k.a, UV map. Our UV map is designed to preserve isometry with respect to the underlying 3D surface by making use of the 3D surface normals predicted from the image. Our approach captures the underlying geometry of the garment in a self-supervised way, requiring no ground truth annotation of UV maps and can be readily extended to predict temporally coherent UV maps. We demonstrate that our method outperforms the state-of-the-art human UV map estimation approaches on both real and synthetic data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">410.Computational Flash Photography Through Intrinsics</span><br>
                <span class="as">Maralan, SepidehSarajianandCareaga, ChrisandAksoy, Yagiz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Maralan_Computational_Flash_Photography_Through_Intrinsics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16654-16662.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究闪光灯在摄影中的计算控制，特别是在有或无闪光灯的情况下。<br>
                    动机：目前，闪光灯的使用是二分的，一旦拍摄照片，对闪光灯的特性（如强度或颜色）的控制是有限的。<br>
                    方法：我们提出了一个物理驱动的内在公式来形成闪光照片，并开发了分别用于闪光和无闪光照片的闪光分解和生成方法。<br>
                    效果：我们的实验结果表明，这种内在公式优于文献中的其他方法，使我们能够计算控制自然环境下图像中的闪光灯。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Flash is an essential tool as it often serves as the sole controllable light source in everyday photography. However, the use of flash is a binary decision at the time a photograph is captured with limited control over its characteristics such as strength or color. In this work, we study the computational control of the flash light in photographs taken with or without flash. We present a physically motivated intrinsic formulation for flash photograph formation and develop flash decomposition and generation methods for flash and no-flash photographs, respectively. We demonstrate that our intrinsic formulation outperforms alternatives in the literature and allows us to computationally control flash in in-the-wild images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">411.BITE: Beyond Priors for Improved Three-D Dog Pose Estimation</span><br>
                <span class="as">R\&quot;uegg, NadineandTripathi, ShashankandSchindler, KonradandBlack, MichaelJ.andZuffi, Silvia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8867-8876.png><br>
            
            <span class="tt"><span class="t0">研究问题：从图像中推断狗的3D形状和姿势。<br>
                    动机：由于缺乏3D训练数据，这个问题具有挑战性，最好的方法落后于那些用于估计人类形状和姿势的方法。<br>
                    方法：首先，学习一种狗专用的3D参数模型，称为D-SMAL。其次，利用与地面的接触作为侧信息来解决这个问题。然后，开发一种新的神经网络架构来推断和利用这种接触信息。最后，创建包含扫描的3D狗的渲染图像的合成数据集进行评估。<br>
                    效果：通过这些进步，我们的方法在恢复狗的形状和姿势方面比最先进的技术有了显著的改进，并在3D中进行了评估。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">412.SeSDF: Self-Evolved Signed Distance Field for Implicit 3D Clothed Human Reconstruction</span><br>
                <span class="as">Cao, YukangandHan, KaiandWong, Kwan-YeeK.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_SeSDF_Self-Evolved_Signed_Distance_Field_for_Implicit_3D_Clothed_Human_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4647-4657.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从单张图像或未校准的多视图图像重建穿衣人体的问题。<br>
                    动机：现有的方法在重建穿衣人体的详细几何形状上存在困难，且多视图重建通常需要校准设置。<br>
                    方法：提出一个灵活的框架，利用参数化的SMPL-X模型，可以在未校准的设置下，根据任意数量的输入图像重建穿衣人体模型。核心是我们的新型自我进化有符号距离场（SeSDF）模块，该模块可以学习变形从拟合的SMPL-X模型得到的有符号距离场（SDF），从而编码出反映实际穿衣人体的详细几何形状，以实现更好的重建。<br>
                    效果：我们在公共基准上全面评估了我们的框架，无论在定性还是定量上都明显优于最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of clothed human reconstruction from a single image or uncalibrated multi-view images. Existing methods struggle with reconstructing detailed geometry of a clothed human and often require a calibrated setting for multi-view reconstruction. We propose a flexible framework which, by leveraging the parametric SMPL-X model, can take an arbitrary number of input images to reconstruct a clothed human model under an uncalibrated setting. At the core of our framework is our novel self-evolved signed distance field (SeSDF) module which allows the framework to learn to deform the signed distance field (SDF) derived from the fitted SMPL-X model, such that detailed geometry reflecting the actual clothed human can be encoded for better reconstruction. Besides, we propose a simple method for self-calibration of multi-view images via the fitted SMPL-X parameters. This lifts the requirement of tedious manual calibration and largely increases the flexibility of our method. Further, we introduce an effective occlusion-aware feature fusion strategy to account for the most useful features to reconstruct the human model. We thoroughly evaluate our framework on public benchmarks, demonstrating significant superiority over the state-of-the-arts both qualitatively and quantitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">413.Deep Depth Estimation From Thermal Image</span><br>
                <span class="as">Shin, UkcheolandPark, JinsunandKweon, InSo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1043-1053.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现自动驾驶汽车在恶劣天气条件下的鲁棒和精确的几何理解，以实现高度自主性。<br>
                    动机：目前的自动驾驶算法依赖于可见光波段，容易受到天气和光照条件的影响。长波红外相机（热成像相机）可能是实现高度鲁棒性的潜在解决方案，但缺乏大型数据集和公开基准结果。<br>
                    方法：本文首先构建了一个大规模的多光谱立体（MS^2）数据集，包括立体RGB、立体NIR、立体热成像和立体激光雷达数据以及GNSS/IMU信息。收集的数据集提供了约195K个同步的数据对，来自城市、住宅、道路、校园和郊区的早晨、白天和夜晚，在晴朗、多云和雨天的条件下。其次，我们对基于可见光波段设计的单目和双目深度估计算法进行了详尽的验证过程，以在热成像领域基准它们的性能。最后，我们提出了一个统一的深度网络，从条件随机场的角度有效地连接了单目深度和双目深度任务。<br>
                    效果：我们的数据集和源代码可在https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset获取。实验结果表明，我们的方法在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust and accurate geometric understanding against adverse weather conditions is one top prioritized conditions to achieve a high-level autonomy of self-driving cars. However, autonomous driving algorithms relying on the visible spectrum band are easily impacted by weather and lighting conditions. A long-wave infrared camera, also known as a thermal imaging camera, is a potential rescue to achieve high-level robustness. However, the missing necessities are the well-established large-scale dataset and public benchmark results. To this end, in this paper, we first built a large-scale Multi-Spectral Stereo (MS^2) dataset, including stereo RGB, stereo NIR, stereo thermal, and stereo LiDAR data along with GNSS/IMU information. The collected dataset provides about 195K synchronized data pairs taken from city, residential, road, campus, and suburban areas in the morning, daytime, and nighttime under clear-sky, cloudy, and rainy conditions. Secondly, we conduct an exhaustive validation process of monocular and stereo depth estimation algorithms designed on visible spectrum bands to benchmark their performance in the thermal image domain. Lastly, we propose a unified depth network that effectively bridges monocular depth and stereo depth tasks from a conditional random field approach perspective. Our dataset and source code are available at https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">414.Building Rearticulable Models for Arbitrary 3D Objects From 4D Point Clouds</span><br>
                <span class="as">Liu, ShaoweiandGupta, SaurabhandWang, Shenlong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Building_Rearticulable_Models_for_Arbitrary_3D_Objects_From_4D_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21138-21147.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何构建可重构模型，用于处理日常生活中由任意数量部分组成的任意连接方式的对象。<br>
                    动机：为了能够从点云视频中识别出不同的对象部分，以及各部分之间的连接关系和关节属性。<br>
                    方法：通过使用新颖的能量最小化框架，联合优化部件分割、变换和运动学，实现对对象的建模。<br>
                    效果：在新的关节式机器人数据集和包含日常物品的Sapiens数据集上进行测试，实验表明该方法在各项指标上都优于两种领先的先前工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We build rearticulable models for arbitrary everyday man-made objects containing an arbitrary number of parts that are connected together in arbitrary ways via 1-degree-of-freedom joints. Given point cloud videos of such everyday objects, our method identifies the distinct object parts, what parts are connected to what other parts, and the properties of the joints connecting each part pair. We do this by jointly optimizing the part segmentation, transformation, and kinematics using a novel energy minimization framework. Our inferred animatable models, enables retargeting to novel poses with sparse point correspondences guidance. We test our method on a new articulating robot dataset and the Sapiens dataset with common daily objects. Experiments show that our method outperforms two leading prior works on various metrics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">415.Towards Stable Human Pose Estimation via Cross-View Fusion and Foot Stabilization</span><br>
                <span class="as">Zhuo, Li{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuo_Towards_Stable_Human_Pose_Estimation_via_Cross-View_Fusion_and_Foot_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/650-659.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目图像中稳定人体姿态估计存在两个主要难题，一是不同视角（如前视图、侧视图和顶视图）由于深度模糊导致表现不一致；二是在复杂的人体姿态估计中，如舞蹈和运动，脚部姿势以及脚与地面的交互作用起着重要作用，但大多数通用方法和数据集都忽略了这一点。<br>
                    动机：为了解决上述问题，本文提出了一种基于视觉变换器编码器的跨视角融合（CVF）模块，以获得更好的3D中间表示并减轻视角不一致的问题。同时，引入了基于优化的方法来重建一般多视角数据集（包括AIST++和Human3.6M）中的脚部姿势和脚地接触。此外，还创新了可逆的运动学拓扑策略，将接触信息用于带有脚部姿势回归器的全身。<br>
                    方法：通过在流行的基准测试集上进行大量实验，证明了该方法优于最先进的方法，在3DPW测试集上实现了40.1mm的PA-MPJPE，在AIST++测试集上实现了43.8mm。<br>
                    效果：本文提出的方法在各种知识驱动任务上取得了显著改进，并在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Towards stable human pose estimation from monocular images, there remain two main dilemmas. On the one hand, the different perspectives, i.e., front view, side view, and top view, appear the inconsistent performances due to the depth ambiguity. On the other hand, foot posture plays a significant role in complicated human pose estimation, i.e., dance and sports, and foot-ground interaction, but unfortunately, it is omitted in most general approaches and datasets. In this paper, we first propose the Cross-View Fusion (CVF) module to catch up with better 3D intermediate representation and alleviate the view inconsistency based on the vision transformer encoder. Then the optimization-based method is introduced to reconstruct the foot pose and foot-ground contact for the general multi-view datasets including AIST++ and Human3.6M. Besides, the reversible kinematic topology strategy is innovated to utilize the contact information into the full-body with foot pose regressor. Extensive experiments on the popular benchmarks demonstrate that our method outperforms the state-of-the-art approaches by achieving 40.1mm PA-MPJPE on the 3DPW test set and 43.8mm on the AIST++ test set.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">416.Few-Shot Non-Line-of-Sight Imaging With Signal-Surface Collaborative Regularization</span><br>
                <span class="as">Liu, XintongandWang, JianyuandXiao, LepingandFu, XingandQiu, LingyunandShi, Zuoqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Few-Shot_Non-Line-of-Sight_Imaging_With_Signal-Surface_Collaborative_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13303-13312.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过非视线成像技术，从多次反射的光中重建目标？<br>
                    动机：现有的大多数方法需要对中继面上的密集点进行光栅扫描以获取高质量的重建，这需要很长的采集时间。<br>
                    方法：我们提出了一个信号-表面协同正则化（SSCR）框架，该框架使用贝叶斯推理设计了信号、物体的3D体素表示和目标的2D表面描述的联合正则化。<br>
                    效果：在合成和实验数据集上的实验表明，该方法在共聚焦和非共聚焦设置下都有效。我们报告了仅使用公共数据集中的5 x 5共聚焦测量来重建具有复杂几何结构的隐藏目标，这表明常规测量过程的速度提高了10,000倍。此外，该方法在稀疏测量下具有低时间和内存复杂度。该方法在实时非视线成像应用中具有巨大潜力，如救援行动和自动驾驶。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The non-line-of-sight imaging technique aims to reconstruct targets from multiply reflected light. For most existing methods, dense points on the relay surface are raster scanned to obtain high-quality reconstructions, which requires a long acquisition time. In this work, we propose a signal-surface collaborative regularization (SSCR) framework that provides noise-robust reconstructions with a minimal number of measurements. Using Bayesian inference, we design joint regularizations of the estimated signal, the 3D voxel-based representation of the objects, and the 2D surface-based description of the targets. To our best knowledge, this is the first work that combines regularizations in mixed dimensions for hidden targets. Experiments on synthetic and experimental datasets illustrated the efficiency of the proposed method under both confocal and non-confocal settings. We report the reconstruction of the hidden targets with complex geometric structures with only 5 x 5 confocal measurements from public datasets, indicating an acceleration of the conventional measurement process by a factor of 10,000. Besides, the proposed method enjoys low time and memory complexity with sparse measurements. Our approach has great potential in real-time non-line-of-sight imaging applications such as rescue operations and autonomous driving.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">417.RelightableHands: Efficient Neural Relighting of Articulated Hand Models</span><br>
                <span class="as">Iwase, ShunandSaito, ShunsukeandSimon, TomasandLombardi, StephenandBagautdinov, TimurandJoshi, RohanandPrada, FabianandShiratori, TakaakiandSheikh, YaserandSaragih, Jason</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16663-16673.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实时渲染高保真个性化手部，并在新的照明条件下进行动画？<br>
                    动机：目前的神经光照方法在合成手部时需要大量的计算，且无法实时渲染。<br>
                    方法：采用教师-学生框架，教师模型通过单点光源从光场图像中学习外观，生成的手部可以在任意照明下渲染，但计算量大。利用教师模型渲染的图像作为训练数据，学生模型直接预测自然光照下的外观，实现实时渲染。为了实现泛化，我们使用物理启发的光照特征（如可见性、漫反射和镜面反射）对学生模型进行条件设置，这些特征与后续的全局光照效果有强相关性，可以作为神经光照网络的条件数据。<br>
                    效果：实验表明，我们的光照特征表示优于基线方法，能够在实时速度下对两只互动的手进行照片级真实感光照。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present the first neural relighting approach for rendering high-fidelity personalized hands that can be animated in real-time under novel illumination. Our approach adopts a teacher-student framework, where the teacher learns appearance under a single point light from images captured in a light-stage, allowing us to synthesize hands in arbitrary illuminations but with heavy compute. Using images rendered by the teacher model as training data, an efficient student model directly predicts appearance under natural illuminations in real-time. To achieve generalization, we condition the student model with physics-inspired illumination features such as visibility, diffuse shading, and specular reflections computed on a coarse proxy geometry, maintaining a small computational overhead. Our key insight is that these features have strong correlation with subsequent global light transport effects, which proves sufficient as conditioning data for the neural relighting network. Moreover, in contrast to bottleneck illumination conditioning, these features are spatially aligned based on underlying geometry, leading to better generalization to unseen illuminations and poses. In our experiments, we demonstrate the efficacy of our illumination feature representations, outperforming baseline approaches. We also show that our approach can photorealistically relight two interacting hands at real-time speeds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">418.AnyFlow: Arbitrary Scale Optical Flow With Implicit Neural Representation</span><br>
                <span class="as">Jung, HyunyoungandHui, ZhuoandLuo, LeiandYang, HaitaoandLiu, FengandYoo, SungjooandRanjan, RakeshandDemandolx, Denis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_AnyFlow_Arbitrary_Scale_Optical_Flow_With_Implicit_Neural_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5455-5465.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确估计低分辨率输入的光学流？<br>
                    动机：在实际应用中，为了降低计算成本，通常需要将输入调整为较小的尺寸。然而，这会使估计变得更具挑战性，因为物体和运动范围会变小。尽管现有的方法在高分辨率输入上表现良好，但在低分辨率输入时，它们往往无法准确建模小物体和精确边界。<br>
                    方法：我们提出了AnyFlow，这是一种能够从各种分辨率的图像中准确估计流的强大网络。通过将光学流表示为连续的基于坐标的表示，AnyFlow可以从低分辨率输入生成任意尺度的输出，从而在捕获微小物体和保留细节方面优于现有方法。<br>
                    效果：我们在KITTI数据集上建立了新的跨数据集泛化性能的最先进水平，同时在其他SOTA方法上实现了相当的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To apply optical flow in practice, it is often necessary to resize the input to smaller dimensions in order to reduce computational costs. However, downsizing inputs makes the estimation more challenging because objects and motion ranges become smaller. Even though recent approaches have demonstrated high-quality flow estimation, they tend to fail to accurately model small objects and precise boundaries when the input resolution is lowered, restricting their applicability to high-resolution inputs. In this paper, we introduce AnyFlow, a robust network that estimates accurate flow from images of various resolutions. By representing optical flow as a continuous coordinate-based representation, AnyFlow generates outputs at arbitrary scales from low-resolution inputs, demonstrating superior performance over prior works in capturing tiny objects with detail preservation on a wide range of scenes. We establish a new state-of-the-art performance of cross-dataset generalization on the KITTI dataset, while achieving comparable accuracy on the online benchmarks to other SOTA methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">419.Shakes on a Plane: Unsupervised Depth Estimation From Unstabilized Photography</span><br>
                <span class="as">Chugunov, IlyaandZhang, YuxuanandHeide, Felix</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chugunov_Shakes_on_a_Plane_Unsupervised_Depth_Estimation_From_Unstabilized_Photography_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13240-13251.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的移动摄影流程在拍摄和合并短序列帧以恢复增强图像时，往往忽视了他们捕捉到的场景的3D特性，将图像间的像素运动视为2D聚合问题。<br>
                    动机：作者们发现，在两秒钟内捕获的42张12百万像素的RAW帧的"长曝光"中，仅自然的手震就有足够的视差信息来恢复高质量的场景深度。<br>
                    方法：作者们设计了一种测试时间优化方法，该方法将神经RGB-D表示拟合到长曝光数据，并同时估计场景深度和相机运动。他们的平面加深度模型是端到端训练的，并通过控制网络在训练过程中何时可以访问哪些多分辨率体积特征来进行粗到精的细化。<br>
                    效果：实验验证了这种方法，无需额外的硬件或单独的数据预处理和姿态估计步骤，就可以实现几何精确的深度重建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern mobile burst photography pipelines capture and merge a short sequence of frames to recover an enhanced image, but often disregard the 3D nature of the scene they capture, treating pixel motion between images as a 2D aggregation problem. We show that in a "long-burst", forty-two 12-megapixel RAW frames captured in a two-second sequence, there is enough parallax information from natural hand tremor alone to recover high-quality scene depth. To this end, we devise a test-time optimization approach that fits a neural RGB-D representation to long-burst data and simultaneously estimates scene depth and camera motion. Our plane plus depth model is trained end-to-end, and performs coarse-to-fine refinement by controlling which multi-resolution volume features the network has access to at what time during training. We validate the method experimentally, and demonstrate geometrically accurate depth reconstructions with no additional hardware or separate data pre-processing and pose-estimation steps.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">420.ShapeClipper: Scalable 3D Shape Learning From Single-View Images via Geometric and CLIP-Based Consistency</span><br>
                <span class="as">Huang, ZixuanandJampani, VarunandThai, AnhandLi, YuanzhenandStojanov, StefanandRehg, JamesM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_ShapeClipper_Scalable_3D_Shape_Learning_From_Single-View_Images_via_Geometric_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12912-12922.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法，通过单视图RGB图像重建3D物体形状。<br>
                    动机：现有的3D模型重建方法需要大量的3D、多视图或相机姿态标注，过程繁琐。<br>
                    方法：作者提出了ShapeClipper，该方法通过一组单视图分割图像学习形状重建。主要思想是通过基于CLIP的形状一致性促进形状学习，鼓励具有相似CLIP编码的对象共享相似的形状。同时，利用现成的法线作为额外的几何约束，使模型能够更好地进行详细的表面几何的自下而上推理。这两种新的一致性约束用于正则化模型，提高了其学习全局形状结构和局部几何细节的能力。<br>
                    效果：在Pix3D、Pascal3D+和OpenImages三个具有挑战性的现实世界数据集上评估了该方法，结果优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ShapeClipper, a novel method that reconstructs 3D object shapes from real-world single-view RGB images. Instead of relying on laborious 3D, multi-view or camera pose annotation, ShapeClipper learns shape reconstruction from a set of single-view segmented images. The key idea is to facilitate shape learning via CLIP-based shape consistency, where we encourage objects with similar CLIP encodings to share similar shapes. We also leverage off-the-shelf normals as an additional geometric constraint so the model can learn better bottom-up reasoning of detailed surface geometry. These two novel consistency constraints, when used to regularize our model, improve its ability to learn both global shape structure and local geometric details. We evaluate our method over three challenging real-world datasets, Pix3D, Pascal3D+, and OpenImages, where we achieve superior performance over state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">421.Exact-NeRF: An Exploration of a Precise Volumetric Parameterization for Neural Radiance Fields</span><br>
                <span class="as">Isaac-Medina, BrianK.S.andWillcocks, ChrisG.andBreckon, TobyP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Isaac-Medina_Exact-NeRF_An_Exploration_of_a_Precise_Volumetric_Parameterization_for_Neural_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/66-75.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经辐射场（NeRF）模型在渲染场景视图时，由于采样点的宽度为零可能导致模糊的表示，从而产生诸如混叠等渲染伪影。<br>
                    动机：为了解决这个问题，最近的mip-NeRF提出了一种基于锥形视景体的整体位置编码（IPE）。然而，这种方法在处理深度较大的场景对象时，会产生高度拉长的区域，导致其性能下降。<br>
                    方法：本文提出了一种使用金字塔式积分公式计算IPE的精确方法，称为Exact-NeRF。这种方法为NeRF领域提供了第一个精确的解析解决方案。<br>
                    效果：实验结果表明，这种精确的方法（Exact-NeRF）与mip-NeRF的精度相匹配，并且无需进一步修改即可自然扩展到更具挑战性的场景，如无界场景。这项贡献旨在解决早期NeRF工作中关于视景体近似的问题，并为未来NeRF扩展中考虑解析解决方案提供见解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRF) have attracted significant attention due to their ability to synthesize novel scene views with great accuracy. However, inherent to their underlying formulation, the sampling of points along a ray with zero width may result in ambiguous representations that lead to further rendering artifacts such as aliasing in the final scene. To address this issue, the recent variant mip-NeRF proposes an Integrated Positional Encoding (IPE) based on a conical view frustum. Although this is expressed with an integral formulation, mip-NeRF instead approximates this integral as the expected value of a multivariate Gaussian distribution. This approximation is reliable for short frustums but degrades with highly elongated regions, which arises when dealing with distant scene objects under a larger depth of field. In this paper, we explore the use of an exact approach for calculating the IPE by using a pyramid-based integral formulation instead of an approximated conical-based one. We denote this formulation as Exact-NeRF and contribute the first approach to offer a precise analytical solution to the IPE within the NeRF domain. Our exploratory work illustrates that such an exact formulation (Exact-NeRF) matches the accuracy of mip-NeRF and furthermore provides a natural extension to more challenging scenarios without further modification, such as in the case of unbounded scenes. Our contribution aims to both address the hitherto unexplored issues of frustum approximation in earlier NeRF work and additionally provide insight into the potential future consideration of analytical solutions in future NeRF extensions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">422.Non-Line-of-Sight Imaging With Signal Superresolution Network</span><br>
                <span class="as">Wang, JianyuandLiu, XintongandXiao, LepingandShi, ZuoqiangandQiu, LingyunandFu, Xing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Non-Line-of-Sight_Imaging_With_Signal_Superresolution_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17420-17429.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决非视距（NLOS）成像中，由于长时间曝光的限制，对隐藏物体的重建质量下降的问题。<br>
                    动机：NLOS成像在各个领域都有巨大的潜力，但由于其需要长时间的曝光，对于如自动驾驶等实时性要求高的应用来说，限制了其实用化。<br>
                    方法：本文提出了一种基于学习的管道，通过训练神经网络来恢复高空间分辨率的信号，从而在少量扫描点的情况下提高成像质量。<br>
                    效果：实验结果表明，该方法在共焦和非共焦设置下都能忠实地重建隐藏场景。与原始测量相比，该方法的采集速度提高了16倍，同时保持了相似的重建质量。此外，该方法可以作为即插即用的模块直接应用于现有的光学系统和成像算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Non-line-of-sight (NLOS) imaging aims at reconstructing the location, shape, albedo, and surface normal of the hidden object around the corner with measured transient data. Due to its strong potential in various fields, it has drawn much attention in recent years. However, long exposure time is not always available for applications such as auto-driving, which hinders the practical use of NLOS imaging. Although scanning fewer points can reduce the total measurement time, it also brings the problem of imaging quality degradation. This paper proposes a general learning-based pipeline for increasing imaging quality with only a few scanning points. We tailor a neural network to learn the operator that recovers a high spatial resolution signal. Experiments on synthetic and measured data indicate that the proposed method provides faithful reconstructions of the hidden scene under both confocal and non-confocal settings. Compared with original measurements, the acquisition of our approach is 16 times faster while maintaining similar reconstruction quality. Besides, the proposed pipeline can be applied directly to existing optical systems and imaging algorithms as a plug-in-and-play module. We believe the proposed pipeline is powerful in increasing the frame rate in NLOS video imaging.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">423.WildLight: In-the-Wild Inverse Rendering With a Flashlight</span><br>
                <span class="as">Cheng, ZiangandLi, JunxuanandLi, Hongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_WildLight_In-the-Wild_Inverse_Rendering_With_a_Flashlight_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4305-4314.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在未知环境光照下，野外逆向渲染的挑战性问题。<br>
                    动机：目前的逆向渲染方法需要成对的闪光/非闪光图像，且难以处理环境反射的问题。<br>
                    方法：提出一种实用的光度解决方案，利用智能手机内置的手电筒作为受控光源，将图像强度分解为静态外观和动态反射两个光度分量。<br>
                    效果：实验证明，该方法易于实施，设置方便，并持续优于现有的野外逆向渲染技术。最终的神经重建可以方便地导出到适用于工业渲染器的PBR纹理三角形网格。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a practical photometric solution for the challenging problem of in-the-wild inverse rendering under unknown ambient lighting. Our system recovers scene geometry and reflectance using only multi-view images captured by a smartphone. The key idea is to exploit smartphone's built-in flashlight as a minimally controlled light source, and decompose image intensities into two photometric components -- a static appearance corresponds to ambient flux, plus a dynamic reflection induced by the moving flashlight. Our method does not require flash/non-flash images to be captured in pairs. Building on the success of neural light fields, we use an off-the-shelf method to capture the ambient reflections, while the flashlight component enables physically accurate photometric constraints to decouple reflectance and illumination. Compared to existing inverse rendering methods, our setup is applicable to non-darkroom environments yet sidesteps the inherent difficulties of explicit solving ambient reflections. We demonstrate by extensive experiments that our method is easy to implement, casual to set up, and consistently outperforms existing in-the-wild inverse rendering techniques. Finally, our neural reconstruction can be easily exported to PBR textured triangle mesh ready for industrial renderers. Our source code and data are released to https://github.com/za-cheng/WildLight</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">424.A Probabilistic Attention Model With Occlusion-Aware Texture Regression for 3D Hand Reconstruction From a Single RGB Image</span><br>
                <span class="as">Jiang, ZhehengandRahmani, HosseinandBlack, SueandWilliams, BryanM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_A_Probabilistic_Attention_Model_With_Occlusion-Aware_Texture_Regression_for_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/758-767.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中重建3D手部模型，同时解决现有方法对模型参数空间的过度依赖和深度模糊的问题。<br>
                    动机：目前的深度学习方法在处理这些问题时存在不足，需要一种新的方法来提高重建的准确性和鲁棒性。<br>
                    方法：提出了一种新颖的概率模型，该模型结合了基于模型的网络作为先验网络来估计关节和顶点的先验概率分布，并利用注意力机制的网格顶点不确定性回归模型来捕捉顶点之间的依赖关系以及关节与网格顶点之间的相关性，以提高其特征表示。此外，还提出了一种学习型的遮挡感知手部纹理回归模型来实现高保真度的纹理重建。<br>
                    效果：实验结果表明，所提出的概率模型在监督和弱监督两种训练模式下都能实现最先进的重建精度，即使在严重遮挡的情况下也能保持良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, deep learning based approaches have shown promising results in 3D hand reconstruction from a single RGB image. These approaches can be roughly divided into model-based approaches, which are heavily dependent on the model's parameter space, and model-free approaches, which require large numbers of 3D ground truths to reduce depth ambiguity and struggle in weakly-supervised scenarios. To overcome these issues, we propose a novel probabilistic model to achieve the robustness of model-based approaches and reduced dependence on the model's parameter space of model-free approaches. The proposed probabilistic model incorporates a model-based network as a prior-net to estimate the prior probability distribution of joints and vertices. An Attention-based Mesh Vertices Uncertainty Regression (AMVUR) model is proposed to capture dependencies among vertices and the correlation between joints and mesh vertices to improve their feature representation. We further propose a learning based occlusion-aware Hand Texture Regression model to achieve high-fidelity texture reconstruction. We demonstrate the flexibility of the proposed probabilistic model to be trained in both supervised and weakly-supervised scenarios. The experimental results demonstrate our probabilistic model's state-of-the-art accuracy in 3D hand and texture reconstruction from a single image in both training schemes, including in the presence of severe occlusions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">425.MixNeRF: Modeling a Ray With Mixture Density for Novel View Synthesis From Sparse Inputs</span><br>
                <span class="as">Seo, SeunghyeonandHan, DonghoonandChang, YeonjinandKwak, Nojun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seo_MixNeRF_Modeling_a_Ray_With_Mixture_Density_for_Novel_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20659-20668.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的Neural Radiance Field (NeRF)模型在训练时需要大量的不同相机姿态的图像，这限制了其在实际中的应用。<br>
                    动机：为了解决这一问题，本文提出了一种新的训练策略MixNeRF，通过将光线建模为混合密度模型，实现从稀疏输入中进行新颖视图合成。<br>
                    方法：MixNeRF通过将光线样本上的RGB颜色沿光线估计为混合分布来估计联合分布。同时，提出了一个新的任务——射线深度估计，作为有用的训练目标，该目标与3D场景几何高度相关。此外，还根据估计的射线深度重新生成混合权重，进一步提高了对颜色和视点的鲁棒性。<br>
                    效果：实验结果表明，MixNeRF在各种标准基准测试中优于其他最先进的方法，具有优越的训练和推理效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">426.Cross-Domain 3D Hand Pose Estimation With Dual Modalities</span><br>
                <span class="as">Lin, QiuxiaandYang, LinlinandYao, Angela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17184-17193.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用合成数据训练神经网络进行手部姿态估计，并解决由于领域差距导致的对真实世界数据的泛化问题。<br>
                    动机：现有的手部姿态估计方法主要依赖合成数据进行训练，但这种方法往往无法很好地泛化到真实世界的数据上。<br>
                    方法：提出了一种跨领域的半监督手部姿态估计框架，该框架通过双模态网络同时利用RGB和深度合成图像进行训练。在预训练阶段，网络使用多模态对比学习和注意力融合监督来学习有效的RGB图像表示。在微调阶段，引入了一种新的自我蒸馏技术来减少伪标签噪声。<br>
                    效果：实验表明，该方法在基准测试中显著提高了3D手部姿态估计和2D关键点检测的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in hand pose estimation have shed light on utilizing synthetic data to train neural networks, which however inevitably hinders generalization to real-world data due to domain gaps. To solve this problem, we present a framework for cross-domain semi-supervised hand pose estimation and target the challenging scenario of learning models from labelled multi-modal synthetic data and unlabelled real-world data. To that end, we propose a dual-modality network that exploits synthetic RGB and synthetic depth images. For pre-training, our network uses multi-modal contrastive learning and attention-fused supervision to learn effective representations of the RGB images. We then integrate a novel self-distillation technique during fine-tuning to reduce pseudo-label noise. Experiments show that the proposed method significantly improves 3D hand pose estimation and 2D keypoint detection on benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">427.Inverse Rendering of Translucent Objects Using Physical and Neural Renderers</span><br>
                <span class="as">Li, ChenhaoandNgo, TrungThanhandNagahara, Hajime</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Inverse_Rendering_of_Translucent_Objects_Using_Physical_and_Neural_Renderers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12510-12520.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种逆渲染模型，仅通过一对透明物体的捕获图像来估计3D形状、空间变化的反射率、均匀亚表面散射参数和环境照明。<br>
                    动机：为了解决逆渲染的模糊性问题，我们使用物理基础渲染器和神经渲染器进行场景重建和材料编辑。由于两种渲染器都是可微分的，我们可以计算重构损失以协助参数估计。<br>
                    方法：我们使用闪光灯和非闪光灯图像对作为输入，并构建了一个大型的透明物体合成数据集，包含117K个场景，以监督训练。<br>
                    效果：在合成和真实世界的数据集上的定性和定量结果都证明了该模型的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we propose an inverse rendering model that estimates 3D shape, spatially-varying reflectance, homogeneous subsurface scattering parameters, and an environment illumination jointly from only a pair of captured images of a translucent object. In order to solve the ambiguity problem of inverse rendering, we use a physically-based renderer and a neural renderer for scene reconstruction and material editing. Because two renderers are differentiable, we can compute a reconstruction loss to assist parameter estimation. To enhance the supervision of the proposed neural renderer, we also propose an augmented loss. In addition, we use a flash and no-flash image pair as the input. To supervise the training, we constructed a large-scale synthetic dataset of translucent objects, which consists of 117K scenes. Qualitative and quantitative results on both synthetic and real-world datasets demonstrated the effectiveness of the proposed model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">428.Improving Fairness in Facial Albedo Estimation via Visual-Textual Cues</span><br>
                <span class="as">Ren, XingyuandDeng, JiankangandMa, ChaoandYan, YichaoandYang, Xiaokang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Improving_Fairness_in_Facial_Albedo_Estimation_via_Visual-Textual_Cues_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4511-4520.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确估计人脸的反照率，同时避免由种族偏见和光照限制引起的光皮肤偏差。<br>
                    动机：现有的方法在几何预测方面取得了显著的进步，但在改善反照率方面的进展受到滞后的影响，因为从外观推断反照率是一个病态的问题。<br>
                    方法：我们重新考虑了反照率和面部属性之间的关系，并提出了ID2Albedo来直接估计反照率，而不约束光照。我们的关键洞察是内在的语义属性，如种族、肤色和年龄，可以约束反照率图。我们首先引入视觉文本线索并设计了一个语义损失来监督面部反照率估计。<br>
                    效果：实验结果表明，我们的ID2Albedo在准确性和逼真度上优于最先进的反照率估计方法。此外，我们的方法具有优秀的泛化能力和公平性，特别是在野外数据上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent 3D face reconstruction methods have made significant advances in geometry prediction, yet further cosmetic improvements are limited by lagged albedo because inferring albedo from appearance is an ill-posed problem. Although some existing methods consider prior knowledge from illumination to improve albedo estimation, they still produce a light-skin bias due to racially biased albedo models and limited light constraints. In this paper, we reconsider the relationship between albedo and face attributes and propose an ID2Albedo to directly estimate albedo without constraining illumination. Our key insight is that intrinsic semantic attributes such as race, skin color, and age can constrain the albedo map. We first introduce visual-textual cues and design a semantic loss to supervise facial albedo estimation. Specifically, we pre-define text labels such as race, skin color, age, and wrinkles. Then, we employ the text-image model (CLIP) to compute the similarity between the text and the input image, and assign a pseudo-label to each facial image. We constrain generated albedos in the training phase to have the same attributes as the inputs. In addition, we train a high-quality, unbiased facial albedo generator and utilize the semantic loss to learn the mapping from illumination-robust identity features to the albedo latent codes. Finally, our ID2Albedo is trained in a self-supervised way and outperforms state-of-the-art albedo estimation methods in terms of accuracy and fidelity. It is worth mentioning that our approach has excellent generalizability and fairness, especially on in-the-wild data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">429.SfM-TTR: Using Structure From Motion for Test-Time Refinement of Single-View Depth Networks</span><br>
                <span class="as">Izquierdo, SergioandCivera, Javier</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Izquierdo_SfM-TTR_Using_Structure_From_Motion_for_Test-Time_Refinement_of_Single-View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21466-21476.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单一视角估计稠密深度图？<br>
                    动机：目前的方法主要依赖于深度学习网络学习深度与视觉外观的关系，而结构运动（SfM）方法则利用多视角约束产生精确但稀疏的映射。<br>
                    方法：提出一种新颖的测试时修正（TTR）方法，称为SfM-TTR，该方法在测试时使用SfM多视角线索来提升单视角深度网络的性能。具体来说，我们使用稀疏的SfM点云作为测试时的自监督信号，微调网络编码器以学习更好的测试场景表示。<br>
                    效果：我们的结果显示，将SfM-TTR添加到几种最先进的自我监督和监督网络中，显著提高了它们的性能，主要基于光度学多视角一致性的先前TTR基线相比，表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Estimating a dense depth map from a single view is geometrically ill-posed, and state-of-the-art methods rely on learning depth's relation with visual appearance using deep neural networks. On the other hand, Structure from Motion (SfM) leverages multi-view constraints to produce very accurate but sparse maps, as matching across images is typically limited by locally discriminative texture. In this work, we combine the strengths of both approaches by proposing a novel test-time refinement (TTR) method, denoted as SfM-TTR, that boosts the performance of single-view depth networks at test time using SfM multi-view cues. Specifically, and differently from the state of the art, we use sparse SfM point clouds as test-time self-supervisory signal, fine-tuning the network encoder to learn a better representation of the test scene. Our results show how the addition of SfM-TTR to several state-of-the-art self-supervised and supervised networks improves significantly their performance, outperforming previous TTR baselines mainly based on photometric multi-view consistency. The code is available at https://github.com/serizba/SfM-TTR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">430.Implicit View-Time Interpolation of Stereo Videos Using Multi-Plane Disparities and Non-Uniform Coordinates</span><br>
                <span class="as">Paliwal, AvinashandTsarov, AndriiandKalantari, NimaKhademi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Paliwal_Implicit_View-Time_Interpolation_of_Stereo_Videos_Using_Multi-Plane_Disparities_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/888-898.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种立体视频的视时插值方法。<br>
                    动机：目前的X-Fields在处理大基线相机的视差插值上存在问题，因此需要提出新的技术来克服这些挑战。<br>
                    方法：我们提出了多平面视差和非线性非均匀时间坐标的方法，并进行了多项改进。<br>
                    效果：实验结果表明，我们的方法优于现有技术，同时运行速度快，内存和存储成本低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose an approach for view-time interpolation of stereo videos. Specifically, we build upon X-Fields that approximates an interpolatable mapping between the input coordinates and 2D RGB images using a convolutional decoder. Our main contribution is to analyze and identify the sources of the problems with using X-Fields in our application and propose novel techniques to overcome these challenges. Specifically, we observe that X-Fields struggles to implicitly interpolate the disparities for large baseline cameras. Therefore, we propose multi-plane disparities to reduce the spatial distance of the objects in the stereo views. Moreover, we propose non-uniform time coordinates to handle the non-linear and sudden motion spikes in videos. We additionally introduce several simple, but important, improvements over X-Fields. We demonstrate that our approach is able to produce better results than the state of the art, while running in near real-time rates and having low memory and storage costs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">431.pCON: Polarimetric Coordinate Networks for Neural Scene Representations</span><br>
                <span class="as">Peters, HenryandBa, YunhaoandKadambi, Achuta</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peters_pCON_Polarimetric_Coordinate_Networks_for_Neural_Scene_Representations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16579-16589.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的神经场景表示模型在重建图像时并未优化物理量的保留。<br>
                    动机：虽然现有的架构可以正确重建彩色图像，但在尝试拟合极性量图时会产生伪影。<br>
                    方法：我们提出了极坐标网络（pCON），这是一种新的神经场景表示模型，旨在在准确参数化场景的同时保留极性信息。<br>
                    效果：我们的模型消除了当前坐标网络架构在重建三个关注的极性量时产生的伪影。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural scene representations have achieved great success in parameterizing and reconstructing images, but current state of the art models are not optimized with the preservation of physical quantities in mind. While current architectures can reconstruct color images correctly, they create artifacts when trying to fit maps of polar quantities. We propose polarimetric coordinate networks (pCON), a new model architecture for neural scene representations aimed at preserving polarimetric information while accurately parameterizing the scene. Our model removes artifacts created by current coordinate network architectures when reconstructing three polarimetric quantities of interest.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">432.Visibility Aware Human-Object Interaction Tracking From Single RGB Camera</span><br>
                <span class="as">Xie, XianghuiandBhatnagar, BharatLalandPons-Moll, Gerard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Visibility_Aware_Human-Object_Interaction_Tracking_From_Single_RGB_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4757-4768.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中重建3D人体和物体的交互，并跟踪其在帧间的相对平移。<br>
                    动机：现有的方法在重建3D人体和物体时，由于假设了固定的深度，导致在不同帧之间的相对平移不一致，且当物体被遮挡时性能显著下降。<br>
                    方法：提出一种新的方法，通过预拟合SMPL模型到视频序列来获取每帧的SMPL模型估计，以此作为条件进行神经场重建，提高了神经重建的准确性，并产生了连贯的相对平移。同时，利用可见帧中的人体和物体运动信息推断被遮挡物体。<br>
                    效果：实验表明，该方法在两个数据集上都显著优于现有方法，即使在被遮挡的情况下也能有效地跟踪人体和物体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Capturing the interactions between humans and their environment in 3D is important for many applications in robotics, graphics, and vision. Recent works to reconstruct the 3D human and object from a single RGB image do not have consistent relative translation across frames because they assume a fixed depth. Moreover, their performance drops significantly when the object is occluded. In this work, we propose a novel method to track the 3D human, object, contacts, and relative translation across frames from a single RGB camera, while being robust to heavy occlusions. Our method is built on two key insights. First, we condition our neural field reconstructions for human and object on per-frame SMPL model estimates obtained by pre-fitting SMPL to a video sequence. This improves neural reconstruction accuracy and produces coherent relative translation across frames. Second, human and object motion from visible frames provides valuable information to infer the occluded object. We propose a novel transformer-based neural network that explicitly uses object visibility and human motion to leverage neighboring frames to make predictions for the occluded frames. Building on these insights, our method is able to track both human and object robustly even under occlusions. Experiments on two datasets show that our method significantly improves over the state-of-the-art methods. Our code and pretrained models are available at: https://virtualhumans.mpi-inf.mpg.de/VisTracker.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">433.Uncertainty-Aware Vision-Based Metric Cross-View Geolocalization</span><br>
                <span class="as">Fervers, FlorianandBullinger, SebastianandBodensteiner, ChristophandArens, MichaelandStiefelhagen, Rainer</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fervers_Uncertainty-Aware_Vision-Based_Metric_Cross-View_Geolocalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21621-21631.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过匹配地面车辆的摄像头图像和航空图像来确定车辆的地理位置。<br>
                    动机：航空图像全球范围内可获取且成本低廉，可以作为自动驾驶两种主流方法（使用昂贵的高清预先地图或完全依赖运行时捕获的传感器数据）之间的折衷方案。<br>
                    方法：提出一种新颖的基于视觉的度量跨视图地理定位（CVGL）方法，该方法使用地面和航空图像预测可能的车辆位置的概率分布。<br>
                    效果：在多个车辆数据集和航空图像上进行演示，证明了该方法的可行性。即使没有来自测试区域的地面或航空数据，也能大幅提高先前最先进技术的性能，显示出模型在全球范围内的应用潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a novel method for vision-based metric cross-view geolocalization (CVGL) that matches the camera images captured from a ground-based vehicle with an aerial image to determine the vehicle's geo-pose. Since aerial images are globally available at low cost, they represent a potential compromise between two established paradigms of autonomous driving, i.e. using expensive high-definition prior maps or relying entirely on the sensor data captured at runtime. We present an end-to-end differentiable model that uses the ground and aerial images to predict a probability distribution over possible vehicle poses. We combine multiple vehicle datasets with aerial images from orthophoto providers on which we demonstrate the feasibility of our method. Since the ground truth poses are often inaccurate w.r.t. the aerial images, we implement a pseudo-label approach to produce more accurate ground truth poses and make them publicly available. While previous works require training data from the target region to achieve reasonable localization accuracy (i.e. same-area evaluation), our approach overcomes this limitation and outperforms previous results even in the strictly more challenging cross-area case. We improve the previous state-of-the-art by a large margin even without ground or aerial data from the test region, which highlights the model's potential for global-scale application. We further integrate the uncertainty-aware predictions in a tracking framework to determine the vehicle's trajectory over time resulting in a mean position error on KITTI-360 of 0.78m.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">434.DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering</span><br>
                <span class="as">Li, ZongruiandZheng, QianandShi, BoxinandPan, GangandJiang, Xudong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DANI-Net_Uncalibrated_Photometric_Stereo_by_Differentiable_Shadow_Handling_Anisotropic_Reflectance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8381-8391.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决未校准的光度立体视觉（UPS）问题，特别是对于具有复杂形状和不规则阴影的一般物体以及具有复杂反射性如各向异性反射的材料。<br>
                    动机：由于未知光带来的固有模糊性，未校准的光度立体视觉（UPS）具有挑战性。尽管非朗伯体物体的模糊性得到了缓解，但对于引入不规则阴影和具有复杂反射性（如各向异性反射）的一般材料的物体来说，这个问题仍然难以解决。<br>
                    方法：我们提出了DANI-Net，一个具有可微分阴影处理和各向异性反射建模的逆渲染框架。与大多数使用不可微分阴影映射并假设各向同性材料的先前方法不同，我们的网络通过两条可微分路径受益于阴影和各向异性反射的线索。<br>
                    效果：我们在多个真实世界数据集上进行的实验表明，我们的方法具有优越且稳健的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Uncalibrated photometric stereo (UPS) is challenging due to the inherent ambiguity brought by the unknown light. Although the ambiguity is alleviated on non-Lambertian objects, the problem is still difficult to solve for more general objects with complex shapes introducing irregular shadows and general materials with complex reflectance like anisotropic reflectance. To exploit cues from shadow and reflectance to solve UPS and improve performance on general materials, we propose DANI-Net, an inverse rendering framework with differentiable shadow handling and anisotropic reflectance modeling. Unlike most previous methods that use non-differentiable shadow maps and assume isotropic material, our network benefits from cues of shadow and anisotropic reflectance through two differentiable paths. Experiments on multiple real-world datasets demonstrate our superior and robust performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">435.Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields for Controllable Scene Stylization</span><br>
                <span class="as">Zhang, YuechenandHe, ZexinandXing, JinboandYao, XufengandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ref-NPR_Reference-Based_Non-Photorealistic_Radiance_Fields_for_Controllable_Scene_Stylization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4242-4251.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的3D场景风格化方法在将纹理和颜色作为样式进行转移时，缺乏有意义的语义对应关系。<br>
                    动机：为了解决这个问题，我们提出了基于参考的非真实感辐射场（Ref-NPR）方法。<br>
                    方法：该方法使用单一风格的2D视图作为参考，通过辐射场对3D场景进行风格化。我们还提出了一种基于风格化参考视图的光线注册过程，以在新视图中获得伪光线监督。然后，我们利用内容图像中的语义对应关系，用感知上相似的风格填充遮挡区域，从而生成非真实感且连续的新视图序列。<br>
                    效果：实验结果表明，与现有的场景和视频风格化方法相比，Ref-NPR在视觉质量和语义对应方面表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current 3D scene stylization methods transfer textures and colors as styles using arbitrary style references, lacking meaningful semantic correspondences. We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to address this limitation. This controllable method stylizes a 3D scene using radiance fields with a single stylized 2D view as a reference. We propose a ray registration process based on the stylized reference view to obtain pseudo-ray supervision in novel views. Then we exploit semantic correspondences in content images to fill occluded regions with perceptually similar styles, resulting in non-photorealistic and continuous novel view sequences. Our experimental results demonstrate that Ref-NPR outperforms existing scene and video stylization methods regarding visual quality and semantic correspondence. The code and data are publicly available on the project page at https://ref-npr.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">436.NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization</span><br>
                <span class="as">Min, ZhixiangandZhuang, BingbingandSchulter, SamuelandLiu, BuyuandDunn, EnriqueandChandraker, Manmohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Min_NeurOCS_Neural_NOCS_Supervision_for_Monocular_3D_Object_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21404-21414.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目驾驶场景下的三维物体定位是一项关键任务，但由于其病态特性而具有挑战性。<br>
                    动机：估计物体表面上每个像素的3D坐标具有巨大潜力，因为它为底层PnP问题提供了密集的2D-3D几何约束。然而，由于Lidar数据的稀疏性和各种伪影，以及收集每个实例CAD模型的实际不可行性，在驾驶场景中无法获得高质量的地面真实监督。<br>
                    方法：我们提出了NeurOCS框架，该框架使用实例掩码和3D框作为输入，通过可微分渲染学习3D物体形状，这进一步作为学习密集物体坐标的监督。<br>
                    效果：我们的框架在单目驾驶场景下的三维物体定位方面取得了新的最先进的成果，在KITTI-Object基准测试中排名首位。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular 3D object localization in driving scenes is a crucial task, but challenging due to its ill-posed nature. Estimating 3D coordinates for each pixel on the object surface holds great potential as it provides dense 2D-3D geometric constraints for the underlying PnP problem. However, high-quality ground truth supervision is not available in driving scenes due to sparsity and various artifacts of Lidar data, as well as the practical infeasibility of collecting per-instance CAD models. In this work, we present NeurOCS, a framework that uses instance masks and 3D boxes as input to learn 3D object shapes by means of differentiable rendering, which further serves as supervision for learning dense object coordinates. Our approach rests on insights in learning a category-level shape prior directly from real driving scenes, while properly handling single-view ambiguities. Furthermore, we study and make critical design choices to learn object coordinates more effectively from an object-centric view. Altogether, our framework leads to new state-of-the-art in monocular 3D localization that ranks 1st on the KITTI-Object benchmark among published monocular methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">437.TMO: Textured Mesh Acquisition of Objects With a Mobile Device by Using Differentiable Rendering</span><br>
                <span class="as">Choi, JaehoonandJung, DongkiandLee, TaejaeandKim, SangwookandJung, YoungdongandManocha, DineshandLee, Donghwan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_TMO_Textured_Mesh_Acquisition_of_Objects_With_a_Mobile_Device_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16674-16684.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法，通过智能手机获取野外有质感的3D模型。<br>
                    动机：现有的3D重建和纹理映射方法需要实验室环境或精确的掩膜图像，而本文的方法可以在真实世界中的任何常见物体上应用，无需这些条件。<br>
                    方法：首先，利用RGBD辅助的结构运动从图片、深度图和有效姿态中生成过滤后的深度图并优化相机姿态；然后，采用神经隐式表面重建方法生成高质量的3D网格，并开发一种新的训练过程，应用经典的多视图立体方法提供的正则化；最后，应用可微渲染来微调不完整的纹理地图，生成更接近原始场景的纹理。<br>
                    效果：实验结果表明，该方法可以捕获具有复杂形状的物体，并在数值上优于现有的3D重建和纹理映射方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a new pipeline for acquiring a textured mesh in the wild with a single smartphone which offers access to images, depth maps, and valid poses. Our method first introduces an RGBD-aided structure from motion, which can yield filtered depth maps and refines camera poses guided by corresponding depth. Then, we adopt the neural implicit surface reconstruction method, which allows for high quality mesh and develops a new training process for applying a regularization provided by classical multi-view stereo methods. Moreover, we apply a differentiable rendering to fine-tune incomplete texture maps and generate textures which are perceptually closer to the original scene. Our pipeline can be applied to any common objects in the real world without the need for either in-the-lab environments or accurate mask images. We demonstrate results of captured objects with complex shapes and validate our method numerically against existing 3D reconstruction and texture mapping methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">438.Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction and Rendering</span><br>
                <span class="as">Shao, RuizhiandZheng, ZerongandTu, HanzhangandLiu, BoningandZhang, HongwenandLiu, Yebin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16632-16642.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行动态场景建模。<br>
                    动机：现有的动态场景建模方法存在内存消耗大的问题。<br>
                    方法：提出一种名为Tensor4D的方法，通过高效的4D张量分解技术直接将动态场景表示为一个四维时空张量，并通过层次化分解以紧凑且高效的方式同时捕获空间信息随时间的变化。<br>
                    效果：在合成和真实世界的场景上验证了该方法的有效性，实验表明，该方法能够从稀疏视图的相机架或甚至单眼相机实现高质量的动态重建和渲染。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Tensor4D, an efficient yet effective approach to dynamic scene modeling. The key of our solution is an efficient 4D tensor decomposition method so that the dynamic scene can be directly represented as a 4D spatio-temporal tensor. To tackle the accompanying memory issue, we decompose the 4D tensor hierarchically by projecting it first into three time-aware volumes and then nine compact feature planes. In this way, spatial information over time can be simultaneously captured in a compact and memory-efficient manner. When applying Tensor4D for dynamic scene reconstruction and rendering, we further factorize the 4D fields to different scales in the sense that structural motions and dynamic detailed changes can be learned from coarse to fine. The effectiveness of our method is validated on both synthetic and real-world scenes. Extensive experiments show that our method is able to achieve high-quality dynamic reconstruction and rendering from sparse-view camera rigs or even a monocular camera.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">439.Blowing in the Wind: CycleNet for Human Cinemagraphs From Still Images</span><br>
                <span class="as">Bertiche, HugoandMitra, NiloyJ.andKulkarni, KuldeepandHuang, Chun-HaoP.andWang, TuanfengY.andMadadi, MeysamandEscalera, SergioandCeylan, Duygu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bertiche_Blowing_in_the_Wind_CycleNet_for_Human_Cinemagraphs_From_Still_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/459-468.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种自动方法，从单张RGB图片生成人类动态影像。<br>
                    动机：目前的动态影像生成技术需要艺术家进行繁琐的手动操作，而自动生成动态影像的方法尚未得到充分探索。<br>
                    方法：通过在图像正常空间中工作，学习服装运动动力学，并在合成数据上进行训练，然后推广到真实数据。<br>
                    效果：实验证明，该方法可以在合成和真实数据上生成引人注目且合理的动态影像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cinemagraphs are short looping videos created by adding subtle motions to a static image. This kind of media is popular and engaging. However, automatic generation of cinemagraphs is an underexplored area and current solutions require tedious low-level manual authoring by artists. In this paper, we present an automatic method that allows generating human cinemagraphs from single RGB images. We investigate the problem in the context of dressed humans under the wind. At the core of our method is a novel cyclic neural network that produces looping cinemagraphs for the target loop duration. To circumvent the problem of collecting real data, we demonstrate that it is possible, by working in the image normal space, to learn garment motion dynamics on synthetic data and generalize to real data. We evaluate our method on both synthetic and real data and demonstrate that it is possible to create compelling and plausible cinemagraphs from single RGB images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">440.Panoptic Compositional Feature Field for Editable Scene Rendering With Network-Inferred Labels via Metric Learning</span><br>
                <span class="as">Cheng, XinhuaandWu, YanminandJia, MengxiandWang, QianandZhang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4947-4957.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管神经隐式表示在高质量视图合成方面表现出色，但将其分解为实例级别的对象进行编辑仍然具有挑战性。<br>
                    动机：现有的工作通过地面真实实例注释学习了对象组合的表示，并在场景编辑中产生了有希望的结果。然而，地面真实注释是手动标记的，在实践中成本高昂，限制了其在真实世界场景中的使用。<br>
                    方法：我们尝试通过利用从现成的二维全景分割网络推断出的标签，而不是地面真实注释，来学习可编辑场景渲染的对象组合神经隐式表示。我们提出了一种名为Panoptic Compositional Feature Field（PCFF）的新框架，该框架引入了一个实例四元组度量学习，以建立一个可靠的场景编辑的辨别性全景特征空间。此外，我们还提出了语义相关策略，以进一步挖掘语义和外观属性之间的关联，以实现更好的渲染结果。<br>
                    效果：我们在多个场景数据集上进行了实验，包括ScanNet、Replica和ToyDesk，实验结果表明，我们提出的方法在新颖视图合成方面表现优越，并产生了令人信服的真实世界场景编辑结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite neural implicit representations demonstrating impressive high-quality view synthesis capacity, decomposing such representations into objects for instance-level editing is still challenging. Recent works learn object-compositional representations supervised by ground truth instance annotations and produce promising scene editing results. However, ground truth annotations are manually labeled and expensive in practice, which limits their usage in real-world scenes. In this work, we attempt to learn an object-compositional neural implicit representation for editable scene rendering by leveraging labels inferred from the off-the-shelf 2D panoptic segmentation networks instead of the ground truth annotations. We propose a novel framework named Panoptic Compositional Feature Field (PCFF), which introduces an instance quadruplet metric learning to build a discriminating panoptic feature space for reliable scene editing. In addition, we propose semantic-related strategies to further exploit the correlations between semantic and appearance attributes for achieving better rendering results. Experiments on multiple scene datasets including ScanNet, Replica, and ToyDesk demonstrate that our proposed method achieves superior performance for novel view synthesis and produces convincing real-world scene editing results. The code will be available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">441.Neural Kaleidoscopic Space Sculpting</span><br>
                <span class="as">Ahn, ByeongjooandDeZeeuw, MichaelandGkioulekas, IoannisandSankaranarayanan, AswinC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ahn_Neural_Kaleidoscopic_Space_Sculpting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4349-4358.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从一张万花筒图像中恢复完整的3D重建。<br>
                    动机：全环绕3D重建在增强现实和虚拟现实等许多应用中至关重要，而使用单个相机和多个镜子的万花筒可以方便地实现全环绕覆盖，因此成为单次拍摄和动态全环绕3D重建的理想选择。<br>
                    方法：通过仔细利用万花筒图像中的轮廓、背景、前景和纹理信息，“雕刻”出一种神经表面表示，从而避免了显式估计标签的需要。<br>
                    效果：在一系列模拟和真实实验中，无论是静态场景还是动态场景，该方法都表现出了显著的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a method that recovers full-surround 3D reconstructions from a single kaleidoscopic image using a neural surface representation. Full-surround 3D reconstruction is critical for many applications, such as augmented and virtual reality. A kaleidoscope, which uses a single camera and multiple mirrors, is a convenient way of achieving full-surround coverage, as it redistributes light directions and thus captures multiple viewpoints in a single image. This enables single-shot and dynamic full-surround 3D reconstruction. However, using a kaleidoscopic image for multi-view stereo is challenging, as we need to decompose the image into multi-view images by identifying which pixel corresponds to which virtual camera, a process we call labeling. To address this challenge, pur approach avoids the need to explicitly estimate labels, but instead "sculpts" a neural surface representation through the careful use of silhouette, background, foreground, and texture information present in the kaleidoscopic image. We demonstrate the advantages of our method in a range of simulated and real experiments, on both static and dynamic scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">442.Unsupervised Intrinsic Image Decomposition With LiDAR Intensity</span><br>
                <span class="as">Sato, ShogoandYao, YasuhiroandYoshida, TaigaandKaneko, TakuhiroandAndo, ShingoandShimamura, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sato_Unsupervised_Intrinsic_Image_Decomposition_With_LiDAR_Intensity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13466-13475.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自然图像分解为反射率和阴影的固有图像分解（IID）任务，由于在一般场景中难以观察到地面真实的反射率和阴影，因此通常通过监督学习方法来解决，但这并不理想。<br>
                    动机：目前，由于无监督学习方法无法解决病态问题，其性能不如监督学习方法。最近，由于能够进行高精度距离测量，光探测和测距（LiDAR）得到了广泛应用。因此，我们专注于利用LiDAR，特别是LiDAR强度来解决此问题。<br>
                    方法：本文提出了一种使用LiDAR强度进行无监督固有图像分解的方法（IID-LI）。由于传统的无监督学习方法包括图像到图像的转换，因此简单地输入LiDAR强度并不是一个有效的方法。因此，我们设计了一个强度一致性损失函数，该函数计算LiDAR强度与灰度反射率之间的误差，为病态问题提供了一个标准。此外，由于LiDAR强度的稀疏性和遮挡性，我们提出了一个LiDAR强度密集化模块。<br>
                    效果：我们使用自己的数据集（包括RGB图像、LiDAR强度和人工判断注释）验证了估计质量。结果显示，我们的方法在估计准确性上超过了传统无监督学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Intrinsic image decomposition (IID) is the task that decomposes a natural image into albedo and shade. While IID is typically solved through supervised learning methods, it is not ideal due to the difficulty in observing ground truth albedo and shade in general scenes. Conversely, unsupervised learning methods are currently underperforming supervised learning methods since there are no criteria for solving the ill-posed problems. Recently, light detection and ranging (LiDAR) is widely used due to its ability to make highly precise distance measurements. Thus, we have focused on the utilization of LiDAR, especially LiDAR intensity, to address this issue. In this paper, we propose unsupervised intrinsic image decomposition with LiDAR intensity (IID-LI). Since the conventional unsupervised learning methods consist of image-to-image transformations, simply inputting LiDAR intensity is not an effective approach. Therefore, we design an intensity consistency loss that computes the error between LiDAR intensity and gray-scaled albedo to provide a criterion for the ill-posed problem. In addition, LiDAR intensity is difficult to handle due to its sparsity and occlusion, hence, a LiDAR intensity densification module is proposed. We verified the estimating quality using our own dataset, which include RGB images, LiDAR intensity and human judged annotations. As a result, we achieved an estimation accuracy that outperforms conventional unsupervised learning methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">443.PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces</span><br>
                <span class="as">Wang, YiqunandSkorokhodov, IvanandWonka, Peter</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PET-NeuS_Positional_Encoding_Tri-Planes_for_Neural_Surfaces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12598-12607.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过改进神经网络表面重建方法，提高重建的精度和质量。<br>
                    动机：目前的神经网络表面重建方法（如NeuS）虽然取得了一定的成功，但仍存在噪声干扰和表达性不足的问题。<br>
                    方法：提出了一种基于MLP参数化的距离函数（SDF）的新方法，该方法引入了三个新的组件：一是借鉴EG3D的三平面表示法，将有向距离场表示为三平面和MLP的混合体；二是使用具有可学习权重的新型位置编码来对抗重建过程中的噪声；三是利用自注意力卷积对三平面特征进行可学习的卷积操作，以产生不同频段的特征。<br>
                    效果：实验表明，新方法在标准数据集上实现了高精度的表面重建，相比于NeuS基线，在Nerf-synthetic数据集上提高了57%，在DTU数据集上提高了15.5%。定性评估显示，新方法能更好地控制高频噪声的干扰。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using tri-planes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 57% on Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to 0.84). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">444.Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation From 2D Supervision</span><br>
                <span class="as">Zhang, XiaoshuaiandKundu, AbhijitandFunkhouser, ThomasandGuibas, LeonidasandSu, HaoandGenova, Kyle</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Nerflets_Local_Radiance_Fields_for_Efficient_Structure-Aware_3D_Scene_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8274-8284.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决如何从图像中进行高效且结构感知的3D场景表示。<br>
                    动机：传统的全局NeRFs在表示场景时存在效率低下的问题，需要改进。<br>
                    方法：提出一种新的局部神经辐射场——Nerflets，每个Nerflet都保持自己的空间位置、方向和范围，共同表示一个场景。通过仅使用光度学和推断的全景图像监督，可以直接联合优化一组Nerflets的参数，形成场景的分解表示，其中每个对象实例由一组Nerflets表示。<br>
                    效果：实验结果表明，Nerflets比传统的全球NeRFs更有效地拟合和近似场景，可以从任意视角提取全景和光度渲染，并能够执行对NeRFs来说罕见的任务，如3D全景分割和交互式编辑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address efficient and structure-aware 3D scene representation from images. Nerflets are our key contribution-- a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, orientation, and extent, within which it contributes to panoptic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor environments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) allow the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">445.Multi-View Azimuth Stereo via Tangent Space Consistency</span><br>
                <span class="as">Cao, XuandSanto, HiroakiandOkura, FumioandMatsushita, Yasuyuki</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Multi-View_Azimuth_Stereo_via_Tangent_Space_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/825-834.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种仅使用校准后的多视角表面方位图进行3D重建的方法。<br>
                    动机：针对传统多视角立体方法难以处理的无纹理或镜面表面，我们提出了一种新的方法。<br>
                    方法：引入切线空间一致性概念，通过优化神经隐式表面表示来恢复形状。<br>
                    效果：实验证明，即使没有天顶角，该方法也能准确恢复形状。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a method for 3D reconstruction only using calibrated multi-view surface azimuth maps. Our method, multi-view azimuth stereo, is effective for textureless or specular surfaces, which are difficult for conventional multi-view stereo methods. We introduce the concept of tangent space consistency: Multi-view azimuth observations of a surface point should be lifted to the same tangent space. Leveraging this consistency, we recover the shape by optimizing a neural implicit surface representation. Our method harnesses the robust azimuth estimation capabilities of photometric stereo methods or polarization imaging while bypassing potentially complex zenith angle estimation. Experiments using azimuth maps from various sources validate the accurate shape recovery with our method, even without zenith angles.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">446.Self-Supervised Representation Learning for CAD</span><br>
                <span class="as">Jones, BenjaminT.andHu, MichaelandKodnongbua, MilinandKim, VladimirG.andSchulz, Adriana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jones_Self-Supervised_Representation_Learning_for_CAD_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21327-21336.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用未标记的计算机辅助设计（CAD）几何数据进行有监督学习任务。<br>
                    动机：当前，CAD的原生格式——参数化边界表示（B-Rep）缺乏标签数据，这在当前的研究中是一个难以克服的难题。<br>
                    方法：我们提出了一种新的混合隐式/显式表面表示法，用于对B-Rep几何进行预训练，并证明了这种预训练可以显著提高少次学习的性能，并在几个当前的B-Rep基准测试中实现了最先进的性能。<br>
                    效果：实验结果表明，该方法能有效地利用未标记的CAD几何数据进行有监督学习任务，提高了学习性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Virtually every object in the modern world was created, modified, analyzed and optimized using computer aided design (CAD) tools. An active CAD research area is the use of data-driven machine learning methods to learn from the massive repositories of geometric and program representations. However, the lack of labeled data in CAD's native format, i.e., the parametric boundary representation (B-Rep), poses an obstacle at present difficult to overcome. Several datasets of mechanical parts in B-Rep format have recently been released for machine learning research. However, large-scale databases are mostly unlabeled, and labeled datasets are small. Additionally, task-specific label sets are rare and costly to annotate. This work proposes to leverage unlabeled CAD geometry on supervised learning tasks. We learn a novel, hybrid implicit/explicit surface representation for B-Rep geometry. Further, we show that this pre-training both significantly improves few-shot learning performance and achieves state-of-the-art performance on several current B-Rep benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">447.BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects</span><br>
                <span class="as">Wen, BowenandTremblay, JonathanandBlukis, ValtsandTyree, StephenandM\&quot;uller, ThomasandEvans, AlexandFox, DieterandKautz, JanandBirchfield, Stan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/606-617.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目RGBD视频序列中实时（10Hz）跟踪并重建未知物体的6自由度？<br>
                    动机：目前的方法需要额外的信息或对交互主体的假设，我们的目标是在没有任何额外信息和对交互主体假设的情况下，实现任意刚性物体的实时跟踪和重建。<br>
                    方法：我们提出了一种神经对象场方法，通过在姿态图优化过程中同时学习，将信息稳健地累积到一个同时捕获几何和外观的一致3D表示中。<br>
                    效果：我们在HO3D、YCBInEOAT和BEHAVE数据集上的结果证明，我们的方法显著优于现有的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a near real-time (10Hz) method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that our method significantly outperforms existing approaches. Project page: https://bundlesdf.github.io/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">448.Humans As Light Bulbs: 3D Human Reconstruction From Thermal Reflection</span><br>
                <span class="as">Liu, RuoshiandVondrick, Carl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Humans_As_Light_Bulbs_3D_Human_Reconstruction_From_Thermal_Reflection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12531-12542.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过人体热反射定位和重构人体姿态，即使人眼无法直接看到。<br>
                    动机：由于人体温度较高，会发出长波红外光，这种光的波长大于可见光，因此可以作为红外光源来定位和重构人体姿态。<br>
                    方法：提出一种分析-合成框架，联合建模物体、人和他们的热反射，结合了生成模型和可微分反射渲染。<br>
                    效果：定量和定性实验表明，该方法在高度挑战性的情况下有效，如曲面镜或当人眼完全看不到普通相机时。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The relatively hot temperature of the human body causes people to turn into long-wave infrared light sources. Since this emitted light has a larger wavelength than visible light, many surfaces in typical scenes act as infrared mirrors with strong specular reflections. We exploit the thermal reflections of a person onto objects in order to locate their position and reconstruct their pose, even if they are not visible to a normal camera. We propose an analysis-by-synthesis framework that jointly models the objects, people, and their thermal reflections, which allows us to combine generative models with differentiable rendering of reflections. Quantitative and qualitative experiments show our approach works in highly challenging cases, such as with curved mirrors or when the person is completely unseen by a normal camera.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">449.Hi4D: 4D Instance Segmentation of Close Human Interaction</span><br>
                <span class="as">Yin, YifeiandGuo, ChenandKaufmann, ManuelandZarate, JuanJoseandSong, JieandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17016-17027.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地分析长时间接触下的人体互动？<br>
                    动机：由于遮挡和复杂形状，现有的多视角系统通常将近距离的多个主体的3D表面融合成一个单一的连接网格，这在分离多个接触主体时是一个挑战。<br>
                    方法：我们提出了Hi4D方法，该方法利用i)单独拟合的神经隐性化身；ii)交替优化方案，通过频繁的接近来细化姿势和表面；iii)从而将融合的原始扫描分割成单个实例。<br>
                    效果：我们从这些实例中编译了Hi4D数据集，包含20个主体对、100个序列和总共超过11K帧的4D纹理扫描。Hi4D包含丰富的交互式2D和3D注释以及准确注册的参数化身体模型。我们在该数据集上定义了各种人体姿势和形状估计任务，并提供了最先进的方法在这些基准上的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Hi4D, a method and dataset for the auto analysis of physically close human-human interaction under prolonged contact. Robustly disentangling several in-contact subjects is a challenging task due to occlusions and complex shapes. Hence, existing multi-view systems typically fuse 3D surfaces of close subjects into a single, connected mesh. To address this issue we leverage i) individually fitted neural implicit avatars; ii) an alternating optimization scheme that refines pose and surface through periods of close proximity; and iii) thus segment the fused raw scans into individual instances. From these instances we compile Hi4D dataset of 4D textured scans of 20 subject pairs, 100 sequences, and a total of more than 11K frames. Hi4D contains rich interaction-centric annotations in 2D and 3D alongside accurately registered parametric body models. We define varied human pose and shape estimation tasks on this dataset and provide results from state-of-the-art methods on these benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">450.Hi-LASSIE: High-Fidelity Articulated Shape and Skeleton Discovery From Sparse Image Ensemble</span><br>
                <span class="as">Yao, Chun-HanandHung, Wei-ChihandLi, YuanzhenandRubinstein, MichaelandYang, Ming-HsuanandJampani, Varun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Hi-LASSIE_High-Fidelity_Articulated_Shape_and_Skeleton_Discovery_From_Sparse_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4853-4862.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅从稀疏的野外图像集合中自动估计3D骨架、形状、相机视点和部分关节。<br>
                    动机：大部分现有方法依赖于大规模图像数据集、密集的时间对应关系或人工注释，如相机姿态、2D关键点和形状模板，但这种方法需要大量的用户输入。<br>
                    方法：我们提出了Hi-LASSIE，它只需要20-30张在线野外图像，无需任何用户定义的形状或骨架模板，就可以进行3D关节重建。我们首先从选定的参考图像中自动估计特定类别的骨架，然后通过新的实例特定优化策略改进形状重建，使重建能够忠实地适应每个实例，同时保留所有图像中学习到的类别特定先验知识。<br>
                    效果：实验表明，尽管需要最少的用户输入，Hi-LASSIE仍然可以获得更高保真度的最先进的3D重建结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatically estimating 3D skeleton, shape, camera viewpoints, and part articulation from sparse in-the-wild image ensembles is a severely under-constrained and challenging problem. Most prior methods rely on large-scale image datasets, dense temporal correspondence, or human annotations like camera pose, 2D keypoints, and shape templates. We propose Hi-LASSIE, which performs 3D articulated reconstruction from only 20-30 online images in the wild without any user-defined shape or skeleton templates. We follow the recent work of LASSIE that tackles a similar problem setting and make two significant advances. First, instead of relying on a manually annotated 3D skeleton, we automatically estimate a class-specific skeleton from the selected reference image. Second, we improve the shape reconstructions with novel instance-specific optimization strategies that allow reconstructions to faithful fit on each instance while preserving the class-specific priors learned across all images. Experiments on in-the-wild image ensembles show that Hi-LASSIE obtains higher fidelity state-of-the-art 3D reconstructions despite requiring minimum user input. Project page: chhankyao.github.io/hi-lassie/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">451.PointAvatar: Deformable Point-Based Head Avatars From Videos</span><br>
                <span class="as">Zheng, YufengandYifan, WangandWetzstein, GordonandBlack, MichaelJ.andHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21057-21067.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从随意的视频序列中创建逼真的可动画化和可重光照头部头像，并在通信和娱乐领域广泛应用。<br>
                    动机：现有的方法要么基于明确的3D可变形网格（3DMM），要么利用神经隐含表示，但都存在各种限制。<br>
                    方法：我们提出了PointAvatar，一种基于点的可变形表示，将源颜色解耦为固有反照率和法线依赖的着色。<br>
                    效果：PointAvatar成功地连接了现有的网格和隐含表示之间的差距，结合了高质量的几何和外观、拓扑灵活性、易于变形和高效的渲染。我们的方法是首个能在多种来源（包括手持智能手机、笔记本电脑网络摄像头和互联网视频）的单目视频中生成可动画化3D头像的方法，并在挑战性的情况下（如细发丝）实现了超越先前方法的顶级质量，同时在训练效率上也显著优于竞争方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability to create realistic animatable and relightable head avatars from casual video sequences would open up wide ranging applications in communication and entertainment. Current methods either build on explicit 3D morphable meshes (3DMM) or exploit neural implicit representations. The former are limited by fixed topology, while the latter are non-trivial to deform and inefficient to render. Furthermore, existing approaches entangle lighting and albedo, limiting the ability to re-render the avatar in new environments. In contrast, we propose PointAvatar, a deformable point-based representation that disentangles the source color into intrinsic albedo and normal-dependent shading. We demonstrate that PointAvatar bridges the gap between existing mesh- and implicit representations, combining high-quality geometry and appearance with topological flexibility, ease of deformation and rendering efficiency. We show that our method is able to generate animatable 3D avatars using monocular videos from multiple sources including hand-held smartphones, laptop webcams and internet videos, achieving state-of-the-art quality in challenging cases where previous methods fail, e.g., thin hair strands, while being significantly more efficient in training than competing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">452.Seeing Through the Glass: Neural 3D Reconstruction of Object Inside a Transparent Container</span><br>
                <span class="as">Tong, JinguangandMuthu, SundaramandMaken, FahiraAfzalandNguyen, ChuongandLi, Hongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tong_Seeing_Through_the_Glass_Neural_3D_Reconstruction_of_Object_Inside_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12555-12564.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决透明封闭空间内物体的三维几何重建问题。<br>
                    动机：现有的方法无法准确重建透明封闭空间内物体的三维几何，因为光在空气和玻璃等不同传播介质之间的界面上会产生多次反射和折射，导致严重的图像失真。<br>
                    方法：我们提出了一种新的方法，将场景明确地建模为透明封闭空间内外两个不同的子空间。我们使用一种现有的神经重建方法（NeuS）来隐式表示内部子空间的几何和外观，并开发了一种混合渲染策略，结合体积渲染和光线追踪，以考虑复杂的光交互作用。然后，通过最小化真实图像和渲染图像之间的差异，恢复模型的基本几何和外观。<br>
                    效果：我们在合成数据和真实数据上都进行了评估，实验结果表明，我们的方法优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">453.Neural Voting Field for Camera-Space 3D Hand Pose Estimation</span><br>
                <span class="as">Huang, LinandLin, Chung-ChingandLin, KevinandLiang, LinandWang, LijuanandYuan, JunsongandLiu, Zicheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Voting_Field_for_Camera-Space_3D_Hand_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8969-8978.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中进行摄像机空间的3D手部姿态估计。<br>
                    动机：现有的方法大多需要先通过整体或像素级的密集回归获取相对的3D手部姿态，然后进行复杂的第二阶段操作恢复3D全局根或比例，而本文提出了一种新的统一的3D密集回归方案，直接在摄像机视锥体内进行密集3D点投票来估计摄像机空间的3D手部姿态。<br>
                    方法：通过在3D领域直接建模，借鉴用于3D详细重建的像素对齐隐式函数，提出的神经投票场（NVF）全面地模型了3D密集局部证据和手部全局几何，有助于缓解常见的2D到3D的歧义。具体来说，对于视锥体内的一个3D查询点及其像素对齐的图像特征，由多层感知器表示的NVF会回归：（i）其到手表面的距离；（ii）一组4D偏移向量（1D投票权重和每个手关节的3D方向向量）。按照投票方案，选择近表面点的4D偏移向量，通过加权平均计算3D手关节坐标。<br>
                    效果：实验证明，NVF在FreiHAND数据集上的摄像机空间3D手部姿态估计上优于现有的最先进算法。我们还将NVF适应于经典的根相对3D手部姿态估计任务，NVF在该任务上也在HO3D数据集上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a unified framework for camera-space 3D hand pose estimation from a single RGB image based on 3D implicit representation. As opposed to recent works, most of which first adopt holistic or pixel-level dense regression to obtain relative 3D hand pose and then follow with complex second-stage operations for 3D global root or scale recovery, we propose a novel unified 3D dense regression scheme to estimate camera-space 3D hand pose via dense 3D point-wise voting in camera frustum. Through direct dense modeling in 3D domain inspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction, our proposed Neural Voting Field (NVF) fully models 3D dense local evidence and hand global geometry, helping to alleviate common 2D-to-3D ambiguities. Specifically, for a 3D query point in camera frustum and its pixel-aligned image feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) its signed distance to the hand surface; (ii) a set of 4D offset vectors (1D voting weight and 3D directional vector to each hand joint). Following a vote-casting scheme, 4D offset vectors from near-surface points are selected to calculate the 3D hand joint coordinates by a weighted average. Experiments demonstrate that NVF outperforms existing state-of-the-art algorithms on FreiHAND dataset for camera-space 3D hand pose estimation. We also adapt NVF to the classic task of root-relative 3D hand pose estimation, for which NVF also obtains state-of-the-art results on HO3D dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">454.Pointersect: Neural Rendering With Cloud-Ray Intersection</span><br>
                <span class="as">Chang, Jen-HaoRickandChen, Wei-YuandRanjan, AnuragandYi, KwangMooandTuzel, Oncel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Pointersect_Neural_Rendering_With_Cloud-Ray_Intersection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8359-8369.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将点云直接渲染为表面，并实现无场景特定优化的表面法线估计、房间规模点云渲染、逆渲染和全局照明的光线追踪。<br>
                    动机：现有的方法主要关注将点云转换为其他表示形式，如表面或隐式函数，而我们的方法则直接推断光线与给定点云表示的底层表面的交点。<br>
                    方法：我们训练了一个集合转换器，通过少量的沿光线的局部邻居点，提供交点、表面法线和材质混合权重，用于渲染该光线的结果。通过将问题定位在小的邻域内，我们可以仅使用48个网格模型进行训练，并将其应用于未见过点云的场景。<br>
                    效果：在三个测试集上，我们的方法在表面重建和点云渲染方面实现了比现有方法更高的估计精度。当应用于房间规模的点云时，无需任何场景特定的优化，该方法就可以达到与最先进的新颖视图渲染方法相竞争的质量。此外，我们还展示了对激光扫描点云进行渲染和操作的能力，如光照控制和对象插入。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel method that renders point clouds as if they are surfaces. The proposed method is differentiable and requires no scene-specific optimization. This unique capability enables, out-of-the-box, surface normal estimation, rendering room-scale point clouds, inverse rendering, and ray tracing with global illumination. Unlike existing work that focuses on converting point clouds to other representations--e.g., surfaces or implicit functions--our key idea is to directly infer the intersection of a light ray with the underlying surface represented by the given point cloud. Specifically, we train a set transformer that, given a small number of local neighbor points along a light ray, provides the intersection point, the surface normal, and the material blending weights, which are used to render the outcome of this light ray. Localizing the problem into small neighborhoods enables us to train a model with only 48 meshes and apply it to unseen point clouds. Our model achieves higher estimation accuracy than state-of-the-art surface reconstruction and point-cloud rendering methods on three test sets. When applied to room-scale point clouds, without any scene-specific optimization, the model achieves competitive quality with the state-of-the-art novel-view rendering methods. Moreover, we demonstrate ability to render and manipulate Lidar-scanned point clouds such as lighting control and object insertion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">455.MagicPony: Learning Articulated 3D Animals in the Wild</span><br>
                <span class="as">Wu, ShangzheandLi, RuiningandJakab, TomasandRupprecht, ChristianandVedaldi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_MagicPony_Learning_Articulated_3D_Animals_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8792-8802.png><br>
            
            <span class="tt"><span class="t0">研究问题：预测给定一张测试图像的带有关节的动物（如马）的3D形状、关节、视角、纹理和照明。<br>
                    动机：现有的方法需要大量的假设和数据，我们提出了一种新的方法，MagicPony，只需要在野外拍摄的单视图图像进行训练。<br>
                    方法：我们的方法结合了神经场和网格的优点，通过一个现成的自监督视觉转换器来理解物体的形状和姿态。<br>
                    效果：MagicPony在这个具有挑战性的任务上优于先前的工作，并在重建艺术方面表现出色，尽管它只在实际图像上进行过训练。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the problem of predicting the 3D shape, articulation, viewpoint, texture, and lighting of an articulated animal like a horse given a single test image as input. We present a new method, dubbed MagicPony, that learns this predictor purely from in-the-wild single-view images of the object category, with minimal assumptions about the topology of deformation. At its core is an implicit-explicit representation of articulated shape and appearance, combining the strengths of neural fields and meshes. In order to help the model understand an object's shape and pose, we distil the knowledge captured by an off-the-shelf self-supervised vision transformer and fuse it into the 3D model. To overcome local optima in viewpoint estimation, we further introduce a new viewpoint sampling scheme that comes at no additional training cost. MagicPony outperforms prior work on this challenging task and demonstrates excellent generalisation in reconstructing art, despite the fact that it is only trained on real images. The code can be found on the project page at https://3dmagicpony.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">456.Unsupervised Inference of Signed Distance Functions From Single Sparse Point Clouds Without Learning Priors</span><br>
                <span class="as">Chen, ChaoandLiu, Yu-ShenandHan, Zhizhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Unsupervised_Inference_of_Signed_Distance_Functions_From_Single_Sparse_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17712-17723.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从3D点云中推断有符号距离函数（SDFs）。<br>
                    动机：现有的方法依赖于从大规模监督学习中获取的先验知识，但这些先验知识在面对未见过的各种几何变化时，尤其是在极度稀疏的点云上，泛化能力较差。<br>
                    方法：提出了一种神经网络，可以直接从单个稀疏的点云中推断出SDFs，无需使用有符号距离监督、学习到的先验知识或法线。通过端到端的方式学习表面参数化和SDFs推理。为了弥补稀疏性，利用参数化表面作为粗表面采样器，在训练迭代中提供许多粗表面估计，根据这些估计挖掘监督信息，并使用基于薄板样条网络以统计方式推断平滑的SDFs。<br>
                    效果：该方法显著提高了在未见过点云上的泛化能力和准确性。实验结果表明，在合成数据集和真实扫描下，该方法在稀疏点云的表面重建方面优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>It is vital to infer signed distance functions (SDFs) from 3D point clouds. The latest methods rely on generalizing the priors learned from large scale supervision. However, the learned priors do not generalize well to various geometric variations that are unseen during training, especially for extremely sparse point clouds. To resolve this issue, we present a neural network to directly infer SDFs from single sparse point clouds without using signed distance supervision, learned priors or even normals. Our insight here is to learn surface parameterization and SDFs inference in an end-to-end manner. To make up the sparsity, we leverage parameterized surfaces as a coarse surface sampler to provide many coarse surface estimations in training iterations, according to which we mine supervision and our thin plate splines (TPS) based network infers SDFs as smooth functions in a statistical way. Our method significantly improves the generalization ability and accuracy in unseen point clouds. Our experimental results show our advantages over the state-of-the-art methods in surface reconstruction for sparse point clouds under synthetic datasets and real scans.The code is available at https://github.com/chenchao15/NeuralTPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">457.Learning Neural Duplex Radiance Fields for Real-Time View Synthesis</span><br>
                <span class="as">Wan, ZiyuandRichardt, ChristianandBo\v{z</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wan_Learning_Neural_Duplex_Radiance_Fields_for_Real-Time_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8307-8316.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地渲染出照片级真实的图像？<br>
                    动机：目前的神经辐射场（NeRFs）虽然能产生高质量的新视角合成，但需要对每个像素进行大量的多层感知器评估，导致成本高昂且无法实时渲染。<br>
                    方法：提出一种新的方法，将NeRFs蒸馏并烘焙成高效的基于网格的神经表示，完全兼容大规模并行图形渲染管道。我们将场景表示为编码在两层双工网格上的神经辐射特征，通过学习可靠的光线-表面交点区间的聚合辐射信息，有效克服了3D表面重建中的固有不准确性。<br>
                    效果：通过广泛的实验，证明了该方法在一系列标准数据集上的效果和优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields (NeRFs) enable novel view synthesis with unprecedented visual quality. However, to render photorealistic images, NeRFs require hundreds of deep multilayer perceptron (MLP) evaluations -- for each pixel. This is prohibitively expensive and makes real-time rendering infeasible, even on powerful modern GPUs. In this paper, we propose a novel approach to distill and bake NeRFs into highly efficient mesh-based neural representations that are fully compatible with the massively parallel graphics rendering pipeline. We represent scenes as neural radiance features encoded on a two-layer duplex mesh, which effectively overcomes the inherent inaccuracies in 3D surface reconstruction by learning the aggregated radiance information from a reliable interval of ray-surface intersections. To exploit local geometric relationships of nearby pixels, we leverage screen-space convolutions instead of the MLPs used in NeRFs to achieve high-quality appearance. Finally, the performance of the whole framework is further boosted by a novel multi-view distillation optimization strategy. We demonstrate the effectiveness and superiority of our approach via extensive experiments on a range of standard datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">458.Occlusion-Free Scene Recovery via Neural Radiance Fields</span><br>
                <span class="as">Zhu, ChengxuanandWan, RenjieandTang, YunkaiandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Occlusion-Free_Scene_Recovery_via_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20722-20731.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合优化摄像机参数和场景重建来有效地去除遮挡物，而无需依赖外部监督。<br>
                    动机：尽管已经提出了几种去除遮挡物的方法，但由于依赖于外部监督，其性能仍然不尽人意。<br>
                    方法：我们提出了一种新的去除遮挡物的方法，通过在位置、视角和无遮挡场景细节之间建立直接映射，利用神经辐射场（NeRF）。我们还开发了一种有效的方案，当存在遮挡时，联合优化摄像机参数和场景重建。<br>
                    效果：我们在现有的和新收集的数据集上进行的实验结果验证了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Our everyday lives are filled with occlusions that we strive to see through. By aggregating desired background information from different viewpoints, we can easily eliminate such occlusions without any external occlusion-free supervision. Though several occlusion removal methods have been proposed to empower machine vision systems with such ability, their performances are still unsatisfactory due to reliance on external supervision. We propose a novel method for occlusion removal by directly building a mapping between position and viewing angles and the corresponding occlusion-free scene details leveraging Neural Radiance Fields (NeRF). We also develop an effective scheme to jointly optimize camera parameters and scene reconstruction when occlusions are present. An additional depth constraint is applied to supervise the entire optimization without labeled external data for training. The experimental results on existing and newly collected datasets validate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">459.Learning a 3D Morphable Face Reflectance Model From Low-Cost Data</span><br>
                <span class="as">Han, YuxuanandWang, ZhiboandXu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Learning_a_3D_Morphable_Face_Reflectance_Model_From_Low-Cost_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8598-8608.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用低成本的公开数据建立具有空间变化的BRDF的3D可变形人脸反射模型。<br>
                    动机：现有的工作使用Light Stage数据为漫反射和镜面反射率建立参数化模型，但仅靠漫反射和镜面反射率无法确定完整的BRDF。此外，对研究社区来说，获取Light Stage数据的要求很难满足。<br>
                    方法：本文提出了第一个仅使用低成本的公开数据的具有空间变化的BRDF的3D可变形人脸反射模型。我们将线性光泽权重应用于参数化建模，以表示空间变化的镜面强度和光泽度。然后开发了一种逆渲染算法，从非Light Stage数据中重建反射参数，用于训练初始的可变形反射模型。为了提高模型的泛化能力和表达能力，我们进一步提出了一种通过重建进行更新的策略，在野外数据集上对其进行微调。<br>
                    效果：实验结果表明，我们的方法获得了具有可信面部镜面反射率的良好渲染结果。我们的代码发布在https://yxuhan.github.io/ReflectanceMM/index.html。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modeling non-Lambertian effects such as facial specularity leads to a more realistic 3D Morphable Face Model. Existing works build parametric models for diffuse and specular albedo using Light Stage data. However, only diffuse and specular albedo cannot determine the full BRDF. In addition, the requirement of Light Stage data is hard to fulfill for the research communities. This paper proposes the first 3D morphable face reflectance model with spatially varying BRDF using only low-cost publicly-available data. We apply linear shiness weighting into parametric modeling to represent spatially varying specular intensity and shiness. Then an inverse rendering algorithm is developed to reconstruct the reflectance parameters from non-Light Stage data, which are used to train an initial morphable reflectance model. To enhance the model's generalization capability and expressive power, we further propose an update-by-reconstruction strategy to finetune it on an in-the-wild dataset. Experimental results show that our method obtains decent rendering results with plausible facial specularities. Our code is released at https://yxuhan.github.io/ReflectanceMM/index.html.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">460.SCoDA: Domain Adaptive Shape Completion for Real Scans</span><br>
                <span class="as">Wu, YushuangandYan, ZizhengandChen, CeandWei, LaiandLi, XiaoandLi, GuanbinandLi, YihaoandCui, ShuguangandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_SCoDA_Domain_Adaptive_Shape_Completion_for_Real_Scans_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17630-17641.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从点云中完成3D形状，特别是在真实物体扫描的情况下。<br>
                    动机：由于真实扫描的3D形状缺乏精确的地面真值，现有的工作主要集中在在合成数据上进行基准测试，这限制了这些方法的通用性。<br>
                    方法：提出了一个新的任务SCoDA，用于从合成数据适应真实扫描的形状完成。同时，创建了一个新的数据集ScanSalon，并提出了一种新的跨领域特征融合方法和一种新的体积一致的自我训练框架。<br>
                    效果：实验证明，该方法能有效提高6%-7%的mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D shape completion from point clouds is a challenging task, especially from scans of real-world objects. Considering the paucity of 3D shape ground truths for real scans, existing works mainly focus on benchmarking this task on synthetic data, e.g. 3D computer-aided design models. However, the domain gap between synthetic and real data limits the generalizability of these methods. Thus, we propose a new task, SCoDA, for the domain adaptation of real scan shape completion from synthetic data. A new dataset, ScanSalon, is contributed with a bunch of elaborate 3D models created by skillful artists according to scans. To address this new task, we propose a novel cross-domain feature fusion method for knowledge transfer and a novel volume-consistent self-training framework for robust learning from real data. Extensive experiments prove our method is effective to bring an improvement of 6% 7% mIoU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">461.I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs</span><br>
                <span class="as">Zhu, JingsenandHuo, YuchiandYe, QiandLuan, FujunandLi, JifanandXi, DianbingandWang, LishaandTang, RuiandHua, WeiandBao, HujunandWang, Rui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12489-12498.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的方法，使用可微分蒙特卡洛光线追踪在神经符号距离场（SDFs）上进行室内场景的重建和编辑。<br>
                    动机：现有的方法在大规模室内场景的重建和编辑上存在质量不高的问题。<br>
                    方法：提出了一种名为I^2-SDF的新方法，该方法基于神经SDF框架，通过多视角图像联合恢复底层形状、入射辐射度和材质，并引入了新的气泡损失函数和小物体精细粒度优化以及错误引导的自适应采样方案，以提高大规模室内场景的重建质量。<br>
                    效果：通过一系列的定性和定量实验，证明该方法在室内场景重建、新视图合成和场景编辑等方面的性能优于现有的最佳基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we present I^2-SDF, a new method for intrinsic indoor scene reconstruction and editing using differentiable Monte Carlo raytracing on neural signed distance fields (SDFs). Our holistic neural SDF-based framework jointly recovers the underlying shapes, incident radiance and materials from multi-view images. We introduce a novel bubble loss for fine-grained small objects and error-guided adaptive sampling scheme to largely improve the reconstruction quality on large-scale indoor scenes. Further, we propose to decompose the neural radiance field into spatially-varying material of the scene as a neural field through surface-based, differentiable Monte Carlo raytracing and emitter semantic segmentations, which enables physically based and photorealistic scene relighting and editing applications. Through a number of qualitative and quantitative experiments, we demonstrate the superior quality of our method on indoor scene reconstruction, novel view synthesis, and scene editing compared to state-of-the-art baselines. Our project page is at https://jingsenzhu.github.io/i2-sdf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">462.Canonical Fields: Self-Supervised Learning of Pose-Canonicalized Neural Fields</span><br>
                <span class="as">Agaram, RohithandDewan, ShauryaandSajnani, RahulandPoulenard, AdrienandKrishna, MadhavaandSridhar, Srinath</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Agaram_Canonical_Fields_Self-Supervised_Learning_of_Pose-Canonicalized_Neural_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4500-4510.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在没有ShapeNet等提供“标准化”对象实例的数据集的情况下，为3D计算机视觉中的对象类别构建神经场。<br>
                    动机：尽管有进展，但在没有像ShapeNet这样的数据集的情况下，为对象类别构建神经场仍然具有挑战性。<br>
                    方法：我们提出了Canonical Field Network（CaFi-Net），这是一种自我监督的方法，用于对表示为神经场的对象类别的实例进行3D位姿的标准化，特别是神经辐射场（NeRFs）。<br>
                    效果：我们在一个新的包含1300个NeRF模型和13个对象类别的数据集上进行了广泛的实验，结果表明，我们的方法在性能上与基于3D点云的方法相当或优于这些方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Coordinate-based implicit neural networks, or neural fields, have emerged as useful representations of shape and appearance in 3D computer vision. Despite advances however, it remains challenging to build neural fields for categories of objects without datasets like ShapeNet that provide "canonicalized" object instances that are consistently aligned for their 3D position and orientation (pose). We present Canonical Field Network (CaFi-Net), a self-supervised method to canonicalize the 3D pose of instances from an object category represented as neural fields, specifically neural radiance fields (NeRFs). CaFi-Net directly learns from continuous and noisy radiance fields using a Siamese network architecture that is designed to extract equivariant field features for category-level canonicalization. During inference, our method takes pre-trained neural radiance fields of novel object instances at arbitrary 3D pose, and estimates a canonical field with consistent 3D pose across the entire category. Extensive experiments on a new dataset of 1300 NeRF models across 13 object categories show that our method matches or exceeds the performance of 3D point cloud-based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">463.Multi-View Inverse Rendering for Large-Scale Real-World Indoor Scenes</span><br>
                <span class="as">Li, ZhenandWang, LingliandCheng, MofangandPan, CihuiandYang, Jiaqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Multi-View_Inverse_Rendering_for_Large-Scale_Real-World_Indoor_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12499-12509.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地重建大规模真实世界室内场景的全局照明和物理合理的SVBRDFs。<br>
                    动机：现有的表示方法将大型场景的全局照明简化为多个环境贴图，我们提出一种称为基于纹理的照明（TBL）的紧凑表示方法。<br>
                    方法：我们提出了一种有效的多视角逆渲染方法，用于重建大规模真实世界室内场景的全局照明和物理合理的SVBRDFs。我们的方法包括一个基于3D网格和HDR纹理的紧凑表示，称为基于纹理的照明（TBL），以及一种基于预先计算辐照度的混合照明表示。<br>
                    效果：实验结果表明，我们的方法在数量和质量上都优于最先进的方法，并能够实现物理上合理的混合现实应用，如材料编辑、可编辑的新视图合成和重照明。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a efficient multi-view inverse rendering method for large-scale real-world indoor scenes that reconstructs global illumination and physically-reasonable SVBRDFs. Unlike previous representations, where the global illumination of large scenes is simplified as multiple environment maps, we propose a compact representation called Texture-based Lighting (TBL). It consists of 3D mesh and HDR textures, and efficiently models direct and infinite-bounce indirect lighting of the entire large scene. Based on TBL, we further propose a hybrid lighting representation with precomputed irradiance, which significantly improves the efficiency and alleviates the rendering noise in the material optimization. To physically disentangle the ambiguity between materials, we propose a three-stage material optimization strategy based on the priors of semantic segmentation and room segmentation. Extensive experiments show that the proposed method outperforms the state-of-the-art quantitatively and qualitatively, and enables physically-reasonable mixed-reality applications such as material editing, editable novel view synthesis and relighting. The project page is at https://lzleejean.github.io/TexIR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">464.Marching-Primitives: Shape Abstraction From Signed Distance Function</span><br>
                <span class="as">Liu, WeixiaoandWu, YuweiandRuan, SipuandChirikjian, GregoryS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Marching-Primitives_Shape_Abstraction_From_Signed_Distance_Function_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8771-8780.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用基本的几何原语表示复杂对象，以实现更高的计算效率和准确性。<br>
                    动机：现有的从有向距离函数（SDF）提取多边形网格的方法存在不足，本文提出了一种新的方法——Marching-Primitives，可以直接从SDF获取基于原语的抽象表示。<br>
                    方法：通过分析体素的连通性，在不同级别的有向距离范围内逐步生长几何原语（如超二次体），并同时求解原语参数以捕获底层局部几何形状。<br>
                    效果：在合成和真实世界数据集上评估了该方法的性能，结果表明该方法在准确性方面优于现有技术，并且可直接推广到不同类别和规模。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Representing complex objects with basic geometric primitives has long been a topic in computer vision. Primitive-based representations have the merits of compactness and computational efficiency in higher-level tasks such as physics simulation, collision checking, and robotic manipulation. Unlike previous works which extract polygonal meshes from a signed distance function (SDF), in this paper, we present a novel method, named Marching-Primitives, to obtain a primitive-based abstraction directly from an SDF. Our method grows geometric primitives (such as superquadrics) iteratively by analyzing the connectivity of voxels while marching at different levels of signed distance. For each valid connected volume of interest, we march on the scope of voxels from which a primitive is able to be extracted in a probabilistic sense and simultaneously solve for the parameters of the primitive to capture the underlying local geometry. We evaluate the performance of our method on both synthetic and real-world datasets. The results show that the proposed method outperforms the state-of-the-art in terms of accuracy, and is directly generalizable among different categories and scales. The code is open-sourced at https://github.com/ChirikjianLab/Marching-Primitives.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">465.Revisiting the P3P Problem</span><br>
                <span class="as">Ding, YaqingandYang, JianandLarsson, ViktorandOlsson, Carland\r{A</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Revisiting_the_P3P_Problem_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4872-4880.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过三个2D-to-3D对应关系确定已校准相机的绝对姿态。<br>
                    动机：解决该问题对于许多视觉系统（如定位和从结构到运动）至关重要，而现有的算法在特定配置下可能会失败。<br>
                    方法：我们将问题形式化为寻找两个圆锥体的交集，并利用此公式对多项式系统的实数根进行解析表征，为每个问题实例设计定制的解决方案策略。<br>
                    效果：我们的方法比当前最先进的方法在速度和成功率方面都更胜一筹，且完全稳定，能够正确解决竞争性方法失败的情况。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>One of the classical multi-view geometry problems is the so called P3P problem, where the absolute pose of a calibrated camera is determined from three 2D-to-3D correspondences. Since these solvers form a critical component of many vision systems (e.g. in localization and Structure-from-Motion), there have been significant effort in developing faster and more stable algorithms. While the current state-of-the-art solvers are both extremely fast and stable, there still exist configurations where they break down. In this paper we algebraically formulate the problem as finding the intersection of two conics. With this formulation we are able to analytically characterize the real roots of the polynomial system and employ a tailored solution strategy for each problem instance. The result is a fast and completely stable solver, that is able to correctly solve cases where competing methods fail. Our experimental evaluation shows that we outperform the current state-of-the-art methods both in terms of speed and success rate.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">466.Combining Implicit-Explicit View Correlation for Light Field Semantic Segmentation</span><br>
                <span class="as">Cong, RuixuanandYang, DaandChen, RongshanandWang, SizheandCui, ZhenglongandSheng, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cong_Combining_Implicit-Explicit_View_Correlation_for_Light_Field_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9172-9181.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用光场中的多视图信息进行语义分割。<br>
                    动机：光场同时记录了光线的空间信息和角度信息，对于许多潜在应用（如语义分割）具有巨大潜力。然而，由于光场的高维特性和有限的内存，使得在保持单视图上下文信息的同时充分利用各视图间的关系变得困难。<br>
                    方法：本文提出了一种名为LF-IENet的新型网络来进行光场语义分割。该网络包含两种不同的方式从周围视图中挖掘互补信息来分割中心视图：一种是隐式特征集成，通过注意力机制计算视图间和视图内的相似性以调整中心视图的特征；另一种是显式特征传播，直接在其他视图的指导下将特征扭曲到中心视图。这两种方式相互补充，共同实现了光场中跨视图的互补信息融合。<br>
                    效果：该方法在真实世界和合成光场数据集上都取得了优秀的性能，证明了这种新架构的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Since light field simultaneously records spatial information and angular information of light rays, it is considered to be beneficial for many potential applications, and semantic segmentation is one of them. The regular variation of image information across views facilitates a comprehensive scene understanding. However, in the case of limited memory, the high-dimensional property of light field makes the problem more intractable than generic semantic segmentation, manifested in the difficulty of fully exploiting the relationships among views while maintaining contextual information in single view. In this paper, we propose a novel network called LF-IENet for light field semantic segmentation. It contains two different manners to mine complementary information from surrounding views to segment central view. One is implicit feature integration that leverages attention mechanism to compute inter-view and intra-view similarity to modulate features of central view. The other is explicit feature propagation that directly warps features of other views to central view under the guidance of disparity. They complement each other and jointly realize complementary information fusion across views in light field. The proposed method achieves outperforming performance on both real-world and synthetic light field datasets, demonstrating the effectiveness of this new architecture.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">467.SunStage: Portrait Reconstruction and Relighting Using the Sun as a Light Stage</span><br>
                <span class="as">Wang, YifanandHolynski, AleksanderandZhang, XiumingandZhang, Xuaner</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_SunStage_Portrait_Reconstruction_and_Relighting_Using_the_Sun_as_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20792-20802.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种轻量级的光照台替代方案，以更经济、简便的方式捕捉人脸的外观信息。<br>
                    动机：传统的光照台设备昂贵且技术要求高，限制了其在面部重建和重光照等领域的应用。<br>
                    方法：我们提出了SunStage，这是一个使用智能手机摄像头和太阳作为光源的轻量级光照台替代方案。用户只需在户外旋转拍摄自拍照视频，利用太阳与面部之间的角度变化来指导面部几何、反射率、相机姿态和照明参数的联合重建。<br>
                    效果：尽管是在未校准的自然环境中，我们的方法仍能重建出详细的面部外观和几何信息，实现诸如重光照、新视角合成和反射率编辑等引人注目的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A light stage uses a series of calibrated cameras and lights to capture a subject's facial appearance under varying illumination and viewpoint. This captured information is crucial for facial reconstruction and relighting. Unfortunately, light stages are often inaccessible: they are expensive and require significant technical expertise for construction and operation. In this paper, we present SunStage: a lightweight alternative to a light stage that captures comparable data using only a smartphone camera and the sun. Our method only requires the user to capture a selfie video outdoors, rotating in place, and uses the varying angles between the sun and the face as guidance in joint reconstruction of facial geometry, reflectance, camera pose, and lighting parameters. Despite the in-the-wild un-calibrated setting, our approach is able to reconstruct detailed facial appearance and geometry, enabling compelling effects such as relighting, novel view synthesis, and reflectance editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">468.Multi-View Reconstruction Using Signed Ray Distance Functions (SRDF)</span><br>
                <span class="as">Zins, PierreandXu, YuanluandBoyer, EdmondandWuhrer, StefanieandTung, Tony</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zins_Multi-View_Reconstruction_Using_Signed_Ray_Distance_Functions_SRDF_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16696-16706.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究一种新的多视角3D形状重建优化框架。<br>
                    动机：目前的可微渲染方法和多视图立体方法在3D形状重建上各有优势，但都存在一定问题。前者虽然性能出色，但对几何估计精度不足；后者虽然几何精度高，但对全局优化处理不佳。<br>
                    方法：本文提出了一种新颖的体积形状表示方法，该方法结合了两者的优点，通过像素深度参数化来更好地实现形状表面的一致性。同时，通过体积积分进行优化，提高了优化效果。<br>
                    效果：实验结果表明，该方法在标准3D基准测试中的表现优于现有方法，具有更好的几何估计精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we investigate a new optimization framework for multi-view 3D shape reconstructions. Recent differentiable rendering approaches have provided breakthrough performances with implicit shape representations though they can still lack precision in the estimated geometries. On the other hand multi-view stereo methods can yield pixel wise geometric accuracy with local depth predictions along viewing rays. Our approach bridges the gap between the two strategies with a novel volumetric shape representation that is implicit but parameterized with pixel depths to better materialize the shape surface with consistent signed distances along viewing rays. The approach retains pixel-accuracy while benefiting from volumetric integration in the optimization. To this aim, depths are optimized by evaluating, at each 3D location within the volumetric discretization, the agreement between the depth prediction consistency and the photometric consistency for the corresponding pixels. The optimization is agnostic to the associated photo-consistency term which can vary from a median-based baseline to more elaborate criteria, learned functions. Our experiments demonstrate the benefit of the volumetric integration with depth predictions. They also show that our approach outperforms existing approaches over standard 3D benchmarks with better geometry estimations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">469.Neural Pixel Composition for 3D-4D View Synthesis From Multi-Views</span><br>
                <span class="as">Bansal, AayushandZollh\&quot;ofer, Michael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bansal_Neural_Pixel_Composition_for_3D-4D_View_Synthesis_From_Multi-Views_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/290-299.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅使用离散的多视图观察结果进行连续的3D-4D视图合成。<br>
                    动机：现有的最先进技术需要密集的多视图监督和大量的计算预算，而我们的方法可以在稀疏和宽基线的多视图图像上可靠地运行，并且可以在几秒钟到10分钟内高效地训练高分辨率（12MP）的内容。<br>
                    方法：我们提出了一种新的方法，即神经像素合成（NPC），该方法包括两个核心创新点：1）一个表示特定位置和时间沿视线的像素的颜色和深度信息的累积的像素表示；2）一个多层感知器（MLP），该网络能够合成为一个像素位置提供的丰富信息以获得最终的颜色输出。<br>
                    效果：我们在各种多视图序列上进行了大量实验，并与现有方法进行了比较，在多样化和具有挑战性的环境中取得了更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Neural Pixel Composition (NPC), a novel approach for continuous 3D-4D view synthesis given only a discrete set of multi-view observations as input. Existing state-of-the-art approaches require dense multi-view supervision and an extensive computational budget. The proposed formulation reliably operates on sparse and wide-baseline multi-view imagery and can be trained efficiently within a few seconds to 10 minutes for hi-res (12MP) content, i.e., 200-400X faster convergence than existing methods. Crucial to our approach are two core novelties: 1) a representation of a pixel that contains color and depth information accumulated from multi-views for a particular location and time along a line of sight, and 2) a multi-layer perceptron (MLP) that enables the composition of this rich information provided for a pixel location to obtain the final color output. We experiment with a large variety of multi-view sequences, compare to existing approaches, and achieve better results in diverse and challenging settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">470.Hybrid Neural Rendering for Large-Scale Scenes With Motion Blur</span><br>
                <span class="as">Dai, PengandZhang, YindaandYu, XinandLyu, XiaoyangandQi, Xiaojuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Hybrid_Neural_Rendering_for_Large-Scale_Scenes_With_Motion_Blur_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/154-164.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从在野外拍摄的图像中渲染出高保真度、视角一致的新视图，以解决运动模糊等不可避免的伪影问题。<br>
                    动机：尽管最近取得了一些进展，但仍然具有挑战性，需要开发新的模型来解决这个问题。<br>
                    方法：我们开发了一种混合神经网络渲染模型，该模型将基于图像的表示和神经3D表示结合起来，以渲染高质量的、视角一致的图像。同时，我们还提出了模拟模糊效果的策略，以减轻模糊图像对渲染质量的负面影响。<br>
                    效果：我们在真实和合成数据上的大量实验表明，我们的模型超越了最先进的基于点的新颖视图合成方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rendering novel view images is highly desirable for many applications. Despite recent progress, it remains challenging to render high-fidelity and view-consistent novel views of large-scale scenes from in-the-wild images with inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid neural rendering model that makes image-based representation and neural 3D representation join forces to render high-quality, view-consistent images. Besides, images captured in the wild inevitably contain artifacts, such as motion blur, which deteriorates the quality of rendered images. Accordingly, we propose strategies to simulate blur effects on the rendered images to mitigate the negative influence of blurriness images and reduce their importance during training based on precomputed quality-aware weights. Extensive experiments on real and synthetic data demonstrate our model surpasses state-of-the-art point-based methods for novel view synthesis. The code is available at https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">471.Learning 3D Scene Priors With 2D Supervision</span><br>
                <span class="as">Nie, YinyuandDai, AngelaandHan, XiaoguangandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nie_Learning_3D_Scene_Priors_With_2D_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/792-802.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地估计三维环境中的布局配置和物体几何？<br>
                    动机：现有的方法需要大量的3D标注数据，但收集这些数据既昂贵又困难。<br>
                    方法：提出一种新的方法，通过多视角RGB图像的2D监督来学习三维场景的布局和形状先验，无需任何3D真实值。<br>
                    效果：在3D-FRONT和ScanNet上的实验表明，该方法在单视图重建方面优于现有技术，并在场景合成方面实现了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Holistic 3D scene understanding entails estimation of both layout configuration and object geometry in a 3D environment. Recent works have shown advances in 3D scene estimation from various input modalities (e.g., images, 3D scans), by leveraging 3D supervision (e.g., 3D bounding boxes or CAD models), for which collection at scale is expensive and often intractable. To address this shortcoming, we propose a new method to learn 3D scene priors of layout and shape without requiring any 3D ground truth. Instead, we rely on 2D supervision from multi-view RGB images. Our method represents a 3D scene as a latent vector, from which we can progressively decode to a sequence of objects characterized by their class categories, 3D bounding boxes, and meshes. With our trained autoregressive decoder representing the scene prior, our method facilitates many downstream applications, including scene synthesis, interpolation, and single-view reconstruction. Experiments on 3D-FRONT and ScanNet show that our method outperforms state of the art in single-view reconstruction, and achieves state-of-the-art results in scene synthesis against baselines which require for 3D supervision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">472.Grid-Guided Neural Radiance Fields for Large Urban Scenes</span><br>
                <span class="as">Xu, LinningandXiangli, YuanboandPeng, SidaandPan, XingangandZhao, NanxuanandTheobalt, ChristianandDai, BoandLin, Dahua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Grid-Guided_Neural_Radiance_Fields_for_Large_Urban_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8296-8306.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于多层感知机（MLP）的神经辐射场（NeRF）方法在大规模场景中由于模型容量有限，往往导致渲染结果模糊不清，存在欠拟合的问题。<br>
                    动机：目前的解决方案主要是将场景地理分割，采用多个子NeRF对每个区域进行单独建模，但这会导致训练成本和子NeRF数量随着场景的扩大呈线性增长。另一种解决方案是使用特征网格表示，虽然计算效率高且可以自然地扩展到大场景，但特征网格往往约束性较弱，常常无法得到最优解，尤其在几何和纹理复杂的区域，渲染结果会产生噪声。<br>
                    方法：本文提出了一种新的框架，可以在保持计算效率的同时实现大规模城市场景的高保真渲染。我们提出使用一种紧凑的多分辨率地面特征平面表示来粗略捕捉场景，并通过另一个NeRF分支补充位置编码输入，以联合学习的方式进行渲染。<br>
                    效果：实验表明，这种集成方法能够利用两种替代方案的优点：在特征网格表示的指导下，轻量级的NeRF足以渲染具有精细细节的真实照片视图；同时，经过联合优化的地面特征平面可以获得进一步的细化，形成更准确、更紧凑的特征空间，输出更自然的渲染结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Purely MLP-based neural radiance fields (NeRF-based methods) often suffer from underfitting with blurred renderings on large-scale scenes due to limited model capacity. Recent approaches propose to geographically divide the scene and adopt multiple sub-NeRFs to model each region individually, leading to linear scale-up in training costs and the number of sub-NeRFs as the scene expands. An alternative solution is to use a feature grid representation, which is computationally efficient and can naturally scale to a large scene with increased grid resolutions. However, the feature grid tends to be less constrained and often reaches suboptimal solutions, producing noisy artifacts in renderings, especially in regions with complex geometry and texture. In this work, we present a new framework that realizes high-fidelity rendering on large urban scenes while being computationally efficient. We propose to use a compact multi-resolution ground feature plane representation to coarsely capture the scene, and complement it with positional encoding inputs through another NeRF branch for rendering in a joint learning fashion. We show that such an integration can utilize the advantages of two alternative solutions: a light-weighted NeRF is sufficient, under the guidance of the feature grid representation, to render photorealistic novel views with fine details; and the jointly optimized ground feature planes, can meanwhile gain further refinements, forming a more accurate and compact feature space and output much more natural rendering results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">473.Local Implicit Ray Function for Generalizable Radiance Field Representation</span><br>
                <span class="as">Huang, XinandZhang, QiandFeng, YingandLi, XiaoyuandWang, XuanandWang, Qing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Local_Implicit_Ray_Function_for_Generalizable_Radiance_Field_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/97-107.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种名为LIRF的通用神经渲染方法，用于新颖视图渲染。<br>
                    动机：目前的通用神经辐射场（NeRF）方法对每个像素只采样一条光线，因此在输入视图和渲染视图观察场景内容时分辨率不同，可能会渲染出模糊或锯齿状的视图。<br>
                    方法：为了解决这个问题，我们提出了LIRF，它通过聚合来自锥形截锥体的信息来构造一条光线。给定锥形截锥体内的3D位置，LIRF将3D坐标和锥形截锥体的特征作为输入，预测局部体积辐射场。由于坐标是连续的，LIRF可以通过体积渲染在连续的值尺度上渲染高质量的新颖视图。此外，我们还通过基于变压器的特征匹配预测每个输入视图的可见权重，以提高在遮挡区域的性能。<br>
                    效果：我们在真实世界的场景上的实验结果表明，我们的方法在任意尺度上渲染未见场景的新颖视图方面优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose LIRF (Local Implicit Ray Function), a generalizable neural rendering approach for novel view rendering. Current generalizable neural radiance fields (NeRF) methods sample a scene with a single ray per pixel and may therefore render blurred or aliased views when the input views and rendered views observe scene content at different resolutions. To solve this problem, we propose LIRF to aggregate the information from conical frustums to construct a ray. Given 3D positions within conical frustums, LIRF takes 3D coordinates and the features of conical frustums as inputs and predicts a local volumetric radiance field. Since the coordinates are continuous, LIRF renders high-quality novel views at a continuously-valued scale via volume rendering. Besides, we predict the visible weights for each input view via transformer-based feature matching to improve the performance in occluded areas. Experimental results on real-world scenes validate that our method outperforms state-of-the-art methods on novel view rendering of unseen scenes at arbitrary scales.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">474.FitMe: Deep Photorealistic 3D Morphable Model Avatars</span><br>
                <span class="as">Lattas, AlexandrosandMoschoglou, StylianosandPloumpis, StylianosandGecer, BarisandDeng, JiankangandZafeiriou, Stefanos</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lattas_FitMe_Deep_Photorealistic_3D_Morphable_Model_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8629-8640.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张或多张照片中获取高保真、可渲染的人类头像。<br>
                    动机：现有的技术需要大量的计算资源和时间，而且结果往往不尽人意。<br>
                    方法：本文提出了一种名为FitMe的面部反射模型和可微分渲染优化管道，该模型由一个基于风格的多模态生成器和一个基于PCA的形状模型组成，可以捕获面部外观的漫反射和镜面反射。<br>
                    效果：实验结果表明，FitMe在单张“野外”面部图像上取得了最先进的反射获取和身份保持效果，当给定多个同一身份的无约束面部图像时，它可以产生令人印象深刻的扫描效果。与最近的隐式头像重建相比，FitMe只需要一分钟的时间，就可以生成可重光照的网格和基于纹理的头像，可供终端用户应用程序使用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single "in-the-wild" facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">475.expOSE: Accurate Initialization-Free Projective Factorization Using Exponential Regularization</span><br>
                <span class="as">Iglesias, Jos\&#x27;ePedroandNilsson, AmandaandOlsson, Carl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Iglesias_expOSE_Accurate_Initialization-Free_Projective_Factorization_Using_Exponential_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8959-8968.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高结构从运动系统中的光束调整的准确性和效率。<br>
                    动机：光束调整是结构从运动系统中的关键组成部分，但需要良好的初始化才能收敛到正确的解决方案。<br>
                    方法：提出了一种新的基于因子分解的光束调整误差替代方法——expOSE，该方法通过指数正则化解决了大深度的问题，并使用二次近似实现了与VarPro的迭代解决方案。此外，还通过将物体空间误差分解为径向和切向组件，增强了径向畸变稳健性。<br>
                    效果：实验结果表明，该方法对初始化具有鲁棒性，并且即使不进行光束调整优化，也比最先进的方法提高了重建质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Bundle adjustment is a key component in practically all available Structure from Motion systems. While it is crucial for achieving accurate reconstruction, convergence to the right solution hinges on good initialization. The recently introduced factorization-based pOSE methods formulate a surrogate for the bundle adjustment error without reliance on good initialization. In this paper, we show that pOSE has an undesirable penalization of large depths. To address this we propose expOSE which has an exponential regularization that is negligible for positive depths. To achieve efficient inference we use a quadratic approximation that allows an iterative solution with VarPro. Furthermore, we extend the method with radial distortion robustness by decomposing the Object Space Error into radial and tangential components. Experimental results confirm that the proposed method is robust to initialization and improves reconstruction quality compared to state-of-the-art methods even without bundle adjustment refinement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">476.A Large-Scale Homography Benchmark</span><br>
                <span class="as">Barath, DanielandMishkin, DmytroandPolic, MichalandF\&quot;orstner, WolfgangandMatas, Jiri</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Barath_A_Large-Scale_Homography_Benchmark_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21360-21370.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一个大规模的三维平面数据集Pi3D，并利用该数据集构建了一个大规模的姿态估计基准HEB。<br>
                    动机：为了解决单目深度估计、表面法线估计和图像匹配算法的训练和评估问题，以及在视角和光照变化下进行姿态估计的问题。<br>
                    方法：从1DSfM数据集中提取大约1000个平面的图像，创建了包含大约10000张图像的大规模三维平面数据集Pi3D。同时，利用Pi3D构建了包含226260个单应性矩阵和约4百万对应关系的大规模姿态估计基准HEB。<br>
                    效果：通过严格的评估，建立了当前最先进的鲁棒单应性估计方法。同时，还评估了SIFT方向和尺度相对于底层单应性的不确定性，并为比较自定义检测器的不确定性提供了代码。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a large-scale dataset of Planes in 3D, Pi3D, of roughly 1000 planes observed in 10 000 images from the 1DSfM dataset, and HEB, a large-scale homography estimation benchmark leveraging Pi3D. The applications of the Pi3D dataset are diverse, e.g. training or evaluating monocular depth, surface normal estimation and image matching algorithms. The HEB dataset consists of 226 260 homographies and includes roughly 4M correspondences. The homographies link images that often undergo significant viewpoint and illumination changes. As applications of HEB, we perform a rigorous evaluation of a wide range of robust estimators and deep learning-based correspondence filtering methods, establishing the current state-of-the-art in robust homography estimation. We also evaluate the uncertainty of the SIFT orientations and scales w.r.t. the ground truth coming from the underlying homographies and provide codes for comparing uncertainty of custom detectors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">477.Consistent View Synthesis With Pose-Guided Diffusion Models</span><br>
                <span class="as">Tseng, Hung-YuandLi, QinboandKim, ChangilandAlsisan, SuhibandHuang, Jia-BinandKopf, Johannes</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16773-16783.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图片生成一致的长期视频新视角。<br>
                    动机：现有的技术在相机运动较大时，无法生成一致和高质量的新视角。<br>
                    方法：提出一种基于姿态引导的扩散模型，设计一个注意力层，利用极线作为约束来促进不同视点的关联。<br>
                    效果：实验结果表明，该方法在合成和真实世界的数据集上优于最先进的基于变换器和GAN的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Novel view synthesis from a single image has been a cornerstone problem for many Virtual Reality applications that provide immersive experiences. However, most existing techniques can only synthesize novel views within a limited range of camera motion or fail to generate consistent and high-quality novel views under significant camera movement. In this work, we propose a pose-guided diffusion model to generate a consistent long-term video of novel views from a single image. We design an attention layer that uses epipolar lines as constraints to facilitate the association between different viewpoints. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of the proposed diffusion model against state-of-the-art transformer-based and GAN-based approaches. More qualitative results are available at https://poseguided-diffusion.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">478.Learning Neural Volumetric Representations of Dynamic Humans in Minutes</span><br>
                <span class="as">Geng, ChenandPeng, SidaandXu, ZhenandBao, HujunandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Learning_Neural_Volumetric_Representations_of_Dynamic_Humans_in_Minutes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8759-8770.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从稀疏的多视角视频中重建动态人物的体积视频。<br>
                    动机：现有的方法需要花费大量时间进行场景优化，或者牺牲视觉质量以减少优化时间。<br>
                    方法：提出一种新的方法，通过定义新的基于部分的体素化人体表示和创新的二维运动参数化方案，来学习动态人体的神经体积表示。<br>
                    效果：实验证明，该方法比之前的逐场景优化方法快100倍，同时在渲染质量上具有竞争力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper addresses the challenge of efficiently reconstructing volumetric videos of dynamic humans from sparse multi-view videos. Some recent works represent a dynamic human as a canonical neural radiance field (NeRF) and a motion field, which are learned from input videos through differentiable rendering. But the per-scene optimization generally requires hours. Other generalizable NeRF models leverage learned prior from datasets to reduce the optimization time by only finetuning on new scenes at the cost of visual fidelity. In this paper, we propose a novel method for learning neural volumetric representations of dynamic humans in minutes with competitive visual quality. Specifically, we define a novel part-based voxelized human representation to better distribute the representational power of the network to different human parts. Furthermore, we propose a novel 2D motion parameterization scheme to increase the convergence rate of deformation field learning. Experiments demonstrate that our model can be learned 100 times faster than previous per-scene optimization methods while being competitive in the rendering quality. Training our model on a 512x512 video with 100 frames typically takes about 5 minutes on a single RTX 3090 GPU. The code is available on our project page: https://zju3dv.github.io/instant_nvr</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">479.Symmetric Shape-Preserving Autoencoder for Unsupervised Real Scene Point Cloud Completion</span><br>
                <span class="as">Ma, ChangfengandChen, YinuoandGuo, PengxiaoandGuo, JieandWang, ChongjunandGuo, Yanwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Symmetric_Shape-Preserving_Autoencoder_for_Unsupervised_Real_Scene_Point_Cloud_Completion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13560-13569.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现真实场景物体的无监督补全，同时保留输入形状、预测准确结果并适应多类别数据。<br>
                    动机：现有的方法在保持输入形状、预测准确结果和适应多类别数据方面存在困难。<br>
                    方法：提出一种名为USSPA的无监督对称形状保持自动编码网络，通过学习自然和人造物体的显著对称性来预测真实场景中物体的完整点云。设计了一个对称性学习模块来学习和保留结构对称性，并通过精心设计的上采样细化模块从初始粗预测器开始细化完整形状。<br>
                    效果：实验结果表明，该方法在真实场景物体的无监督补全方面取得了最先进的性能，并在单类别训练中具有一致的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised completion of real scene objects is of vital importance but still remains extremely challenging in preserving input shapes, predicting accurate results, and adapting to multi-category data. To solve these problems, we propose in this paper an Unsupervised Symmetric Shape-Preserving Autoencoding Network, termed USSPA, to predict complete point clouds of objects from real scenes. One of our main observations is that many natural and man-made objects exhibit significant symmetries. To accommodate this, we devise a symmetry learning module to learn from those objects and to preserve structural symmetries. Starting from an initial coarse predictor, our autoencoder refines the complete shape with a carefully designed upsampling refinement module. Besides the discriminative process on the latent space, the discriminators of our USSPA also take predicted point clouds as direct guidance, enabling more detailed shape prediction. Clearly different from previous methods which train each category separately, our USSPA can be adapted to the training of multi-category data in one pass through a classifier-guided discriminator, with consistent performance on single category. For more accurate evaluation, we contribute to the community a real scene dataset with paired CAD models as ground truth. Extensive experiments and comparisons demonstrate our superiority and generalization and show that our method achieves state-of-the-art performance on unsupervised completion of real scene objects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">480.Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling</span><br>
                <span class="as">Hu, ZhanhaoandChu, WendaandZhu, XiaopeiandZhang, HuiandZhang, BoandHu, Xiaolin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Physically_Realizable_Natural-Looking_Clothing_Textures_Evade_Person_Detectors_via_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16975-16984.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何制作出能避开人体检测器的对抗性衣物纹理？<br>
                    动机：目前的对抗性衣物大多只在有限的视角有效，或对人类来说过于明显。<br>
                    方法：基于3D建模来制作对抗性衣物纹理，利用Voronoi图和Gumbel-softmax技巧参数化迷彩纹理并通过3D建模优化参数。同时提出一种结合拓扑合理投影（TopoProj）和薄板样条（TPS）的高效3D网格增强管道。<br>
                    效果：实验表明，这些衣物对多种探测器的攻击成功率很高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narrow the gap between digital and real-world objects. We printed the developed 3D texture pieces on fabric materials and tailored them into T-shirts and trousers. Experiments show high attack success rates of these clothes against multiple detectors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">481.SurfelNeRF: Neural Surfel Radiance Fields for Online Photorealistic Reconstruction of Indoor Scenes</span><br>
                <span class="as">Gao, YimingandCao, Yan-PeiandShan, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_SurfelNeRF_Neural_Surfel_Radiance_Fields_for_Online_Photorealistic_Reconstruction_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/108-118.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在线重建和渲染大规模室内场景。<br>
                    动机：目前的在线重建方法（如SLAM）可以实时重建3D场景几何，但不能产生照片级的真实结果；而基于NeRF的方法虽然能产生有前景的新颖视图合成结果，但其离线优化时间长且缺乏几何约束，对处理在线输入构成挑战。<br>
                    方法：我们提出了SurfelNeRF，这是一种结合了显式几何表示和NeRF渲染的新型方法，通过使用灵活可扩展的神经表面表示来存储几何属性和从输入图像中提取的外观特征，并进一步将输入帧逐步集成到重建的全局神经场景表示中。此外，我们还提出了一种高效的可微分光栅化方案，用于渲染神经表面辐射场，使SurfelNeRF的训练和推理时间分别提高了10倍。<br>
                    效果：实验结果表明，我们的方法在ScanNet上实现了最先进的23.82 PSNR和29.58 PSNR，在前向推理和每场景优化设置下分别达到了最佳效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Online reconstructing and rendering of large-scale indoor scenes is a long-standing challenge. SLAM-based methods can reconstruct 3D scene geometry progressively in real time but can not render photorealistic results. While NeRF-based methods produce promising novel view synthesis results, their long offline optimization time and lack of geometric constraints pose challenges to efficiently handling online input. Inspired by the complementary advantages of classical 3D reconstruction and NeRF, we thus investigate marrying explicit geometric representation with NeRF rendering to achieve efficient online reconstruction and high-quality rendering. We introduce SurfelNeRF, a variant of neural radiance field which employs a flexible and scalable neural surfel representation to store geometric attributes and extracted appearance features from input images. We further extend conventional surfel-based fusion scheme to progressively integrate incoming input frames into the reconstructed global neural scene representation. In addition, we propose a highly-efficient differentiable rasterization scheme for rendering neural surfel radiance fields, which helps SurfelNeRF achieve 10x speedups in training and inference time, respectively. Experimental results show that our method achieves the state-of-the-art 23.82 PSNR and 29.58 PSNR on ScanNet in feedforward inference and per-scene optimization settings, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">482.NeUDF: Leaning Neural Unsigned Distance Fields With Volume Rendering</span><br>
                <span class="as">Liu, Yu-TaoandWang, LiandYang, JieandChen, WeikaiandMeng, XiaoxuandYang, BoandGao, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_NeUDF_Leaning_Neural_Unsigned_Distance_Fields_With_Volume_Rendering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/237-247.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于符号距离函数（SDF）的方法仅限于重建封闭表面，无法重建包含开放表面结构的广泛真实世界对象。<br>
                    动机：为了解决这一问题，我们引入了一种新的神经渲染框架——编码NeUDF，它仅通过多视图监督就可以从任意拓扑结构的表面进行重建。<br>
                    方法：NeUDF利用无符号距离函数（UDF）作为表面表示，以获得表示任意表面的灵活性。同时，我们提出了两种专为基于UDF的体积渲染设计的权重函数新公式，并针对开放表面渲染（其中入/出测试不再有效）提出了专门的法线正则化策略来解决表面方向模糊问题。<br>
                    效果：我们在包括DTU、MGN和Deep Fashion 3D在内的多个具有挑战性的数据集上进行了广泛的评估。实验结果表明，NeUDF在多视图表面重建任务中的性能明显优于最先进的方法，尤其是在处理具有开放边界的复杂形状时。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-view shape reconstruction has achieved impressive progresses thanks to the latest advances in neural implicit surface rendering. However, existing methods based on signed distance function (SDF) are limited to closed surfaces, failing to reconstruct a wide range of real-world objects that contain open-surface structures. In this work, we introduce a new neural rendering framework, coded NeUDF, that can reconstruct surfaces with arbitrary topologies solely from multi-view supervision. To gain the flexibility of representing arbitrary surfaces, NeUDF leverages the unsigned distance function (UDF) as surface representation. While a naive extension of SDF-based neural renderer cannot scale to UDF, we propose two new formulations of weight function specially tailored for UDF-based volume rendering. Furthermore, to cope with open surface rendering, where the in/out test is no longer valid, we present a dedicated normal regularization strategy to resolve the surface orientation ambiguity. We extensively evaluate our method over a number of challenging datasets, including DTU, MGN, and Deep Fashion 3D. Experimental results demonstrate that NeUDF can significantly outperform the state-of-the-art method in the task of multi-view surface reconstruction, especially for the complex shapes with open boundaries.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">483.NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction From Multi-View Images</span><br>
                <span class="as">Ye, YunfanandYi, RenjiaoandGao, ZhiruiandZhu, ChenyangandCai, ZhipingandXu, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_NEF_Neural_Edge_Fields_for_3D_Parametric_Curve_Reconstruction_From_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8486-8495.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从一组校准的多视图图像中重建物体的3D特征曲线。<br>
                    动机：现有的方法需要3D边缘的监督或几何运算，我们提出一种无需这些操作的新方法。<br>
                    方法：我们学习了一种表示3D边缘密度分布的神经隐式场（NEF），并通过基于视图的渲染损失进行优化。<br>
                    效果：在合成数据上，我们的NEF方法在所有指标上都优于现有的最佳方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of reconstructing 3D feature curves of an object from a set of calibrated multi-view images. To do so, we learn a neural implicit field representing the density distribution of 3D edges which we refer to as Neural Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based rendering loss where a 2D edge map is rendered at a given view and is compared to the ground-truth edge map extracted from the image of that view. The rendering-based differentiable optimization of NEF fully exploits 2D edge detection, without needing a supervision of 3D edges, a 3D geometric operator or cross-view edge correspondence. Several technical designs are devised to ensure learning a range-limited and view-independent NEF for robust edge extraction. The final parametric 3D curves are extracted from NEF with an iterative optimization method. On our benchmark with synthetic data, we demonstrate that NEF outperforms existing state-of-the-art methods on all metrics. Project page: https://yunfan1202.github.io/NEF/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">484.Inverting the Imaging Process by Learning an Implicit Camera Model</span><br>
                <span class="as">Huang, XinandZhang, QiandFeng, YingandLi, HongdongandWang, Qing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Inverting_the_Imaging_Process_by_Learning_an_Implicit_Camera_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21456-21465.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地替代传统的离散信号表示，用隐式坐标基神经网络来表示视觉信号。<br>
                    动机：现有的隐式神经网络表示主要关注场景建模，本文提出了一种新的隐式相机模型，将相机的物理成像过程表示为深度神经网络。<br>
                    方法：通过多聚焦堆栈和多曝光包围监督，联合学习隐式场景模型和隐式相机模型。设计了隐式模糊生成器和隐式色调映射器分别模拟相机的光圈和曝光过程。<br>
                    效果：在大量的测试图像和视频上展示了新模型的有效性，能产生准确且视觉上吸引人的全焦点和高动态范围图像。原则上，新的隐式神经网络相机模型有可能使各种其他逆成像任务受益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Representing visual signals with implicit coordinate-based neural networks, as an effective replacement of the traditional discrete signal representation, has gained considerable popularity in computer vision and graphics. In contrast to existing implicit neural representations which focus on modelling the scene only, this paper proposes a novel implicit camera model which represents the physical imaging process of a camera as a deep neural network. We demonstrate the power of this new implicit camera model on two inverse imaging tasks: i) generating all-in-focus photos, and ii) HDR imaging. Specifically, we devise an implicit blur generator and an implicit tone mapper to model the aperture and exposure of the camera's imaging process, respectively. Our implicit camera model is jointly learned together with implicit scene models under multi-focus stack and multi-exposure bracket supervision. We have demonstrated the effectiveness of our new model on large number of test images and videos, producing accurate and visually appealing all-in-focus and high dynamic range images. In principle, our new implicit neural camera model has the potential to benefit a wide array of other inverse imaging tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">485.Detecting Human-Object Contact in Images</span><br>
                <span class="as">Chen, YixinandDwivedi, SaiKumarandBlack, MichaelJ.andTzionas, Dimitrios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Detecting_Human-Object_Contact_in_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17100-17110.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前，对于从图像中检测人体与物体的接触情况，尚无稳健的方法和数据集。<br>
                    动机：人类在执行任务时会不断接触物体，因此，检测人体与物体的接触对于构建以人为中心的人工智能至关重要。<br>
                    方法：我们创建了一个新的数据集HOT（"Human-Object conTact"），通过结合两个数据源来构建HOT：一是使用3D人体网格在3D场景中移动的PROX数据集，并通过3D网格接近度和投影自动注释2D图像区域；二是使用V-COCO、HAKE和Watch-n-Patch数据集，并让训练过的标注者在发生接触的2D图像区域周围画多边形。我们还对接触的身体部位进行了标注。然后，我们使用HOT数据集训练了一个新的接触探测器，该探测器接受单色图像作为输入，输出2D接触热图以及接触的身体部位标签。<br>
                    效果：我们的探测器在广泛的评估中表现出色，定量结果显示，我们的模型优于基线，所有组件都有助于提高性能。在线资源库中的图像结果显示出合理的检测结果和泛化能力。我们的HOT数据集和模型可在https://hot.is.tue.mpg.de进行研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT ("Human-Object conTact"), a new dataset of human-object contacts in images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons around the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task, that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. Our HOT data and model are available for research at https://hot.is.tue.mpg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">486.Human Body Shape Completion With Implicit Shape and Flow Learning</span><br>
                <span class="as">Zhou, BoyaoandMeng, DiandFranco, Jean-S\&#x27;ebastienandBoyer, Edmond</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Human_Body_Shape_Completion_With_Implicit_Shape_and_Flow_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12901-12911.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过结合形状和流动估计，利用两个连续深度图像完成人体形状模型。<br>
                    动机：在考虑部分深度观察时，形状补全是计算机视觉中一个具有挑战性的任务，且高度缺乏约束。<br>
                    方法：采用学习基础的方法，并探索两个连续帧之间的运动流如何对形状补全任务做出贡献。为了有效利用流动信息，我们的架构结合了两种估计，并实现了两个用于提高稳健性的特征：一是所有到全部的注意力模块，用于编码同一帧内点和不同帧对应点之间的相关性；二是从粗到细，再到稀疏的策略，以平衡表示能力和计算成本。<br>
                    效果：实验证明，流动实际上有助于人体模型的完成。同时，对于不同的人体形状、姿势和服装，该方法在两个基准测试上都优于最先进的形状补全方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we investigate how to complete human body shape models by combining shape and flow estimation given two consecutive depth images. Shape completion is a challenging task in computer vision that is highly under-constrained when considering partial depth observations. Besides model based strategies that exploit strong priors, and consequently struggle to preserve fine geometric details, learning based approaches build on weaker assumptions and can benefit from efficient implicit representations. We adopt such a representation and explore how the motion flow between two consecutive frames can contribute to the shape completion task. In order to effectively exploit the flow information, our architecture combines both estimations and implements two features for robustness: First, an all-to-all attention module that encodes the correlation between points in the same frame and between corresponding points in different frames; Second, a coarse-dense to fine-sparse strategy that balances the representation ability and the computational cost. Our experiments demonstrate that the flow actually benefits human body model completion. They also show that our method outperforms the state-of-the-art approaches for shape completion on 2 benchmarks, considering different human shapes, poses, and clothing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">487.Towards Unbiased Volume Rendering of Neural Implicit Surfaces With Geometry Priors</span><br>
                <span class="as">Zhang, YongqiangandHu, ZhipengandWu, HaoqianandZhao, MindaandLi, LinchengandZou, ZhengxiaandFan, Changjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4359-4368.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经表面重建方法在多视角重建中的准确度有限。<br>
                    动机：这种限制是由于其体积渲染策略的偏差，特别是在观察方向接近与表面相切时。<br>
                    方法：我们提出了一种新的渲染方法，通过将SDF场与观察方向和表面法线向量之间的角度进行缩放来消除偏差。<br>
                    效果：实验结果表明，我们的渲染方法减少了基于SDF的体积渲染的偏差，并在DTU数据集上超越了最先进的神经隐式表面方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning surface by neural implicit rendering has been a promising way for multi-view reconstruction in recent years. Existing neural surface reconstruction methods, such as NeuS and VolSDF, can produce reliable meshes from multi-view posed images. Although they build a bridge between volume rendering and Signed Distance Function (SDF), the accuracy is still limited. In this paper, we argue that this limited accuracy is due to the bias of their volume rendering strategies, especially when the viewing direction is close to be tangent to the surface. We revise and provide an additional condition for the unbiased volume rendering. Following this analysis, we propose a new rendering method by scaling the SDF field with the angle between the viewing direction and the surface normal vector. Experiments on simulated data indicate that our rendering method reduces the bias of SDF-based volume rendering. Moreover, there still exists non-negligible bias when the learnable standard deviation of SDF is large at early stage, which means that it is hard to supervise the rendered depth with depth priors. Alternatively we supervise zero-level set with surface points obtained from a pre-trained Multi-View Stereo network. We evaluate our method on the DTU dataset and show that it outperforms the state-of-the-arts neural implicit surface methods without mask supervision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">488.NeRFLight: Fast and Light Neural Radiance Fields Using a Shared Feature Grid</span><br>
                <span class="as">Rivas-Manzaneque, FernandoandSierra-Acosta, JorgeandPenate-Sanchez, AdrianandMoreno-Noguer, FrancescandRibeiro, Angela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rivas-Manzaneque_NeRFLight_Fast_and_Light_Neural_Radiance_Fields_Using_a_Shared_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12417-12427.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的Neural Radiance Fields（NeRF）模型虽然在场景外观建模上表现出色，但无法实现实时渲染。<br>
                    动机：为了解决这一问题，研究人员尝试将NeRF的输出烘焙到数据结构中或将可训练参数排列在显式特征网格中，但这些方法会大大增加模型的内存占用，限制了其在带宽受限的应用中的部署。<br>
                    方法：本文提出了一种新的架构，将基于NeRF表示的密度场分割为N个区域，并使用N个不同的解码器对密度进行建模，这些解码器共享同一个特征网格。这种方法生成了一个较小的网格，其中每个特征位于多个空间位置，迫使它们学习一个对场景不同部分都有效的紧凑表示。<br>
                    效果：通过在每个区域上对称地处理特征，进一步减小了最终模型的大小，这既有利于训练后的特征剪枝，又允许相邻体素之间平滑的梯度过渡。实验表明，该方法实现了实时性能和质量指标，与最先进的方法相比，FPS/MB比率提高了2倍以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While original Neural Radiance Fields (NeRF) have shown impressive results in modeling the appearance of a scene with compact MLP architectures, they are not able to achieve real-time rendering. This has been recently addressed by either baking the outputs of NeRF into a data structure or arranging trainable parameters in an explicit feature grid. These strategies, however, significantly increase the memory footprint of the model which prevents their deployment on bandwidth-constrained applications. In this paper, we extend the grid-based approach to achieve real-time view synthesis at more than 150 FPS using a lightweight model. Our main contribution is a novel architecture in which the density field of NeRF-based representations is split into N regions and the density is modeled using N different decoders which reuse the same feature grid. This results in a smaller grid where each feature is located in more than one spatial position, forcing them to learn a compact representation that is valid for different parts of the scene. We further reduce the size of the final model by disposing of the features symmetrically on each region, which favors feature pruning after training while also allowing smooth gradient transitions between neighboring voxels. An exhaustive evaluation demonstrates that our method achieves real-time performance and quality metrics on a pair with state-of-the-art with an improvement of more than 2x in the FPS/MB ratio.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">489.Compressing Volumetric Radiance Fields to 1 MB</span><br>
                <span class="as">Li, LingzhiandShen, ZhenandWang, ZhongshuandShen, LiandBo, Liefeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Compressing_Volumetric_Radiance_Fields_to_1_MB_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4222-4231.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改进NeRFs，提高训练速度和渲染效率。<br>
                    动机：现有的方法如DVGO、Plenoxels和TensoRF等在改进NeRFs上取得了显著效果，但需要大量的存储空间，且运行内存消耗大。<br>
                    方法：本文提出了一种名为向量量化辐射场（VQRF）的简单有效框架，用于压缩基于体积网格的辐射场。通过引入可训练的向量量化来提高网格模型的紧凑性，并结合有效的联合调整策略和后处理，实现了对模型大小的压缩，同时保持视觉质量。<br>
                    效果：实验表明，该方法在多种具有不同体积结构的方法中表现出色，能够实现无可比拟的性能和良好的泛化能力，为实际应用中的体积辐射场方法提供了广泛的使用可能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Approximating radiance fields with discretized volumetric grids is one of promising directions for improving NeRFs, represented by methods like DVGO, Plenoxels and TensoRF, which achieve super-fast training convergence and real-time rendering. However, these methods typically require a tremendous storage overhead, costing up to hundreds of megabytes of disk space and runtime memory for a single scene. We address this issue in this paper by introducing a simple yet effective framework, called vector quantized radiance fields (VQRF), for compressing these volume-grid-based radiance fields. We first present a robust and adaptive metric for estimating redundancy in grid models and performing voxel pruning by better exploring intermediate outputs of volumetric rendering. A trainable vector quantization is further proposed to improve the compactness of grid models. In combination with an efficient joint tuning strategy and post-processing, our method can achieve a compression ratio of 100x by reducing the overall model size to 1 MB with negligible loss on visual quality. Extensive experiments demonstrate that the proposed framework is capable of achieving unrivaled performance and well generalization across multiple methods with distinct volumetric structures, facilitating the wide use of volumetric radiance fields methods in real-world applications. Code is available at https://github.com/AlgoHunt/VQRF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">490.Gated Stereo: Joint Depth Estimation From Gated and Wide-Baseline Active Stereo Cues</span><br>
                <span class="as">Walz, StefanieandBijelic, MarioandRamazzina, AndreaandWalia, AmanpreetandMannan, FahimandHeide, Felix</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13252-13262.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种高分辨率、长距离深度估计技术，即门控立体视觉。<br>
                    动机：利用主动和高动态范围被动捕获，结合多视角线索和来自主动门控的飞行时间强度线索进行深度估计。<br>
                    方法：提出一种带有单眼和立体深度预测分支的深度估计方法，并在最终融合阶段进行组合。每个模块通过有监督和门控自我监督损失的组合进行监督。<br>
                    效果：该方法在最远160米的距离上，比次优RGB立体方法提高了50%以上的MAE，比现有的单眼门控方法提高了74%的MAE。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Gated Stereo, a high-resolution and long-range depth estimation technique that operates on active gated stereo images. Using active and high dynamic range passive captures, Gated Stereo exploits multi-view cues alongside time-of-flight intensity cues from active gating. To this end, we propose a depth estimation method with a monocular and stereo depth prediction branch which are combined in a final fusion stage. Each block is supervised through a combination of supervised and gated self-supervision losses. To facilitate training and validation, we acquire a long-range synchronized gated stereo dataset for automotive scenarios. We find that the method achieves an improvement of more than 50 % MAE compared to the next best RGB stereo method, and 74 % MAE to existing monocular gated methods for distances up to 160 m. Our code, models and datasets are available here: https://light.princeton.edu/gatedstereo/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">491.Hand Avatar: Free-Pose Hand Animation and Rendering From Monocular Video</span><br>
                <span class="as">Chen, XingyuandWang, BaoyuanandShum, Heung-Yeung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8683-8693.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的手部动画和渲染表示方法，即HandAvatar，以生成平滑的组成几何和自遮挡感知纹理。<br>
                    动机：现有的手部动画和渲染技术无法生成高质量的个性化手部形状，以及逼真的组成几何和自遮挡感知纹理。<br>
                    方法：首先，我们开发了一个MANO-HD模型作为高分辨率网格拓扑，以适应个性化的手部形状。然后，我们将手部几何分解为每根骨头的刚性部分，并重新组合成对的几何编码，以获得一致的占用场。对于纹理建模，我们提出了一个自遮挡感知的着色场（SelF）。在SelF中，我们在MANO-HD表面上铺设了可驱动的锚点，以记录各种手部姿势下的反射率信息。此外，我们还设计了有向软占用，用于描述光线到表面的关系，从而生成照明场，实现与姿势无关的反射率和与姿势有关光照的解耦。<br>
                    效果：通过单目视频数据训练的HandAvatar可以在进行自由手部动画和渲染的同时，实现出色的外观保真度。我们还证明，HandAvatar为手部外观编辑提供了一种途径。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present HandAvatar, a novel representation for hand animation and rendering, which can generate smoothly compositional geometry and self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as a high-resolution mesh topology to fit personalized hand shapes. Sequentially, we decompose hand geometry into per-bone rigid parts, and then re-compose paired geometry encodings to derive an across-part consistent occupancy field. As for texture modeling, we propose a self-occlusion-aware shading field (SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record albedo information under a wide variety of hand poses. Moreover, directed soft occupancy is designed to describe the ray-to-surface relation, which is leveraged to generate an illumination field for the disentanglement of pose-independent albedo and pose-dependent illumination. Trained from monocular video data, our HandAvatar can perform free-pose hand animation and rendering while at the same time achieving superior appearance fidelity. We also demonstrate that HandAvatar provides a route for hand appearance editing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">492.DiffRF: Rendering-Guided 3D Radiance Field Diffusion</span><br>
                <span class="as">M\&quot;uller, NormanandSiddiqui, YawarandPorzi, LorenzoandBul\`o, SamuelRotaandKontschieder, PeterandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4328-4338.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的3D辐射场合成方法DiffRF，该方法基于去噪扩散概率模型。<br>
                    动机：现有的基于扩散的方法主要在图像、潜在代码或点云数据上操作，而我们是第一个直接生成体积辐射场的。<br>
                    方法：我们提出了一个直接在显式体素网格表示上操作的3D去噪模型。为了解决从一组拍摄的图像生成的辐射场可能模糊并包含伪影的问题，我们将去噪公式与渲染损失相结合，使模型能够学习偏向良好图像质量的偏离先验，而不是尝试复制漂浮伪影等拟合误差。<br>
                    效果：与2D扩散模型相比，我们的模型学习了多视图一致的先验，支持自由视点合成和精确的形状生成。与3D GANs相比，我们的基于扩散的方法自然地支持条件生成，如遮罩完成或单视图3D合成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce DiffRF, a novel approach for 3D radiance field synthesis based on denoising diffusion probabilistic models. While existing diffusion-based methods operate on images, latent codes, or point cloud data, we are the first to directly generate volumetric radiance fields. To this end, we propose a 3D denoising model which directly operates on an explicit voxel grid representation. However, as radiance fields generated from a set of posed images can be ambiguous and contain artifacts, obtaining ground truth radiance field samples is non-trivial. We address this challenge by pairing the denoising formulation with a rendering loss, enabling our model to learn a deviated prior that favours good image quality instead of trying to replicate fitting errors like floating artifacts. In contrast to 2D-diffusion models, our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation. Compared to 3D GANs, our diffusion-based approach naturally enables conditional generation like masked completion or single-view 3D synthesis at inference time.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">493.SUDS: Scalable Urban Dynamic Scenes</span><br>
                <span class="as">Turki, HaithemandZhang, JasonY.andFerroni, FrancescoandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Turki_SUDS_Scalable_Urban_Dynamic_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12375-12385.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩展神经辐射场（NeRFs）以处理大规模的动态城市场景。<br>
                    动机：目前的工作主要针对短时视频片段进行重建，且需要通过3D边界框和全景标签进行监督。<br>
                    方法：将场景分解为三个独立的哈希表数据结构，有效地编码静态、动态和远场辐射场；利用未标记的目标信号，包括RGB图像、稀疏LiDAR、现成的自监督2D描述符以及最重要的2D光流。<br>
                    效果：在1700个视频的地理覆盖范围内，实现了对数千个对象的重建，并超越了依赖真实3D边界框注释的最新技术，同时训练速度提高了10倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We extend neural radiance fields (NeRFs) to dynamic large-scale urban scenes. Prior work tends to reconstruct single video clips of short durations (up to 10 seconds). Two reasons are that such methods (a) tend to scale linearly with the number of moving objects and input videos because a separate model is built for each and (b) tend to require supervision via 3D bounding boxes and panoptic labels, obtained manually or via category-specific models. As a step towards truly open-world reconstructions of dynamic cities, we introduce two key innovations: (a) we factorize the scene into three separate hash table data structures to efficiently encode static, dynamic, and far-field radiance fields, and (b) we make use of unlabeled target signals consisting of RGB images, sparse LiDAR, off-the-shelf self-supervised 2D descriptors, and most importantly, 2D optical flow. Operationalizing such inputs via photometric, geometric, and feature-metric reconstruction losses enables SUDS to decompose dynamic scenes into the static background, individual objects, and their motions. When combined with our multi-branch table representation, such reconstructions can be scaled to tens of thousands of objects across 1.2 million frames from 1700 videos spanning geospatial footprints of hundreds of kilometers, (to our knowledge) the largest dynamic NeRF built to date. We present qualitative initial results on a variety of tasks enabled by our representations, including novel-view synthesis of dynamic urban scenes, unsupervised 3D instance segmentation, and unsupervised 3D cuboid detection. To compare to prior work, we also evaluate on KITTI and Virtual KITTI 2, surpassing state-of-the-art methods that rely on ground truth 3D bounding box annotations while being 10x quicker to train.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">494.HandNeRF: Neural Radiance Fields for Animatable Interacting Hands</span><br>
                <span class="as">Guo, ZhiyangandZhou, WengangandWang, MinandLi, LiandLi, Houqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_HandNeRF_Neural_Radiance_Fields_for_Animatable_Interacting_Hands_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21078-21087.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的框架，利用神经辐射场（NeRF）重建交互手的准确外观和几何形状。<br>
                    动机：为了实现从任意视角进行手势动画的真实感渲染，需要对手部外观和几何形状进行精确建模。<br>
                    方法：首先使用现成的骨架估计器对手部姿态进行参数化，然后设计一个基于姿态的变形场，建立不同姿态之间的对应关系，优化单手的姿态解耦NeRF。这种统一的建模方式有效地补充了两种手部在罕见观察区域中的几何和纹理线索。同时，利用姿态先验生成伪深度图，作为遮挡感知密度学习的指导。此外，还提出了一种神经特征蒸馏方法，实现颜色优化的跨域对齐。<br>
                    效果：通过在大规模InterHand2.6M数据集上进行大量实验，验证了提出的HandNeRF的优点，并在定性和定量上都取得了一系列最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel framework to reconstruct accurate appearance and geometry with neural radiance fields (NeRF) for interacting hands, enabling the rendering of photo-realistic images and videos for gesture animation from arbitrary views. Given multi-view images of a single hand or interacting hands, an off-the-shelf skeleton estimator is first employed to parameterize the hand poses. Then we design a pose-driven deformation field to establish correspondence from those different poses to a shared canonical space, where a pose-disentangled NeRF for one hand is optimized. Such unified modeling efficiently complements the geometry and texture cues in rarely-observed areas for both hands. Meanwhile, we further leverage the pose priors to generate pseudo depth maps as guidance for occlusion-aware density learning. Moreover, a neural feature distillation method is proposed to achieve cross-domain alignment for color optimization. We conduct extensive experiments to verify the merits of our proposed HandNeRF and report a series of state-of-the-art results both qualitatively and quantitatively on the large-scale InterHand2.6M dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">495.Weakly-Supervised Single-View Image Relighting</span><br>
                <span class="as">Yi, RenjiaoandZhu, ChenyangandXu, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Weakly-Supervised_Single-View_Image_Relighting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8402-8411.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地为朗伯体和低频镜面物体的单张图片重新打光。<br>
                    动机：为了实现AR应用中将照片中的物体插入新场景并按照新环境光照进行重新打光，需要解决图像的逆渲染和再渲染问题。<br>
                    方法：提出了一种基于学习的方法，通过弱监督低秩约束解决逆渲染问题，并使用可微分的镜面渲染层对低频非朗伯材料进行各种球谐光照下的再渲染。<br>
                    效果：通过大规模的数据集和实验验证，该方法实现了最先进的性能，可以用于移动设备的AR物体插入应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a learning-based approach to relight a single image of Lambertian and low-frequency specular objects. Our method enables inserting objects from photographs into new scenes and relighting them under the new environment lighting, which is essential for AR applications. To relight the object, we solve both inverse rendering and re-rendering. To resolve the ill-posed inverse rendering, we propose a weakly-supervised method by a low-rank constraint. To facilitate the weakly-supervised training, we contribute Relit, a large-scale (750K images) dataset of videos with aligned objects under changing illuminations. For re-rendering, we propose a differentiable specular rendering layer to render low-frequency non-Lambertian materials under various illuminations of spherical harmonics. The whole pipeline is end-to-end and efficient, allowing for a mobile app implementation of AR object insertion. Extensive evaluations demonstrate that our method achieves state-of-the-art performance. Project page: https://renjiaoyi.github.io/relighting/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">496.H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction</span><br>
                <span class="as">Xu, HaoandWang, TianyuandTang, XiaoandFu, Chi-Wing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_H2ONet_Hand-Occlusion-and-Orientation-Aware_Network_for_Real-Time_3D_Hand_Mesh_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17048-17058.png><br>
            
            <span class="tt"><span class="t0">研究问题：实时3D手部网格重建，特别是在手部持有物体时的挑战。<br>
                    动机：以前的重建方法无法充分利用多帧非遮挡信息来提高重建质量。<br>
                    方法：设计了H2ONet模型，将手部网格重建分为两个分支，一个利用手指级别的非遮挡信息，另一个利用全局手部方向。同时提出手指级别和手级别遮挡感知特征融合策略，以获取跨时间帧的非遮挡信息。<br>
                    效果：在Dex-YCB和HO3D-v2数据集上进行的实验表明，H2ONet能够实时运行并在手部网格和姿态精度上都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-time 3D hand mesh reconstruction is challenging, especially when the hand is holding some object. Beyond the previous methods, we design H2ONet to fully exploit non-occluded information from multiple frames to boost the reconstruction quality. First, we decouple hand mesh reconstruction into two branches, one to exploit finger-level non-occluded information and the other to exploit global hand orientation, with lightweight structures to promote real-time inference. Second, we propose finger-level occlusion-aware feature fusion, leveraging predicted finger-level occlusion information as guidance to fuse finger-level information across time frames. Further, we design hand-level occlusion-aware feature fusion to fetch non-occluded information from nearby time frames. We conduct experiments on the Dex-YCB and HO3D-v2 datasets with challenging hand-object occlusion cases, manifesting that H2ONet is able to run in real-time and achieves state-of-the-art performance on both the hand mesh and pose precision. The code will be released on GitHub.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">497.Structured 3D Features for Reconstructing Controllable Avatars</span><br>
                <span class="as">Corona, EnricandZanfir, MihaiandAlldieck, ThiemoandBazavan, EduardGabrielandZanfir, AndreiandSminchisescu, Cristian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Corona_Structured_3D_Features_for_Reconstructing_Controllable_Avatars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16954-16964.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用新型隐式3D表示，将像素对齐的图像特征聚合到从参数化、统计人体网格表面采样的密集3D点上，以优化覆盖感兴趣的人，并生成可动画化的3D重建。<br>
                    动机：现有的模型仅能捕捉到身体形状，无法有效处理配饰、头发和松散的衣物等细节。因此，提出一种基于3D变换器的注意框架，能够从单一视角的无约束姿势图像生成带有反照率和照明分解的可动画化3D重建。<br>
                    方法：采用新型隐式3D表示，将像素对齐的图像特征聚合到从参数化、统计人体网格表面采样的密集3D点上，形成有语义的3D点，并在3D空间中自由移动。然后，通过一个端到端的模型进行训练，实现单目3D重建以及反照率和照明分解。<br>
                    效果：S3F模型在各种任务上都超越了先前最先进的技术，包括单目3D重建以及反照率和照明估计。此外，该方法还支持新的视角合成、重新照明和重新定位重建，并可以自然地扩展到处理多个输入图像（例如，同一人的多个视图或视频中的不同姿势）。最后，展示了该模型在3D虚拟试穿应用中的编辑能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Structured 3D Features, a model based on a novel implicit 3D representation that pools pixel-aligned image features onto dense 3D points sampled from a parametric, statistical human mesh surface. The 3D points have associated semantics and can move freely in 3D space. This allows for optimal coverage of the person of interest, beyond just the body shape, which in turn, additionally helps modeling accessories, hair, and loose clothing. Owing to this, we present a complete 3D transformer-based attention framework which, given a single image of a person in an unconstrained pose, generates an animatable 3D reconstruction with albedo and illumination decomposition, as a result of a single end-to-end model, trained semi-supervised, and with no additional postprocessing. We show that our S3F model surpasses the previous state-of-the-art on various tasks, including monocular 3D reconstruction, as well as albedo & shading estimation. Moreover, we show that the proposed methodology allows novel view synthesis, relighting, and re-posing the reconstruction, and can naturally be extended to handle multiple input images (e.g. different views of a person, or the same view, in different poses, in video). Finally, we demonstrate the editing capabilities of our model for 3D virtual try-on applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">498.In-Hand 3D Object Scanning From an RGB Sequence</span><br>
                <span class="as">Hampali, ShreyasandHodan, TomasandTran, LuanandMa, LingniandKeskin, CemandLepetit, Vincent</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hampali_In-Hand_3D_Object_Scanning_From_an_RGB_Sequence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17079-17088.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单目相机对未知物体进行手持3D扫描。<br>
                    动机：目前的大多数基于NeRF的方法都需要已知的相机-物体相对位姿，而我们的方法不需要这个假设。<br>
                    方法：我们提出了一种增量方法，首先将序列分割成精心选择的重叠段，然后在每个段内独立地重建物体形状和跟踪其位姿，最后在所有段合并后进行全局优化。<br>
                    效果：我们的方法能够重建有纹理和无纹理挑战物体的形状和颜色，优于只依赖外观特征的经典方法，且其性能接近于假设已知相机位姿的最新方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method for in-hand 3D scanning of an unknown object with a monocular camera. Our method relies on a neural implicit surface representation that captures both the geometry and the appearance of the object, however, by contrast with most NeRF-based methods, we do not assume that the camera-object relative poses are known. Instead, we simultaneously optimize both the object shape and the pose trajectory. As direct optimization over all shape and pose parameters is prone to fail without coarse-level initialization, we propose an incremental approach that starts by splitting the sequence into carefully selected overlapping segments within which the optimization is likely to succeed. We reconstruct the object shape and track its poses independently within each segment, then merge all the segments before performing a global optimization. We show that our method is able to reconstruct the shape and color of both textured and challenging texture-less objects, outperforms classical methods that rely only on appearance features, and that its performance is close to recent methods that assume known camera poses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">499.OmniVidar: Omnidirectional Depth Estimation From Multi-Fisheye Images</span><br>
                <span class="as">Xie, ShengandWang, DaochuanandLiu, Yun-Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_OmniVidar_Omnidirectional_Depth_Estimation_From_Multi-Fisheye_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21529-21538.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从四个大视场（FoV）摄像头中估计深度，这是一个困难且未被充分研究的问题。<br>
                    动机：现有的方法无法有效解决这个问题，因此需要提出新的解决方案。<br>
                    方法：我们提出了一种名为OmniVidar的新系统，该系统将复杂的多视点深度估计问题简化为更易处理的双目深度估计问题。OmniVidar包含三个部分：（1）一种新的相机模型，用于解决现有模型的缺点；（2）一种新的基于多鱼眼相机的极线纠正方法，用于解决图像畸变并简化深度估计问题；（3）一种改进的双目深度估计网络，实现了准确性和效率之间的更好平衡。<br>
                    效果：实验结果表明，OmniVidar在准确性和性能上都优于其他所有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Estimating depth from four large field of view (FoV) cameras has been a difficult and understudied problem. In this paper, we proposed a novel and simple system that can convert this difficult problem into easier binocular depth estimation. We name this system OmniVidar, as its results are similar to LiDAR, but rely only on vision. OmniVidar contains three components: (1) a new camera model to address the shortcomings of existing models, (2) a new multi-fisheye camera based epipolar rectification method for solving the image distortion and simplifying the depth estimation problem, (3) an improved binocular depth estimation network, which achieves a better balance between accuracy and efficiency. Unlike other omnidirectional stereo vision methods, OmniVidar does not contain any 3D convolution, so it can achieve higher resolution depth estimation at fast speed. Results demonstrate that OmniVidar outperforms all other methods in terms of accuracy and performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">500.Octree Guided Unoriented Surface Reconstruction</span><br>
                <span class="as">Koneputugodage, ChaminHewaandBen-Shabat, YizhakandGould, Stephen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Koneputugodage_Octree_Guided_Unoriented_Surface_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16717-16726.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决从无方向点云进行表面重建的问题。<br>
                    动机：目前的隐式神经表示（INRs）在这项任务中很受欢迎，但当形状内部和外部的信息不可用时（如形状占据、有符号的距离或表面法线方向），优化依赖于启发式方法和正则化器来恢复表面，这可能导致收敛缓慢并容易陷入局部最小值。<br>
                    方法：我们提出了一个两步法，OG-INR，首先构建一个离散的八叉树并标记内部和外部，然后使用初始由八叉树标签引导的INR对连续且高保真度的形状进行优化。为了解决我们的标记问题，我们在离散结构上提出了一个能量函数，并提供了一个高效的移动生成算法，该算法探索了许多可能的标记。此外，我们还展示了可以很容易地将知识注入到离散的八叉树中，从而简单地影响连续INR的结果。<br>
                    效果：我们在两个无方向表面重建数据集上评估了我们方法的有效性，并与其他无方向和一些有向的方法进行了比较，结果显示出良好的竞争力。我们的结果表明，通过移动生成算法的探索避免了纯梯度下降优化方法所达到的许多不良局部最小值（见图1）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of surface reconstruction from unoriented point clouds. Implicit neural representations (INRs) have become popular for this task, but when information relating to the inside versus outside of a shape is not available (such as shape occupancy, signed distances or surface normal orientation) optimization relies on heuristics and regularizers to recover the surface. These methods can be slow to converge and easily get stuck in local minima. We propose a two-step approach, OG-INR, where we (1) construct a discrete octree and label what is inside and outside (2) optimize for a continuous and high-fidelity shape using an INR that is initially guided by the octree's labelling. To solve for our labelling, we propose an energy function over the discrete structure and provide an efficient move-making algorithm that explores many possible labellings. Furthermore we show that we can easily inject knowledge into the discrete octree, providing a simple way to influence the result from the continuous INR. We evaluate the effectiveness of our approach on two unoriented surface reconstruction datasets and show competitive performance compared to other unoriented, and some oriented, methods. Our results show that the exploration by the move-making algorithm avoids many of the bad local minima reached by purely gradient descent optimized methods (see Figure 1).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">501.Rigidity-Aware Detection for 6D Object Pose Estimation</span><br>
                <span class="as">Hai, YangandSong, RuiandLi, JiaojiaoandSalzmann, MathieuandHu, Yinlin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hai_Rigidity-Aware_Detection_for_6D_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8927-8936.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的6D物体姿态估计方法在实际应用中，由于初始的2D边界框定位不准确，导致后续的姿态网络训练效果不佳。<br>
                    动机：为了解决这一问题，本文提出了一种刚体感知检测方法，利用6D姿态估计中目标物体是刚体的特性，从整个可见物体区域采样正样本，而非简单地从可能被遮挡的边界框中心采样。<br>
                    方法：通过构建一个可见性图，该图使用边界框内每个像素到边界的最小距离，使得每个可见的物体部分都能对最终的边界框预测做出贡献，从而提高检测鲁棒性。<br>
                    效果：在七个具有挑战性的6D姿态估计数据集上进行实验，结果表明该方法比通用检测框架有大幅度的提升。结合姿态回归网络，本文的方法在具有挑战性的BOP基准测试上取得了最先进的姿态估计结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most recent 6D object pose estimation methods first use object detection to obtain 2D bounding boxes before actually regressing the pose. However, the general object detection methods they use are ill-suited to handle cluttered scenes, thus producing poor initialization to the subsequent pose network. To address this, we propose a rigidity-aware detection method exploiting the fact that, in 6D pose estimation, the target objects are rigid. This lets us introduce an approach to sampling positive object regions from the entire visible object area during training, instead of naively drawing samples from the bounding box center where the object might be occluded. As such, every visible object part can contribute to the final bounding box prediction, yielding better detection robustness. Key to the success of our approach is a visibility map, which we propose to build using a minimum barrier distance between every pixel in the bounding box and the box boundary. Our results on seven challenging 6D pose estimation datasets evidence that our method outperforms general detection frameworks by a large margin. Furthermore, combined with a pose regression network, we obtain state-of-the-art pose estimation results on the challenging BOP benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">502.DP-NeRF: Deblurred Neural Radiance Field With Physical Scene Priors</span><br>
                <span class="as">Lee, DogyoonandLee, MinhyeokandShin, ChajinandLee, Sangyoun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12386-12396.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的NeRF模型在处理模糊图像时，没有考虑到三维空间中的几何和外观一致性，导致重建场景的感知质量下降。<br>
                    动机：为了解决这一问题，本文提出了一种名为DP-NeRF的新型清晰NeRF框架，用于处理模糊图像。<br>
                    方法：DP-NeRF采用了两个物理先验来约束模型，这两个先验是从相机在图像采集过程中的实际模糊过程推导出来的。具体来说，DP-NeRF提出了刚性模糊核来利用物理先验实现3D一致性，并采用自适应权重提议来考虑深度和模糊之间的关系，从而优化颜色合成误差。<br>
                    效果：实验结果表明，DP-NeRF成功地提高了重建NeRF的感知质量，确保了3D几何和外观一致性。通过全面的消融分析，进一步证明了该模型的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Field (NeRF) has exhibited outstanding three-dimensional (3D) reconstruction quality via the novel view synthesis from multi-view images and paired calibrated camera parameters. However, previous NeRF-based systems have been demonstrated under strictly controlled settings, with little attention paid to less ideal scenarios, including with the presence of noise such as exposure, illumination changes, and blur. In particular, though blur frequently occurs in real situations, NeRF that can handle blurred images has received little attention. The few studies that have investigated NeRF for blurred images have not considered geometric and appearance consistency in 3D space, which is one of the most important factors in 3D reconstruction. This leads to inconsistency and the degradation of the perceptual quality of the constructed scene. Hence, this paper proposes a DP-NeRF, a novel clean NeRF framework for blurred images, which is constrained with two physical priors. These priors are derived from the actual blurring process during image acquisition by the camera. DP-NeRF proposes rigid blurring kernel to impose 3D consistency utilizing the physical priors and adaptive weight proposal to refine the color composition error in consideration of the relationship between depth and blur. We present extensive experimental results for synthetic and real scenes with two types of blur: camera motion blur and defocus blur. The results demonstrate that DP-NeRF successfully improves the perceptual quality of the constructed NeRF ensuring 3D geometric and appearance consistency. We further demonstrate the effectiveness of our model with comprehensive ablation analysis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">503.MACARONS: Mapping and Coverage Anticipation With RGB Online Self-Supervision</span><br>
                <span class="as">Gu\&#x27;edon, AntoineandMonnier, TomandMonasse, PascalandLepetit, Vincent</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guedon_MACARONS_Mapping_and_Coverage_Anticipation_With_RGB_Online_Self-Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/940-951.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅通过彩色图像同时探索和重建大型环境，并解决下一步最佳视角问题。<br>
                    动机：目前的方法大多依赖深度传感器，需要3D监督且无法处理大规模场景。<br>
                    方法：提出一种自我监督的方法，仅使用彩色相机预测体积占用场，并从该场预测下一步最佳视角。<br>
                    效果：在各种3D场景的数据集上进行测试，表现优于依赖深度传感器的最新方法，适用于无人机拍摄的户外场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a method that simultaneously learns to explore new large environments and to reconstruct them in 3D from color images only. This is closely related to the Next Best View problem (NBV), where one has to identify where to move the camera next to improve the coverage of an unknown scene. However, most of the current NBV methods rely on depth sensors, need 3D supervision and/or do not scale to large scenes. Our method requires only a color camera and no 3D supervision. It simultaneously learns in a self-supervised fashion to predict a volume occupancy field from color images and, from this field, to predict the NBV. Thanks to this approach, our method performs well on new scenes as it is not biased towards any training 3D data. We demonstrate this on a recent dataset made of various 3D scenes and show it performs even better than recent methods requiring a depth sensor, which is not a realistic assumption for outdoor scenes captured with a flying drone.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">504.REC-MV: REconstructing 3D Dynamic Cloth From Monocular Videos</span><br>
                <span class="as">Qiu, LingtengandChen, GuanyingandZhou, JiapengandXu, MutianandWang, JunleandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_REC-MV_REconstructing_3D_Dynamic_Cloth_From_Monocular_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4637-4646.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单目视频中重建具有开放边界的动态3D服装表面？<br>
                    动机：现有的神经渲染方法无法将服装表面与身体分离，基于特征曲线表示的服装重建方法在视频输入上难以生成时间一致的表面。<br>
                    方法：本文将此任务表述为3D服装特征曲线和表面重建的优化问题，提出了一种名为REC-MV的新方法，用于联合优化显式特征曲线和服装的隐式符号距离场（SDF）。然后通过在规范空间中的服装模板注册提取开放的服装网格。<br>
                    效果：实验表明，该方法优于现有方法，可以生成高质量的动态服装表面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstructing dynamic 3D garment surfaces with open boundaries from monocular videos is an important problem as it provides a practical and low-cost solution for clothes digitization. Recent neural rendering methods achieve high-quality dynamic clothed human reconstruction results from monocular video, but these methods cannot separate the garment surface from the body. Moreover, despite existing garment reconstruction methods based on feature curve representation demonstrating impressive results for garment reconstruction from a single image, they struggle to generate temporally consistent surfaces for the video input. To address the above limitations, in this paper, we formulate this task as an optimization problem of 3D garment feature curves and surface reconstruction from monocular video. We introduce a novel approach, called REC-MV to jointly optimize the explicit feature curves and the implicit signed distance field (SDF) of the garments. Then the open garment meshes can be extracted via garment template registration in the canonical space. Experiments on multiple casually captured datasets show that our approach outperforms existing methods and can produce high-quality dynamic garment surfaces.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">505.RUST: Latent Neural Scene Representations From Unposed Imagery</span><br>
                <span class="as">Sajjadi, MehdiS.M.andMahendran, AravindhandKipf, ThomasandPot, EtienneandDuckworth, DanielandLu\v{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sajjadi_RUST_Latent_Neural_Scene_Representations_From_Unposed_Imagery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17297-17306.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从2D观察中推断3D场景的结构是计算机视觉中的一个基本挑战。<br>
                    动机：目前流行的基于神经场景表示的方法已经在各种应用中取得了巨大的影响，但训练一个能提供有效泛化到单个场景之外的潜表示的单一模型仍然是这个领域的主要挑战之一。<br>
                    方法：我们提出了RUST（真正无姿态的场景表示转换器），这是一种仅使用RGB图像进行训练的无姿态新颖视图合成方法。我们的主要见解是，可以训练一个窥视目标图像并学习用于视图合成的潜在姿态嵌入的姿态编码器。<br>
                    效果：我们对学习到的潜在姿态结构进行了实证研究，结果表明，它允许有意义的测试时间相机变换和准确的显式姿态读出。令人惊讶的是，RUST实现了与具有完美相机姿态的方法相当的质量，从而解锁了大规模训练共享神经场景表示的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation into the learned latent pose structure and show that it allows meaningful test-time camera transformations and accurate explicit pose readouts. Perhaps surprisingly, RUST achieves similar quality as methods which have access to perfect camera pose, thereby unlocking the potential for large-scale training of amortized neural scene representations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">506.Spatio-Focal Bidirectional Disparity Estimation From a Dual-Pixel Image</span><br>
                <span class="as">Kim, DonggunandJang, HyeonjoongandKim, InchulandKim, MinH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Spatio-Focal_Bidirectional_Disparity_Estimation_From_a_Dual-Pixel_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5023-5032.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何充分利用双倍像素摄影的超高清分辨率，并解决其深度估计性能下降的问题。<br>
                    动机：双倍像素摄影具有方向性视差，需要浅景深才能捕获广泛的双倍像素视差，但这会导致图像严重模糊，降低深度估计性能。<br>
                    方法：提出一种自我监督学习方法，通过利用双倍像素摄影中的各向异性模糊核的性质来学习双向视差。<br>
                    效果：该方法不需要依赖不存在的双倍像素视差训练数据集，能从双倍像素图像中估计出完整的视差图，超越基线双倍像素方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dual-pixel photography is monocular RGB-D photography with an ultra-high resolution, enabling many applications in computational photography. However, there are still several challenges to fully utilizing dual-pixel photography. Unlike the conventional stereo pair, the dual pixel exhibits a bidirectional disparity that includes positive and negative values, depending on the focus plane depth in an image. Furthermore, capturing a wide range of dual-pixel disparity requires a shallow depth of field, resulting in a severely blurred image, degrading depth estimation performance. Recently, several data-driven approaches have been proposed to mitigate these two challenges. However, due to the lack of the ground-truth dataset of the dual-pixel disparity, existing data-driven methods estimate either inverse depth or blurriness map. In this work, we propose a self-supervised learning method that learns bidirectional disparity by utilizing the nature of anisotropic blur kernels in dual-pixel photography. We observe that the dual-pixel left/right images have reflective-symmetric anisotropic kernels, so their sum is equivalent to that of a conventional image. We take a self-supervised training approach with the novel kernel-split symmetry loss accounting for the phenomenon. Our method does not rely on a training dataset of dual-pixel disparity that does not exist yet. Our method can estimate a complete disparity map with respect to the focus-plane depth from a dual-pixel image, outperforming the baseline dual-pixel methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">507.Four-View Geometry With Unknown Radial Distortion</span><br>
                <span class="as">Hruby, PetrandKorotynskiy, ViktorandDuff, TimothyandOeding, LukeandPollefeys, MarcandPajdla, TomasandLarsson, Viktor</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hruby_Four-View_Geometry_With_Unknown_Radial_Distortion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8990-9000.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文解决了在相机标定参数（焦距和径向畸变）未知的情况下，从图像中估计相对位姿的问题。<br>
                    动机：现有的方法需要模型化这些参数才能进行度量重建，而我们的方法不需要这样做。<br>
                    方法：我们提出了一种新的解决方案，通过将已知和未知的相机都视为4视图中的13个点，将问题分解为一系列子问题进行求解。<br>
                    效果：实验结果表明，我们的方法在模拟数据和真实数据上都优于以往的无标定解决方案，可以有效地启动带有径向相机的SfM管道。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present novel solutions to previously unsolved problems of relative pose estimation from images whose calibration parameters, namely focal lengths and radial distortion, are unknown. Our approach enables metric reconstruction without modeling these parameters. The minimal case for reconstruction requires 13 points in 4 views for both the calibrated and uncalibrated cameras. We describe and implement the first solution to these minimal problems. In the calibrated case, this may be modeled as a polynomial system of equations with 3584 solutions. Despite the apparent intractability, the problem decomposes spectacularly. Each solution falls into a Euclidean symmetry class of size 16, and we can estimate 224 class representatives by solving a sequence of three subproblems with 28, 2, and 4 solutions. We highlight the relationship between internal constraints on the radial quadrifocal tensor and the relations among the principal minors of a 4x4 matrix. We also address the case of 4 upright cameras, where 7 points are minimal. Finally, we evaluate our approach on simulated and real data and benchmark against previous calibration-free solutions, and show that our method provides an efficient startup for an SfM pipeline with radial cameras.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">508.HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics</span><br>
                <span class="as">Grigorev, ArturandBlack, MichaelJ.andHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16965-16974.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种利用图神经网络、多级消息传递和无监督训练实时预测真实服装动态的方法。<br>
                    动机：现有的基于线性混合蒙皮的方法必须针对特定服装进行训练，而我们的方法对体型不敏感，适用于紧身服装和宽松的流动服装。<br>
                    方法：我们的方法进一步处理拓扑变化（如带有纽扣或拉链的服装）和材料属性在推理时的变化。我们提出了一种分层的消息传递方案，有效地传播了刚性拉伸模式，同时保留了局部细节。<br>
                    效果：实验结果表明，我们的方法在数量上优于强大的基线，并且其结果被认为比最先进的方法更真实。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method that leverages graph neural networks, multi-level message passing, and unsupervised training to enable real-time prediction of realistic clothing dynamics. Whereas existing methods based on linear blend skinning must be trained for specific garments, our method is agnostic to body shape and applies to tight-fitting garments as well as loose, free-flowing clothing. Our method furthermore handles changes in topology (e.g., garments with buttons or zippers) and material properties at inference time. As one key contribution, we propose a hierarchical message-passing scheme that efficiently propagates stiff stretching modes while preserving local detail. We empirically show that our method outperforms strong baselines quantitatively and that its results are perceived as more realistic than state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">509.HyperReel: High-Fidelity 6-DoF Video With Ray-Conditioned Sampling</span><br>
                <span class="as">Attal, BenjaminandHuang, Jia-BinandRichardt, ChristianandZollh\&quot;ofer, MichaelandKopf, JohannesandO{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16610-16620.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的体积场景表示方法在质量、渲染速度和内存效率之间需要做出精细的权衡，特别是在实时性能、小内存占用和高质量渲染方面。<br>
                    动机：为了解决现有方法在处理具有挑战性的现实世界场景时无法同时实现实时性能、小内存占用和高质量渲染的问题。<br>
                    方法：提出了一种新的6-DoF视频表示方法——HyperReel，包括两个核心组件：（1）一种射线条件样本预测网络，能够在高分辨率下实现高保真、高帧率的渲染；（2）一种紧凑且内存效率高的动态体积表示。<br>
                    效果：与先前和同时代的其他方法相比，HyperReel在视觉质量方面表现最佳，同时内存需求较小，并且在没有自定义CUDA代码的情况下，以每秒18帧的速率在百万像素分辨率下进行渲染。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Volumetric scene representations enable photorealistic view synthesis for static scenes and form the basis of several existing 6-DoF video techniques. However, the volume rendering procedures that drive these representations necessitate careful trade-offs in terms of quality, rendering speed, and memory efficiency. In particular, existing methods fail to simultaneously achieve real-time performance, small memory footprint, and high-quality rendering for challenging real-world scenes. To address these issues, we present HyperReel --- a novel 6-DoF video representation. The two core components of HyperReel are: (1) a ray-conditioned sample prediction network that enables high-fidelity, high frame rate rendering at high resolutions and (2) a compact and memory-efficient dynamic volume representation. Our 6-DoF video pipeline achieves the best performance compared to prior and contemporary approaches in terms of visual quality with small memory requirements, while also rendering at up to 18 frames-per-second at megapixel resolution without any custom CUDA code.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">510.Pose Synchronization Under Multiple Pair-Wise Relative Poses</span><br>
                <span class="as">Sun, YifanandHuang, Qixing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Pose_Synchronization_Under_Multiple_Pair-Wise_Relative_Poses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13072-13081.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在多个对象对之间存在大量错误相对位姿估计的情况下，如何进行位姿同步的问题。<br>
                    动机：现有的通过恢复编码相对位姿的低秩矩阵来解决位姿同步的方法在存在大量错误相对位姿估计的情况下无法有效工作。<br>
                    方法：提出了一种三步算法来进行多相对位姿输入下的位姿同步。第一步进行扩散和聚类以计算输入对象的候选位姿；第二步联合优化每个对象的最佳位姿；第三步细化第二步的输出结果。<br>
                    效果：在结构从运动和基于扫描的几何重建基准数据集上的实验结果表明，该方法比最先进的位姿同步技术提供了更准确的绝对位姿。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pose synchronization, which seeks to estimate consistent absolute poses among a collection of objects from noisy relative poses estimated between pairs of objects in isolation, is a fundamental problem in many inverse applications. This paper studies an extreme setting where multiple relative pose estimates exist between each object pair, and the majority is incorrect. Popular methods that solve pose synchronization via recovering a low-rank matrix that encodes relative poses in block fail under this extreme setting. We introduce a three-step algorithm for pose synchronization under multiple relative pose inputs. The first step performs diffusion and clustering to compute the candidate poses of the input objects. We present a theoretical result to justify our diffusion formulation. The second step jointly optimizes the best pose for each object. The final step refines the output of the second step. Experimental results on benchmark datasets of structurefrom-motion and scan-based geometry reconstruction show that our approach offers more accurate absolute poses than state-of-the-art pose synchronization techniques.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">511.Virtual Occlusions Through Implicit Depth</span><br>
                <span class="as">Watson, JamieandSayed, MohamedandQureshi, ZawarandBrostow, GabrielJ.andVicente, SaraandMacAodha, OisinandFirman, Michael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Watson_Virtual_Occlusions_Through_Implicit_Depth_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9053-9064.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高虚拟现实元素在真实世界中的遮挡效果，使其看起来更自然。<br>
                    动机：目前的深度估计模型在边界或时间变化时会出现不一致性，影响虚拟现实元素的遮挡效果。<br>
                    方法：提出一种隐式深度模型，直接预测遮挡掩码，输入为一张或多张彩色图像和虚拟几何体的已知深度。<br>
                    效果：实验结果表明，该方法比传统深度估计模型的预测更准确、更稳定，在ScanNetv2数据集上取得了最先进的遮挡效果，并在真实场景上获得了优越的定性结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For augmented reality (AR), it is important that virtual assets appear to 'sit among' real world objects. The virtual element should variously occlude and be occluded by real matter, based on a plausible depth ordering. This occlusion should be consistent over time as the viewer's camera moves. Unfortunately, small mistakes in the estimated scene depth can ruin the downstream occlusion mask, and thereby the AR illusion. Especially in real-time settings, depths inferred near boundaries or across time can be inconsistent. In this paper, we challenge the need for depth-regression as an intermediate step. We instead propose an implicit model for depth and use that to predict the occlusion mask directly. The inputs to our network are one or more color images, plus the known depths of any virtual geometry. We show how our occlusion predictions are more accurate and more temporally stable than predictions derived from traditional depth-estimation models. We obtain state-of-the-art occlusion results on the challenging ScanNetv2 dataset and superior qualitative results on real scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">512.Instant Multi-View Head Capture Through Learnable Registration</span><br>
                <span class="as">Bolkart, TimoandLi, TianyeandBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bolkart_Instant_Multi-View_Head_Capture_Through_Learnable_Registration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/768-779.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接从校准的多视图图像中推断密集对应关系的3D头部。<br>
                    动机：现有的方法在捕获3D头部数据集时速度慢，且通常分两步进行；多视图立体重建（MVS）重建后进行非刚性配准。<br>
                    方法：提出TEMPEH模型，通过直接从校准的多视图图像中推断密集对应关系的3D头部，同时注册3D头部数据集。<br>
                    效果：预测一个头部大约需要0.3秒，中位重建误差为0.26毫米，比当前最先进的方法低64%，能够有效地捕获包含多个人和不同面部运动的大数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods for capturing datasets of 3D heads in dense semantic correspondence are slow and commonly address the problem in two separate steps; multi-view stereo (MVS) reconstruction followed by non-rigid registration. To simplify this process, we introduce TEMPEH (Towards Estimation of 3D Meshes from Performances of Expressive Heads) to directly infer 3D heads in dense correspondence from calibrated multi-view images. Registering datasets of 3D scans typically requires manual parameter tuning to find the right balance between accurately fitting the scans' surfaces and being robust to scanning noise and outliers. Instead, we propose to jointly register a 3D head dataset while training TEMPEH. Specifically, during training, we minimize a geometric loss commonly used for surface registration, effectively leveraging TEMPEH as a regularizer. Our multi-view head inference builds on a volumetric feature representation that samples and fuses features from each view using camera calibration information. To account for partial occlusions and a large capture volume that enables head movements, we use view- and surface-aware feature fusion, and a spatial transformer-based head localization module, respectively. We use raw MVS scans as supervision during training, but, once trained, TEMPEH directly predicts 3D heads in dense correspondence without requiring scans. Predicting one head takes about 0.3 seconds with a median reconstruction error of 0.26 mm, 64% lower than the current state-of-the-art. This enables the efficient capture of large datasets containing multiple people and diverse facial motions. Code, model, and data are publicly available at https://tempeh.is.tue.mpg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">513.NeAT: Learning Neural Implicit Surfaces With Arbitrary Topologies From Multi-View Images</span><br>
                <span class="as">Meng, XiaoxuandChen, WeikaiandYang, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_NeAT_Learning_Neural_Implicit_Surfaces_With_Arbitrary_Topologies_From_Multi-View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/248-258.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有神经隐式函数在重建高保真3D形状时只能处理封闭表面的问题。<br>
                    动机：目前的神经渲染方法受限于需要用符号距离场表示的表面，因此只能处理封闭的3D形状。<br>
                    方法：本文提出了一种新的神经渲染框架NeAT，该框架可以从多视图图像中学习具有任意拓扑的隐式表面。具体来说，NeAT将3D表面表示为带有有效性分支的符号距离函数（SDF）的水平集，用于估计查询位置的表面存在概率。<br>
                    效果：实验结果表明，NeAT不仅能够忠实地重建无缝隙的表面，而且在开放表面重建任务上显著优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent progress in neural implicit functions has set new state-of-the-art in reconstructing high-fidelity 3D shapes from a collection of images. However, these approaches are limited to closed surfaces as they require the surface to be represented by a signed distance field. In this paper, we propose NeAT, a new neural rendering framework that can learn implicit surfaces with arbitrary topologies from multi-view images. In particular, NeAT represents the 3D surface as a level set of a signed distance function (SDF) with a validity branch for estimating the surface existence probability at the query positions. We also develop a novel neural volume rendering method, which uses SDF and validity to calculate the volume opacity and avoids rendering points with low validity. NeAT supports easy field-to-mesh conversion using the classic Marching Cubes algorithm. Extensive experiments on DTU, MGN, and Deep Fashion 3D datasets indicate that our approach is able to faithfully reconstruct both watertight and non-watertight surfaces. In particular, NeAT significantly outperforms the state-of-the-art methods in the task of open surface reconstruction both quantitatively and qualitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">514.SPARF: Neural Radiance Fields From Sparse and Noisy Poses</span><br>
                <span class="as">Truong, PruneandRakotosaona, Marie-JulieandManhardt, FabianandTombari, Federico</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4190-4200.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用稀疏输入视图进行新颖的视图合成。<br>
                    动机：现有的神经辐射场（NeRF）模型需要密集的输入视图和高精度的相机姿态，限制了其在现实世界中的应用。<br>
                    方法：提出稀疏姿态调整辐射场（SPARF）方法，通过利用多视图几何约束来联合学习NeRF并优化相机姿态。<br>
                    效果：在多个具有挑战性的数据集上，该方法在稀疏视图领域取得了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Field (NeRF) has recently emerged as a powerful representation to synthesize photorealistic novel views. While showing impressive performance, it relies on the availability of dense input views with highly accurate camera poses, thus limiting its application in real-world scenarios. In this work, we introduce Sparse Pose Adjusting Radiance Field (SPARF), to address the challenge of novel-view synthesis given only few wide-baseline input images (as low as 3) with noisy camera poses. Our approach exploits multi-view geometry constraints in order to jointly learn the NeRF and refine the camera poses. By relying on pixel matches extracted between the input views, our multi-view correspondence objective enforces the optimized scene and camera poses to converge to a global and geometrically accurate solution. Our depth consistency loss further encourages the reconstructed scene to be consistent from any viewpoint. Our approach sets a new state of the art in the sparse-view regime on multiple challenging datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">515.ABLE-NeRF: Attention-Based Rendering With Learnable Embeddings for Neural Radiance Field</span><br>
                <span class="as">Tang, ZheJunandCham, Tat-JenandZhao, Haiyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_ABLE-NeRF_Attention-Based_Rendering_With_Learnable_Embeddings_for_Neural_Radiance_Field_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16559-16568.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的神经辐射场（NeRF）方法在渲染3D场景时，存在光泽和透明物体表现不佳的问题。<br>
                    动机：为了解决这一问题，本文提出了一种基于自注意力机制的体积光线框架，并借鉴游戏引擎中的光照探针技术，引入可学习嵌入来捕捉场景内的视点相关效应。<br>
                    方法：本文提出的ABLE-NeRF方法通过优化连续体积场景函数来表示3D场景，同时利用自注意力机制对体积进行约束，以及引入可学习嵌入来捕捉视点相关效应。<br>
                    效果：实验结果表明，ABLE-NeRF在Blender数据集上取得了最先进的结果，并在所有三个图像质量指标（PSNR、SSIM、LPIPS）上都超过了Ref-NeRF，显著减少了渲染中“模糊”的光泽表面，并产生了真实感强的透明表面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Field (NeRF) is a popular method in representing 3D scenes by optimising a continuous volumetric scene function. Its large success which lies in applying volumetric rendering (VR) is also its Achilles' heel in producing view-dependent effects. As a consequence, glossy and transparent surfaces often appear murky. A remedy to reduce these artefacts is to constrain this VR equation by excluding volumes with back-facing normal. While this approach has some success in rendering glossy surfaces, translucent objects are still poorly represented. In this paper, we present an alternative to the physics-based VR approach by introducing a self-attention-based framework on volumes along a ray. In addition, inspired by modern game engines which utilise Light Probes to store local lighting passing through the scene, we incorporate Learnable Embeddings to capture view dependent effects within the scene. Our method, which we call ABLE-NeRF, significantly reduces 'blurry' glossy surfaces in rendering and produces realistic translucent surfaces which lack in prior art. In the Blender dataset, ABLE-NeRF achieves SOTA results and surpasses Ref-NeRF in all 3 image quality metrics PSNR, SSIM, LPIPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">516.PermutoSDF: Fast Multi-View Reconstruction With Implicit Surfaces Using Permutohedral Lattices</span><br>
                <span class="as">Rosu, RaduAlexandruandBehnke, Sven</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rosu_PermutoSDF_Fast_Multi-View_Reconstruction_With_Implicit_Surfaces_Using_Permutohedral_Lattices_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8466-8475.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过混合神经网络辐射密度场方法和基于位置编码的哈希方法，提高新视角渲染的准确性和效率。<br>
                    动机：当前的方法在恢复表面几何形状时存在困难，且训练和推断速度较慢。<br>
                    方法：提出一种新的基于哈希的隐式表面表示方法，使用排列八面体格对体积哈希编码进行优化，并引入一种用于恢复高频几何细节的正则化方案。<br>
                    效果：实验结果表明，该方法能在保持高帧率的同时，准确恢复毛孔和皱纹等微观几何细节，并在多个数据集上进行了有效评估。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance-density field methods have become increasingly popular for the task of novel-view rendering. Their recent extension to hash-based positional encoding ensures fast training and inference with visually pleasing results. However, density-based methods struggle with recovering accurate surface geometry. Hybrid methods alleviate this issue by optimizing the density based on an underlying SDF. However, current SDF methods are overly smooth and miss fine geometric details. In this work, we combine the strengths of these two lines of work in a novel hash-based implicit surface representation. We propose improvements to the two areas by replacing the voxel hash encoding with a permutohedral lattice which optimizes faster, especially for higher dimensions. We additionally propose a regularization scheme which is crucial for recovering high-frequency geometric detail. We evaluate our method on multiple datasets and show that we can recover geometric detail at the level of pores and wrinkles while using only RGB images for supervision. Furthermore, using sphere tracing we can render novel views at 30 fps on an RTX 3090. Code is publicly available at https://radualexandru.github.io/permuto_sdf</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">517.1000 FPS HDR Video With a Spike-RGB Hybrid Camera</span><br>
                <span class="as">Chang, YakunandZhou, ChuandHong, YuchenandHu, LiwenandXu, ChaoandHuang, TiejunandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_1000_FPS_HDR_Video_With_a_Spike-RGB_Hybrid_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22180-22190.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用传统的帧基相机在高速场景中捕捉高帧率和高动态范围（HFR&HDR）彩色视频。<br>
                    动机：使用传统帧基相机在高速场景中捕捉高帧率和高动态范围的彩色视频是非常具有挑战性的，因为提高帧率通常需要缩短曝光时间，从而使得捕获的视频受到噪声的严重干扰。<br>
                    方法：我们引入了一个由脉冲相机和交替曝光RGB相机组成的混合相机系统，以高保真度捕捉HFR&HDR场景。首先，我们将脉冲帧进行重构以获取运动信息，然后基于这些脉冲帧指导恢复中长曝光RGB图像的丢失时间信息，同时保留其可靠的颜色外观。最后，利用从脉冲序列中估计出的强时间约束，恢复了丢失和失真的颜色交叉RGB帧，生成了时间一致的高帧率彩色帧。<br>
                    效果：我们收集了一个新的Spike-RGB数据集，其中包含300个合成数据序列和20组真实世界数据，实验结果表明，我们的方法能够产生超过1000FPS的HDR视频，性能超过了HDR视频重建方法和商业高速相机。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Capturing high frame rate and high dynamic range (HFR&HDR) color videos in high-speed scenes with conventional frame-based cameras is very challenging. The increasing frame rate is usually guaranteed by using shorter exposure time so that the captured video is severely interfered by noise. Alternating exposures could alleviate the noise issue but sacrifice frame rate due to involving long-exposure frames. The neuromorphic spiking camera records high-speed scenes of high dynamic range without colors using a completely different sensing mechanism and visual representation. We introduce a hybrid camera system composed of a spiking and an alternating-exposure RGB camera to capture HFR&HDR scenes with high fidelity. Our insight is to bring each camera's superiority into full play. The spike frames, with accurate fast motion information encoded, are first reconstructed for motion representation, from which the spike-based optical flows guide the recovery of missing temporal information for middle- and long-exposure RGB images while retaining their reliable color appearances. With the strong temporal constraint estimated from spike trains, both missing and distorted colors cross RGB frames are recovered to generate time-consistent and HFR color frames. We collect a new Spike-RGB dataset that contains 300 sequences of synthetic data and 20 groups of real-world data to demonstrate 1000 FPS HDR videos outperforming HDR video reconstruction methods and commercial high-speed cameras.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">518.Learning To Fuse Monocular and Multi-View Cues for Multi-Frame Depth Estimation in Dynamic Scenes</span><br>
                <span class="as">Li, RuiandGong, DongandYin, WeiandChen, HaoandZhu, YuandWang, KaixuanandChen, XiaozhiandSun, JinqiuandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_To_Fuse_Monocular_and_Multi-View_Cues_for_Multi-Frame_Depth_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21539-21548.png><br>
            
            <span class="tt"><span class="t0">研究问题：动态场景下的多帧深度估计通常依赖于多视图几何一致性，但在动态区域中，这种一致性通常会被破坏，导致估计结果失真。<br>
                    动机：许多多帧方法通过显式掩码识别动态区域，并用局部单眼深度或特征作为单眼线索来补偿多视图线索，但这种方法的效果有限，因为掩码的质量无法控制，而且两种类型的线索的融合效益没有得到充分利用。<br>
                    方法：本文提出了一种新的方法，无需手动创建掩码，就可以学习融合编码为体积的多视图和单眼线索。通过分析发现，多视图线索在静态区域中能捕获更准确的几何信息，而单眼线索在动态区域中能捕获更有用的上下文信息。为了将静态区域中从多视图线索学习的几何感知传播到动态区域的单眼表示，并让单眼线索增强多视图成本体积的表示，我们提出了跨线索融合（CCF）模块，其中包括跨线索注意力（CCA）以编码空间上非局部的相对内部关系，从而增强另一种表示。<br>
                    效果：在真实世界数据集上的实验证明了该方法的显著有效性和泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-frame depth estimation generally achieves high accuracy relying on the multi-view geometric consistency. When applied in dynamic scenes, e.g., autonomous driving, this consistency is usually violated in the dynamic areas, leading to corrupted estimations. Many multi-frame methods handle dynamic areas by identifying them with explicit masks and compensating the multi-view cues with monocular cues represented as local monocular depth or features. The improvements are limited due to the uncontrolled quality of the masks and the underutilized benefits of the fusion of the two types of cues. In this paper, we propose a novel method to learn to fuse the multi-view and monocular cues encoded as volumes without needing the heuristically crafted masks. As unveiled in our analyses, the multi-view cues capture more accurate geometric information in static areas, and the monocular cues capture more useful contexts in dynamic areas. To let the geometric perception learned from multi-view cues in static areas propagate to the monocular representation in dynamic areas and let monocular cues enhance the representation of multi-view cost volume, we propose a cross-cue fusion (CCF) module, which includes the cross-cue attention (CCA) to encode the spatially non-local relative intra-relations from each source to enhance the representation of the other. Experiments on real-world datasets prove the significant effectiveness and generalization ability of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">519.Neural Volumetric Memory for Visual Locomotion Control</span><br>
                <span class="as">Yang, RuihanandYang, GeandWang, Xiaolong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Neural_Volumetric_Memory_for_Visual_Locomotion_Control_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1430-1440.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使腿式机器人在具有挑战性的地形上自主移动。<br>
                    动机：由于部分可观察性问题，机器人必须依靠过去的观察来推断当前地形下的地形。<br>
                    方法：采用计算机视觉中显式建模场景3D几何的方法，提出神经体积记忆（NVM），一种显式考虑3D世界SE(3)等变的几何记忆架构。<br>
                    效果：通过在物理机器人上测试学习到的视觉-运动策略，表明了我们的方法——使用神经体积记忆学习腿式运动，在具有挑战性的地形上产生了性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Legged robots have the potential to expand the reach of autonomy beyond paved roads. In this work, we consider the difficult problem of locomotion on challenging terrains using a single forward-facing depth camera. Due to the partial observability of the problem, the robot has to rely on past observations to infer the terrain currently beneath it. To solve this problem, we follow the paradigm in computer vision that explicitly models the 3D geometry of the scene and propose Neural Volumetric Memory (NVM), a geometric memory architecture that explicitly accounts for the SE(3) equivariance of the 3D world. NVM aggregates feature volumes from multiple camera views by first bringing them back to the ego-centric frame of the robot. We test the learned visual-locomotion policy on a physical robot and show that our approach, learning legged locomotion with neural volumetric memory, produces performance gains over prior works on challenging terrains. We include ablation studies and show that the representations stored in the neural volumetric memory capture sufficient geometric information to reconstruct the scene. Our project page with videos is https://rchalyang.github.io/NVM/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">520.Propagate and Calibrate: Real-Time Passive Non-Line-of-Sight Tracking</span><br>
                <span class="as">Wang, YihaoandWang, ZhigangandZhao, BinandWang, DongandChen, MulinandLi, Xuelong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Propagate_and_Calibrate_Real-Time_Passive_Non-Line-of-Sight_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/972-981.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现非视线（NLOS）跟踪，即在对象不在视线范围内的情况下进行追踪。<br>
                    动机：现有的NLOS跟踪技术大多依赖主动照明，如激光，成本高且实验条件复杂，且由于设置过于简单化，距离实际应用还有一段距离。<br>
                    方法：提出一种纯被动的追踪方法，通过观察中继墙来追踪人在隐形房间中的行走。引入差异帧作为时间-局部运动信息的重要载体，并构建交替传播和校准的PAC-Net模型，以在帧级别粒度上利用动态和静态信息。<br>
                    效果：构建并发布了首个动态被动NLOS跟踪数据集NLOS-Track，填补了现实NLOS数据集的空白。该数据集包含数千个NLOS视频片段和相应的轨迹，包括实拍和合成数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent years, due to its ability to detect object motion out of sight. Most previous works on NLOS tracking rely on active illumination, e.g., laser, and suffer from high cost and elaborate experimental conditions. Besides, these techniques are still far from practical application due to oversimplified settings. In contrast, we propose a purely passive method to track a person walking in an invisible room by only observing a relay wall, which is more in line with real application scenarios, e.g., security. To excavate imperceptible changes in videos of the relay wall, we introduce difference frames as an essential carrier of temporal-local motion messages. In addition, we propose PAC-Net, which consists of alternating propagation and calibration, making it capable of leveraging both dynamic and static messages on a frame-level granularity. To evaluate the proposed method, we build and publish the first dynamic passive NLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS datasets. NLOS-Track contains thousands of NLOS video clips and corresponding trajectories. Both real-shot and synthetic data are included. Our codes and dataset are available at https://againstentropy.github.io/NLOS-Track/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">521.Neural Fields Meet Explicit Geometric Representations for Inverse Rendering of Urban Scenes</span><br>
                <span class="as">Wang, ZianandShen, TianchangandGao, JunandHuang, ShengyuandMunkberg, JacobandHasselgren, JonandGojcic, ZanandChen, WenzhengandFidler, Sanja</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Fields_Meet_Explicit_Geometric_Representations_for_Inverse_Rendering_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8370-8380.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从捕获的图像中重建和内在分解场景，以实现诸如重照明和虚拟对象插入等许多应用。<br>
                    动机：现有的基于NeRF的方法在3D重建的准确性上取得了令人印象深刻的成果，但将光照和阴影烘焙到辐射场中，而通过可微分渲染促进内在分解的基于网格的方法尚未扩展到户外场景的复杂性和规模。<br>
                    方法：我们提出了一种新的逆渲染框架，能够从一组带有可选深度的已定位RGB图像中联合重建场景几何、空间变化的材质和HDR光照。具体来说，我们使用一个神经场来表示主光线，并使用一个显式网格（从底层神经场重建）来模拟产生高阶光照效应（如投射阴影）的次级光线。<br>
                    效果：通过忠实地将复杂的几何和材质与光照效应分离，我们的方法能够在多个户外数据集上实现具有镜面和阴影效果的照片般真实的重照明。此外，它还支持物理基础的场景操作，如带有射线追踪阴影投射的虚拟物体插入。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reconstruction and intrinsic decomposition of scenes from captured imagery would enable many applications such as relighting and virtual object insertion. Recent NeRF based methods achieve impressive fidelity of 3D reconstruction, but bake the lighting and shadows into the radiance field, while mesh-based methods that facilitate intrinsic decomposition through differentiable rendering have not yet scaled to the complexity and scale of outdoor scenes. We present a novel inverse rendering framework for large urban scenes capable of jointly reconstructing the scene geometry, spatially-varying materials, and HDR lighting from a set of posed RGB images with optional depth. Specifically, we use a neural field to account for the primary rays, and use an explicit mesh (reconstructed from the underlying neural field) for modeling secondary rays that produce higher-order lighting effects such as cast shadows. By faithfully disentangling complex geometry and materials from lighting effects, our method enables photorealistic relighting with specular and shadow effects on several outdoor datasets. Moreover, it supports physics-based scene manipulations such as virtual object insertion with ray-traced shadow casting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">522.NeRF-RPN: A General Framework for Object Detection in NeRFs</span><br>
                <span class="as">Hu, BenranandHuang, JunkaiandLiu, YichenandTai, Yu-WingandTang, Chi-Keung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23528-23538.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了首个直接在NeRF上操作的目标检测框架，即NeRF-RPN。<br>
                    动机：现有的目标检测框架无法直接在NeRF上进行操作，因此作者提出了一种新方法来解决这个问题。<br>
                    方法：通过利用一种新的体素表示法，该方法结合了多尺度的3D神经体积特征，可以在不渲染任何视角的NeRF的情况下直接回归物体的3D边界框。<br>
                    效果：实验结果表明，NeRF-RPN可以有效地在NeRF上进行目标检测，并且可以应用于无类别标签的对象检测。此外，作者还建立了一个新的基准数据集，以促进未来在NeRF上进行目标检测的研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents the first significant object detection framework, NeRF-RPN, which directly operates on NeRF. Given a pre-trained NeRF model, NeRF-RPN aims to detect all bounding boxes of objects in a scene. By exploiting a novel voxel representation that incorporates multi-scale 3D neural volumetric features, we demonstrate it is possible to regress the 3D bounding boxes of objects in NeRF directly without rendering the NeRF at any viewpoint. NeRF-RPN is a general framework and can be applied to detect objects without class labels. We experimented NeRF-RPN with various backbone architectures, RPN head designs, and loss functions. All of them can be trained in an end-to-end manner to estimate high quality 3D bounding boxes. To facilitate future research in object detection for NeRF, we built a new benchmark dataset which consists of both synthetic and real-world data with careful labeling and clean up. Code and dataset are available at https://github.com/lyclyc52/NeRF_RPN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">523.Masked Wavelet Representation for Compact Neural Radiance Fields</span><br>
                <span class="as">Rho, DanielandLee, ByeonghyeonandNam, SeungtaeandLee, JooChanandKo, JongHwanandPark, Eunbyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rho_Masked_Wavelet_Representation_for_Compact_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20680-20690.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何减少使用多层感知器（MLP）表示3D场景或对象所需的巨大计算资源和时间。<br>
                    动机：尽管最近的研究通过使用额外的数据结构（如网格或树）来降低这些计算效率，但这些显式的数据结构需要大量的内存。<br>
                    方法：我们提出了一种在不牺牲额外数据结构优点的情况下减小其大小的方法。具体来说，我们建议在基于网格的神经场中使用小波变换。<br>
                    效果：实验结果表明，非空间网格系数（如小波系数）能够实现比空间网格系数更高的稀疏性，从而得到更紧凑的表示。通过我们的掩码和压缩管道，我们在2MB的内存预算内实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural radiance fields (NeRF) have demonstrated the potential of coordinate-based neural representation (neural fields or implicit neural representation) in neural rendering. However, using a multi-layer perceptron (MLP) to represent a 3D scene or object requires enormous computational resources and time. There have been recent studies on how to reduce these computational inefficiencies by using additional data structures, such as grids or trees. Despite the promising performance, the explicit data structure necessitates a substantial amount of memory. In this work, we present a method to reduce the size without compromising the advantages of having additional data structures. In detail, we propose using the wavelet transform on grid-based neural fields. Grid-based neural fields are for fast convergence, and the wavelet transform, whose efficiency has been demonstrated in high-performance standard codecs, is to improve the parameter efficiency of grids. Furthermore, in order to achieve a higher sparsity of grid coefficients while maintaining reconstruction quality, we present a novel trainable masking approach. Experimental results demonstrate that non-spatial grid coefficients, such as wavelet coefficients, are capable of attaining a higher level of sparsity than spatial grid coefficients, resulting in a more compact representation. With our proposed mask and compression pipeline, we achieved state-of-the-art performance within a memory budget of 2 MB. Our code is available at https://github.com/daniel03c1/masked_wavelet_nerf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">524.PersonNeRF: Personalized Reconstruction From Photo Collections</span><br>
                <span class="as">Weng, Chung-YiandSrinivasan, PratulP.andCurless, BrianandKemelmacher-Shlizerman, Ira</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weng_PersonNeRF_Personalized_Reconstruction_From_Photo_Collections_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/524-533.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从多角度、多姿态和多外观的人物照片中，生成具有任意新视角、姿态和外观的人物3D模型。<br>
                    动机：现有的方法在处理稀疏的观测数据时存在困难，即同一姿态可能只有单一视角和单一外观的观察，同一外观也只有少数不同姿态的观察。<br>
                    方法：提出PersonNeRF方法，通过建立一个定制化的神经体积3D模型来捕捉人物的全部空间，包括相机视角、姿态和外观。同时，通过恢复一个标准的T姿态神经体积表示，允许在不同的观察中改变外观，但使用所有观察共享的姿态依赖运动场。<br>
                    效果：该方法能够有效地从这些具有挑战性的非结构化照片集合中，生成令人信服的新视角、姿态和外观的人物图像，优于先前的自由视点人类渲染工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present PersonNeRF, a method that takes a collection of photos of a subject (e.g., Roger Federer) captured across multiple years with arbitrary body poses and appearances, and enables rendering the subject with arbitrary novel combinations of viewpoint, body pose, and appearance. PersonNeRF builds a customized neural volumetric 3D model of the subject that is able to render an entire space spanned by camera viewpoint, body pose, and appearance. A central challenge in this task is dealing with sparse observations; a given body pose is likely only observed by a single viewpoint with a single appearance, and a given appearance is only observed under a handful of different body poses. We address this issue by recovering a canonical T-pose neural volumetric representation of the subject that allows for changing appearance across different observations, but uses a shared pose-dependent motion field across all observations. We demonstrate that this approach, along with regularization of the recovered volumetric geometry to encourage smoothness, is able to recover a model that renders compelling images from novel combinations of viewpoint, pose, and appearance from these challenging unstructured photo collections, outperforming prior work for free-viewpoint human rendering.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">525.Learning a Depth Covariance Function</span><br>
                <span class="as">Dexheimer, EricandDavison, AndrewJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dexheimer_Learning_a_Depth_Covariance_Function_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13122-13131.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种深度协方差函数学习方法，并将其应用于几何视觉任务。<br>
                    动机：利用RGB图像作为输入，协方差函数可以灵活地定义深度函数的先验、观察后的预测分布以及主动点选择方法。<br>
                    方法：通过这些技术，我们为一系列下游任务（如深度补全、光束法平差和单目密集视觉里程计）提供了支持。<br>
                    效果：实验结果表明，该方法在各种几何视觉任务上取得了良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose learning a depth covariance function with applications to geometric vision tasks. Given RGB images as input, the covariance function can be flexibly used to define priors over depth functions, predictive distributions given observations, and methods for active point selection. We leverage these techniques for a selection of downstream tasks: depth completion, bundle adjustment, and monocular dense visual odometry.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">526.What You Can Reconstruct From a Shadow</span><br>
                <span class="as">Liu, RuoshiandMenon, SachitandMao, ChengzhiandPark, DennisandStent, SimonandVondrick, Carl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_What_You_Can_Reconstruct_From_a_Shadow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17059-17068.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的基本问题——3D重建，特别是在部分或完全遮挡的对象进行重建时的挑战。<br>
                    动机：当需要重建的对象被部分或完全遮挡时，3D重建任务变得尤其困难。因此，我们提出了一种使用未被观察到的物体投射的阴影来推断遮挡下的可能3D体积的方法。<br>
                    方法：我们创建了一个可微分的图像形成模型，该模型可以联合推断物体的3D形状、其姿态和光源的位置。由于该方法是端到端可微分的，我们可以整合学到的物体几何先验知识，以生成不同类别物体的真实3D形状。<br>
                    效果：实验和可视化结果显示，该方法能够生成多个与阴影观察一致的可能解决方案。即使在光源位置和物体姿态都未知的情况下，该方法也能正常工作。此外，对于真实世界的图像，即使不知道地面真值阴影掩码，该方法也是鲁棒的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D reconstruction is a fundamental problem in computer vision, and the task is especially challenging when the object to reconstruct is partially or fully occluded. We introduce a method that uses the shadows cast by an unobserved object in order to infer the possible 3D volumes under occlusion. We create a differentiable image formation model that allows us to jointly infer the 3D shape of an object, its pose, and the position of a light source. Since the approach is end-to-end differentiable, we are able to integrate learned priors of object geometry in order to generate realistic 3D shapes of different object categories. Experiments and visualizations show that the method is able to generate multiple possible solutions that are consistent with the observation of the shadow. Our approach works even when the position of the light source and object pose are both unknown. Our approach is also robust to real-world images where ground-truth shadow mask is unknown.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">527.HelixSurf: A Robust and Efficient Neural Implicit Surface Learning of Indoor Scenes With Iterative Intertwined Regularization</span><br>
                <span class="as">Liang, ZhihaoandHuang, ZhangjinandDing, ChangxingandJia, Kui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_HelixSurf_A_Robust_and_Efficient_Neural_Implicit_Surface_Learning_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13165-13174.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从多视角图像中恢复底层场景几何？<br>
                    动机：现有的方法在处理复杂场景表面时表现不佳，而传统的多视角立体视觉在处理具有丰富纹理的场景时效果较好。<br>
                    方法：提出一种名为HelixSurf的方法，该方法利用两种策略的互补优势，通过迭代地在训练过程中进行相互约束来优化学习过程。<br>
                    效果：实验表明，HelixSurf在室内场景的表面重建方面优于现有方法，并且即使对于一些使用辅助训练数据的方法，HelixSurf的速度也快了几个数量级。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recovery of an underlying scene geometry from multi-view images stands as a long-time challenge in computer vision research. The recent promise leverages neural implicit surface learning and differentiable volume rendering, and achieves both the recovery of scene geometry and synthesis of novel views, where deep priors of neural models are used as an inductive smoothness bias. While promising for object-level surfaces, these methods suffer when coping with complex scene surfaces. In the meanwhile, traditional multi-view stereo can recover the geometry of scenes with rich textures, by globally optimizing the local, pixel-wise correspondences across multiple views. We are thus motivated to make use of the complementary benefits from the two strategies, and propose a method termed Helix-shaped neural implicit Surface learning or HelixSurf; HelixSurf uses the intermediate prediction from one strategy as the guidance to regularize the learning of the other one, and conducts such intertwined regularization iteratively during the learning process. We also propose an efficient scheme for differentiable volume rendering in HelixSurf. Experiments on surface reconstruction of indoor scenes show that our method compares favorably with existing methods and is orders of magnitude faster, even when some of existing methods are assisted with auxiliary training data. The source code is available at https://github.com/Gorilla-Lab-SCUT/HelixSurf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">528.3D-Aware Facial Landmark Detection via Multi-View Consistent Training on Synthetic Data</span><br>
                <span class="as">Zeng, LibingandChen, LeleandBao, WentaoandLi, ZhongandXu, YiandYuan, JunsongandKalantari, NimaKhademi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_3D-Aware_Facial_Landmark_Detection_via_Multi-View_Consistent_Training_on_Synthetic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12747-12758.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高在野外图像中准确检测面部地标的能力。<br>
                    动机：由于缺乏多视角的野外训练数据，现有的方法在检测3D/2D面部地标时难以保持3D一致性。<br>
                    方法：利用生成视觉模型和神经渲染的最新进展，构建一个合成数据集，并提出一种新的多视角一致学习策略来提高在野外图像中的3D面部地标检测准确性。<br>
                    效果：所提出的3D感知模块可以插入到任何基于学习的地标检测算法中以提高其准确性。通过在多个真实和合成数据集上与最先进的方法进行广泛比较，证明了所提出的插件模块的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Accurate facial landmark detection on wild images plays an essential role in human-computer interaction, entertainment, and medical applications. Existing approaches have limitations in enforcing 3D consistency while detecting 3D/2D facial landmarks due to the lack of multi-view in-the-wild training data. Fortunately, with the recent advances in generative visual models and neural rendering, we have witnessed rapid progress towards high quality 3D image synthesis. In this work, we leverage such approaches to construct a synthetic dataset and propose a novel multi-view consistent learning strategy to improve 3D facial landmark detection accuracy on in-the-wild images. The proposed 3D-aware module can be plugged into any learning-based landmark detection algorithm to enhance its accuracy. We demonstrate the superiority of the proposed plug-in module with extensive comparison against state-of-the-art methods on several real and synthetic datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">529.MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures</span><br>
                <span class="as">Chen, ZhiqinandFunkhouser, ThomasandHedman, PeterandTagliasacchi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16569-16578.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile phones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">530.POEM: Reconstructing Hand in a Point Embedded Multi-View Stereo</span><br>
                <span class="as">Yang, LixinandXu, JianandZhong, LichengandZhan, XinyuandWang, ZhichengandWu, KejianandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_POEM_Reconstructing_Hand_in_a_Point_Embedded_Multi-View_Stereo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21108-21117.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使神经网络在多视角视觉任务中捕捉到3D几何感知特征。<br>
                    动机：以前的多视角立体视觉方法通常将3D信息编码到2D特征中，而我们提出了一种直接在3D点上操作的新方法POEM，用于重建手部网格。<br>
                    方法：POEM利用了嵌入在多视图立体视觉中的3D点来表示复杂的3D手部网格，这些点携带了来自不同视图的特征，并环绕着手部。我们设计了基于点的特诊融合和交叉集点注意力机制两种操作。<br>
                    效果：在三个具有挑战性的多视角数据集上的评估表明，POEM在手部网格重建方面优于最先进的技术。代码和模型可以在github.com/lixiny/POEM进行研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Enable neural networks to capture 3D geometrical-aware features is essential in multi-view based vision tasks. Previous methods usually encode the 3D information of multi-view stereo into the 2D features. In contrast, we present a novel method, named POEM, that directly operates on the 3D POints Embedded in the Multi-view stereo for reconstructing hand mesh in it. Point is a natural form of 3D information and an ideal medium for fusing features across views, as it has different projections on different views. Our method is thus in light of a simple yet effective idea, that a complex 3D hand mesh can be represented by a set of 3D points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encircle the hand. To leverage the power of points, we design two operations: point-based feature fusion and cross-set point attention mechanism. Evaluation on three challenging multi-view datasets shows that POEM outperforms the state-of-the-art in hand mesh reconstruction. Code and models are available for research at github.com/lixiny/POEM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">531.DrapeNet: Garment Generation and Self-Supervised Draping</span><br>
                <span class="as">DeLuigi, LucaandLi, RenandGuillard, Beno{\^\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/De_Luigi_DrapeNet_Garment_Generation_and_Self-Supervised_Draping_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1451-1460.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个能快速覆盖任意人体服装的模型，同时减少对大型训练集的依赖。<br>
                    动机：目前的服装覆盖模型需要为每一件服装训练一个网络，限制了其泛化能力。<br>
                    方法：利用自我监督训练一个能覆盖多种服装的网络，通过预测基于生成网络潜在代码的3D变形场来实现。<br>
                    效果：该模型可以生成和覆盖以前未见过的任意拓扑结构的服装，形状可以通过操作其潜在代码进行编辑。并且可以从部分观察（如图像或3D扫描）中通过梯度下降恢复准确的服装3D模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent approaches to drape garments quickly over arbitrary human bodies leverage self-supervision to eliminate the need for large training sets. However, they are designed to train one network per clothing item, which severely limits their generalization abilities. In our work, we rely on self-supervision to train a single network to drape multiple garments. This is achieved by predicting a 3D deformation field conditioned on the latent codes of a generative network, which models garments as unsigned distance fields. Our pipeline can generate and drape previously unseen garments of any topology, whose shape can be edited by manipulating their latent codes. Being fully differentiable, our formulation makes it possible to recover accurate 3D models of garments from partial observations -- images or 3D scans -- via gradient descent. Our code is publicly available at https://github.com/liren2515/DrapeNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">532.Progressively Optimized Local Radiance Fields for Robust View Synthesis</span><br>
                <span class="as">Meuleman, Andr\&#x27;easandLiu, Yu-LunandGao, ChenandHuang, Jia-BinandKim, ChangilandKim, MinH.andKopf, Johannes</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16539-16548.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张随意捕获的视频中重建大规模场景的辐射场。<br>
                    动机：大多数现有的辐射场重建方法依赖于结构从运动算法准确预先估计的相机位姿，这在野外视频中经常失败；使用具有有限表示能力的单个全局辐射场无法扩展到无界场景中的更长轨迹。<br>
                    方法：我们逐步联合估计相机位姿和辐射场以处理未知位姿，动态分配新训练的局部辐射场以处理大型无界场景。<br>
                    效果：我们在坦克和寺庙数据集以及我们收集的户外数据集静态远足上进行了广泛的评估，结果表明我们的方法与最先进的方法相比具有优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present an algorithm for reconstructing the radiance field of a large-scale scene from a single casually captured video. The task poses two core challenges. First, most existing radiance field reconstruction approaches rely on accurate pre-estimated camera poses from Structure-from-Motion algorithms, which frequently fail on in-the-wild videos. Second, using a single, global radiance field with finite representational capacity does not scale to longer trajectories in an unbounded scene. For handling unknown poses, we jointly estimate the camera poses with radiance field in a progressive manner. We show that progressive optimization significantly improves the robustness of the reconstruction. For handling large unbounded scenes, we dynamically allocate new local radiance fields trained with frames within a temporal window. This further improves robustness (e.g., performs well even under moderate pose drifts) and allows us to scale to large scenes. Our extensive evaluation on the Tanks and Temples dataset and our collected outdoor dataset, Static Hikes, show that our approach compares favorably with the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">533.AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware Training</span><br>
                <span class="as">Jiang, YifanandHedman, PeterandMildenhall, BenandXu, DejiaandBarron, JonathanT.andWang, ZhangyangandXue, Tianfan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_AligNeRF_High-Fidelity_Neural_Radiance_Fields_via_Alignment-Aware_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/46-55.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索Neural Radiance Fields (NeRFs)在高分辨率场景重建中的局限性，并提出相应的解决方案。<br>
                    动机：现有的基于NeRF的方法在重建高分辨率真实场景时面临一些挑战，包括参数数量过多、输入数据不匹配以及过于平滑的细节。<br>
                    方法：本文提出了一种新颖的NeRF训练策略，包括将多层感知器与卷积层结合以编码更多的邻域信息并减少参数数量；解决由移动物体或小相机校准错误引起的不匹配问题的新方法；以及一种高频感知损失函数。<br>
                    效果：实验结果表明，该方法可以恢复比当前最先进的NeRF模型更多的高频细节，且几乎不需要额外的训练/测试成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Radiance Fields (NeRFs) are a powerful representation for modeling a 3D scene as a continuous function. Though NeRF is able to render complex 3D scenes with view-dependent effects, few efforts have been devoted to exploring its limits in a high-resolution setting. Specifically, existing NeRF-based methods face several limitations when reconstructing high-resolution real scenes, including a very large number of parameters, misaligned input data, and overly smooth details. In this work, we conduct the first pilot study on training NeRF with high-resolution data and propose the corresponding solutions: 1) marrying the multilayer perceptron (MLP) with convolutional layers which can encode more neighborhood information while reducing the total number of parameters; 2) a novel training strategy to address misalignment caused by moving objects or small camera calibration errors; and 3) a high-frequency aware loss. Our approach is nearly free without introducing obvious training/testing costs, while experiments on different datasets demonstrate that it can recover more high-frequency details compared with the current state-of-the-art NeRF models. Project page: https://yifanjiang19.github.io/alignerf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">534.Implicit 3D Human Mesh Recovery Using Consistency With Pose and Shape From Unseen-View</span><br>
                <span class="as">Cho, HanbyelandCho, YooshinandAhn, JaesungandKim, Junmo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Implicit_3D_Human_Mesh_Recovery_Using_Consistency_With_Pose_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21148-21158.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从人像图像中推断出人的自然3D姿态和形状，即使存在模糊性。<br>
                    动机：由于现有方法的结构限制，它们只考虑了图像拍摄的方向，而我们提出的方法可以隐式地通过神经特征场在特征级别上想象人在3D空间中的形象。<br>
                    方法：我们提出了“隐式3D人体网格恢复（ImpHMR）”方法，该方法通过基于CNN的图像编码器生成特征场，然后根据给定的查看方向从特征场中进行2D特征图的体积渲染，并从特征中回归姿态和形状参数。<br>
                    效果：广泛的评估表明，该方法是有效的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>From an image of a person, we can easily infer the natural 3D pose and shape of the person even if ambiguity exists. This is because we have a mental model that allows us to imagine a person's appearance at different viewing directions from a given image and utilize the consistency between them for inference. However, existing human mesh recovery methods only consider the direction in which the image was taken due to their structural limitations. Hence, we propose "Implicit 3D Human Mesh Recovery (ImpHMR)" that can implicitly imagine a person in 3D space at the feature-level via Neural Feature Fields. In ImpHMR, feature fields are generated by CNN-based image encoder for a given image. Then, the 2D feature map is volume-rendered from the feature field for a given viewing direction, and the pose and shape parameters are regressed from the feature. To utilize consistency with pose and shape from unseen-view, if there are 3D labels, the model predicts results including the silhouette from an arbitrary direction and makes it equal to the rotated ground-truth. In the case of only 2D labels, we perform self-supervised learning through the constraint that the pose and shape parameters inferred from different directions should be the same. Extensive evaluations show the efficacy of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">535.Teleidoscopic Imaging System for Microscale 3D Shape Reconstruction</span><br>
                <span class="as">Kawahara, RyoandKuo, Meng-YuJenniferandNobuhara, Shohei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kawahara_Teleidoscopic_Imaging_System_for_Microscale_3D_Shape_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20813-20822.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种实用的显微三维形状捕获方法，通过远程光学成像系统实现。<br>
                    动机：微型三维形状重建的主要挑战在于从多个视角捕捉目标，同时需要足够大的景深。<br>
                    方法：采用由三个平面镜和单心透镜组成的远程光学测量系统。平面镜通过多次反射虚拟定义多个视角，单心透镜即使在近距离成像中也能实现高放大倍率、低模糊度和全景视图。<br>
                    效果：我们的贡献包括一个处理折射和反射投影射线的结构化光线像素相机模型，对远程光学成像系统的景深进行解析评估，以及远程光学成像系统的实用校准算法。通过真实图像的评估证明了我们的测量系统的概念。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a practical method of microscale 3D shape capturing by a teleidoscopic imaging system. The main challenge in microscale 3D shape reconstruction is to capture the target from multiple viewpoints with a large enough depth-of-field. Our idea is to employ a teleidoscopic measurement system consisting of three planar mirrors and monocentric lens. The planar mirrors virtually define multiple viewpoints by multiple reflections, and the monocentric lens realizes a high magnification with less blurry and surround view even in closeup imaging. Our contributions include, a structured ray-pixel camera model which handles refractive and reflective projection rays efficiently, analytical evaluations of depth of field of our teleidoscopic imaging system, and a practical calibration algorithm of the teleidoscppic imaging system. Evaluations with real images prove the concept of our measurement system.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">536.UV Volumes for Real-Time Rendering of Editable Free-View Human Performance</span><br>
                <span class="as">Chen, YueandWang, XuanandChen, XingyuandZhang, QiandLi, XiaoyuandGuo, YuandWang, JueandWang, Fei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16621-16631.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低神经体渲染在沉浸式VR/AR应用中对高计算成本的依赖，实现人类表演者的实时、可编辑的自由视点视频渲染。<br>
                    动机：现有的神经体渲染方法由于高昂的计算成本，在实践中受到严重限制。<br>
                    方法：提出UV Volumes新方法，将高频（即非平滑）的人体外观与3D体积分离，编码为2D神经纹理堆栈（NTS）。通过参数化人体模型与平滑纹理坐标之间的映射，实现了对新姿态和形状的更好泛化。<br>
                    效果：在CMU Panoptic、ZJU Mocap和H36M数据集上的大量实验表明，该方法可以在平均30FPS的速度下渲染出具有与最先进方法相当的照片写实度的960 x 540图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural volume rendering enables photo-realistic renderings of a human performer in free-view, a critical task in immersive VR/AR applications. But the practice is severely limited by high computational costs in the rendering process. To solve this problem, we propose the UV Volumes, a new approach that can render an editable free-view video of a human performer in real-time. It separates the high-frequency (i.e., non-smooth) human appearance from the 3D volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV volumes allow much smaller and shallower neural networks to obtain densities and texture coordinates in 3D while capturing detailed appearance in 2D NTS. For editability, the mapping between the parameterized human model and the smooth texture coordinates allows us a better generalization on novel poses and shapes. Furthermore, the use of NTS enables interesting applications, e.g., retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M datasets show that our model can render 960 x 540 images in 30FPS on average with comparable photo-realism to state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">537.Multi-View Stereo Representation Revist: Region-Aware MVSNet</span><br>
                <span class="as">Zhang, YisuandZhu, JiankeandLin, Lixiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Multi-View_Stereo_Representation_Revist_Region-Aware_MVSNet_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17376-17385.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习多视角立体重建方法通常只通过最小化预测点与光线和表面交点的间隙来估计像素级的深度值，这通常会忽略表面的拓扑结构，导致纹理缺失区域和表面边界无法正确重建。<br>
                    动机：为了解决这个问题，我们提出利用点到表面的距离，使模型能够感知更广泛的表面。为此，我们从成本体积预测距离体积以估计表面周围点的有符号距离。<br>
                    方法：我们提出的RA-MVSNet是分块的，因为通过将假设平面与表面补丁关联起来，感知范围得到了增强。因此，它可以增加纹理缺失区域的完成度并减少边界的异常值。此外，引入的距离体积可以生成具有精细细节的网格拓扑结构。<br>
                    效果：与传统的深度学习多视角立体重建方法相比，我们的RA-MVSNet方法通过利用有符号距离监督获得了更完整的重建结果。在DTU和Tanks & Temples数据集上的实验表明，我们的方法实现了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning-based multi-view stereo has emerged as a powerful paradigm for reconstructing the complete geometrically-detailed objects from multi-views. Most of the existing approaches only estimate the pixel-wise depth value by minimizing the gap between the predicted point and the intersection of ray and surface, which usually ignore the surface topology. It is essential to the textureless regions and surface boundary that cannot be properly reconstructed.To address this issue, we suggest to take advantage of point-to-surface distance so that the model is able to perceive a wider range of surfaces. To this end, we predict the distance volume from cost volume to estimate the signed distance of points around the surface. Our proposed RA-MVSNet is patch-awared, since the perception range is enhanced by associating hypothetical planes with a patch of surface. Therefore, it could increase the completion of textureless regions and reduce the outliers at the boundary. Moreover, the mesh topologies with fine details can be generated by the introduced distance volume. Comparing to the conventional deep learning-based multi-view stereo methods, our proposed RA-MVSNet approach obtains more complete reconstruction results by taking advantage of signed distance supervision. The experiments on both the DTU and Tanks & Temples datasets demonstrate that our proposed approach achieves the state-of-the-art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">538.Robust Dynamic Radiance Fields</span><br>
                <span class="as">Liu, Yu-LunandGao, ChenandMeuleman, Andr\&#x27;easandTseng, Hung-YuandSaraf, AyushandKim, ChangilandChuang, Yung-YuandKopf, JohannesandHuang, Jia-Bin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13-23.png><br>
            
            <span class="tt"><span class="t0">研究问题：动态场景的结构和外观建模。<br>
                    动机：现有的方法依赖于结构从运动（SfM）算法来估计准确的相机姿态，但在具有高度动态对象、纹理表面较差和旋转相机运动的挑战性视频中，这些方法往往失败或产生错误的姿态。<br>
                    方法：我们通过联合估计静态和动态辐射场以及相机参数（姿态和焦距）来解决此问题。<br>
                    效果：我们的方法在广泛的定量和定性实验中表现出强大的稳健性，性能优于最先进的动态视图合成方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dynamic radiance field reconstruction methods aim to model the time-varying structure and appearance of a dynamic scene. Existing methods, however, assume that accurate camera poses can be reliably estimated by Structure from Motion (SfM) algorithms. These methods, thus, are unreliable as SfM algorithms often fail or produce erroneous poses on challenging videos with highly dynamic objects, poorly textured surfaces, and rotating camera motion. We address this issue by jointly estimating the static and dynamic radiance fields along with the camera parameters (poses and focal length). We demonstrate the robustness of our approach via extensive quantitative and qualitative experiments. Our results show favorable performance over the state-of-the-art dynamic view synthesis methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">539.PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation</span><br>
                <span class="as">Shetty, KarthikandBirkhold, AnnetteandJaganathan, SrikrishnaandStrobel, NorbertandKowarschik, MarkusandMaier, AndreasandEgger, Bernhard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shetty_PLIKS_A_Pseudo-Linear_Inverse_Kinematic_Solver_for_3D_Human_Body_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/574-584.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过单张2D图像重建3D人体模型。<br>
                    动机：目前的直接回归方法对外部影响缺乏灵活性，我们希望通过模型循环优化解决这个问题。<br>
                    方法：我们提出了PLIKS（伪线性逆运动学解算器），它基于参数化SMPL模型的线性化形式，可以通过2D像素对齐顶点进行人体模型的解析重建。<br>
                    效果：实验结果表明，与其他最先进的方法相比，PLIKS在标准的3D人体姿态和形状基准上实现了10%以上的更准确的重建，同时在新的AGORA数据集上，重建误差改善了12.9毫米。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce PLIKS (Pseudo-Linear Inverse Kinematic Solver) for reconstruction of a 3D mesh of the human body from a single 2D image. Current techniques directly regress the shape, pose, and translation of a parametric model from an input image through a non-linear mapping with minimal flexibility to any external influences. We approach the task as a model-in-the-loop optimization problem. PLIKS is built on a linearized formulation of the parametric SMPL model. Using PLIKS, we can analytically reconstruct the human model via 2D pixel-aligned vertices. This enables us with the flexibility to use accurate camera calibration information when available. PLIKS offers an easy way to introduce additional constraints such as shape and translation. We present quantitative evaluations which confirm that PLIKS achieves more accurate reconstruction with greater than 10% improvement compared to other state-of-the-art methods with respect to the standard 3D human pose and shape benchmarks while also obtaining a reconstruction error improvement of 12.9 mm on the newer AGORA dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">540.gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction</span><br>
                <span class="as">Chen, ZeruiandChen, ShizheandSchmid, CordeliaandLaptev, Ivan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_gSDF_Geometry-Driven_Signed_Distance_Functions_for_3D_Hand-Object_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12890-12900.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用手部结构作为指导，对基于SDF的3D形状重建进行改进。<br>
                    动机：虽然SDFs在3D形状重建中表现出色，但它们缺乏对底层3D几何的显式建模。<br>
                    方法：我们使用手部结构和姿势来指导基于SDF的形状重建，并预测姿势变换的运动链以与高度关节化的手部姿势对齐。我们还通过几何对齐改善了3D点的视觉特征，并进一步利用时间信息增强了对遮挡和运动模糊的鲁棒性。<br>
                    效果：我们在具有挑战性的ObMan和DexYCB基准上进行了广泛的实验，证明该方法比现有技术有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Signed distance functions (SDFs) is an attractive framework that has recently shown promising results for 3D shape reconstruction from images. SDFs seamlessly generalize to different shape resolutions and topologies but lack explicit modelling of the underlying 3D geometry. In this work, we exploit the hand structure and use it as guidance for SDF-based shape reconstruction. In particular, we address reconstruction of hands and manipulated objects from monocular RGB images. To this end, we estimate poses of hands and objects and use them to guide 3D reconstruction. More specifically, we predict kinematic chains of pose transformations and align SDFs with highly-articulated hand poses. We improve the visual features of 3D points with geometry alignment and further leverage temporal information to enhance the robustness to occlusion and motion blurs. We conduct extensive experiments on the challenging ObMan and DexYCB benchmarks and demonstrate significant improvements of the proposed method over the state of the art.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>