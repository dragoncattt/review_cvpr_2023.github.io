<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic4</h2>
      <b>Topic words : &ensp;</b>point, &ensp;features, &ensp;feature, &ensp;attention, &ensp;local, &ensp;transformer, &ensp;network, &ensp;propose</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">879.CXTrack: Improving 3D Point Cloud Tracking With Contextual Information</span><br>
                <span class="as">Xu, Tian-XingandGuo, Yuan-ChenandLai, Yu-KunandZhang, Song-Hai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_CXTrack_Improving_3D_Point_Cloud_Tracking_With_Contextual_Information_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1084-1093.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用上下文信息进行3D物体追踪。<br>
                    动机：由于外观变化大、遮挡和传感器能力限制导致的点稀疏，使得现有的方法经常忽略并裁剪掉包含有用信息的点，导致重要上下文知识的使用不足。<br>
                    方法：提出一种基于变压器的3D物体追踪网络CXTrack，通过直接从连续两帧的点特征和之前的边界框中获取上下文信息，以改善追踪结果。设计了一种目标为中心的变压器网络，用于探索上下文信息和隐式传播目标线索。<br>
                    效果：在KITTI、nuScenes和Waymo Open Dataset三个大规模数据集上的大量实验表明，CXTrack在运行速度为34FPS的同时，实现了最先进的追踪性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D single object tracking plays an essential role in many applications, such as autonomous driving. It remains a challenging problem due to the large appearance variation and the sparsity of points caused by occlusion and limited sensor capabilities. Therefore, contextual information across two consecutive frames is crucial for effective object tracking. However, points containing such useful information are often overlooked and cropped out in existing methods, leading to insufficient use of important contextual knowledge. To address this issue, we propose CXTrack, a novel transformer-based network for 3D object tracking, which exploits ConteXtual information to improve the tracking results. Specifically, we design a target-centric transformer network that directly takes point features from two consecutive frames and the previous bounding box as input to explore contextual information and implicitly propagate target cues. To achieve accurate localization for objects of all sizes, we propose a transformer-based localization head with a novel center embedding module to distinguish the target from distractors. Extensive experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open Dataset, show that CXTrack achieves state-of-the-art tracking performance while running at 34 FPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">880.Revisiting Self-Similarity: Structural Embedding for Image Retrieval</span><br>
                <span class="as">Lee, SeongwonandLee, SuhyeonandSeong, HongjeandKim, Euntai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Revisiting_Self-Similarity_Structural_Embedding_for_Image_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23412-23421.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管全球图像表示取得了进展，但现有的图像检索方法在全局检索阶段很少考虑几何结构。<br>
                    动机：我们重新审视了传统的自相似性描述符，从卷积的角度出发，对图像的视觉和结构线索进行编码，以实现全局图像表示。<br>
                    方法：我们提出了一种名为Structural Embedding Network（SENet）的网络，该网络能够捕获图像的内部结构，并在学习各种图像的多样化结构的同时，将这些结构逐渐压缩成密集的自相似性描述符。这些自相似性描述符和原始图像特征被融合并池化为全局嵌入，使全局嵌入能够同时表示图像的几何和视觉线索。<br>
                    效果：通过这种新颖的结构嵌入，我们的网络在几个图像检索基准测试中设置了新的最先进的性能，证明了其对类似干扰物的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite advances in global image representation, existing image retrieval approaches rarely consider geometric structure during the global retrieval stage. In this work, we revisit the conventional self-similarity descriptor from a convolutional perspective, to encode both the visual and structural cues of the image to global image representation. Our proposed network, named Structural Embedding Network (SENet), captures the internal structure of the images and gradually compresses them into dense self-similarity descriptors while learning diverse structures from various images. These self-similarity descriptors and original image features are fused and then pooled into global embedding, so that global embedding can represent both geometric and visual cues of the image. Along with this novel structural embedding, our proposed network sets new state-of-the-art performances on several image retrieval benchmarks, convincing its robustness to look-alike distractors. The code and models are available: https://github.com/sungonce/SENet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">881.Decoupling-and-Aggregating for Image Exposure Correction</span><br>
                <span class="as">Wang, YangandPeng, LongandLi, LiangandCao, YangandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Decoupling-and-Aggregating_for_Image_Exposure_Correction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18115-18124.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善在曝光条件不佳下拍摄的图片的对比度和细节？<br>
                    动机：曝光不足会导致图片的低频率和高频率组件混合，限制了统计和结构建模的能力。<br>
                    方法：提出在每个卷积过程中分离对比度增强和细节恢复的方法。通过添加/差分操作，将CA单元和DA单元插入到现有的CNN-based曝光校正网络中，以改善性能。<br>
                    效果：实验证明，该方法可以全面提高现有方法的性能，且不增加额外的计算成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The images captured under improper exposure conditions often suffer from contrast degradation and detail distortion. Contrast degradation will destroy the statistical properties of low-frequency components, while detail distortion will disturb the structural properties of high-frequency components, leading to the low-frequency and high-frequency components being mixed and inseparable. This will limit the statistical and structural modeling capacity for exposure correction. To address this issue, this paper proposes to decouple the contrast enhancement and detail restoration within each convolution process. It is based on the observation that, in the local regions covered by convolution kernels, the feature response of low-/high-frequency can be decoupled by addition/difference operation. To this end, we inject the addition/difference operation into the convolution process and devise a Contrast Aware (CA) unit and a Detail Aware (DA) unit to facilitate the statistical and structural regularities modeling. The proposed CA and DA can be plugged into existing CNN-based exposure correction networks to substitute the Traditional Convolution (TConv) to improve the performance. Furthermore, to maintain the computational costs of the network without changing, we aggregate two units into a single TConv kernel using structural re-parameterization. Evaluations of nine methods and five benchmark datasets demonstrate that our proposed method can comprehensively improve the performance of existing methods without introducing extra computational costs compared with the original networks. The codes will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">882.MarS3D: A Plug-and-Play Motion-Aware Model for Semantic Segmentation on Multi-Scan 3D Point Clouds</span><br>
                <span class="as">Liu, JiahuiandChang, ChiruiandLiu, JianhuiandWu, XiaoyangandMa, LanandQi, Xiaojuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MarS3D_A_Plug-and-Play_Motion-Aware_Model_for_Semantic_Segmentation_on_Multi-Scan_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9372-9381.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D semantic segmentation on multi-scan large-scale point clouds plays an important role in autonomous systems. Unlike the single-scan-based semantic segmentation task, this task requires distinguishing the motion states of points in addition to their semantic categories. However, methods designed for single-scan-based segmentation tasks perform poorly on the multi-scan task due to the lacking of an effective way to integrate temporal information. We propose MarS3D, a plug-and-play motion-aware model for semantic segmentation on multi-scan 3D point clouds. This module can be flexibly combined with single-scan models to allow them to have multi-scan perception abilities. The model encompasses two key designs: the Cross-Frame Feature Embedding module for enriching representation learning and the Motion-Aware Feature Learning module for enhancing motion awareness. Extensive experiments show that MarS3D can improve the performance of the baseline model by a large margin. The code is available at https://github.com/CVMI-Lab/MarS3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">883.MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving</span><br>
                <span class="as">Li, JialeandDai, HangandHan, HaoandDing, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MSeg3D_Multi-Modal_3D_Semantic_Segmentation_for_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21694-21704.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自动驾驶中3D语义分割的两种主要模式，LiDAR和相机之间的差异性、视场交叉限制以及多模态数据增强等问题。<br>
                    动机：目前，仅使用LiDAR的方法在小物体和远距离对象的分割上表现不佳，而多模态解决方案尚未得到充分探索。<br>
                    方法：我们提出了一种多模态3D语义分割模型（MSeg3D），通过联合提取模内特征和融合模间特征来缓解模态异质性问题。MSeg3D的多模态融合包括基于几何的特征融合GF-Phase、跨模态特征补全以及在所有可见点上的基于语义的特征融合SF-Phase。同时，我们还对LiDAR点云和多相机图像分别应用非对称变换，以增强多模态数据增强的效果。<br>
                    效果：实验结果表明，MSeg3D在nuScenes、Waymo和SemanticKITTI数据集上都取得了最先进的结果。即使在多相机输入故障和多帧点云输入的情况下，MSeg3D仍显示出强大的鲁棒性，并优于仅使用LiDAR的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR and camera are two modalities available for 3D semantic segmentation in autonomous driving. The popular LiDAR-only methods severely suffer from inferior segmentation on small and distant objects due to insufficient laser points, while the robust multi-modal solution is under-explored, where we investigate three crucial inherent difficulties: modality heterogeneity, limited sensor field of view intersection, and multi-modal data augmentation. We propose a multi-modal 3D semantic segmentation model (MSeg3D) with joint intra-modal feature extraction and inter-modal feature fusion to mitigate the modality heterogeneity. The multi-modal fusion in MSeg3D consists of geometry-based feature fusion GF-Phase, cross-modal feature completion, and semantic-based feature fusion SF-Phase on all visible points. The multi-modal data augmentation is reinvigorated by applying asymmetric transformations on LiDAR point cloud and multi-camera images individually, which benefits the model training with diversified augmentation transformations. MSeg3D achieves state-of-the-art results on nuScenes, Waymo, and SemanticKITTI datasets. Under the malfunctioning multi-camera input and the multi-frame point clouds input, MSeg3D still shows robustness and improves the LiDAR-only baseline. Our code is publicly available at https://github.com/jialeli1/lidarseg3d.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">884.Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</span><br>
                <span class="as">Huang, YuanhuiandZheng, WenzhaoandZhang, YunpengandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9223-9232.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉自动驾驶感知方法在描述3D场景的精细结构时存在困难。<br>
                    动机：为了解决这个问题，我们提出了一种三视角（TPV）表示法，以补充鸟瞰图（BEV）的不足。<br>
                    方法：我们通过在三个垂直平面上投影特征来对3D空间中的每个点进行建模，并使用转换器基础的TPV编码器（TPVFormer）将图像特征提升到3D TPV空间。<br>
                    效果：实验表明，我们的模型在稀疏监督下有效地预测了所有体素的语义占有率，首次证明了仅使用相机输入就可以在nuScenes上的激光雷达分割任务上实现与基于激光雷达的方法相媲美的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern methods for vision-centric autonomous driving perception widely adopt the bird's-eye-view (BEV) representation to describe a 3D scene. Despite its better efficiency than voxel representation, it has difficulty describing the fine-grained 3D structure of a scene with a single plane. To address this, we propose a tri-perspective view (TPV) representation which accompanies BEV with two additional perpendicular planes. We model each point in the 3D space by summing its projected features on the three planes. To lift image features to the 3D TPV space, we further propose a transformer-based TPV encoder (TPVFormer) to obtain the TPV features effectively. We employ the attention mechanism to aggregate the image features corresponding to each query in each TPV plane. Experiments show that our model trained with sparse supervision effectively predicts the semantic occupancy for all voxels. We demonstrate for the first time that using only camera inputs can achieve comparable performance with LiDAR-based methods on the LiDAR segmentation task on nuScenes. Code: https://github.com/wzzheng/TPVFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">885.Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference</span><br>
                <span class="as">You, HaoranandXiong, YunyangandDai, XiaoliangandWu, BichenandZhang, PeizhaoandFan, HaoqiandVajda, PeterandLin, Yingyan(Celine)</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14431-14442.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器（ViTs）能否在推理过程中同时学习全局和局部上下文，并提高效率？<br>
                    动机：现有的高效ViTs采用局部或线性注意力，牺牲了捕捉全局或局部上下文的能力。<br>
                    方法：提出一个名为Castling-ViT的框架，训练ViTs使用线性-角度注意力和基于掩码的二次注意力，但在推理时只使用线性-角度注意力。<br>
                    效果：Castling-ViT利用角度内核通过光谱角度测量查询和键之间的相似性。实验证明其有效性，例如在分类任务上比使用普通softmax注意力的ViTs提高1.8%的准确率或减少40%的MACs，在检测任务上提高1.2的mAP，同时保持相似的FLOPs。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) have shown impressive performance but still require a high computation cost as compared to convolutional neural networks (CNNs), one reason is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of input tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling-ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling-ViT leverages angular kernels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an auxiliary masked softmax attention to help learn global and local information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling-ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based attentions. Project page is available at https://www.haoranyou.com/castling-vit.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">886.Robust 3D Shape Classification via Non-Local Graph Attention Network</span><br>
                <span class="as">Qin, ShengweiandLi, ZhongandLiu, Ligang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Robust_3D_Shape_Classification_via_Non-Local_Graph_Attention_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5374-5383.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过设计新的网络结构，实现对3D形状的鲁棒分类。<br>
                    动机：现有的方法在处理稀疏点云和旋转不变性上存在困难，需要设计新的方法来提高分类效果。<br>
                    方法：提出了一种非局部图注意力网络（NLGAT），该网络包含两个子网络。第一个子网络通过全局关系网络（GRN）捕获点之间的全局关系；第二个子网络利用几何形状注意力图增强局部特征，该图由全局结构网络（GSN）生成。所有子网络都使用不同维度的Gram矩阵作为输入，以提取更多信息并保持旋转不变性。<br>
                    效果：实验结果表明，NLGAT模型在各种数据集上的分类效果优于其他最先进的模型。特别是在64个点的稀疏点云和任意SO(3)旋转噪声的情况下，NLGAT的分类结果（85.4%）比其他方法的最佳发展提高了39.4%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a non-local graph attention network (NLGAT), which generates a novel global descriptor through two sub-networks for robust 3D shape classification. In the first sub-network, we capture the global relationships between points (i.e., point-point features) by designing a global relationship network (GRN). In the second sub-network, we enhance the local features with a geometric shape attention map obtained from a global structure network (GSN). To keep rotation invariant and extract more information from sparse point clouds, all sub-networks use the Gram matrices with different dimensions as input for working with robust classification. Additionally, GRN effectively preserves the low-frequency features and improves the classification results. Experimental results on various datasets exhibit that the classification effect of the NLGAT model is better than other state-of-the-art models. Especially, in the case of sparse point clouds (64 points) with noise under arbitrary SO(3) rotation, the classification result (85.4%) of NLGAT is improved by 39.4% compared with the best development of other methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">887.Bitstream-Corrupted JPEG Images Are Restorable: Two-Stage Compensation and Alignment Framework for Image Restoration</span><br>
                <span class="as">Liu, WenyangandWang, YiandYap, Kim-HuiandChau, Lap-Pui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Bitstream-Corrupted_JPEG_Images_Are_Restorable_Two-Stage_Compensation_and_Alignment_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9979-9988.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了加密比特流中存在误比特的JPEG图像恢复问题。<br>
                    动机：误比特在解码图像内容上带来不可预测的颜色偏移和块移位，现有的主要依赖像素域预定义退化模型的图像恢复方法无法轻易解决这些问题。<br>
                    方法：提出了一个鲁棒的JPEG解码器，然后通过两阶段补偿和对齐框架来恢复比特流损坏的JPEG图像。具体来说，鲁棒的JPEG解码器采用错误弹性机制来解码损坏的JPEG比特流。两阶段框架由自我补偿和对齐（SCA）阶段和引导补偿和对齐（GCA）阶段组成。SCA根据通过图像内容相似性估计的颜色和块偏移进行自适应的块状图像颜色补偿和对齐。GCA利用从JPEG头中提取的低分辨率缩略图以粗到精的方式指导全分辨率像素级图像恢复。这是通过粗引导pix2pix网络和精引导双向拉普拉斯金字塔融合网络实现的。<br>
                    效果：在三个不同误比特率基准上进行了实验。实验结果和消融研究表明了我们提出的方法的优势。代码将在https://github.com/wenyang001/Two-ACIR上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study a real-world JPEG image restoration problem with bit errors on the encrypted bitstream. The bit errors bring unpredictable color casts and block shifts on decoded image contents, which cannot be trivially resolved by existing image restoration methods mainly relying on pre-defined degradation models in the pixel domain. To address these challenges, we propose a robust JPEG decoder, followed by a two-stage compensation and alignment framework to restore bitstream-corrupted JPEG images. Specifically, the robust JPEG decoder adopts an error-resilient mechanism to decode the corrupted JPEG bitstream. The two-stage framework is composed of the self-compensation and alignment (SCA) stage and the guided-compensation and alignment (GCA) stage. The SCA adaptively performs block-wise image color compensation and alignment based on the estimated color and block offsets via image content similarity. The GCA leverages the extracted low-resolution thumbnail from the JPEG header to guide full-resolution pixel-wise image restoration in a coarse-to-fine manner. It is achieved by a coarse-guided pix2pix network and a refine-guided bi-directional Laplacian pyramid fusion network. We conduct experiments on three benchmarks with varying degrees of bit error rates. Experimental results and ablation studies demonstrate the superiority of our proposed method. The code will be released at https://github.com/wenyang001/Two-ACIR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">888.Histopathology Whole Slide Image Analysis With Heterogeneous Graph Representation Learning</span><br>
                <span class="as">Chan, TsaiHorandCendra, FernandoJulioandMa, LanandYin, GuoshengandYu, Lequan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15661-15670.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用图模型挖掘全切片组织病理学图像（WSI）中不同细胞类型的复杂结构关系。<br>
                    动机：现有的方法主要关注同质图模型的WSI分析，无法充分挖掘生物实体间的复杂交互关系。<br>
                    方法：提出一种新颖的异质图基框架，将WSI构建为带有"核类型"属性和语义相似性属性的异质图，设计了一种新的异质图边属性转换器（HEAT）进行信息聚合，并设计了一种新的基于伪标签的语义一致池化机制获取图级别特征。<br>
                    效果：在三个公共TCGA基准数据集上的大量实验表明，该框架在各种任务上显著优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph-based methods have been extensively applied to whole slide histopathology image (WSI) analysis due to the advantage of modeling the spatial relationships among different entities. However, most of the existing methods focus on modeling WSIs with homogeneous graphs (e.g., with homogeneous node type). Despite their successes, these works are incapable of mining the complex structural relations between biological entities (e.g., the diverse interaction among different cell types) in the WSI. We propose a novel heterogeneous graph-based framework to leverage the inter-relationships among different types of nuclei for WSI analysis. Specifically, we formulate the WSI as a heterogeneous graph with "nucleus-type" attribute to each node and a semantic similarity attribute to each edge. We then present a new heterogeneous-graph edge attribute transformer (HEAT) to take advantage of the edge and node heterogeneity during massage aggregating. Further, we design a new pseudo-label-based semantic-consistent pooling mechanism to obtain graph-level features, which can mitigate the over-parameterization issue of conventional cluster-based pooling. Additionally, observing the limitations of existing association-based localization methods, we propose a causal-driven approach attributing the contribution of each node to improve the interpretability of our framework. Extensive experiments on three public TCGA benchmark datasets demonstrate that our framework outperforms the state-of-the-art methods with considerable margins on various tasks. Our codes are available at https://github.com/HKU-MedAI/WSI-HGNN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">889.Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning</span><br>
                <span class="as">Wu, XiaoyangandWen, XinandLiu, XihuiandZhao, Hengshuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Masked_Scene_Contrast_A_Scalable_Framework_for_Unsupervised_3D_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9415-9424.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过对比学习进行无监督的3D表示学习，解决RGB-D帧匹配效率低下和模式崩溃的问题。<br>
                    动机：现有的PointContrast方法在各种下游任务上表现出色，但大规模无监督3D学习的趋势尚未形成，主要受限于RGB-D帧匹配效率低和模式崩溃问题。<br>
                    方法：我们提出了一种高效且有效的对比学习框架，通过精心策划的数据增强管道和实用的视图混合策略，直接在场景级别的点云上生成对比视图。同时，我们在对比学习框架中引入了重建学习，设计了对比交叉掩码，专门针对点颜色和曲面法线的重建。<br>
                    效果：我们的Masked Scene Contrast（MSC）框架能够更高效、有效地提取全面的3D表示。它至少将预训练过程加速了3倍，与之前的工作相比，性能没有妥协。此外，MSC还支持跨多个数据集的大规模3D预训练，进一步提升了性能，并在几个下游任务上实现了最先进的微调结果，例如在ScanNet语义分割验证集上达到了75.5%的mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As a pioneering work, PointContrast conducts unsupervised 3D representation learning via leveraging contrastive learning over raw RGB-D frames and proves its effectiveness on various downstream tasks. However, the trend of large-scale unsupervised learning in 3D has yet to emerge due to two stumbling blocks: the inefficiency of matching RGB-D frames as contrastive views and the annoying mode collapse phenomenon mentioned in previous works. Turning the two stumbling blocks into empirical stepping stones, we first propose an efficient and effective contrastive learning framework, which generates contrastive views directly on scene-level point clouds by a well-curated data augmentation pipeline and a practical view mixing strategy. Second, we introduce reconstructive learning on the contrastive learning framework with an exquisite design of contrastive cross masks, which targets the reconstruction of point color and surfel normal. Our Masked Scene Contrast (MSC) framework is capable of extracting comprehensive 3D representations more efficiently and effectively. It accelerates the pre-training procedure by at least 3x and still achieves an uncompromised performance compared with previous work. Besides, MSC also enables large-scale 3D pre-training across multiple datasets, which further boosts the performance and achieves state-of-the-art fine-tuning results on several downstream tasks, e.g., 75.5% mIoU on ScanNet semantic segmentation validation set.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">890.A Simple Baseline for Video Restoration With Grouped Spatial-Temporal Shift</span><br>
                <span class="as">Li, DasongandShi, XiaoyuandZhang, YiandCheung, KaChunandSee, SimonandWang, XiaogangandQin, HongweiandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9822-9832.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种简单有效的视频恢复框架，以降低复杂网络架构带来的高计算成本。<br>
                    动机：现有的深度学习方法在视频恢复中需要复杂的网络架构，如光流估计、可变形卷积和跨帧自注意力层，导致计算成本高昂。<br>
                    方法：我们提出了基于分组时空位移的简单有效框架，这是一种轻量级直接的技术，可以隐式捕获多帧聚合的帧间对应关系。通过引入分组空间位移，我们可以获得广泛的有效感受野。结合基本的二维卷积，这个简单的框架可以有效地聚合帧间信息。<br>
                    效果：大量实验表明，我们的框架在视频去模糊和视频去噪任务上优于先前最先进的方法，同时使用的计算成本不到其四分之一。这些结果表明，我们的方法有潜力在保持高质量结果的同时显著降低计算开销。代码可在https://github.com/dasongli1/Shift-Net获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video restoration, which aims to restore clear frames from degraded videos, has numerous important applications. The key to video restoration depends on utilizing inter-frame information. However, existing deep learning methods often rely on complicated network architectures, such as optical flow estimation, deformable convolution, and cross-frame self-attention layers, resulting in high computational costs. In this study, we propose a simple yet effective framework for video restoration. Our approach is based on grouped spatial-temporal shift, which is a lightweight and straightforward technique that can implicitly capture inter-frame correspondences for multi-frame aggregation. By introducing grouped spatial shift, we attain expansive effective receptive fields. Combined with basic 2D convolution, this simple framework can effectively aggregate inter-frame information. Extensive experiments demonstrate that our framework outperforms the previous state-of-the-art method, while using less than a quarter of its computational cost, on both video deblurring and video denoising tasks. These results indicate the potential for our approach to significantly reduce computational overhead while maintaining high-quality results. Code is avaliable at https://github.com/dasongli1/Shift-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">891.SliceMatch: Geometry-Guided Aggregation for Cross-View Pose Estimation</span><br>
                <span class="as">Lentsch, TedandXia, ZiminandCaesar, HolgerandKooij, JulianF.P.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lentsch_SliceMatch_Geometry-Guided_Aggregation_for_Cross-View_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17225-17234.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决跨视角的相机位姿估计问题，即确定给定地面图像相对于局部区域航拍图像的3自由度相机位姿。<br>
                    动机：现有的方法在处理跨视角的相机位姿估计问题上效果不佳，需要更高效准确的算法。<br>
                    方法：提出SliceMatch方法，包括地面和航拍特征提取器、特征聚合器和位姿预测器。特征提取器从地面和航拍图像中提取密集特征；特征聚合器在给定一组候选相机位姿的情况下，构造单个地面描述符和一组与位姿相关的航拍描述符。特别的是，我们的新型航拍特征聚合器具有一个用于地面引导的航拍特征选择的跨视图注意力模块，并利用地面相机视锥体在航拍图像上的几何投影来池化特征。使用预先计算的掩码来实现高效的航拍描述符构建。SliceMatch使用对比学习进行训练，将位姿估计形式化为地面描述符和航拍描述符之间的相似性比较。<br>
                    效果：与最先进的方法相比，SliceMatch在VIGOR基准测试上使用相同的VGG16主干网络实现了19%的中值定位误差降低，每秒150帧，使用ResNet50主干网络时误差降低了50%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work addresses cross-view camera pose estimation, i.e., determining the 3-Degrees-of-Freedom camera pose of a given ground-level image w.r.t. an aerial image of the local area. We propose SliceMatch, which consists of ground and aerial feature extractors, feature aggregators, and a pose predictor. The feature extractors extract dense features from the ground and aerial images. Given a set of candidate camera poses, the feature aggregators construct a single ground descriptor and a set of pose-dependent aerial descriptors. Notably, our novel aerial feature aggregator has a cross-view attention module for ground-view guided aerial feature selection and utilizes the geometric projection of the ground camera's viewing frustum on the aerial image to pool features. The efficient construction of aerial descriptors is achieved using precomputed masks. SliceMatch is trained using contrastive learning and pose estimation is formulated as a similarity comparison between the ground descriptor and the aerial descriptors. Compared to the state-of-the-art, SliceMatch achieves a 19% lower median localization error on the VIGOR benchmark using the same VGG16 backbone at 150 frames per second, and a 50% lower error when using a ResNet50 backbone.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">892.Learning Rotation-Equivariant Features for Visual Correspondence</span><br>
                <span class="as">Lee, JongminandKim, ByungjinandKim, SeungwookandCho, Minsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Learning_Rotation-Equivariant_Features_for_Visual_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21887-21897.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提取具有旋转不变性的判别性局部特征，以建立图像之间的对应关系。<br>
                    动机：现有的方法需要复杂的数据增强才能学习到旋转等变的特征和它们的取向，而我们的方法通过使用群等变CNNs，可以有效地学习到旋转等变的特征和它们的取向。<br>
                    方法：我们提出了一种自监督学习框架，利用群等变CNNs来提取判别性的旋转不变描述符。我们的方法进一步通过群体对齐这一新颖的不变映射技术处理所得的特征和它们的取向，该技术沿着群维度按其取向移动群等变特征，从而实现旋转不变性，同时避免了群维度的塌陷和判别性的丧失。<br>
                    效果：我们的方法在端到端的自我监督方式下进行训练，并在各种旋转条件下实现了最先进的匹配精度，同时也在关键点匹配和相机位姿估计任务上表现出了竞争力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Extracting discriminative local features that are invariant to imaging variations is an integral part of establishing correspondences between images. In this work, we introduce a self-supervised learning framework to extract discriminative rotation-invariant descriptors using group-equivariant CNNs. Thanks to employing group-equivariant CNNs, our method effectively learns to obtain rotation-equivariant features and their orientations explicitly, without having to perform sophisticated data augmentations. The resultant features and their orientations are further processed by group aligning, a novel invariant mapping technique that shifts the group-equivariant features by their orientations along the group dimension. Our group aligning technique achieves rotation-invariance without any collapse of the group dimension and thus eschews loss of discriminability. The proposed method is trained end-to-end in a self-supervised manner, where we use an orientation alignment loss for the orientation estimation and a contrastive descriptor loss for robust local descriptors to geometric/photometric variations. Our method demonstrates state-of-the-art matching accuracy among existing rotation-invariant descriptors under varying rotation and also shows competitive results when transferred to the task of keypoint matching and camera pose estimation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">893.Dynamic Focus-Aware Positional Queries for Semantic Segmentation</span><br>
                <span class="as">He, HaoyuandCai, JianfeiandPan, ZizhengandLiu, JingandZhang, JingandTao, DachengandZhuang, Bohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Dynamic_Focus-Aware_Positional_Queries_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11299-11308.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高语义分割的准确性和精度。<br>
                    动机：现有的端到端训练的查询集在语义分割中取得了突破，但依赖于可学习的参数化位置查询，这可能导致数据集统计信息的编码不准确，从而影响定位准确性。<br>
                    方法：提出了一种名为动态焦点感知位置查询（DFPQ）的简单而有效的查询设计，根据前解码器块的交叉注意力分数和相应图像特征的位置编码动态生成位置查询。同时，通过仅基于低分辨率交叉注意力分数聚合上下文令牌来执行局部关系聚合，以高效处理高分辨率交叉注意力。<br>
                    效果：在ADE20K和Cityscapes上的大量实验表明，通过对Mask2former进行两项修改，我们的框架实现了最先进的性能，并在ADE20K验证集上分别以ResNet-50、Swin-T和Swin-B主干网络获得了1.1%、1.9%和1.1%的单尺度mIoU，明显优于Mask2former。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The DETR-like segmentors have underpinned the most recent breakthroughs in semantic segmentation, which end-to-end train a set of queries representing the class prototypes or target segments. Recently, masked attention is proposed to restrict each query to only attend to the foreground regions predicted by the preceding decoder block for easier optimization. Although promising, it relies on the learnable parameterized positional queries which tend to encode the dataset statistics, leading to inaccurate localization for distinct individual queries. In this paper, we propose a simple yet effective query design for semantic segmentation termed Dynamic Focus-aware Positional Queries (DFPQ), which dynamically generates positional queries conditioned on the cross-attention scores from the preceding decoder block and the positional encodings for the corresponding image features, simultaneously. Therefore, our DFPQ preserves rich localization information for the target segments and provides accurate and fine-grained positional priors. In addition, we propose to efficiently deal with high-resolution cross-attention by only aggregating the contextual tokens based on the low-resolution cross-attention scores to perform local relation aggregation. Extensive experiments on ADE20K and Cityscapes show that with the two modifications on Mask2former, our framework achieves SOTA performance and outperforms Mask2former by clear margins of 1.1%, 1.9%, and 1.1% single-scale mIoU with ResNet-50, Swin-T, and Swin-B backbones on the ADE20K validation set, respectively. Source code is available at https://github.com/ziplab/FASeg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">894.PointConvFormer: Revenge of the Point-Based Convolution</span><br>
                <span class="as">Wu, WenxuanandFuxin, LiandShan, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_PointConvFormer_Revenge_of_the_Point-Based_Convolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21802-21813.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的点云深度学习网络架构构建模块——PointConvFormer。<br>
                    动机：受到泛化理论的启发，PointConvFormer结合了基于相对位置的点卷积和利用特征注意力的Transformers的思想。<br>
                    方法：在PointConvFormer中，通过计算点在邻域内的特征差来得到注意力，并用该注意力修改每个点的卷积权重。这样既保留了点卷积的不变性，又通过注意力选择了邻域内的相关信息进行卷积。<br>
                    效果：我们在多个数据集上进行了语义分割和场景流估计任务的实验，包括ScanNet、SemanticKitti、FlyingThings3D和KITTI。实验结果显示，PointConvFormer在性能上大大超过了经典的卷积、常规的Transformers以及体素化的稀疏卷积方法，并且其网络更小更快。可视化结果也显示，PointConvFormer在平坦区域的表现与卷积相近，而在物体边界的选择效应更强，证明它兼具两者的优点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce PointConvFormer, a novel building block for point cloud based deep network architectures. Inspired by generalization theory, PointConvFormer combines ideas from point convolution, where filter weights are only based on relative position, and Transformers which utilize feature-based attention. In PointConvFormer, attention computed from feature difference between points in the neighborhood is used to modify the convolutional weights at each point. Hence, we preserved the invariances from point convolution, whereas attention helps to select relevant points in the neighborhood for convolution. We experiment on both semantic segmentation and scene flow estimation tasks on point clouds with multiple datasets including ScanNet, SemanticKitti, FlyingThings3D and KITTI. Our results show that PointConvFormer substantially outperforms classic convolutions, regular transformers, and voxelized sparse convolution approaches with much smaller and faster networks. Visualizations show that PointConvFormer performs similarly to convolution on flat areas, whereas the neighborhood selection effect is stronger on object boundaries, showing that it has got the best of both worlds. The code will be available with the final version.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">895.BiFormer: Vision Transformer With Bi-Level Routing Attention</span><br>
                <span class="as">Zhu, LeiandWang, XinjiangandKe, ZhanghanandZhang, WayneandLau, RynsonW.H.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10323-10333.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低视觉转换器中的注意力机制的计算负担和内存占用。<br>
                    动机：现有的方法通过限制注意力操作在局部窗口、轴向条纹或扩张窗口内，引入了手工制作的和与内容无关的稀疏性来解决这个问题。<br>
                    方法：我们提出了一种新的动态稀疏注意力机制，通过双层路由实现更灵活的内容感知计算分配。具体来说，对于查询，首先在粗粒度的区域级别上过滤掉无关的键值对，然后在剩余的候选区域（即路由区域）上应用细粒度的token-to-token注意力。<br>
                    效果：我们提出的双层路由注意力机制在节省计算和内存的同时，只涉及到GPU友好的密集矩阵乘法。基于这种注意力机制，我们构建了一个新的通用视觉转换器，名为BiFormer。实验结果表明，这种方法在图像分类、目标检测和语义分割等计算机视觉任务上具有很好的性能和高效的计算效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As the core building block of vision transformers, attention is a powerful tool to capture long-range dependency. However, such power comes at a cost: it incurs a huge computation burden and heavy memory footprint as pairwise token interaction across all spatial locations is computed. A series of works attempt to alleviate this problem by introducing handcrafted and content-agnostic sparsity into attention, such as restricting the attention operation to be inside local windows, axial stripes, or dilated windows. In contrast to these approaches, we propose a novel dynamic sparse attention via bi-level routing to enable a more flexible allocation of computations with content awareness. Specifically, for a query, irrelevant key-value pairs are first filtered out at a coarse region level, and then fine-grained token-to-token attention is applied in the union of remaining candidate regions (i.e., routed regions). We provide a simple yet effective implementation of the proposed bi-level routing attention, which utilizes the sparsity to save both computation and memory while involving only GPU-friendly dense matrix multiplications. Built with the proposed bi-level routing attention, a new general vision transformer, named BiFormer, is then presented. As BiFormer attends to a small subset of relevant tokens in a query-adaptive manner without distraction from other irrelevant ones, it enjoys both good performance and high computational efficiency, especially in dense prediction tasks. Empirical results across several computer vision tasks such as image classification, object detection, and semantic segmentation verify the effectiveness of our design. Code is available at https://github.com/rayleizhu/BiFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">896.RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo</span><br>
                <span class="as">Cai, ChangjiangandJi, PanandYan, QinganandXu, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_RIAV-MVS_Recurrent-Indexing_an_Asymmetric_Volume_for_Multi-View_Stereo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/919-928.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从已定位的图像中进行多视角深度估计。<br>
                    动机：现有的方法在处理多视角几何信息的编码上存在不足，需要改进。<br>
                    方法：提出一种学习优化的方法，通过迭代索引平面扫描成本体积和利用卷积门控循环单元（GRU）回归深度图。同时，对参考图像引入变换器块打破Siamese网络的对称性以提取全局特征，并使用残差姿态网络纠正参考和源图像之间的位姿误差。<br>
                    效果：在真实世界的MVS数据集上进行的大量实验表明，该方法在数据集内评估和跨数据集泛化方面均达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a learning-based method for multi-view depth estimation from posed images. Our core idea is a "learning-to-optimize" paradigm that iteratively indexes a plane-sweeping cost volume and regresses the depth map via a convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a paramount role in encoding the multi-view geometry, we aim to improve its construction both at pixel- and frame- levels. At the pixel level, we propose to break the symmetry of the Siamese network (which is typically used in MVS to extract image features) by introducing a transformer block to the reference image (but not to the source images). Such an asymmetric volume allows the network to extract global features from the reference image to predict its depth map. Given potential inaccuracies in the poses between reference and source images, we propose to incorporate a residual pose network to correct the relative poses. This essentially rectifies the cost volume at the frame level. We conduct extensive experiments on real-world MVS datasets and show that our method achieves state-of-the-art performance in terms of both within-dataset evaluation and cross-dataset generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">897.VectorFloorSeg: Two-Stream Graph Attention Network for Vectorized Roughcast Floorplan Segmentation</span><br>
                <span class="as">Yang, BingchenandJiang, HaiyongandPan, HaoandXiao, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_VectorFloorSeg_Two-Stream_Graph_Attention_Network_for_Vectorized_Roughcast_Floorplan_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1358-1367.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决矢量图形（VG）中典型场景——粗糙地平面图的语义分割问题，其输出可直接用于进一步的应用如室内装饰和房间空间建模。<br>
                    动机：由于像素级分割忽略了矢量平面图中的常规元素（如线段），以往的语义分割工作在处理带有裸露墙体结构的粗糙地平面图时，常常产生模糊的边界和分割房间中的离群片段。<br>
                    方法：我们提出的方法充分利用了矢量平面图中的常规元素进行更完整的分割。通过将线段分类为房间边界，并将由线段划分的区域划分为房间部分，我们的流程从矢量平面图预测房间分割。为了充分利用线条和区域之间的结构关系，我们使用两流图神经网络分别处理线段和分区区域，并设计了一种新颖的调制图注意力层来融合来自一个流的异构信息到另一个流。<br>
                    效果：大量实验表明，直接在矢量平面图上操作，我们在mIoU和mAcc上都优于基于图像的方法。此外，我们提出了一种新的度量标准，可以捕获房间完整性和边界规则性，这证实了我们的方法产生的分割更加规则。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vector graphics (VG) are ubiquitous in industrial designs. In this paper, we address semantic segmentation of a typical VG, i.e., roughcast floorplans with bare wall structures, whose output can be directly used for further applications like interior furnishing and room space modeling. Previous semantic segmentation works mostly process well-decorated floorplans in raster images and usually yield aliased boundaries and outlier fragments in segmented rooms, due to pixel-level segmentation that ignores the regular elements (e.g. line segments) in vector floorplans. To overcome these issues, we propose to fully utilize the regular elements in vector floorplans for more integral segmentation. Our pipeline predicts room segmentation from vector floorplans by dually classifying line segments as room boundaries, and regions partitioned by line segments as room segments. To fully exploit the structural relationships between lines and regions, we use two-stream graph neural networks to process the line segments and partitioned regions respectively, and devise a novel modulated graph attention layer to fuse the heterogeneous information from one stream to the other. Extensive experiments show that by directly operating on vector floorplans, we outperform image-based methods in both mIoU and mAcc. In addition, we propose a new metric that captures room integrity and boundary regularity, which confirms that our method produces much more regular segmentations. Source code is available at https://github.com/DrZiji/VecFloorSeg</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">898.Dynamic Aggregated Network for Gait Recognition</span><br>
                <span class="as">Ma, KangandFu, YingandZheng, DezhiandCao, ChunshuiandHu, XuecaiandHuang, Yongzhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Dynamic_Aggregated_Network_for_Gait_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22076-22085.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决步态识别在现实场景中受到多种外部因素影响的问题，如携带条件、穿着外套和不同视角等。<br>
                    动机：现有的深度学习步态识别方法往往只提取一个显著特征，没有充分考虑关键区域步态特征之间的关系，并忽视了完整运动模式的聚合。<br>
                    方法：本文提出了一种新的观点，即实际步态特征包括多个关键区域的全局运动模式，每个全局运动模式由一系列局部运动模式组成。为此，我们提出了一种动态聚合网络（DANet）来学习更具判别性的步态特征。具体来说，我们在相邻像素的特征之间创建了一个动态注意力机制，该机制不仅自适应地关注关键区域，而且生成更具表现力的局部运动模式。此外，我们还开发了一种自注意力机制来选择代表性的局部运动模式，并进一步学习稳健的全局运动模式。<br>
                    效果：在三个流行的公共步态数据集（CASIA-B、OUMVLP和Gait3D）上进行的大量实验表明，所提出的方法可以显著提高当前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gait recognition is beneficial for a variety of applications, including video surveillance, crime scene investigation, and social security, to mention a few. However, gait recognition often suffers from multiple exterior factors in real scenes, such as carrying conditions, wearing overcoats, and diverse viewing angles. Recently, various deep learning-based gait recognition methods have achieved promising results, but they tend to extract one of the salient features using fixed-weighted convolutional networks, do not well consider the relationship within gait features in key regions, and ignore the aggregation of complete motion patterns. In this paper, we propose a new perspective that actual gait features include global motion patterns in multiple key regions, and each global motion pattern is composed of a series of local motion patterns. To this end, we propose a Dynamic Aggregation Network (DANet) to learn more discriminative gait features. Specifically, we create a dynamic attention mechanism between the features of neighboring pixels that not only adaptively focuses on key regions but also generates more expressive local motion patterns. In addition, we develop a self-attention mechanism to select representative local motion patterns and further learn robust global motion patterns. Extensive experiments on three popular public gait datasets, i.e., CASIA-B, OUMVLP, and Gait3D, demonstrate that the proposed method can provide substantial improvements over the current state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">899.3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud</span><br>
                <span class="as">Feng, MingtaoandHou, HaoranandZhang, LiangandWu, ZijieandGuo, YulanandMian, Ajmal</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_3D_Spatial_Multimodal_Knowledge_Accumulation_for_Scene_Graph_Prediction_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9182-9191.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地理解和预测3D场景中的对象关系和交互？<br>
                    动机：由于3D场景包含部分扫描的对象，物理连接紧密，大小不断变化，以及各种复杂的关系，现有的方法在有限的训练样本下表现不佳。<br>
                    方法：利用3D场景的物理空间固有的层次结构，将视觉内容和文本事实结合形成一个3D空间多模态知识图谱，并利用外部知识库作为基准。同时，提出一个知识驱动的场景图预测模块，利用3D空间知识有效规范关系语义空间。<br>
                    效果：实验证明该方法优于当前最先进的竞争对手。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In-depth understanding of a 3D scene not only involves locating/recognizing individual objects, but also requires to infer the relationships and interactions among them. However, since 3D scenes contain partially scanned objects with physical connections, dense placement, changing sizes, and a wide variety of challenging relationships, existing methods perform quite poorly with limited training samples. In this work, we find that the inherently hierarchical structures of physical space in 3D scenes aid in the automatic association of semantic and spatial arrangements, specifying clear patterns and leading to less ambiguous predictions. Thus, they well meet the challenges due to the rich variations within scene categories. To achieve this, we explicitly unify these structural cues of 3D physical spaces into deep neural networks to facilitate scene graph prediction. Specifically, we exploit an external knowledge base as a baseline to accumulate both contextualized visual content and textual facts to form a 3D spatial multimodal knowledge graph. Moreover, we propose a knowledge-enabled scene graph prediction module benefiting from the 3D spatial knowledge to effectively regularize semantic space of relationships. Extensive experiments demonstrate the superiority of the proposed method over current state-of-the-art competitors. Our code is available at https://github.com/HHrEtvP/SMKA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">900.Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation</span><br>
                <span class="as">Zhang, GuozhenandZhu, YuhanandWang, HaonanandChen, YouxinandWu, GangshanandWang, Limin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Extracting_Motion_and_Appearance_via_Inter-Frame_Attention_for_Efficient_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5682-5692.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地提取视频帧插值中帧间运动和外观信息。<br>
                    动机：以前的工作要么混合提取这两种类型的信息，要么为每种类型设计单独的模块，导致表示模糊和效率低下。<br>
                    方法：我们提出了一种新的模块，通过统一的操作显式提取运动和外观信息。具体来说，我们重新思考了帧间注意力的信息处理过程，并重用其注意力图进行外观特征增强和运动信息提取。<br>
                    效果：实验结果表明，对于固定和任意时间步长的插值，我们的方法在各种数据集上都达到了最先进的性能。同时，与性能相近的模型相比，我们的方法计算开销更小。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Effectively extracting inter-frame motion and appearance information is important for video frame interpolation (VFI). Previous works either extract both types of information in a mixed way or devise separate modules for each type of information, which lead to representation ambiguity and low efficiency. In this paper, we propose a new module to explicitly extract motion and appearance information via a unified operation. Specifically, we rethink the information process in inter-frame attention and reuse its attention map for both appearance feature enhancement and motion information extraction. Furthermore, for efficient VFI, our proposed module could be seamlessly integrated into a hybrid CNN and Transformer architecture. This hybrid pipeline can alleviate the computational complexity of inter-frame attention as well as preserve detailed low-level structure information. Experimental results demonstrate that, for both fixed- and arbitrary-timestep interpolation, our method achieves state-of-the-art performance on various datasets. Meanwhile, our approach enjoys a lighter computation overhead over models with close performance. The source code and models are available at https://github.com/MCG-NJU/EMA-VFI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">901.ViTs for SITS: Vision Transformers for Satellite Image Time Series</span><br>
                <span class="as">Tarasiou, MichailandChavez, ErikandZafeiriou, Stefanos</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tarasiou_ViTs_for_SITS_Vision_Transformers_for_Satellite_Image_Time_Series_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10418-10428.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文介绍了一种基于视觉转换器的全注意力模型，用于处理卫星图像时间序列（SITS）。<br>
                    动机：与自然图像不同，对于SITS处理，先时空后的方式更为直观。<br>
                    方法：将SITS记录分割成非重叠的空间和时间块进行标记化，然后通过分解的时空编码器进行处理。同时引入了两个新的机制，即获取时间的特定时间位置编码和多个可学习的类别令牌，以提高模型的判别能力。<br>
                    效果：通过广泛的消融研究评估了所有新设计选择的效果。所提出的架构在三个公开可用的SITS语义分割和分类数据集上取得了最先进的性能，大大超过了以前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classification datasets. All model, training and evaluation codes can be found at https://github.com/michaeltrs/DeepSatModels.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">902.Graph Transformer GANs for Graph-Constrained House Generation</span><br>
                <span class="as">Tang, HaoandZhang, ZhenyuandShi, HumphreyandLi, BoandShao, LingandSebe, NicuandTimofte, RaduandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Graph_Transformer_GANs_for_Graph-Constrained_House_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2173-2182.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地学习图节点关系，以应对具有挑战性的图约束房屋生成任务。<br>
                    动机：现有的方法在处理图约束的房屋生成任务时，无法有效地捕捉图节点间的全局和局部交互信息。<br>
                    方法：提出了一种新颖的图变压器生成对抗网络（GTGAN），该网络包括一个结合了图卷积和自注意力的变压器编码器，用于建模连接和非连接图节点之间的局部和全局交互。同时，还提出了一种新的基于节点分类的判别器，以保留不同房屋组件的高级别语义和判别性节点特征。<br>
                    效果：在两个具有挑战性的图约束房屋生成任务（即房屋布局和屋顶生成）上进行的实验表明，GTGAN在客观定量评分和主观视觉逼真度方面均表现出良好的效果，并在这两个任务上都取得了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel graph Transformer generative adversarial network (GTGAN) to learn effective graph node relations in an end-to-end fashion for the challenging graph-constrained house generation task. The proposed graph-Transformer-based generator includes a novel graph Transformer encoder that combines graph convolutions and self-attentions in a Transformer to model both local and global interactions across connected and non-connected graph nodes. Specifically, the proposed connected node attention (CNA) and non-connected node attention (NNA) aim to capture the global relations across connected nodes and non-connected nodes in the input graph, respectively. The proposed graph modeling block (GMB) aims to exploit local vertex interactions based on a house layout topology. Moreover, we propose a new node classification-based discriminator to preserve the high-level semantic and discriminative node features for different house components. Finally, we propose a novel graph-based cycle-consistency loss that aims at maintaining the relative spatial relationships between ground truth and predicted graphs. Experiments on two challenging graph-constrained house generation tasks (i.e., house layout and roof generation) with two public datasets demonstrate the effectiveness of GTGAN in terms of objective quantitative scores and subjective visual realism. New state-of-the-art results are established by large margins on both tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">903.LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising</span><br>
                <span class="as">Wang, ZichunandFu, YingandLiu, JiandZhang, Yulun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LG-BPN_Local_and_Global_Blind-Patch_Network_for_Self-Supervised_Real-World_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18156-18165.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数自我监督的去噪方法在真实噪声下失败，因为存在强烈的空间噪声相关性。<br>
                    动机：针对现实世界的去噪方法，要么忽视了这种空间相关性，要么由于未充分考虑相关性而破坏了精细纹理。<br>
                    方法：提出了一种新的自监督真实世界去噪方法LG-BPN，该方法将空间相关性统计纳入网络设计中进行局部细节恢复，并为先前基于CNN的BSN方法带来了长范围依赖性建模能力。具体包括基于相关性统计的密集采样补丁掩蔽卷积模块和允许在BSN中利用远程上下文的扩张Transformer块。<br>
                    效果：广泛的实验结果证明，LG-BPN能够充分利用详细结构和全局交互，并在真实世界数据集上表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the significant results on synthetic noise under simplified assumptions, most self-supervised denoising methods fail under real noise due to the strong spatial noise correlation, including the advanced self-supervised blind-spot networks (BSNs). For recent methods targeting real-world denoising, they either suffer from ignoring this spatial correlation, or are limited by the destruction of fine textures for under-considering the correlation. In this paper, we present a novel method called LG-BPN for self-supervised real-world denoising, which takes the spatial correlation statistic into our network design for local detail restoration, and also brings the long-range dependencies modeling ability to previously CNN-based BSN methods. First, based on the correlation statistic, we propose a densely-sampled patch-masked convolution module. By taking more neighbor pixels with low noise correlation into account, we enable a denser local receptive field, preserving more useful information for enhanced fine structure recovery. Second, we propose a dilated Transformer block to allow distant context exploitation in BSN. This global perception addresses the intrinsic deficiency of BSN, whose receptive field is constrained by the blind spot requirement, which can not be fully resolved by the previous CNN-based BSNs. These two designs enable LG-BPN to fully exploit both the detailed structure and the global interaction in a blind manner. Extensive results on real-world datasets demonstrate the superior performance of our method. https://github.com/Wang-XIaoDingdd/LGBPN</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">904.Self-Positioning Point-Based Transformer for Point Cloud Understanding</span><br>
                <span class="as">Park, JinyoungandLee, SanghyeokandKim, SihyeonandXiong, YunyangandKim, HyunwooJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Self-Positioning_Point-Based_Transformer_for_Point_Cloud_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21814-21823.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将Transformers直接应用于点云数据，以解决其二次成本问题。<br>
                    动机：尽管Transformers在计算机视觉任务上表现出色，但直接应用于点云数据的复杂性较高。<br>
                    方法：提出一种基于自我定位点的Transformer（SPoTr）架构，该架构由局部自我注意力和基于输入形状自适应定位的自我定位点全局交叉注意力组成。<br>
                    效果：实验证明，SPoTr在形状分类、部分分割和场景分割等三个点云任务上均有效，并在形状分类任务上比之前的最佳模型提高了2.6%的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformers have shown superior performance on various computer vision tasks with their capabilities to capture long-range dependencies. Despite the success, it is challenging to directly apply Transformers on point clouds due to their quadratic cost in the number of points. In this paper, we present a Self-Positioning point-based Transformer (SPoTr), which is designed to capture both local and global shape contexts with reduced complexity. Specifically, this architecture consists of local self- attention and self-positioning point-based global cross-attention. The self-positioning points, adaptively located based on the input shape, consider both spatial and semantic information with disentangled attention to improve expressive power. With the self-positioning points, we propose a novel global cross-attention mechanism for point clouds, which improves the scalability of global self-attention by allowing the attention module to compute attention weights with only a small set of self-positioning points. Experiments show the effectiveness of SPoTr on three point cloud tasks such as shape classification, part segmentation, and scene segmentation. In particular, our proposed model achieves an accuracy gain of 2.6% over the previous best models on shape classification with ScanObjectNN. We also provide qualitative analyses to demonstrate the interpretability of self-positioning points. The code of SPoTr is available at https://github.com/mlvlab/SPoTr.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">905.Learning Dynamic Style Kernels for Artistic Style Transfer</span><br>
                <span class="as">Xu, WenjuandLong, ChengjiangandNie, Yongwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Dynamic_Style_Kernels_for_Artistic_Style_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10083-10092.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行艺术图像生成的任意风格转换。<br>
                    动机：现有的方法要么忽视局部细节全局地修改内容特征，要么过于关注局部结构细节导致风格泄露。<br>
                    方法：提出一种新的“风格内核”方案，通过学习空间自适应内核进行逐像素的风格化，其中卷积核是从全局风格-内容对齐的特征动态生成的，然后应用学到的内核来调整每个空间位置的内容特征。<br>
                    效果：该方法在视觉质量和效率方面优于现有方法，表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Arbitrary style transfer has been demonstrated to be efficient in artistic image generation. Previous methods either globally modulate the content feature ignoring local details, or overly focus on the local structure details leading to style leakage. In contrast to the literature, we propose a new scheme "style kernel" that learns spatially adaptive kernel for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned feature and then the learned kernels are applied to modulate the content feature at each spatial position. This new scheme allows flexible both global and local interactions between the content and style features such that the wanted styles can be easily transferred to the content image while at the same time the content structure can be easily preserved. To further enhance the flexibility of our style transfer method, we propose a Style Alignment Encoding (SAE) module complemented with a Content-based Gating Modulation (CGM) module for learning the dynamic style kernels in focusing regions. Extensive experiments strongly demonstrate that our proposed method outperforms state-of-the-art methods and exhibits superior performance in terms of visual quality and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">906.OcTr: Octree-Based Transformer for 3D Object Detection</span><br>
                <span class="as">Zhou, ChaoandZhang, YananandChen, JiaxinandHuang, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_OcTr_Octree-Based_Transformer_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5166-5175.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从大规模的3D场景中捕获足够的特征，特别是对于遥远或被遮挡的对象。<br>
                    动机：尽管Transformers具有长序列建模能力，但由于其感受野不足或全局关联粗糙，导致在准确性和效率上无法达到平衡。<br>
                    方法：本文提出了一种基于八叉树的Transformer模型OcTr。首先，通过在顶层进行自注意力运算构建了一个动态的八叉树，然后递归地向下一层传播，受限于八分体，以粗到细的方式捕捉丰富的全局上下文，同时保持计算复杂度在可控范围内。此外，为了增强前景感知，我们提出了一种混合位置嵌入，由语义感知位置嵌入和注意力掩码组成，以充分利用语义和几何线索。<br>
                    效果：在Waymo开放数据集和KITTI数据集上进行了大量实验，OcTr达到了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A key challenge for LiDAR-based 3D object detection is to capture sufficient features from large scale 3D scenes especially for distant or/and occluded objects. Albeit recent efforts made by Transformers with the long sequence modeling capability, they fail to properly balance the accuracy and efficiency, suffering from inadequate receptive fields or coarse-grained holistic correlations. In this paper, we propose an Octree-based Transformer, named OcTr, to address this issue. It first constructs a dynamic octree on the hierarchical feature pyramid through conducting self-attention on the top level and then recursively propagates to the level below restricted by the octants, which captures rich global context in a coarse-to-fine manner while maintaining the computational complexity under control. Furthermore, for enhanced foreground perception, we propose a hybrid positional embedding, composed of the semantic-aware positional embedding and attention mask, to fully exploit semantic and geometry clues. Extensive experiments are conducted on the Waymo Open Dataset and KITTI Dataset, and OcTr reaches newly state-of-the-art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">907.GeoMAE: Masked Geometric Target Prediction for Self-Supervised Point Cloud Pre-Training</span><br>
                <span class="as">Tian, XiaoyuandRan, HaoxiandWang, YueandZhao, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_GeoMAE_Masked_Geometric_Target_Prediction_for_Self-Supervised_Point_Cloud_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13570-13580.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决点云自监督学习中的一个重要问题：在没有标注的情况下，我们应该利用什么信号来从点云中学习特征？<br>
                    动机：现有的直接采用掩蔽自动编码器（MAE）并仅从被遮蔽的点云中预测原始坐标或占用率的方法，忽视了图像和点云之间的差异。<br>
                    方法：本文提出了一种基于几何特征重建的点云表示学习框架。通过重新审视图像和点云的差异，确定了三个特定于点云的自监督学习目标，即质心预测、法线估计和曲率预测。这三个目标共同构成了一个非平凡的自监督学习任务，并使模型更好地推理点云的精细几何结构。<br>
                    效果：在nuScene数据集上，该方法在物体检测、分割和多目标跟踪方面分别实现了3.38 mAP、2.1 mIoU和1.7 AMOTA的性能提升。在Waymo开放数据集上也取得了显著的性能改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper tries to address a fundamental question in point cloud self-supervised learning: what is a good signal we should leverage to learn features from point clouds without annotations? To answer that, we introduce a point cloud representation learning framework, based on geometric feature reconstruction. In contrast to recent papers that directly adopt masked autoencoder (MAE) and only predict original coordinates or occupancy from masked point clouds, our method revisits differences between images and point clouds and identifies three self-supervised learning objectives peculiar to point clouds, namely centroid prediction, normal estimation, and curvature prediction. Combined, these three objectives yield an nontrivial self-supervised learning task and mutually facilitate models to better reason fine-grained geometry of point clouds. Our pipeline is conceptually simple and it consists of two major steps: first, it randomly masks out groups of points, followed by a Transformer-based point cloud encoder; second, a lightweight Transformer decoder predicts centroid, normal, and curvature for points in each voxel. We transfer the pre-trained Transformer encoder to a downstream peception model. On the nuScene Datset, our model achieves 3.38 mAP improvment for object detection, 2.1 mIoU gain for segmentation, and 1.7 AMOTA gain for multi-object tracking. We also conduct experiments on the Waymo Open Dataset and achieve significant performance improvements over baselines as well.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">908.PVT-SSD: Single-Stage 3D Object Detector With Point-Voxel Transformer</span><br>
                <span class="as">Yang, HonghuiandWang, WenxiaoandChen, MinghaoandLin, BinbinandHe, TongandChen, HuaandHe, XiaofeiandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_PVT-SSD_Single-Stage_3D_Object_Detector_With_Point-Voxel_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13476-13487.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于Transformer的3D物体检测器在点云特征学习上存在时间消耗大或引入量化误差的问题。<br>
                    动机：提出一种新的单阶段3D检测方法，结合了点和体素两种表示的优点。<br>
                    方法：首先使用体素稀疏卷积进行有效的特征编码，然后提出一个点-体素变换器（PVT）模块，从体素中获取长范围的上下文信息，同时从点中获得准确的位置信息。通过输入依赖的查询初始化模块将这两种不同的表示关联起来。<br>
                    效果：实验证明该方法在自动驾驶基准测试上具有高效性和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent Transformer-based 3D object detectors learn point cloud features either from point- or voxel-based representations. However, the former requires time-consuming sampling while the latter introduces quantization errors. In this paper, we present a novel Point-Voxel Transformer for single-stage 3D detection (PVT-SSD) that takes advantage of these two representations. Specifically, we first use voxel-based sparse convolutions for efficient feature encoding. Then, we propose a Point-Voxel Transformer (PVT) module that obtains long-range contexts in a cheap manner from voxels while attaining accurate positions from points. The key to associating the two different representations is our introduced input-dependent Query Initialization module, which could efficiently generate reference points and content queries. Then, PVT adaptively fuses long-range contextual and local geometric information around reference points into content queries. Further, to quickly find the neighboring points of reference points, we design the Virtual Range Image module, which generalizes the native range image to multi-sensor and multi-frame. The experiments on several autonomous driving benchmarks verify the effectiveness and efficiency of the proposed method. Code will be available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">909.Harmonious Feature Learning for Interactive Hand-Object Pose Estimation</span><br>
                <span class="as">Lin, ZhifengandDing, ChangxingandYao, HuanandKuang, ZengshengandHuang, Shaoli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12989-12998.png><br>
            
            <span class="tt"><span class="t0">研究问题：单张图像中手和物体的姿态估计由于严重遮挡而极具挑战性。<br>
                    动机：现有的方法通常首先从单个主干网络提取粗略的手和物体特征，然后通过交互模块相互参考进一步优化它们，但这些方法通常忽视了手和物体在特征学习中的竞争力。<br>
                    方法：本文提出了一种新的和谐特征学习网络（HFL-Net）。HFL-Net引入了一个新的框架，结合了单流和双流主干的优缺点：共享通用ResNet-50模型的手和物体的低级别和高级别卷积层的参数，中间层不共享。这种策略使得中间层能够将手和物体作为唯一的目标进行提取，避免了它们在特征学习中的竞争力。共享的高级别层也迫使它们的特征保持和谐，从而促进它们的相互特征增强。<br>
                    效果：实验结果表明，我们的方法在流行的HO3D和Dex-YCB数据库上始终优于最先进的方法。特别地，我们的模型在手部姿态估计上的性能甚至超过了只执行单手姿态估计任务的现有工作。代码可在https://github.com/lzfff12/HFL-Net获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Joint hand and object pose estimation from a single image is extremely challenging as serious occlusion often occurs when the hand and object interact. Existing approaches typically first extract coarse hand and object features from a single backbone, then further enhance them with reference to each other via interaction modules. However, these works usually ignore that the hand and object are competitive in feature learning, since the backbone takes both of them as foreground and they are usually mutually occluded. In this paper, we propose a novel Harmonious Feature Learning Network (HFL-Net). HFL-Net introduces a new framework that combines the advantages of single- and double-stream backbones: it shares the parameters of the low- and high-level convolutional layers of a common ResNet-50 model for the hand and object, leaving the middle-level layers unshared. This strategy enables the hand and the object to be extracted as the sole targets by the middle-level layers, avoiding their competition in feature learning. The shared high-level layers also force their features to be harmonious, thereby facilitating their mutual feature enhancement. In particular, we propose to enhance the feature of the hand via concatenation with the feature in the same location from the object stream. A subsequent self-attention layer is adopted to deeply fuse the concatenated feature. Experimental results show that our proposed approach consistently outperforms state-of-the-art methods on the popular HO3D and Dex-YCB databases. Notably, the performance of our model on hand pose estimation even surpasses that of existing works that only perform the single-hand pose estimation task. Code is available at https://github.com/lzfff12/HFL-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">910.Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition</span><br>
                <span class="as">Guo, LemingandXue, WanliandGuo, QingandLiu, BoandZhang, KaihuaandYuan, TiantianandChen, Shengyong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Distilling_Cross-Temporal_Contexts_for_Continuous_Sign_Language_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10771-10780.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决手语识别中空间感知模块训练不足的问题。<br>
                    动机：现有的手语识别方法通常包含空间感知和时间聚合两个模块，但空间感知模块往往训练不足。<br>
                    方法：提出一种跨时态上下文聚合（CTCA）模型，构建双路径网络分别捕捉局部时态和全局时态的上下文信息，并设计了一种跨上下文知识蒸馏学习目标来聚合这两种类型和语言先验的上下文。<br>
                    效果：实验结果表明，该方法在具有挑战性的手语识别基准测试上优于所有最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continuous sign language recognition (CSLR) aims to recognize glosses in a sign language video. State-of-the-art methods typically have two modules, a spatial perception module and a temporal aggregation module, which are jointly learned end-to-end. Existing results in [9,20,25,36] have indicated that, as the frontal component of the overall model, the spatial perception module used for spatial feature extraction tends to be insufficiently trained. In this paper, we first conduct empirical studies and show that a shallow temporal aggregation module allows more thorough training of the spatial perception module. However, a shallow temporal aggregation module cannot well capture both local and global temporal context information in sign language. To address this dilemma, we propose a cross-temporal context aggregation (CTCA) model. Specifically, we build a dual-path network that contains two branches for perceptions of local temporal context and global temporal context. We further design a cross-context knowledge distillation learning objective to aggregate the two types of context and the linguistic prior. The knowledge distillation enables the resultant one-branch temporal aggregation module to perceive local-global temporal and semantic context. This shallow temporal perception module structure facilitates spatial perception module learning. Extensive experiments on challenging CSLR benchmarks demonstrate that our method outperforms all state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">911.ProxyFormer: Proxy Alignment Assisted Point Cloud Completion With Missing Part Sensitive Transformer</span><br>
                <span class="as">Li, ShanshanandGao, PanandTan, XiaoyangandWei, Mingqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ProxyFormer_Proxy_Alignment_Assisted_Point_Cloud_Completion_With_Missing_Part_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9466-9475.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从部分点云中恢复完整的点云？<br>
                    动机：由于设备缺陷或视角有限，捕获的点云可能是不完整的。因此，从部分点云中恢复完整的点云在许多实际任务中起着关键作用。<br>
                    方法：本文提出了一种新的点云补全方法——ProxyFormer，它将点云分为现有（输入）和缺失（待预测）两部分，并通过代理进行信息交流。具体来说，我们通过特征和位置提取器将信息融合到点代理中，并从现有点代理的特征生成缺失点代理的特征。然后，为了更好地感知缺失点的位置，我们设计了一个缺失部分敏感的变压器，它将随机正态分布转换为合理的位置信息，并使用代理对齐来细化缺失的代理。这使得预测的点代理更能感知缺失部分的特征和位置，从而使这些代理更适合后续的粗到细的过程。<br>
                    效果：实验结果表明，我们的方法在几个基准数据集上优于最先进的补全网络，并且具有最快的推理速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Problems such as equipment defects or limited viewpoints will lead the captured point clouds to be incomplete. Therefore, recovering the complete point clouds from the partial ones plays an vital role in many practical tasks, and one of the keys lies in the prediction of the missing part. In this paper, we propose a novel point cloud completion approach namely ProxyFormer that divides point clouds into existing (input) and missing (to be predicted) parts and each part communicates information through its proxies. Specifically, we fuse information into point proxy via feature and position extractor, and generate features for missing point proxies from the features of existing point proxies. Then, in order to better perceive the position of missing points, we design a missing part sensitive transformer, which converts random normal distribution into reasonable position information, and uses proxy alignment to refine the missing proxies. It makes the predicted point proxies more sensitive to the features and positions of the missing part, and thus makes these proxies more suitable for subsequent coarse-to-fine processes. Experimental results show that our method outperforms state-of-the-art completion networks on several benchmark datasets and has the fastest inference speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">912.FrustumFormer: Adaptive Instance-Aware Resampling for Multi-View 3D Detection</span><br>
                <span class="as">Wang, YuqiandChen, YuntaoandZhang, Zhaoxiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_FrustumFormer_Adaptive_Instance-Aware_Resampling_for_Multi-View_3D_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5096-5105.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将2D视角空间的特征转换为3D空间，以实现多视角3D物体检测。<br>
                    动机：目前的方法主要关注于视图转换的设计，但如何选择要转换的内容却很少被讨论。<br>
                    方法：本文提出了一种名为FrustumFormer的新框架，通过自适应实例感知重采样，更加关注实例区域的特征。模型利用图像视图对象提案在鸟瞰图上获取实例锥体，并学习实例锥体内的自适应占用掩码来细化实例位置。此外，时间锥体交会可以进一步减少对象的定位不确定性。<br>
                    效果：在nuScenes数据集上的全面实验证明了FrustumFormer的有效性，并在基准测试中实现了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The transformation of features from 2D perspective space to 3D space is essential to multi-view 3D object detection. Recent approaches mainly focus on the design of view transformation, either pixel-wisely lifting perspective view features into 3D space with estimated depth or grid-wisely constructing BEV features via 3D projection, treating all pixels or grids equally. However, choosing what to transform is also important but has rarely been discussed before. The pixels of a moving car are more informative than the pixels of the sky. To fully utilize the information contained in images, the view transformation should be able to adapt to different image regions according to their contents. In this paper, we propose a novel framework named FrustumFormer, which pays more attention to the features in instance regions via adaptive instance-aware resampling. Specifically, the model obtains instance frustums on the bird's eye view by leveraging image view object proposals. An adaptive occupancy mask within the instance frustum is learned to refine the instance location. Moreover, the temporal frustum intersection could further reduce the localization uncertainty of objects. Comprehensive experiments on the nuScenes dataset demonstrate the effectiveness of FrustumFormer, and we achieve a new state-of-the-art performance on the benchmark. Codes and models will be made available at https://github.com/Robertwyq/Frustum.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">913.Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation</span><br>
                <span class="as">Zhang, NingandNex, FrancescoandVosselman, GeorgeandKerle, Norman</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Lite-Mono_A_Lightweight_CNN_and_Transformer_Architecture_for_Self-Supervised_Monocular_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18537-18546.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一种轻量级但有效的自我监督单目深度估计模型，以部署在边缘设备上。<br>
                    动机：现有的架构大多使用更重的骨干网络，但牺牲了模型的尺寸。因此，设计轻量级的模型具有很高的研究价值。<br>
                    方法：本文提出了一种名为Lite-Mono的混合架构，该架构结合了CNNs和Transformers。其中，连续扩张卷积（CDC）模块用于提取丰富的多尺度局部特征，局部-全局特征交互（LGFI）模块则利用自注意力机制将长范围的全局信息编码到特征中。<br>
                    效果：实验证明，Lite-Mono在准确性上大大超过了Monodepth2，且参数数量减少了约80%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised monocular depth estimation that does not require ground truth for training has attracted attention in recent years. It is of high interest to design lightweight but effective models so that they can be deployed on edge devices. Many existing architectures benefit from using heavier backbones at the expense of model sizes. This paper achieves comparable results with a lightweight architecture. Specifically, the efficient combination of CNNs and Transformers is investigated, and a hybrid architecture called Lite-Mono is presented. A Consecutive Dilated Convolutions (CDC) module and a Local-Global Features Interaction (LGFI) module are proposed. The former is used to extract rich multi-scale local features, and the latter takes advantage of the self-attention mechanism to encode long-range global information into the features. Experiments demonstrate that Lite-Mono outperforms Monodepth2 by a large margin in accuracy, with about 80% fewer trainable parameters. Our codes and models are available at https://github.com/noahzn/Lite-Mono.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">914.Starting From Non-Parametric Networks for 3D Point Cloud Analysis</span><br>
                <span class="as">Zhang, RenruiandWang, LiuhuiandWang, YaliandGao, PengandLi, HongshengandShi, Jianbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Starting_From_Non-Parametric_Networks_for_3D_Point_Cloud_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5344-5353.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种无需参数或训练的非参数网络Point-NN，用于3D点云分析。<br>
                    动机：现有的3D模型需要大量的参数和训练，而Point-NN通过使用最远点采样、k近邻和池化操作等非学习组件，以及三角函数，可以无需参数或训练就能在各种3D任务上表现良好，甚至超越已完全训练的模型。<br>
                    方法：Point-NN由最远点采样、k近邻、池化操作和三角函数等非学习组件构成，无需参数或训练。此外，作者还提出了两种扩展方式：一是将Point-NN作为基础架构框架，插入线性层以构建参数网络；二是将Point-NN视为已训练3D模型的即插即用模块，在推理过程中增强现有方法的性能。<br>
                    效果：实验结果表明，Point-NN在各种3D任务上表现优秀，且其派生的Point-PN在性能效率方面具有优秀的权衡，只需少量可学习的参数。同时，Point-NN还可以作为已训练3D模型的插件模块，在不重新训练的情况下提升现有方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">915.3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention</span><br>
                <span class="as">Tang, ZhenhuaandQiu, ZhaofanandHao, YanbinandHong, RichangandYao, Ting</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4790-4799.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于变换器的3D人体姿态估计方法在计算关节间亲和性矩阵时，随着关节数量的增加，计算成本呈二次增长，尤其在视频序列中的姿态估计问题上，这一问题更为严重。<br>
                    动机：为了解决上述问题，本文提出了一种新的空间-时间交叉注意力（STC）模块，将相关性学习分解为空间和时间两部分。<br>
                    方法：STC首先将其输入特征沿通道维度均匀地切片成两个部分，然后分别对每个部分进行空间和时间的注意力处理。接着，STC通过连接来自注意力层的输出来同时模拟同一帧中的关节和同一轨迹中的关节之间的交互。在此基础上，我们设计了STCFormer，通过堆叠多个STC块，并在STCFormer中集成了一种新的结构增强的位置嵌入（SPE），以考虑人体结构。<br>
                    效果：在Human3.6M和MPI-INF-3DHP基准测试集上进行了大量实验，与最先进的方法相比，取得了优越的结果。特别是在具有挑战性的Human3.6M数据集上，STCFormer实现了迄今为止最好的公开性能：40.5mm的P1误差。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent transformer-based solutions have shown great success in 3D human pose estimation. Nevertheless, to calculate the joint-to-joint affinity matrix, the computational cost has a quadratic growth with the increasing number of joints. Such drawback becomes even worse especially for pose estimation in a video sequence, which necessitates spatio-temporal correlation spanning over the entire video. In this paper, we facilitate the issue by decomposing correlation learning into space and time, and present a novel Spatio-Temporal Criss-cross attention (STC) block. Technically, STC first slices its input feature into two partitions evenly along the channel dimension, followed by performing spatial and temporal attention respectively on each partition. STC then models the interactions between joints in an identical frame and joints in an identical trajectory simultaneously by concatenating the outputs from attention layers. On this basis, we devise STCFormer by stacking multiple STC blocks and further integrate a new Structure-enhanced Positional Embedding (SPE) into STCFormer to take the structure of human body into consideration. The embedding function consists of two components: spatio-temporal convolution around neighboring joints to capture local structure, and part-aware embedding to indicate which part each joint belongs to. Extensive experiments are conducted on Human3.6M and MPI-INF-3DHP benchmarks, and superior results are reported when comparing to the state-of-the-art approaches. More remarkably, STCFormer achieves to-date the best published performance: 40.5mm P1 error on the challenging Human3.6M dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">916.LoGoNet: Towards Accurate 3D Object Detection With Local-to-Global Cross-Modal Fusion</span><br>
                <span class="as">Li, XinandMa, TaoandHou, YuenanandShi, BotianandYang, YuchenandLiu, YouquanandWu, XingjiaoandChen, QinandLi, YikangandQiao, YuandHe, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LoGoNet_Towards_Accurate_3D_Object_Detection_With_Local-to-Global_Cross-Modal_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17524-17534.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用LiDAR和相机的融合方法进行3D物体检测。<br>
                    动机：现有的多模态方法主要进行全局融合，缺乏精细的区域级信息，导致融合性能不佳。<br>
                    方法：提出局部到全局融合网络（LoGoNet），在局部和全局级别进行LiDAR-camera融合。具体来说，全局融合部分基于先前的研究，使用点质心更精确地表示体素特征的位置，实现更好的跨模态对齐。对于局部融合部分，首先将每个提案划分为均匀网格，然后将这些网格中心投影到图像上。采样图像特征周围的投影网格点，与位置装饰的点云特征融合，最大限度地利用了提案周围的丰富上下文信息。进一步提出了特征动态聚合（FDA）模块，实现了局部和全局融合特征之间的信息交互，生成更具信息量的多模态特征。<br>
                    效果：在Waymo开放数据集（WOD）和KITTI数据集上的大量实验表明，LoGoNet优于所有最先进的3D检测方法。特别是在Waymo 3D物体检测排行榜上排名第一，获得了81.02 mAPH (L2)的检测性能。值得注意的是，首次同时在三个类别上的检测性能超过了80 APH (L2)。代码将在https://github.com/sankin97/LoGoNet上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR-camera fusion methods have shown impressive performance in 3D object detection. Recent advanced multi-modal methods mainly perform global fusion, where image features and point cloud features are fused across the whole scene. Such practice lacks fine-grained region-level information, yielding suboptimal fusion performance. In this paper, we present the novel Local-to-Global fusion network (LoGoNet), which performs LiDAR-camera fusion at both local and global levels. Concretely, the Global Fusion (GoF) of LoGoNet is built upon previous literature, while we exclusively use point centroids to more precisely represent the position of voxel features, thus achieving better cross-modal alignment. As to the Local Fusion (LoF), we first divide each proposal into uniform grids and then project these grid centers to the images. The image features around the projected grid points are sampled to be fused with position-decorated point cloud features, maximally utilizing the rich contextual information around the proposals. The Feature Dynamic Aggregation (FDA) module is further proposed to achieve information interaction between these locally and globally fused features, thus producing more informative multi-modal features. Extensive experiments on both Waymo Open Dataset (WOD) and KITTI datasets show that LoGoNet outperforms all state-of-the-art 3D detection methods. Notably, LoGoNet ranks 1st on Waymo 3D object detection leaderboard and obtains 81.02 mAPH (L2) detection performance. It is noteworthy that, for the first time, the detection performance on three classes surpasses 80 APH (L2) simultaneously. Code will be available at https://github.com/sankin97/LoGoNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">917.Representation Learning for Visual Object Tracking by Masked Appearance Transfer</span><br>
                <span class="as">Zhao, HaojieandWang, DongandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18696-18705.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究视觉对象跟踪中的表现学习方法。<br>
                    动机：目前大多数跟踪器直接使用ImageNet预训练表示，但很少有工作研究针对跟踪的特定表示学习方法。<br>
                    方法：提出一种基于编码器-解码器架构的简单而有效的跟踪特定表示学习方法——掩蔽外观转移。首先，对模板和搜索区域的视觉外观进行联合编码，然后分别解码。在解码过程中，原始搜索区域图像被重建。然而，对于模板，使解码器在搜索区域内重建目标外观。通过这种目标外观转移，学习到跟踪特定的表示。随机屏蔽输入，从而使学习到的表示更具判别性。<br>
                    效果：设计了一个简单的轻量级跟踪器来评估表示的目标定位和框回归。大量实验表明，该方法是有效的，学习到的表示可以使简单的跟踪器在六个数据集上获得最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual representation plays an important role in visual object tracking. However, few works study the tracking-specified representation learning method. Most trackers directly use ImageNet pre-trained representations. In this paper, we propose masked appearance transfer, a simple but effective representation learning method for tracking, based on an encoder-decoder architecture. First, we encode the visual appearances of the template and search region jointly, and then we decode them separately. During decoding, the original search region image is reconstructed. However, for the template, we make the decoder reconstruct the target appearance within the search region. By this target appearance transfer, the tracking-specified representations are learned. We randomly mask out the inputs, thereby making the learned representations more discriminative. For sufficient evaluation, we design a simple and lightweight tracker that can evaluate the representation for both target localization and box regression. Extensive experiments show that the proposed method is effective, and the learned representations can enable the simple tracker to obtain state-of-the-art performance on six datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">918.Neural Fourier Filter Bank</span><br>
                <span class="as">Wu, ZhijieandJin, YuheandYi, KwangMoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Neural_Fourier_Filter_Bank_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14153-14163.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新颖的方法，以提供高效且高度详细的重建。<br>
                    动机：受到小波的启发，学习一个能在空间和频率上分解信号的神经场。<br>
                    方法：采用基于网格的空间分解新近范例，通过傅里叶特征编码鼓励在每个网格中存储特定频率。然后应用多层感知机并激活正弦，将适当层的这些傅里叶编码的特征输入，使得高频组件依次累积在低频组件之上，最后将其求和形成最终输出。<br>
                    效果：在多个任务（2D图像拟合、3D形状重建和神经辐射场）上，该方法在模型紧凑性和收敛速度方面优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel method to provide efficient and highly detailed reconstructions. Inspired by wavelets, we learn a neural field that decompose the signal both spatially and frequency-wise. We follow the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourage specific frequencies to be stored in each grid via Fourier features encodings. We then apply a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which we sum up to form the final output. We demonstrate that our method outperforms the state of the art regarding model compactness and convergence speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">919.Self-Supervised Non-Uniform Kernel Estimation With Flow-Based Motion Prior for Blind Image Deblurring</span><br>
                <span class="as">Fang, ZhenxuanandWu, FangfangandDong, WeishengandLi, XinandWu, JinjianandShi, Guangming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Self-Supervised_Non-Uniform_Kernel_Estimation_With_Flow-Based_Motion_Prior_for_Blind_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18105-18114.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习方法在处理模糊图像恢复时，忽视了运动模糊的重要先验信息，导致实际场景下性能下降。<br>
                    动机：为了解决这一问题，我们提出了一种利用正则化流将运动模糊核场表示为潜在空间，并设计卷积神经网络预测潜在码而非运动核的新方法。<br>
                    方法：我们引入了不确定性学习来提高非均匀核估计的准确性和鲁棒性，并提出了一个多尺度核注意力模块以更好地整合图像特征与估计的核。<br>
                    效果：大量的实验结果，特别是在真实世界的模糊数据集上，表明我们的方法在主观和客观质量以及非均匀图像去模糊的泛化性能方面都取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many deep learning-based solutions to blind image deblurring estimate the blur representation and reconstruct the target image from its blurry observation. However, these methods suffer from severe performance degradation in real-world scenarios because they ignore important prior information about motion blur (e.g., real-world motion blur is diverse and spatially varying). Some methods have attempted to explicitly estimate non-uniform blur kernels by CNNs, but accurate estimation is still challenging due to the lack of ground truth about spatially varying blur kernels in real-world images. To address these issues, we propose to represent the field of motion blur kernels in a latent space by normalizing flows, and design CNNs to predict the latent codes instead of motion kernels. To further improve the accuracy and robustness of non-uniform kernel estimation, we introduce uncertainty learning into the process of estimating latent codes and propose a multi-scale kernel attention module to better integrate image features with estimated kernels. Extensive experimental results, especially on real-world blur datasets, demonstrate that our method achieves state-of-the-art results in terms of both subjective and objective quality as well as excellent generalization performance for non-uniform image deblurring. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/UFPNet.htm.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">920.Burstormer: Burst Image Restoration and Enhancement Transformer</span><br>
                <span class="as">Dudhane, AkshayandZamir, SyedWaqasandKhan, SalmanandKhan, FahadShahbazandYang, Ming-Hsuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dudhane_Burstormer_Burst_Image_Restoration_and_Enhancement_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5703-5712.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对由于不可避免的移动导致的连拍照片中的各帧进行正确对齐，并融合其互补信息以生成高质量的图像。<br>
                    动机：现有的技术在处理由于相机晃动导致的照片模糊、对焦不准等问题时效果不佳。<br>
                    方法：提出Burstormer，一种基于变压器的新型架构，用于突发图像恢复和增强。通过利用多尺度局部和非局部特征实现改进的对齐和特征融合。<br>
                    效果：实验结果表明，Burstormer在突发超分辨率、突发去噪和突发低光增强等任务上优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>On a shutter press, modern handheld cameras capture multiple images in rapid succession and merge them to generate a single image. However, individual frames in a burst are misaligned due to inevitable motions and contain multiple degradations. The challenge is to properly align the successive image shots and merge their complimentary information to achieve high-quality outputs. Towards this direction, we propose Burstormer: a novel transformer-based architecture for burst image restoration and enhancement. In comparison to existing works, our approach exploits multi-scale local and non-local features to achieve improved alignment and feature fusion. Our key idea is to enable inter-frame communication in the burst neighborhoods for information aggregation and progressive fusion while modeling the burst-wide context. However, the input burst frames need to be properly aligned before fusing their information. Therefore, we propose an enhanced deformable alignment module for aligning burst features with regards to the reference frame. Unlike existing methods, the proposed alignment module not only aligns burst features but also exchanges feature information and maintains focused communication with the reference frame through the proposed reference-based feature enrichment mechanism, which facilitates handling complex motions. After multi-level alignment and enrichment, we re-emphasize on inter-frame communication within burst using a cyclic burst sampling module. Finally, the inter-frame information is aggregated using the proposed burst feature fusion module followed by progressive upsampling. Our Burstormer outperforms state-of-the-art methods on burst super-resolution, burst denoising and burst low-light enhancement. Our codes and pre-trained models are available at https://github.com/akshaydudhane16/Burstormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">921.DeepMapping2: Self-Supervised Large-Scale LiDAR Map Optimization</span><br>
                <span class="as">Chen, ChaoandLiu, XinhaoandLi, YimingandDing, LiandFeng, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DeepMapping2_Self-Supervised_Large-Scale_LiDAR_Map_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9306-9316.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR mapping is important yet challenging in self-driving and mobile robotics. To tackle such a global point cloud registration problem, DeepMapping converts the complex map estimation into a self-supervised training of simple deep networks. Despite its broad convergence range on small datasets, DeepMapping still cannot produce satisfactory results on large-scale datasets with thousands of frames. This is due to the lack of loop closures and exact cross-frame point correspondences, and the slow convergence of its global localization network. We propose DeepMapping2 by adding two novel techniques to address these issues: (1) organization of training batch based on map topology from loop closing, and (2) self-supervised local-to-global point consistency loss leveraging pairwise registration. Our experiments and ablation studies on public datasets such as KITTI, NCLT, and Nebula, demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">922.WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding</span><br>
                <span class="as">Li, MengzeandWang, HanandZhang, WenqiaoandMiao, JiaxuandZhao, ZhouandZhang, ShengyuandJi, WeiandWu, Fei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23090-23099.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频与语言的对齐问题，即如何将语言查询定位到对应的视觉视频上。<br>
                    动机：现有的技术需要密集的边界和边界框注释来实现这种对齐，这可能会非常昂贵。为了解决这个问题，我们研究了弱监督设置，其中模型从易于访问的视频-语言数据中学习，而无需注释。<br>
                    方法：我们提出了一种新的框架WINNER，用于分层的视频-文本理解。WINNER首先以自下而上的方式构建语言分解树，然后在此基础上，结构注意力机制和自上而下的特征回溯共同构建了一个多模态分解树，实现了对无结构化视频的分层理解。这个多模态分解树是进行多层次语言-管道匹配的基础。<br>
                    效果：我们设计了一个分层对比学习目标，以学习多层次的对应关系和区分度，包括样本内的和样本间的视频-文本分解结构，从而实现了视频-语言分解结构的对齐。大量的实验表明，我们的方法在弱监督设置下的效果超过了最先进的方法，甚至一些监督方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spatio-temporal video grounding aims to localize the aligned visual tube corresponding to a language query. Existing techniques achieve such alignment by exploiting dense boundary and bounding box annotations, which can be prohibitively expensive. To bridge the gap, we investigate the weakly-supervised setting, where models learn from easily accessible video-language data without annotations. We identify that intra-sample spurious correlations among video-language components can be alleviated if the model captures the decomposed structures of video and language data. In this light, we propose a novel framework, namely WINNER, for hierarchical video-text understanding. WINNER first builds the language decomposition tree in a bottom-up manner, upon which the structural attention mechanism and top-down feature backtracking jointly build a multi-modal decomposition tree, permitting a hierarchical understanding of unstructured videos. The multi-modal decomposition tree serves as the basis for multi-hierarchy language-tube matching. A hierarchical contrastive learning objective is proposed to learn the multi-hierarchy correspondence and distinguishment with intra-sample and inter-sample video-text decomposition structures, achieving video-language decomposition structure alignment. Extensive experiments demonstrate the rationality of our design and its effectiveness beyond state-of-the-art weakly supervised methods, even some supervised methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">923.Decompose More and Aggregate Better: Two Closer Looks at Frequency Representation Learning for Human Motion Prediction</span><br>
                <span class="as">Gao, XuehaoandDu, ShaoyiandWu, YangandYang, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Decompose_More_and_Aggregate_Better_Two_Closer_Looks_at_Frequency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6451-6460.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从原始姿势空间转换运动表示到频率空间进行人类运动预测。<br>
                    动机：最近，人们受到在频率域内编码时间动态的有效性的启发，倾向于首先将运动表示从原始姿势空间转换为频率空间。<br>
                    方法：我们开发了两种强大的单元，通过一种新的分解-聚合两阶段策略来分解和聚合频率表示学习任务：（1）频率分解单元通过将输入身体运动的频率特征嵌入多个空间中，从输入身体运动中解开多视图频率表示；（2）特征聚合单元部署了一系列的 intra-space 和 inter-space 特征聚合层，从这些空间收集全面的频率表示以进行稳健的人类运动预测。<br>
                    效果：我们在大规模数据集上进行了评估，开发出一个强大的基线模型用于人类运动预测任务，该模型比最先进的方法高出很大的百分比：在Human3.6M上高出8%-12%，在CMU MoCap上高出3%-7%，在3DPW上高出7%-10%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Encouraged by the effectiveness of encoding temporal dynamics within the frequency domain, recent human motion prediction systems prefer to first convert the motion representation from the original pose space into the frequency space. In this paper, we introduce two closer looks at effective frequency representation learning for robust motion prediction and summarize them as: decompose more and aggregate better. Motivated by these two insights, we develop two powerful units that factorize the frequency representation learning task with a novel decomposition-aggregation two-stage strategy: (1) frequency decomposition unit unweaves multi-view frequency representations from an input body motion by embedding its frequency features into multiple spaces; (2) feature aggregation unit deploys a series of intra-space and inter-space feature aggregation layers to collect comprehensive frequency representations from these spaces for robust human motion prediction. As evaluated on large-scale datasets, we develop a strong baseline model for the human motion prediction task that outperforms state-of-the-art methods by large margins: 8% 12% on Human3.6M, 3% 7% on CMU MoCap, and 7% 10% on 3DPW.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">924.BAAM: Monocular 3D Pose and Shape Reconstruction With Bi-Contextual Attention Module and Attention-Guided Modeling</span><br>
                <span class="as">Lee, Hyo-JunandKim, HanulandChoi, Su-MinandJeong, Seong-GyunandKoh, YeongJun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9011-9020.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地重建3D交通场景中汽车对象的形状和姿态，同时考虑到对象间的相对上下文关系和场景环境。<br>
                    动机：目前的研究对重建详细形状的关注较少，且大多数方法将每个3D对象视为独立的个体，导致丢失了反映道路环境的相对对象间上下文和场景上下文。<br>
                    方法：本文提出了一种基于双上下文注意力和注意力引导建模（BAAM）的新型单目3D姿态和形状重建算法。首先，根据2D原始数据，我们通过考虑检测到的对象与车辆形状先验之间的相关性，使用注意力引导建模来重建3D对象形状。其次，我们通过利用对象间的关系上下文和对象与道路环境的场景上下文的双上下文注意力来估计3D对象的姿态。最后，我们提出了一种基于鸟瞰视图距离的3D非最大抑制算法，以消除基于噪声的对象。<br>
                    效果：大量实验证明，所提出的BAAM在ApolloCar3D上取得了最先进的性能。此外，实验还表明，所提出的BAAM可以插入到KITTI上的任何成熟的单目3D对象检测器中，并显著提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D traffic scene comprises various 3D information about car objects, including their pose and shape. However, most recent studies pay relatively less attention to reconstructing detailed shapes. Furthermore, most of them treat each 3D object as an independent one, resulting in losses of relative context inter-objects and scene context reflecting road circumstances. A novel monocular 3D pose and shape reconstruction algorithm, based on bi-contextual attention and attention-guided modeling (BAAM), is proposed in this work. First, given 2D primitives, we reconstruct 3D object shape based on attention-guided modeling that considers the relevance between detected objects and vehicle shape priors. Next, we estimate 3D object pose through bi-contextual attention, which leverages relation-context inter objects and scene-context between an object and road environment. Finally, we propose a 3D non maximum suppression algorithm to eliminate spurious objects based on their Bird-Eye-View distance. Extensive experiments demonstrate that the proposed BAAM yields state-of-the-art performance on ApolloCar3D. Also, they show that the proposed BAAM can be plugged into any mature monocular 3D object detector on KITTI and significantly boost their performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">925.Visual Dependency Transformers: Dependency Tree Emerges From Reversed Attention</span><br>
                <span class="as">Ding, MingyuandShen, YikangandFan, LijieandChen, ZhenfangandChen, ZitianandLuo, PingandTenenbaum, JoshuaB.andGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Visual_Dependency_Transformers_Dependency_Tree_Emerges_From_Reversed_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14528-14539.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何模仿人类视觉系统，提取图像中实体及其部分的结构化表示，并获取它们之间的依赖关系。<br>
                    动机：目前的模型在提取图像中的结构化信息时，缺乏对长范围依赖关系的捕捉能力。<br>
                    方法：提出一种新的反向注意力机制，通过构建依赖图，让子节点关注其父节点，并按照归一化概率分布发送信息，从而自然地捕获图像补丁之间的长范围视觉依赖关系。<br>
                    效果：该方法能在无监督的情况下从叶子节点到根节点逐步诱导出依赖树，实现了图像中实体及其部分的差异化表示，支持动态视觉聚合，并在多个数据集和任务上表现出良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans possess a versatile mechanism for extracting structured representations of our visual world. When looking at an image, we can decompose the scene into entities and their parts as well as obtain the dependencies between them. To mimic such capability, we propose Visual Dependency Transformers (DependencyViT) that can induce visual dependencies without any labels. We achieve that with a novel neural operator called reversed attention that can naturally capture long-range visual dependencies between image patches. Specifically, we formulate it as a dependency graph where a child token in reversed attention is trained to attend to its parent tokens and send information following a normalized probability distribution rather than gathering information in conventional self-attention. With such a design, hierarchies naturally emerge from reversed attention layers, and a dependency tree is progressively induced from leaf nodes to the root node unsupervisedly. DependencyViT offers several appealing benefits. (i) Entities and their parts in an image are represented by different subtrees, enabling part partitioning from dependencies; (ii) Dynamic visual pooling is made possible. The leaf nodes which rarely send messages can be pruned without hindering the model performance, based on which we propose the lightweight DependencyViT-Lite to reduce the computational and memory footprints; (iii) DependencyViT works well on both self- and weakly-supervised pretraining paradigms on ImageNet, and demonstrates its effectiveness on 8 datasets and 5 tasks, such as unsupervised part and saliency segmentation, recognition, and detection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">926.SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy</span><br>
                <span class="as">Li, JiafengandWen, YingandHe, Lianghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6153-6162.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何减少卷积神经网络在提取特征时的计算资源消耗。<br>
                    动机：现有的卷积神经网络虽然在计算机视觉任务上表现优秀，但需要大量的计算资源，部分原因是卷积层提取了冗余的特征。<br>
                    方法：提出一种名为SCConv（空间和通道重建卷积）的高效卷积模块，通过利用特征的空间和通道冗余性进行压缩，降低冗余计算并促进代表性特征学习。<br>
                    效果：实验结果表明，使用SCConv嵌入的模型能够通过显著降低复杂性和计算成本来减少冗余特征，从而实现更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Convolutional Neural Networks (CNNs) have achieved remarkable performance in various computer vision tasks but this comes at the cost of tremendous computational resources, partly due to convolutional layers extracting redundant features. Recent works either compress well-trained large-scale models or explore well-designed lightweight models. In this paper, we make an attempt to exploit spatial and channel redundancy among features for CNN compression and propose an efficient convolution module, called SCConv (Spatial and Channel reconstruction Convolution), to decrease redundant computing and facilitate representative feature learning. The proposed SCConv consists of two units: spatial reconstruction unit (SRU) and channel reconstruction unit (CRU). SRU utilizes a separate-and-reconstruct method to suppress the spatial redundancy while CRU uses a split-transform-and-fuse strategy to diminish the channel redundancy. In addition, SCConv is a plug-and-play architectural unit that can be used to replace standard convolution in various convolutional neural networks directly. Experimental results show that SCConv-embedded models are able to achieve better performance by reducing redundant features with significantly lower complexity and computational costs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">927.Probability-Based Global Cross-Modal Upsampling for Pansharpening</span><br>
                <span class="as">Zhu, ZeyuandCao, XiangyongandZhou, ManandHuang, JunhaoandMeng, Deyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Probability-Based_Global_Cross-Modal_Upsampling_for_Pansharpening_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14039-14048.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的深度学习方法在遥感图像处理中的升采样步骤中，只利用了低分辨率多光谱图像的局部信息，忽视了全局信息和引导的全色图像的跨模态信息，限制了性能的提升。<br>
                    动机：为了解决这个问题，本文开发了一种新颖的概率基础全局跨模态升采样（PGCU）方法进行pan-sharpening。<br>
                    方法：首先从概率的角度对PGCU方法进行形式化，然后设计了一个有效的网络模块来实现它，充分利用上述信息，同时考虑通道特异性。PGCU模块由三个块组成，即信息提取（IE），分布和期望估计（DEE），和微调（FA）。<br>
                    效果：大量的实验验证了PGCU方法优于其他流行的升采样方法。此外，实验还表明，PGCU模块可以帮助提高现有最先进的深度学习pansharpening方法的性能。代码可在https://github.com/Zeyu-Zhu/PGCU获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pansharpening is an essential preprocessing step for remote sensing image processing. Although deep learning (DL) approaches performed well on this task, current upsampling methods used in these approaches only utilize the local information of each pixel in the low-resolution multispectral (LRMS) image while neglecting to exploit its global information as well as the cross-modal information of the guiding panchromatic (PAN) image, which limits their performance improvement. To address this issue, this paper develops a novel probability-based global cross-modal upsampling (PGCU) method for pan-sharpening. Precisely, we first formulate the PGCU method from a probabilistic perspective and then design an efficient network module to implement it by fully utilizing the information mentioned above while simultaneously considering the channel specificity. The PGCU module consists of three blocks, i.e., information extraction (IE), distribution and expectation estimation (DEE), and fine adjustment (FA). Extensive experiments verify the superiority of the PGCU method compared with other popular upsampling methods. Additionally, experiments also show that the PGCU module can help improve the performance of existing SOTA deep learning pansharpening methods. The codes are available at https://github.com/Zeyu-Zhu/PGCU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">928.PHA: Patch-Wise High-Frequency Augmentation for Transformer-Based Person Re-Identification</span><br>
                <span class="as">Zhang, GuiweiandZhang, YongfeiandZhang, TianyuandLi, BoandPu, Shiliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14133-14142.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管有实证研究表明将卷积神经网络（CNNs）注入视觉变换器（ViTs）可以提高人员再识别的性能，但其背后的原理仍然不清楚。<br>
                    动机：从频率的角度来看，我们发现ViTs在保留关键高频组件（如衣物纹理细节）方面的表现不如CNNs，因为由于ViTs内在的自我注意力机制，高频组件不可避免地会被低频组件稀释。<br>
                    方法：我们提出了一种Patch-wise High-frequency Augmentation（PHA）方法，主要有两个核心设计。首先，为了增强高频组件的特征表示能力，我们使用离散哈尔小波变换来分割含有高频组件的补丁，然后让ViT将这些分割后的补丁作为辅助输入。其次，为了防止在网络优化过程中以整个序列作为输入时高频组件被低频组件稀释，我们提出了一种新的补丁对比损失函数。从梯度优化的角度来看，它可以作为一种隐式的增强，以提高关键高频组件的表示能力。<br>
                    效果：我们在广泛使用的ReID数据集上进行了大量实验，验证了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although recent studies empirically show that injecting Convolutional Neural Networks (CNNs) into Vision Transformers (ViTs) can improve the performance of person re-identification, the rationale behind it remains elusive. From a frequency perspective, we reveal that ViTs perform worse than CNNs in preserving key high-frequency components (e.g, clothes texture details) since high-frequency components are inevitably diluted by low-frequency ones due to the intrinsic Self-Attention within ViTs. To remedy such inadequacy of the ViT, we propose a Patch-wise High-frequency Augmentation (PHA) method with two core designs. First, to enhance the feature representation ability of high-frequency components, we split patches with high-frequency components by the Discrete Haar Wavelet Transform, then empower the ViT to take the split patches as auxiliary input. Second, to prevent high-frequency components from being diluted by low-frequency ones when taking the entire sequence as input during network optimization, we propose a novel patch-wise contrastive loss. From the view of gradient optimization, it acts as an implicit augmentation to improve the representation ability of key high-frequency components. This benefits the ViT to capture key high-frequency components to extract discriminative person representations. PHA is necessary during training and can be removed during inference, without bringing extra complexity. Extensive experiments on widely-used ReID datasets validate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">929.AnchorFormer: Point Cloud Completion From Discriminative Nodes</span><br>
                <span class="as">Chen, ZhikaiandLong, FuchenandQiu, ZhaofanandYao, TingandZhou, WengangandLuo, JieboandMei, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_AnchorFormer_Point_Cloud_Completion_From_Discriminative_Nodes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13581-13590.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决点云补全中由于全局特征向量无法充分表征一个物体的多样化模式而导致的高质形状生成问题。<br>
                    动机：目前的点云补全方法通常将观察到的点编码为全局特征向量，然后通过在这个向量上的生成过程预测完整的点，但这种方法可能会因为全局特征向量无法充分表征物体的多样化模式而产生高质形状生成问题。<br>
                    方法：本文提出了一种新的点云补全架构——AnchorFormer，该架构创新地利用了模式感知的判别节点（即锚点）来动态捕捉物体的区域信息。具体来说，AnchorFormer通过学习一组基于输入部分观察点的点特征的锚点来对区域进行建模和判别。这些锚点通过估计特定的偏移量分散到观察到和未观察到的位置，并与输入观察点的下采样点一起形成稀疏点。为了重建精细的物体模式，AnchorFormer进一步采用调制方案将稀疏点的个体位置处的规范2D网格变形为详细的3D结构。<br>
                    效果：在PCN、ShapeNet-55/34和KITTI数据集上的大量实验从数量和质量两方面证明了AnchorFormer优于最先进的点云补全方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud completion aims to recover the completed 3D shape of an object from its partial observation. A common strategy is to encode the observed points to a global feature vector and then predict the complete points through a generative process on this vector. Nevertheless, the results may suffer from the high-quality shape generation problem due to the fact that a global feature vector cannot sufficiently characterize diverse patterns in one object. In this paper, we present a new shape completion architecture, namely AnchorFormer, that innovatively leverages pattern-aware discriminative nodes, i.e., anchors, to dynamically capture regional information of objects. Technically, AnchorFormer models the regional discrimination by learning a set of anchors based on the point features of the input partial observation. Such anchors are scattered to both observed and unobserved locations through estimating particular offsets, and form sparse points together with the down-sampled points of the input observation. To reconstruct the fine-grained object patterns, AnchorFormer further employs a modulation scheme to morph a canonical 2D grid at individual locations of the sparse points into a detailed 3D structure. Extensive experiments on the PCN, ShapeNet-55/34 and KITTI datasets quantitatively and qualitatively demonstrate the efficacy of AnchorFormer over the state-of-the-art point cloud completion approaches. Source code is available at https://github.com/chenzhik/AnchorFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">930.PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR Point Clouds</span><br>
                <span class="as">Li, JinyuandLuo, ChenxuandYang, Xiaodong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_PillarNeXt_Rethinking_Network_Designs_for_3D_Object_Detection_in_LiDAR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17567-17576.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效处理稀疏和无结构的原始点云，提高基于LiDAR的3D物体检测的性能。<br>
                    动机：大多数现有的3D物体检测研究主要关注设计专门的局部点聚合器进行精细的几何建模，但这种方法在性能和延迟方面表现不佳。<br>
                    方法：本文从分配计算资源的角度重新审视了局部点聚合器，发现最简单的支柱模型在准确性和延迟方面表现出色。此外，我们还借鉴了2D物体检测的成功经验，如扩大感受野，显著提高了性能。<br>
                    效果：通过大量的实验，我们发现基于支柱网络的现代设计和训练方法在两个流行的基准测试集（Waymo Open Dataset和nuScenes）上达到了最先进的性能。这些结果挑战了常见的直觉，即详细的几何建模对于实现高性能的3D物体检测至关重要。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In order to deal with the sparse and unstructured raw point clouds, most LiDAR based 3D object detection research focuses on designing dedicated local point aggregators for fine-grained geometrical modeling. In this paper, we revisit the local point aggregators from the perspective of allocating computational resources. We find that the simplest pillar based models perform surprisingly well considering both accuracy and latency. Additionally, we show that minimal adaptions from the success of 2D object detection, such as enlarging receptive field, significantly boost the performance. Extensive experiments reveal that our pillar based networks with modernized designs in terms of architecture and training render the state-of-the-art performance on two popular benchmarks: Waymo Open Dataset and nuScenes. Our results challenge the common intuition that detailed geometry modeling is essential to achieve high performance for 3D object detection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">931.BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual Camera via Key-Points</span><br>
                <span class="as">Wang, RuihaoandQin, JianandLi, KaiyingandLi, YaochenandCao, DongandXu, Jintao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1002-1011.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行三维车道检测，以解决自动驾驶中车辆路径规划的关键问题。<br>
                    动机：现有的三维车道检测方法由于复杂的空间变换和对三维车道的刚性表示，实用性较差。<br>
                    方法：我们提出了一种高效且稳健的单目三维车道检测方法，称为BEV-LaneDet，主要有三个贡献。首先，我们引入了虚拟摄像头，统一了安装在不同车辆上的摄像头的内外参数，以保证摄像头之间的空间关系的一致性。其次，我们提出了一种简单但有效的三维车道表示方法，称为关键点表示法，这种方法更适合表示复杂多样的三维车道结构。最后，我们提出了一种轻量级、易于集成到芯片的空间变换模块，名为空间变换金字塔，用于将多尺度的前视图特征转换为BEV特征。<br>
                    效果：实验结果表明，我们的方法在F-Score上超过了最先进的方法，在OpenLane数据集上提高了10.6%，在Apollo 3D合成数据集上提高了4.0%，并且速度达到了185FPS。代码已在GitHub上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D lane detection which plays a crucial role in vehicle routing, has recently been a rapidly developing topic in autonomous driving. Previous works struggle with practicality due to their complicated spatial transformations and inflexible representations of 3D lanes. Faced with the issues, our work proposes an efficient and robust monocular 3D lane detection called BEV-LaneDet with three main contributions. First, we introduce the Virtual Camera that unifies the in/extrinsic parameters of cameras mounted on different vehicles to guarantee the consistency of the spatial relationship among cameras. It can effectively promote the learning procedure due to the unified visual space. We secondly propose a simple but efficient 3D lane representation called Key-Points Representation. This module is more suitable to represent the complicated and diverse 3D lane structures. At last, we present a light-weight and chip-friendly spatial transformation module named Spatial Transformation Pyramid to transform multiscale front-view features into BEV features. Experimental results demonstrate that our work outperforms the state-of-the-art approaches in terms of F-Score, being 10.6% higher on the OpenLane dataset and 4.0% higher on the Apollo 3D synthetic dataset, with a speed of 185 FPS. Code is released at https://github.com/gigo-team/bev_lane_det.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">932.Self-Supervised 3D Scene Flow Estimation Guided by Superpoints</span><br>
                <span class="as">Shen, YaqiandHui, LeandXie, JinandYang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Self-Supervised_3D_Scene_Flow_Estimation_Guided_by_Superpoints_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5271-5280.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的3D场景流估计方法中，由于使用离线聚类生成的超点无法准确捕捉复杂3D场景中具有相似运动局部区域的问题。<br>
                    动机：目前的3D场景流估计方法采用离线聚类生成超点，这种方法在复杂的3D场景中无法准确地捕捉到具有相似运动的局部区域，导致场景流估计不准确。<br>
                    方法：提出一种端到端的迭代超点基场景流估计框架，该框架包含一个流动引导的超点生成模块和一个超点引导的流动精化模块。在超点生成模块中，利用前一次迭代的双向流动信息获取点的匹配点和超点中心的配对，构建软点到超点的关联，为配对点云生成超点。然后，利用生成的超点重构每个点的流动，并编码配对点云重构流动之间的一致性。最后，将一致性编码与重构的流动一起输入GRU进行点级流动的精化。<br>
                    效果：在多个不同数据集上的大量实验表明，该方法能够取得良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">933.Guided Depth Super-Resolution by Deep Anisotropic Diffusion</span><br>
                <span class="as">Metzger, NandoandDaudt, RodrigoCayeandSchindler, Konrad</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Metzger_Guided_Depth_Super-Resolution_by_Deep_Anisotropic_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18237-18246.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用RGB图像的指导对深度图像进行超分辨率处理。<br>
                    动机：尽管深度学习方法在此问题上取得了良好的效果，但最近的研究表明，将现代方法与更正式的框架相结合可以进一步提高性能。<br>
                    方法：提出了一种新的方法，该方法结合了引导各向异性扩散和深度卷积网络，并在引导深度超分辨率方面取得了最先进的成果。通过现代网络的上下文推理能力增强了扩散的边缘传递/增强属性，严格的调整步骤确保完全遵循源图像。<br>
                    效果：在三个常用的引导深度超分辨率基准测试中，我们的方法取得了前所未有的结果。与其他方法相比，在大尺度（如x32缩放）上的性能增益最大。为了促进结果的可重复性，我们将提供所提出方法的代码。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Performing super-resolution of a depth image using the guidance from an RGB image is a problem that concerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work highlighted the value of combining modern methods with more formal frameworks. In this work we propose a novel approach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transferring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super resolution. The performance gain compared to other methods is the largest at larger scales, such as x32 scaling. Code for the proposed method will be made available to promote reproducibility of our results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">934.Edge-Aware Regional Message Passing Controller for Image Forgery Localization</span><br>
                <span class="as">Li, DongandZhu, JiayingandWang, MengluandLiu, JiaweiandFu, XueyangandZha, Zheng-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Edge-Aware_Regional_Message_Passing_Controller_for_Image_Forgery_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8222-8232.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习在图像伪造定位中的特征耦合问题。<br>
                    动机：尽管基于深度学习的方法取得了显著的进步，但在图像伪造定位中，伪造和真实区域的严重特征耦合问题仍然存在。<br>
                    方法：提出了一种两步边缘感知的区域消息传递控制策略。第一步是充分利用边缘信息，包括上下文增强的图构建和阈值自适应的可微二值化边缘算法；第二步是在可学习的边缘指导下，设计区域消息传递控制器来减弱伪造和真实区域之间的消息传递。<br>
                    效果：实验结果表明，该方法在多个具有挑战性的基准测试上优于最先进的图像伪造定位方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Digital image authenticity has promoted research on image forgery localization. Although deep learning-based methods achieve remarkable progress, most of them usually suffer from severe feature coupling between the forged and authentic regions. In this work, we propose a two-step Edge-aware Regional Message Passing Controlling strategy to address the above issue. Specifically, the first step is to account for fully exploiting the edge information. It consists of two core designs: context-enhanced graph construction and threshold-adaptive differentiable binarization edge algorithm. The former assembles the global semantic information to distinguish the features between the forged and authentic regions, while the latter stands on the output of the former to provide the learnable edges. In the second step, guided by the learnable edges, a region message passing controller is devised to weaken the message passing between the forged and authentic regions. In this way, our ERMPC is capable of explicitly modeling the inconsistency between the forged and authentic regions and enabling it to perform well on refined forged images. Extensive experiments on several challenging benchmarks show that our method is superior to state-of-the-art image forgery localization methods qualitatively and quantitatively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">935.Frequency-Modulated Point Cloud Rendering With Easy Editing</span><br>
                <span class="as">Zhang, YiandHuang, XiaoyangandNi, BingbingandLi, TengandZhang, Wenjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frequency-Modulated_Point_Cloud_Rendering_With_Easy_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/119-129.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种有效的点云渲染管道，实现新的视图合成，高保真局部细节重建，实时渲染和用户友好的编辑。<br>
                    动机：现有的方法在频率表现能力上不足，且需要大量的计算资源。<br>
                    方法：我们提出了一个自适应频率调制模块（AFNet），利用超网络学习局部纹理频率编码，然后注入到自适应频率激活层中，以调制隐式辐射信号。同时，我们还提出了一个预处理模块，通过点透明度估计优化点云几何。<br>
                    效果：在NeRF-Synthetic、ScanNet、DTU和Tanks and Temples数据集上的大量实验结果表明，我们的方法在PSNR、SSIM和LPIPS等指标上都优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We develop an effective point cloud rendering pipeline for novel view synthesis, which enables high fidelity local detail reconstruction, real-time rendering and user-friendly editing. In the heart of our pipeline is an adaptive frequency modulation module called Adaptive Frequency Net (AFNet), which utilizes a hypernetwork to learn the local texture frequency encoding that is consecutively injected into adaptive frequency activation layers to modulate the implicit radiance signal. This mechanism improves the frequency expressive ability of the network with richer frequency basis support, only at a small computational budget. To further boost performance, a preprocessing module is also proposed for point cloud geometry optimization via point opacity estimation. In contrast to implicit rendering, our pipeline supports high-fidelity interactive editing based on point cloud manipulation. Extensive experimental results on NeRF-Synthetic, ScanNet, DTU and Tanks and Temples datasets demonstrate the superior performances achieved by our method in terms of PSNR, SSIM and LPIPS, in comparison to the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">936.SE-ORNet: Self-Ensembling Orientation-Aware Network for Unsupervised Point Cloud Shape Correspondence</span><br>
                <span class="as">Deng, JiachengandWang, ChuxinandLu, JiahaoandHe, JianfengandZhang, TianzhuandYu, JiyangandZhang, Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_SE-ORNet_Self-Ensembling_Orientation-Aware_Network_for_Unsupervised_Point_Cloud_Shape_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5364-5373.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何获得点云之间的密集点对对应关系，同时解决人类和动物的对称性和各种方向性导致的严重预测错误以及点云噪声干扰的问题。<br>
                    动机：目前的无监督点云形状对应方法存在预测错误严重、受噪声干扰影响大等问题。<br>
                    方法：提出一种名为SE-ORNet的自我集成方向感知网络，通过利用方向估计模块和领域自适应鉴别器来对齐点云对的方向，以显著减轻对称部分的预测错误。同时设计了一种自我集成框架，通过不同的数据增强来扰动学生和教师网络的输入，并约束预测的一致性，以克服点云噪声的干扰。<br>
                    效果：在人类和动物数据集上的大量实验表明，我们的SE-ORNet可以超越最先进的无监督点云形状对应方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised point cloud shape correspondence aims to obtain dense point-to-point correspondences between point clouds without manually annotated pairs. However, humans and some animals have bilateral symmetry and various orientations, which leads to severe mispredictions of symmetrical parts. Besides, point cloud noise disrupts consistent representations for point cloud and thus degrades the shape correspondence accuracy. To address the above issues, we propose a Self-Ensembling ORientation-aware Network termed SE-ORNet. The key of our approach is to exploit an orientation estimation module with a domain adaptive discriminator to align the orientations of point cloud pairs, which significantly alleviates the mispredictions of symmetrical parts. Additionally, we design a self-ensembling framework for unsupervised point cloud shape correspondence. In this framework, the disturbances of point cloud noise are overcome by perturbing the inputs of the student and teacher networks with different data augmentations and constraining the consistency of predictions. Extensive experiments on both human and animal datasets show that our SE-ORNet can surpass state-of-the-art unsupervised point cloud shape correspondence methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">937.Raw Image Reconstruction With Learned Compact Metadata</span><br>
                <span class="as">Wang, YufeiandYu, YiandYang, WenhanandGuo, LanqingandChau, Lap-PuiandKot, AlexC.andWen, Bihan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Raw_Image_Reconstruction_With_Learned_Compact_Metadata_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18206-18215.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地压缩原始图像，同时保持高质量的图像重建效果？<br>
                    动机：尽管原始图像具有线性和细粒度量化级别等优点，但由于存储需求大，普通用户并未广泛使用。<br>
                    方法：提出一种新的框架，在潜在空间中学习紧凑的表示形式作为元数据，并设计了一种新的sRGB引导上下文模型，改进了熵估计策略。<br>
                    效果：实验结果表明，该方法可以在较小的元数据大小下实现优秀的原始图像重建效果，并在未压缩的sRGB图像和JPEG图像上都表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While raw images exhibit advantages over sRGB images (e.g. linearity and fine-grained quantization level), they are not widely used by common users due to the large storage requirements. Very recent works propose to compress raw images by designing the sampling masks in the raw image pixel space, leading to suboptimal image representations and redundant metadata. In this paper, we propose a novel framework to learn a compact representation in the latent space serving as the metadata in an end-to-end manner. Furthermore, we propose a novel sRGB-guided context model with the improved entropy estimation strategies, which leads to better reconstruction quality, smaller size of metadata, and faster speed. We illustrate how the proposed raw image compression scheme can adaptively allocate more bits to image regions that are important from a global perspective. The experimental results show that the proposed method can achieve superior raw image reconstruction results using a smaller size of the metadata on both uncompressed sRGB images and JPEG images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">938.MonoATT: Online Monocular 3D Object Detection With Adaptive Token Transformer</span><br>
                <span class="as">Zhou, YunsongandZhu, HongziandLiu, QuanandChang, ShanandGuo, Minyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_MonoATT_Online_Monocular_3D_Object_Detection_With_Adaptive_Token_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17493-17503.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mobile monocular 3D object detection (Mono3D) (e.g., on a vehicle, a drone, or a robot) is an important yet challenging task. Existing transformer-based offline Mono3D models adopt grid-based vision tokens, which is suboptimal when using coarse tokens due to the limited available computational power. In this paper, we propose an online Mono3D framework, called MonoATT, which leverages a novel vision transformer with heterogeneous tokens of varying shapes and sizes to facilitate mobile Mono3D. The core idea of MonoATT is to adaptively assign finer tokens to areas of more significance before utilizing a transformer to enhance Mono3D. To this end, we first use prior knowledge to design a scoring network for selecting the most important areas of the image, and then propose a token clustering and merging network with an attention mechanism to gradually merge tokens around the selected areas in multiple stages. Finally, a pixel-level feature map is reconstructed from heterogeneous tokens before employing a SOTA Mono3D detector as the underlying detection core. Experiment results on the real-world KITTI dataset demonstrate that MonoATT can effectively improve the Mono3D accuracy for both near and far objects and guarantee low latency. MonoATT yields the best performance compared with the state-of-the-art methods by a large margin and is ranked number one on the KITTI 3D benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">939.Object Discovery From Motion-Guided Tokens</span><br>
                <span class="as">Bao, ZhipengandTokmakov, PavelandWang, Yu-XiongandGaidon, AdrienandHebert, Martial</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_Object_Discovery_From_Motion-Guided_Tokens_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22972-22981.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从背景中分离出对象，而无需手动标签？<br>
                    动机：现有的方法主要依赖于低层次的线索进行聚类，无论是手工制作的（如颜色、纹理）还是学习得到的（如自动编码器）。<br>
                    方法：本文在自动编码器表示学习框架中加入了两个关键组件：运动引导和中层次特征标记化。通过引入一个新的变压器解码器，证明了运动引导和标记化的协同作用可以提升性能。<br>
                    效果：该方法有效地利用了运动和标记化的协同作用，在合成数据和真实数据集上都超越了现有技术。同时，该方法能够产生可解释的对象特定中层次特征，展示了运动引导（无需标注）和量化（可解释性、内存效率）的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object discovery -- separating objects from the background without manual labels -- is a fundamental open challenge in computer vision. Previous methods struggle to go beyond clustering of low-level cues, whether handcrafted (e.g., color, texture) or learned (e.g., from auto-encoders). In this work, we augment the auto-encoder representation learning framework with two key components: motion-guidance and mid-level feature tokenization. Although both have been separately investigated, we introduce a new transformer decoder showing that their benefits can compound thanks to motion-guided vector quantization. We show that our architecture effectively leverages the synergy between motion and tokenization, improving upon the state of the art on both synthetic and real datasets. Our approach enables the emergence of interpretable object-specific mid-level features, demonstrating the benefits of motion-guidance (no labeling) and quantization (interpretability, memory efficiency).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">940.Hyperspherical Embedding for Point Cloud Completion</span><br>
                <span class="as">Zhang, JunmingandZhang, HaomengandVasudevan, RamandJohnson-Roberson, Matthew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Hyperspherical_Embedding_for_Point_Cloud_Completion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5323-5332.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从深度传感器的不完整3D测量中预测物体的完整形状。<br>
                    动机：现有的点云补全任务通常采用编码器-解码器架构，但学习到的嵌入在特征空间中的分布稀疏，导致测试时的泛化结果较差。<br>
                    方法：提出一个超球模块，将编码器提取的嵌入进行转换和归一化，使其位于单位超球上。优化仅针对方向信息，从而得到更稳定训练和更紧凑的嵌入分布。<br>
                    效果：实验结果表明，该方法在单任务和多任务学习中都能显著提高点云补全的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most real-world 3D measurements from depth sensors are incomplete, and to address this issue the point cloud completion task aims to predict the complete shapes of objects from partial observations. Previous works often adapt an encoder-decoder architecture, where the encoder is trained to extract embeddings that are used as inputs to generate predictions from the decoder. However, the learned embeddings have sparse distribution in the feature space, which leads to worse generalization results during testing. To address these problems, this paper proposes a hyperspherical module, which transforms and normalizes embeddings from the encoder to be on a unit hypersphere. With the proposed module, the magnitude and direction of the output hyperspherical embedding are decoupled and only the directional information is optimized. We theoretically analyze the hyperspherical embedding and show that it enables more stable training with a wider range of learning rates and more compact embedding distributions. Experiment results show consistent improvement of point cloud completion in both single-task and multi-task learning, which demonstrates the effectiveness of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">941.Unsupervised Deep Asymmetric Stereo Matching With Spatially-Adaptive Self-Similarity</span><br>
                <span class="as">Song, TaeyongandKim, SunokandSohn, Kwanghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Unsupervised_Deep_Asymmetric_Stereo_Matching_With_Spatially-Adaptive_Self-Similarity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13672-13680.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的无监督立体匹配算法大多假设左右图像具有一致的视觉属性，即对称，但在非对称立体图像上容易失败。<br>
                    动机：提出一种新颖的空间自适应自相似性（SASS）方法，用于解决非对称立体图像的无监督立体匹配问题。<br>
                    方法：扩展自相似性的概念，生成对非对称性具有鲁棒性的深层特征。在整个图像区域内自适应地生成计算自相似性的采样模式，以有效地编码不同的模式。设计一种带有正负权重的对比相似性损失函数，进一步鼓励SASS编码非对称无关的特征，同时保持立体对应的特殊性。<br>
                    效果：通过大量的实验结果，包括消融研究和与不同方法的比较，证明了该方法在分辨率和噪声非对称性下的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised stereo matching has received a lot of attention since it enables the learning of disparity estimation without ground-truth data. However, most of the unsupervised stereo matching algorithms assume that the left and right images have consistent visual properties, i.e., symmetric, and easily fail when the stereo images are asymmetric. In this paper, we present a novel spatially-adaptive self-similarity (SASS) for unsupervised asymmetric stereo matching. It extends the concept of self-similarity and generates deep features that are robust to the asymmetries. The sampling patterns to calculate self-similarities are adaptively generated throughout the image regions to effectively encode diverse patterns. In order to learn the effective sampling patterns, we design a contrastive similarity loss with positive and negative weights. Consequently, SASS is further encouraged to encode asymmetry-agnostic features, while maintaining the distinctiveness for stereo correspondence. We present extensive experimental results including ablation studies and comparisons with different methods, demonstrating effectiveness of the proposed method under resolution and noise asymmetries.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">942.MV-JAR: Masked Voxel Jigsaw and Reconstruction for LiDAR-Based Self-Supervised Pre-Training</span><br>
                <span class="as">Xu, RunsenandWang, TaiandZhang, WenweiandChen, RunjianandCao, JinkunandPang, JiangmiaoandLin, Dahua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MV-JAR_Masked_Voxel_Jigsaw_and_Reconstruction_for_LiDAR-Based_Self-Supervised_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13445-13454.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces the Masked Voxel Jigsaw and Reconstruction (MV-JAR) method for LiDAR-based self-supervised pre-training and a carefully designed data-efficient 3D object detection benchmark on the Waymo dataset. Inspired by the scene-voxel-point hierarchy in downstream 3D object detectors, we design masking and reconstruction strategies accounting for voxel distributions in the scene and local point distributions within the voxel. We employ a Reversed-Furthest-Voxel-Sampling strategy to address the uneven distribution of LiDAR points and propose MV-JAR, which combines two techniques for modeling the aforementioned distributions, resulting in superior performance. Our experiments reveal limitations in previous data-efficient experiments, which uniformly sample fine-tuning splits with varying data proportions from each LiDAR sequence, leading to similar data diversity across splits. To address this, we propose a new benchmark that samples scene sequences for diverse fine-tuning splits, ensuring adequate model convergence and providing a more accurate evaluation of pre-training methods. Experiments on our Waymo benchmark and the KITTI dataset demonstrate that MV-JAR consistently and significantly improves 3D detection performance across various data scales, achieving up to a 6.3% increase in mAPH compared to training from scratch. Codes and the benchmark are available at https://github.com/SmartBot-PJLab/MV-JAR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">943.Learning a Sparse Transformer Network for Effective Image Deraining</span><br>
                <span class="as">Chen, XiangandLi, HaoandLi, MingqiangandPan, Jinshan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_a_Sparse_Transformer_Network_for_Effective_Image_Deraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5896-5905.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的Transformer模型在图像去雨任务中，由于对查询和键的相似性进行特征聚合时，可能会干扰清晰的图像恢复。<br>
                    动机：为了解决现有Transformer模型在图像去雨任务中的干扰问题，提高图像重建质量。<br>
                    方法：提出一种有效的去雨网络——稀疏Transformer（DRSformer）。通过学习可调整的top-k选择操作符，自适应地保留每个查询最重要的注意力分数，以更好地进行特征聚合。同时，开发混合尺度前馈网络生成更好的图像去雨特征。<br>
                    效果：实验结果表明，该方法在常用的基准测试上取得了优于现有最先进技术的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformers-based methods have achieved significant performance in image deraining as they can model the non-local information which is vital for high-quality image reconstruction. In this paper, we find that most existing Transformers usually use all similarities of the tokens from the query-key pairs for the feature aggregation. However, if the tokens from the query are different from those of the key, the self-attention values estimated from these tokens also involve in feature aggregation, which accordingly interferes with the clear image restoration. To overcome this problem, we propose an effective DeRaining network, Sparse Transformer (DRSformer) that can adaptively keep the most useful self-attention values for feature aggregation so that the aggregated features better facilitate high-quality image reconstruction. Specifically, we develop a learnable top-k selection operator to adaptively retain the most crucial attention scores from the keys for each query for better feature aggregation. Simultaneously, as the naive feed-forward network in Transformers does not model the multi-scale information that is important for latent clear image restoration, we develop an effective mixed-scale feed-forward network to generate better features for image deraining. To learn an enriched set of hybrid features, which combines local context from CNN operators, we equip our model with mixture of experts feature compensator to present a cooperation refinement deraining scheme. Extensive experimental results on the commonly used benchmarks demonstrate that the proposed method achieves favorable performance against state-of-the-art approaches. The source code and trained models are available at https://github.com/cschenxiang/DRSformer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">944.DA-DETR: Domain Adaptive Detection Transformer With Information Fusion</span><br>
                <span class="as">Zhang, JingyiandHuang, JiaxingandLuo, ZhipengandZhang, GongjieandZhang, XiaoqinandLu, Shijian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DA-DETR_Domain_Adaptive_Detection_Transformer_With_Information_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23787-23798.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用简单有效的DETR架构进行领域自适应目标检测。<br>
                    动机：尽管DETR简化了目标检测流程，但如何将其应用于领域自适应对象检测的问题尚未得到充分关注。<br>
                    方法：设计了一种名为DA-DETR的领域自适应对象检测变换器，通过引入信息融合，实现从有标签源域到无标签目标域的有效转移。具体来说，DA-DETR引入了一种新颖的CNN-Transformer Blender（CTBlender），巧妙地融合了CNN特征和Transformer特征，以实现跨领域的有效特征对齐和知识转移。<br>
                    效果：大量实验表明，DA-DETR在多个广泛采用的领域适应基准测试中始终表现出优越的检测性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent detection transformer (DETR) simplifies the object detection pipeline by removing hand-crafted designs and hyperparameters as employed in conventional two-stage object detectors. However, how to leverage the simple yet effective DETR architecture in domain adaptive object detection is largely neglected. Inspired by the unique DETR attention mechanisms, we design DA-DETR, a domain adaptive object detection transformer that introduces information fusion for effective transfer from a labeled source domain to an unlabeled target domain. DA-DETR introduces a novel CNN-Transformer Blender (CTBlender) that fuses the CNN features and Transformer features ingeniously for effective feature alignment and knowledge transfer across domains. Specifically, CTBlender employs the Transformer features to modulate the CNN features across multiple scales where the high-level semantic information and the low-level spatial information are fused for accurate object identification and localization. Extensive experiments show that DA-DETR achieves superior detection performance consistently across multiple widely adopted domain adaptation benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">945.Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation</span><br>
                <span class="as">Shen, XiaolongandYang, ZongxinandWang, XiaohanandMa, JianxinandZhou, ChangandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Global-to-Local_Modeling_for_Video-Based_3D_Human_Pose_and_Shape_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8887-8896.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频中三维人体姿态和形状估计的问题，特别是在处理短期和长期时间相关性时的挑战。<br>
                    动机：现有的最先进方法将这两种度量标准视为统一的问题，并使用单一的模型结构（如RNN或基于注意力的模块）来设计网络，这在平衡学习短期和长期时间相关性上存在困难，可能导致预测结果出现位置偏移、时间不一致性和局部细节不足等问题。<br>
                    方法：为了解决这些问题，我们提出了一种端到端的框架——全局到局部转换器(GLoT)，该框架在结构上将长期和短期相关性的建模进行解耦。首先，引入了一个全局转换器，并采用了一种被遮盖的姿态和形状估计策略来进行长期建模。其次，局部转换器负责利用人体网格上的局部细节，并通过利用交叉注意力与全局转换器进行交互。此外，我们还进一步引入了一种分层空间相关性回归器，通过解耦的全局-局部表示和隐式运动学约束来细化帧内估计。<br>
                    效果：我们的GLoT在流行的基准测试——3DPW, MPI-INF-3DHP, 和Human3.6M上超越了之前最先进的方法，同时具有最低的模型参数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video-based 3D human pose and shape estimations are evaluated by intra-frame accuracy and inter-frame smoothness. Although these two metrics are responsible for different ranges of temporal consistency, existing state-of-the-art methods treat them as a unified problem and use monotonous modeling structures (e.g., RNN or attention-based block) to design their networks. However, using a single kind of modeling structure is difficult to balance the learning of short-term and long-term temporal correlations, and may bias the network to one of them, leading to undesirable predictions like global location shift, temporal inconsistency, and insufficient local details. To solve these problems, we propose to structurally decouple the modeling of long-term and short-term correlations in an end-to-end framework, Global-to-Local Transformer (GLoT). First, a global transformer is introduced with a Masked Pose and Shape Estimation strategy for long-term modeling. The strategy stimulates the global transformer to learn more inter-frame correlations by randomly masking the features of several frames. Second, a local transformer is responsible for exploiting local details on the human mesh and interacting with the global transformer by leveraging cross-attention. Moreover, a Hierarchical Spatial Correlation Regressor is further introduced to refine intra-frame estimations by decoupled global-local representation and implicit kinematic constraints. Our GLoT surpasses previous state-of-the-art methods with the lowest model parameters on popular benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M. Codes are available at https://github.com/sxl142/GLoT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">946.Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields</span><br>
                <span class="as">Hu, TaoandXu, XiaogangandLiu, ShuandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Point2Pix_Photo-Realistic_Point_Cloud_Rendering_via_Neural_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8349-8358.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将稀疏的点云表示与2D密集图像像素链接起来，以合成高质量的图像。<br>
                    动机：由于点云表示的稀疏性，从点云合成逼真的图像具有挑战性。<br>
                    方法：提出了一种新的点渲染器Point2Pix，利用点云3D先验和NeRF渲染管道，从彩色点云中合成高质量的图像。<br>
                    效果：通过提出点引导采样、多尺度辐射场的点编码以及融合编码等方法，显著提高了图像合成的效率和质量。在ScanNet和ArkitScenes数据集上的大量实验证明了该方法的有效性和泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesizing photo-realistic images from a point cloud is challenging because of the sparsity of point cloud representation. Recent Neural Radiance Fields and extensions are proposed to synthesize realistic images from 2D input. In this paper, we present Point2Pix as a novel point renderer to link the 3D sparse point clouds with 2D dense image pixels. Taking advantage of the point cloud 3D prior and NeRF rendering pipeline, our method can synthesize high-quality images from colored point clouds, generally for novel indoor scenes. To improve the efficiency of ray sampling, we propose point-guided sampling, which focuses on valid samples. Also, we present Point Encoding to build Multi-scale Radiance Fields that provide discriminative 3D point features. Finally, we propose Fusion Encoding to efficiently synthesize high-quality images. Extensive experiments on the ScanNet and ArkitScenes datasets demonstrate the effectiveness and generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">947.Multiplicative Fourier Level of Detail</span><br>
                <span class="as">Dou, YishunandZheng, ZhongandJin, QiaoqiaoandNi, Bingbing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_Multiplicative_Fourier_Level_of_Detail_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1808-1817.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种名为多倍频细节级别的简单而有效的隐式表示方案（MFLOD）。<br>
                    动机：受最近乘法滤波网络成功的影响，我们提出了MFLOD。<br>
                    方法：基于多分辨率特征网格/体积（如稀疏体素八叉树），每一层的特征首先被正弦函数调制，然后逐元素地与前一层表示的线性变换进行层到层的递归乘法，从而产生用于后续简单线性前向的尺度聚合编码以获得最终输出。<br>
                    效果：通过在隐式神经表示学习任务上进行实验，包括图像拟合、3D形状表示和神经辐射场等，结果证明了MFLOD方案的优越性和通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We develop a simple yet surprisingly effective implicit representing scheme called Multiplicative Fourier Level of Detail (MFLOD) motivated by the recent success of multiplicative filter network. Built on multi-resolution feature grid/volume (e.g., the sparse voxel octree), each level's feature is first modulated by a sinusoidal function and then element-wisely multiplied by a linear transformation of previous layer's representation in a layer-to-layer recursive manner, yielding the scale-aggregated encodings for a subsequent simple linear forward to get final output. In contrast to previous hybrid representations relying on interleaved multilevel fusion and nonlinear activation-based decoding, MFLOD could be elegantly characterized as a linear combination of sine basis functions with varying amplitude, frequency, and phase upon the learned multilevel features, thus offering great feasibility in Fourier analysis. Comprehensive experimental results on implicit neural representation learning tasks including image fitting, 3D shape representation, and neural radiance fields well demonstrate the superior quality and generalizability achieved by the proposed MFLOD scheme.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">948.Low-Light Image Enhancement via Structure Modeling and Guidance</span><br>
                <span class="as">Xu, XiaogangandWang, RuixingandLu, Jiangbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Low-Light_Image_Enhancement_via_Structure_Modeling_and_Guidance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9893-9903.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的低光图像增强框架，通过同时进行外观和结构建模。<br>
                    动机：现有的方法在处理低光图像时，往往只关注图像的外观信息，忽视了图像的结构信息。<br>
                    方法：该框架采用结构特征引导外观增强，实现了边缘检测和图像增强。具体来说，通过设计一个结构感知的特征提取器和生成器，实现了结构建模；并通过一个简单的U-Net网络进行外观建模。<br>
                    效果：实验结果表明，该方法在所有数据集上都取得了最先进的性能，证明了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a new framework for low-light image enhancement by simultaneously conducting the appearance as well as structure modeling. It employs the structural feature to guide the appearance enhancement, leading to sharp and realistic results. The structure modeling in our framework is implemented as the edge detection in low-light images. It is achieved with a modified generative model via designing a structure-aware feature extractor and generator. The detected edge maps can accurately emphasize the essential structural information, and the edge prediction is robust towards the noises in dark areas. Moreover, to improve the appearance modeling, which is implemented with a simple U-Net, a novel structure-guided enhancement module is proposed with structure-guided feature synthesis layers. The appearance modeling, edge detector, and enhancement module can be trained end-to-end. The experiments are conducted on representative datasets (sRGB and RAW domains), showing that our model consistently achieves SOTA performance on all datasets with the same architecture.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">949.Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection</span><br>
                <span class="as">Li, LongandHan, JunweiandZhang, NiandLiu, NianandKhan, SalmanandCholakkal, HishamandAnwer, RaoMuhammadandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Discriminative_Co-Saliency_and_Background_Mining_Transformer_for_Co-Salient_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7247-7256.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数先前的共显著对象检测工作主要关注通过挖掘图像间的一致性关系来提取共显著线索，而忽视了对背景区域的显式探索。<br>
                    动机：本文提出了一种基于经济多粒度关联模块的判别性共显著性和背景挖掘Transformer框架（DMT），以显式地挖掘共显著性和背景信息，并有效地模型它们的区分能力。<br>
                    方法：首先，我们提出了区域到区域关联模块，以经济地为像素级分割特征建模图像间关系。然后，我们使用两种预定义的标记来通过我们提出的对比诱导像素到标记和共显著性标记到标记关联模块来挖掘共显著性和背景信息。我们还设计了一个标记引导的特征细化模块，以在学到的标记的指导下增强分割特征的可区分性。我们对分割特征提取和标记构建进行了迭代相互促进。<br>
                    效果：我们在三个基准数据集上进行的实验结果表明了我们提出的方法的有效性。源代码可在以下网址获取：https://github.com/dragonlee258079/DMT。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most previous co-salient object detection works mainly focus on extracting co-salient cues via mining the consistency relations across images while ignoring the explicit exploration of background regions. In this paper, we propose a Discriminative co-saliency and background Mining Transformer framework (DMT) based on several economical multi-grained correlation modules to explicitly mine both co-saliency and background information and effectively model their discrimination. Specifically, we first propose region-to-region correlation modules to economically model inter-image relations for pixel-wise segmentation features. Then, we use two types of predefined tokens to mine co-saliency and background information via our proposed contrast-induced pixel-to-token and co-saliency token-to-token correlation modules. We also design a token-guided feature refinement module to enhance the discriminability of the segmentation features under the guidance of the learned tokens. We perform iterative mutual promotion for the segmentation feature extraction and token construction. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method. The source code is available at: https://github.com/dragonlee258079/DMT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">950.Binary Latent Diffusion</span><br>
                <span class="as">Wang, ZeandWang, JiangandLiu, ZichengandQiu, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Binary_Latent_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22576-22585.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用二进制潜在空间进行紧凑且富有表现力的图片表示。<br>
                    动机：现有的图片表示方法通常需要多阶段的潜在空间层次结构，或者使用像素或连续的潜在表示，这限制了其效率和分辨率。<br>
                    方法：通过训练一个具有伯努利编码分布的自动编码器，实现图像与其对应二进制潜在表示之间的双向映射。<br>
                    效果：实验结果表明，该方法在多个数据集上的表现与最先进的方法相当，同时显著提高了采样效率，可以在不使用任何测试时间加速的情况下仅用16步生成图像。此外，该方法还可以无缝扩展到1024 x 1024的高分辨率图像生成，无需依赖潜在空间层次结构或多阶段细化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we show that a binary latent space can be explored for compact yet expressive image representations. We model the bi-directional mappings between an image and the corresponding latent binary representation by training an auto-encoder with a Bernoulli encoding distribution. On the one hand, the binary latent space provides a compact discrete image representation of which the distribution can be modeled more efficiently than pixels or continuous latent representations. On the other hand, we now represent each image patch as a binary vector instead of an index of a learned cookbook as in discrete image representations with vector quantization. In this way, we obtain binary latent representations that allow for better image quality and high-resolution image representations without any multi-stage hierarchy in the latent space. In this binary latent space, images can now be generated effectively using a binary latent diffusion model tailored specifically for modeling the prior over the binary image representations. We present both conditional and unconditional image generation experiments with multiple datasets, and show that the proposed method performs comparably to state-of-the-art methods while dramatically improving the sampling efficiency to as few as 16 steps without using any test-time acceleration. The proposed framework can also be seamlessly scaled to 1024 x 1024 high-resolution image generation without resorting to latent hierarchy or multi-stage refinements.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">951.Adaptive Assignment for Geometry Aware Local Feature Matching</span><br>
                <span class="as">Huang, DiheandChen, YingandLiu, YongandLiu, JianlinandXu, ShangandWu, WenlongandDing, YikangandTang, FanandWang, Chengjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Adaptive_Assignment_for_Geometry_Aware_Local_Feature_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5425-5434.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前无检测器的特征匹配方法在大规模和视角变化上表现不佳，因为应用相互最近邻标准（即一对一分配）在补丁级别匹配中导致几何不一致。<br>
                    动机：为了解决这个问题，我们提出了AdaMatcher，它首先通过精心设计的特征交互模块完成特征相关性和共可见区域估计，然后在估计图像之间的比例的同时进行自适应分配的补丁级别匹配，最后通过比例对齐和亚像素回归模块细化共可见匹配。<br>
                    方法：AdaMatcher采用特征交互模块进行特征相关性和共可见区域估计，然后进行自适应分配的补丁级别匹配并估计图像之间的比例，最后通过比例对齐和亚像素回归模块细化共可见匹配。<br>
                    效果：实验表明，AdaMatcher优于坚实的基线并实现了许多下游任务的最先进的结果。此外，自适应分配和亚像素细化模块可以用作其他匹配方法（如SuperGlue）的细化网络，以进一步提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The detector-free feature matching approaches are currently attracting great attention thanks to their excellent performance. However, these methods still struggle at large-scale and viewpoint variations, due to the geometric inconsistency resulting from the application of the mutual nearest neighbour criterion (i.e., one-to-one assignment) in patch-level matching. Accordingly, we introduce AdaMatcher, which first accomplishes the feature correlation and co-visible area estimation through an elaborate feature interaction module, then performs adaptive assignment on patch-level matching while estimating the scales between images, and finally refines the co-visible matches through scale alignment and sub-pixel regression module. Extensive experiments show that AdaMatcher outperforms solid baselines and achieves state-of-the-art results on many downstream tasks. Additionally, the adaptive assignment and sub-pixel refinement module can be used as a refinement network for other matching methods, such as SuperGlue, to boost their performance further. The code will be publicly available at https://github.com/AbyssGaze/AdaMatcher.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">952.FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER</span><br>
                <span class="as">Zheng, CeandMendieta, MatiasandYang, TaojiannanandQi, Guo-JunandChen, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_FeatER_An_Efficient_Network_for_Human_Reconstruction_via_Feature_Map-Based_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13945-13954.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉转换器无法直接处理特征图输入，需要对位置敏感的人体结构信息进行不自然的展平，同时计算和内存需求也在不断增加。<br>
                    动机：为了解决这些问题，我们提出了FeatER，一种能够保留特征图表示固有结构的新型转换器设计，同时降低内存和计算成本。<br>
                    方法：利用FeatER，我们构建了一个高效的网络用于一系列人体重建任务，包括2D人体姿态估计、3D人体姿态估计和人体网格重建。我们还应用了特征图重建模块来提高估计的人体姿态和网格的性能。<br>
                    效果：实验表明，FeatER在各种人体姿态和网格数据集上表现出色。例如，在Human3.6M和3DPW数据集上，FeatER比最先进的SOTA方法MeshGraphormer所需的参数减少了5%，乘法累加操作减少了16%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps (encodes each joint's location into a feature map with a Gaussian distribution) for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel transformer design which preserves the inherent structure of feature map representations when modeling attention while reducing the memory and computational costs. Taking advantage of FeatER, we build an efficient network for a set of human reconstruction tasks including 2D HPE, 3D HPE, and HMR. A feature map reconstruction module is applied to improve the performance of the estimated human pose and mesh. Extensive experiments demonstrate the effectiveness of FeatER on various human pose and mesh datasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by requiring 5% of Params (total parameters) and 16% of MACs (the Multiply-Accumulate Operations) on Human3.6M and 3DPW datasets. Code will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">953.Residual Degradation Learning Unfolding Framework With Mixing Priors Across Spectral and Spatial for Compressive Spectral Imaging</span><br>
                <span class="as">Dong, YuboandGao, DahuaandQiu, TianandLi, YuyanandYang, MinxiandShi, Guangming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Residual_Degradation_Learning_Unfolding_Framework_With_Mixing_Priors_Across_Spectral_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22262-22271.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从2D测量中恢复可靠且精细的底层3D光谱立方体。<br>
                    动机：CASSI系统的核心问题是从2D测量中恢复可靠的底层3D光谱立方体，而现有的深度展开方法在数据子问题和先验子问题上存在不足。<br>
                    方法：提出了残差退化学习展开框架（RDLUF）和MixS2 Transformer，前者弥合了感知矩阵与退化过程之间的差距，后者通过混合光谱和空间先验来增强光谱-空间表示能力。<br>
                    效果：实验结果表明，所提出的方法优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To acquire a snapshot spectral image, coded aperture snapshot spectral imaging (CASSI) is proposed. A core problem of the CASSI system is to recover the reliable and fine underlying 3D spectral cube from the 2D measurement. By alternately solving a data subproblem and a prior subproblem, deep unfolding methods achieve good performance. However, in the data subproblem, the used sensing matrix is ill-suited for the real degradation process due to the device errors caused by phase aberration, distortion; in the prior subproblem, it is important to design a suitable model to jointly exploit both spatial and spectral priors. In this paper, we propose a Residual Degradation Learning Unfolding Framework (RDLUF), which bridges the gap between the sensing matrix and the degradation process. Moreover, a MixS2 Transformer is designed via mixing priors across spectral and spatial to strengthen the spectral-spatial representation capability. Finally, plugging the MixS2 Transformer into the RDLUF leads to an end-to-end trainable and interpretable neural network RDLUF-MixS2. Experimental results establish the superior performance of the proposed method over existing ones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">954.PanelNet: Understanding 360 Indoor Environment via Panel Representation</span><br>
                <span class="as">Yu, HaozhengandHe, LuandJian, BingandFeng, WeiweiandLiu, Shan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PanelNet_Understanding_360_Indoor_Environment_via_Panel_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/878-887.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用室内360度全景的两个关键特性（连续性和无缝性，以及重力在室内环境设计中的重要性）来理解室内环境。<br>
                    动机：现有的方法往往忽视了室内360度全景的这些特性，导致对室内环境的理解和分析不够准确。<br>
                    方法：提出了PanelNet框架，该框架将等距投影图表示为连续的垂直面板，并结合了面板的几何特征。同时，引入了面板几何嵌入网络和局部到全局转换器，以减少全景失真的负面影响，并捕捉房间设计的几何上下文。<br>
                    效果：实验结果表明，该方法在室内360度深度估计、室内布局估计和语义分割等任务上均优于现有方法，且训练开销低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Indoor 360 panoramas have two essential properties. (1) The panoramas are continuous and seamless in the horizontal direction. (2) Gravity plays an important role in indoor environment design. By leveraging these properties, we present PanelNet, a framework that understands indoor environments using a novel panel representation of 360 images. We represent an equirectangular projection (ERP) as consecutive vertical panels with corresponding 3D panel geometry. To reduce the negative impact of panoramic distortion, we incorporate a panel geometry embedding network that encodes both the local and global geometric features of a panel. To capture the geometric context in room design, we introduce Local2Global Transformer, which aggregates local information within a panel and panel-wise global context. It greatly improves the model performance with low training overhead. Our method outperforms existing methods on indoor 360 depth estimation and shows competitive results against state-of-the-art approaches on the task of indoor layout estimation and semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">955.Correspondence Transformers With Asymmetric Feature Learning and Matching Flow Super-Resolution</span><br>
                <span class="as">Sun, YixuanandZhao, DongyangandYin, ZhangyueandHuang, YiwenandGui, TaoandZhang, WenqiangandGe, Weifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Correspondence_Transformers_With_Asymmetric_Feature_Learning_and_Matching_Flow_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17787-17796.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决在只有稀疏标注的情况下，学习同一类别不同物体实例之间的密集视觉对应关系的问题。<br>
                    动机：现有的方法需要大量的标注数据才能进行准确的像素级语义匹配，而本文提出的方法只需要稀疏的标注即可实现。<br>
                    方法：将像素级的语义匹配问题分解为两个较简单的子问题：首先，将源图像和目标图像的局部特征描述符映射到共享的语义空间中以获得粗略的匹配流；其次，对低分辨率的匹配流进行精细化处理以生成精确的点对点匹配结果。为此，提出了基于视觉变换器的非对称特征学习和匹配流超分辨率方法。<br>
                    效果：通过在多个流行基准测试集上进行广泛的实验，如PF-PASCAL、PF-WILLOW和SPair-71K，验证了该方法可以有效地捕捉像素间的微妙语义差异。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper solves the problem of learning dense visual correspondences between different object instances of the same category with only sparse annotations. We decompose this pixel-level semantic matching problem into two easier ones: (i) First, local feature descriptors of source and target images need to be mapped into shared semantic spaces to get coarse matching flows. (ii) Second, matching flows in low resolution should be refined to generate accurate point-to-point matching results. We propose asymmetric feature learning and matching flow super-resolution based on vision transformers to solve the above problems. The asymmetric feature learning module exploits a biased cross-attention mechanism to encode token features of source images with their target counterparts. Then matching flow in low resolutions is enhanced by a super-resolution network to get accurate correspondences. Our pipeline is built upon vision transformers and can be trained in an end-to-end manner. Extensive experimental results on several popular benchmarks, such as PF-PASCAL, PF-WILLOW, and SPair-71K, demonstrate that the proposed method can catch subtle semantic differences in pixels efficiently. Code is available on https://github.com/YXSUNMADMAX/ACTR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">956.Unsupervised 3D Point Cloud Representation Learning by Triangle Constrained Contrast for Autonomous Driving</span><br>
                <span class="as">Pang, BoandXia, HongchiandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Unsupervised_3D_Point_Cloud_Representation_Learning_by_Triangle_Constrained_Contrast_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5229-5239.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于自动驾驶的3D激光雷达数据标注困难，因此需要一种有效的无监督3D表示学习方法。<br>
                    动机：本文设计了针对自动驾驶场景的三角形约束对比（TriCC）框架，通过多模态信息和时间序列动态学习3D无监督表示。<br>
                    方法：我们将一张摄像头图像和两个不同时间戳的激光雷达点云视为一个三元组。关键设计是自动通过"自我循环"找到三元组中的匹配关系的一致性约束，并从中学习表示。利用跨时间和模态的匹配关系，我们可以进一步进行三元组对比以提高学习效率。<br>
                    效果：实验结果表明，TriCC是第一个统一时间和多模态语义的框架，即它几乎利用了自动驾驶场景中的所有信息。与以前的对比方法相比，它可以自动挖掘出更难的对比对，而不是依赖手工制作的对比对。在几个语义分割和3D检测数据集上进行的大量实验表明，TriCC可以在少得多的训练迭代次数下学习到有效的表示，并在所有下游任务上都大大提高了最新结果。代码和模型可在https://bopang1996.github.io/ 找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the difficulty of annotating the 3D LiDAR data of autonomous driving, an efficient unsupervised 3D representation learning method is important. In this paper, we design the Triangle Constrained Contrast (TriCC) framework tailored for autonomous driving scenes which learns 3D unsupervised representations through both the multimodal information and dynamic of temporal sequences. We treat one camera image and two LiDAR point clouds with different timestamps as a triplet. And our key design is the consistent constraint that automatically finds matching relationships among the triplet through "self-cycle" and learns representations from it. With the matching relations across the temporal dimension and modalities, we can further conduct a triplet contrast to improve learning efficiency. To the best of our knowledge, TriCC is the first framework that unifies both the temporal and multimodal semantics, which means it utilizes almost all the information in autonomous driving scenes. And compared with previous contrastive methods, it can automatically dig out contrasting pairs with higher difficulty, instead of relying on handcrafted ones. Extensive experiments are conducted with Minkowski-UNet and VoxelNet on several semantic segmentation and 3D detection datasets. Results show that TriCC learns effective representations with much fewer training iterations and improves the SOTA results greatly on all the downstream tasks. Code and models can be found at https://bopang1996.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">957.Controllable Mesh Generation Through Sparse Latent Point Diffusion Models</span><br>
                <span class="as">Lyu, ZhaoyangandWang, JinyiandAn, YuweiandZhang, YaandLin, DahuaandDai, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lyu_Controllable_Mesh_Generation_Through_Sparse_Latent_Point_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/271-280.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种有效的网格生成模型，以应对网格的非规则数据结构和同一类别中网格的不一致拓扑结构。<br>
                    动机：由于网格的复杂性和不规则性，设计出一种有效的网格生成模型具有很大的挑战性。<br>
                    方法：提出一种新的稀疏潜在点扩散模型进行网格生成。将点云视为网格的中间表示，并对其分布进行建模。进一步将点云编码为一组具有点位语义有意义特征的稀疏潜在点，并在这些潜在点的稀疏空间中训练两个DDPMs，分别对潜在点的位置和特征分布进行建模。<br>
                    效果：在ShapeNet数据集上进行的大量实验表明，与现有方法相比，所提出的稀疏潜在点扩散模型在生成质量和可控性方面表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mesh generation is of great value in various applications involving computer graphics and virtual content, yet designing generative models for meshes is challenging due to their irregular data structure and inconsistent topology of meshes in the same category. In this work, we design a novel sparse latent point diffusion model for mesh generation. Our key insight is to regard point clouds as an intermediate representation of meshes, and model the distribution of point clouds instead. While meshes can be generated from point clouds via techniques like Shape as Points (SAP), the challenges of directly generating meshes can be effectively avoided. To boost the efficiency and controllability of our mesh generation method, we propose to further encode point clouds to a set of sparse latent points with point-wise semantic meaningful features, where two DDPMs are trained in the space of sparse latent points to respectively model the distribution of the latent point positions and features at these latent points. We find that sampling in this latent space is faster than directly sampling dense point clouds. Moreover, the sparse latent points also enable us to explicitly control both the overall structures and local details of the generated meshes. Extensive experiments are conducted on the ShapeNet dataset, where our proposed sparse latent point diffusion model achieves superior performance in terms of generation quality and controllability when compared to existing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">958.SGLoc: Scene Geometry Encoding for Outdoor LiDAR Localization</span><br>
                <span class="as">Li, WenandYu, ShangshuandWang, ChengandHu, GuoshengandShen, SiqiandWen, Chenglu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SGLoc_Scene_Geometry_Encoding_for_Outdoor_LiDAR_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9286-9295.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于激光雷达的定位方法在有效编码场景几何和数据质量方面存在困难，导致准确性有待提高。<br>
                    动机：提出一种新的激光雷达定位框架SGLoc，通过解耦点云对应关系回归和通过对应关系的姿态估计来改善这一问题。<br>
                    方法：SGLoc采用解耦对应关系回归和姿态估计，设计了三尺度空间特征聚合模块和几何间一致性约束损失函数以有效捕捉场景几何。同时，提出了一种姿态质量评估和增强方法来测量和修正地面真值。<br>
                    效果：在牛津雷达机器人汽车和NCLT数据集上的大量实验表明，SGLoc的有效性，其位置精度分别比最先进的基于回归的定位方法提高了68.5%和67.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR-based absolute pose regression estimates the global pose through a deep network in an end-to-end manner, achieving impressive results in learning-based localization. However, the accuracy of existing methods still has room to improve due to the difficulty of effectively encoding the scene geometry and the unsatisfactory quality of the data. In this work, we propose a novel LiDAR localization framework, SGLoc, which decouples the pose estimation to point cloud correspondence regression and pose estimation via this correspondence. This decoupling effectively encodes the scene geometry because the decoupled correspondence regression step greatly preserves the scene geometry, leading to significant performance improvement. Apart from this decoupling, we also design a tri-scale spatial feature aggregation module and inter-geometric consistency constraint loss to effectively capture scene geometry. Moreover, we empirically find that the ground truth might be noisy due to GPS/INS measuring errors, greatly reducing the pose estimation performance. Thus, we propose a pose quality evaluation and enhancement method to measure and correct the ground truth pose. Extensive experiments on the Oxford Radar RobotCar and NCLT datasets demonstrate the effectiveness of SGLoc, which outperforms state-of-the-art regression-based localization methods by 68.5% and 67.6% on position accuracy, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">959.Bridging Search Region Interaction With Template for RGB-T Tracking</span><br>
                <span class="as">Hui, TianruiandXun, ZizhengandPeng, FengguangandHuang, JunshiandWei, XiaomingandWei, XiaolinandDai, JiaoandHan, JizhongandLiu, Si</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hui_Bridging_Search_Region_Interaction_With_Template_for_RGB-T_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13630-13639.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用RGB和TIR模态的相互增强和互补能力，提高各种场景下的跟踪过程。<br>
                    动机：现有的方法直接连接RGB和TIR搜索区域特征进行粗略交互，引入了冗余的背景噪声，或者在局部区域内对孤立的RGB和TIR框对进行各种融合，限制了跨模态交互，导致上下文模型不充分。<br>
                    方法：提出一种新的模板桥接搜索区域交互（TBSI）模块，通过使用模板作为介质来桥接RGB和TIR搜索区域之间的跨模态交互，收集和分发与目标相关的物体和环境上下文。<br>
                    效果：在三个流行的RGB-T跟踪基准上进行的大量实验表明，该方法实现了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>RGB-T tracking aims to leverage the mutual enhancement and complement ability of RGB and TIR modalities for improving the tracking process in various scenarios, where cross-modal interaction is the key component. Some previous methods concatenate the RGB and TIR search region features directly to perform a coarse interaction process with redundant background noises introduced. Many other methods sample candidate boxes from search frames and conduct various fusion approaches on isolated pairs of RGB and TIR boxes, which limits the cross-modal interaction within local regions and brings about inadequate context modeling. To alleviate these limitations, we propose a novel Template-Bridged Search region Interaction (TBSI) module which exploits templates as the medium to bridge the cross-modal interaction between RGB and TIR search regions by gathering and distributing target-relevant object and environment contexts. Original templates are also updated with enriched multimodal contexts from the template medium. Our TBSI module is inserted into a ViT backbone for joint feature extraction, search-template matching, and cross-modal interaction. Extensive experiments on three popular RGB-T tracking benchmarks demonstrate our method achieves new state-of-the-art performances. Code is available at https://github.com/RyanHTR/TBSI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">960.MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding From Object Detection</span><br>
                <span class="as">Zhao, WendaandXie, ShigengandZhao, FanandHe, YouandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13955-13965.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过融合红外和可见图像来提高后续物体检测任务的纹理细节，并研究问题：如何通过融合红外和可见图像来提高后续物体检测任务的纹理细节，并利用物体检测任务提供的对象语义信息改善红外和可见图像的融合效果。<br>
                    动机：现有的方法中，红外和可见图像的融合以及物体检测是两个不同层次的任务，它们之间的特征差距阻碍了融合效果的提升。<br>
                    方法：本文提出了一种基于元特征嵌入的红外和可见图像融合方法，该方法通过设计一个元特征嵌入模型来根据融合网络的能力生成对象语义特征，使得这些语义特征与融合特征自然兼容。并通过模拟元学习进行优化，同时在融合和检测任务之间实施相互促进的学习以提高它们的性能。<br>
                    效果：在三个公共数据集上的全面实验证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fusing infrared and visible images can provide more texture details for subsequent object detection task. Conversely, detection task furnishes object semantic information to improve the infrared and visible image fusion. Thus, a joint fusion and detection learning to use their mutual promotion is attracting more attention. However, the feature gap between these two different-level tasks hinders the progress. Addressing this issue, this paper proposes an infrared and visible image fusion via meta-feature embedding from object detection. The core idea is that meta-feature embedding model is designed to generate object semantic features according to fusion network ability, and thus the semantic features are naturally compatible with fusion features. It is optimized by simulating a meta learning. Moreover, we further implement a mutual promotion learning between fusion and detection tasks to improve their performances. Comprehensive experiments on three public datasets demonstrate the effectiveness of our method. Code and model are available at: https://github.com/wdzhao123/MetaFusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">961.Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising</span><br>
                <span class="as">Li, MiaoyuandLiu, JiandFu, YingandZhang, YulunandDou, Dejing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spectral_Enhanced_Rectangle_Transformer_for_Hyperspectral_Image_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5805-5814.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的高光谱图像（HSI）去噪方法在捕捉非局部自相似性方面存在局限，且现有深度学习方法对空间和光谱相关性的建模效果不佳。<br>
                    动机：为了解决上述问题，本文提出了一种基于变换器的高光谱图像去噪方法，该方法能够更好地探索HSIs的空间相似性和全局光谱低秩特性。<br>
                    方法：具体来说，我们设计了一个光谱增强矩形变换器，通过水平垂直地利用矩形自注意力机制来捕获空间域中的非局部相似性，同时设计了一个光谱增强模块来提取空间-光谱立方体的全局潜在低秩特性以抑制噪声，并允许不重叠的空间矩形之间的交互。<br>
                    效果：我们在合成噪声HSI和真实噪声HSI上进行了大量实验，结果表明，我们的方法在客观度量和主观视觉质量方面都取得了良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Denoising is a crucial step for hyperspectral image (HSI) applications. Though witnessing the great power of deep learning, existing HSI denoising methods suffer from limitations in capturing the non-local self-similarity. Transformers have shown potential in capturing long-range dependencies, but few attempts have been made with specifically designed Transformer to model the spatial and spectral correlation in HSIs. In this paper, we address these issues by proposing a spectral enhanced rectangle Transformer, driving it to explore the non-local spatial similarity and global spectral low-rank property of HSIs. For the former, we exploit the rectangle self-attention horizontally and vertically to capture the non-local similarity in the spatial domain. For the latter, we design a spectral enhancement module that is capable of extracting global underlying low-rank property of spatial-spectral cubes to suppress noise, while enabling the interactions among non-overlapping spatial rectangles. Extensive experiments have been conducted on both synthetic noisy HSIs and real noisy HSIs, showing the effectiveness of our proposed method in terms of both objective metric and subjective visual quality. The code is available at https://github.com/MyuLi/SERT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">962.End-to-End Vectorized HD-Map Construction With Piecewise Bezier Curve</span><br>
                <span class="as">Qiao, LimengandDing, WenjieandQiu, XiandZhang, Chi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13218-13228.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自动驾驶领域中对厘米级环境信息感知的矢量高清地图（HD-map）构建问题。<br>
                    动机：现有的方法主要通过基于分割的流水线获取光栅化地图，然后进行繁重的后处理以实现下游友好的矢量化。这种方法效率低下且不够精确。<br>
                    方法：本文提出了一种简洁而优雅的参数化方法，采用统一的分段贝塞尔曲线来矢量化可变的地图元素。具体来说，我们设计了一种名为Piecewise Bezier HD-map Network（BeMapNet）的简单而有效的架构，该架构直接进行集合预测，无需后处理。<br>
                    效果：实验结果表明，我们的方法在各种指标上均优于其他最先进的方法，最高可达18.0 mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vectorized high-definition map (HD-map) construction, which focuses on the perception of centimeter-level environmental information, has attracted significant research interest in the autonomous driving community. Most existing approaches first obtain rasterized map with the segmentation-based pipeline and then conduct heavy post-processing for downstream-friendly vectorization. In this paper, by delving into parameterization-based methods, we pioneer a concise and elegant scheme that adopts unified piecewise Bezier curve. In order to vectorize changeful map elements end-to-end, we elaborate a simple yet effective architecture, named Piecewise Bezier HD-map Network (BeMapNet), which is formulated as a direct set prediction paradigm and postprocessing-free. Concretely, we first introduce a novel IPM-PE Align module to inject 3D geometry prior into BEV features through common position encoding in Transformer. Then a well-designed Piecewise Bezier Head is proposed to output the details of each map element, including the coordinate of control points and the segment number of curves. In addition, based on the progressively restoration of Bezier curve, we also present an efficient Point-Curve-Region Loss for supervising more robust and precise HD-map modeling. Extensive comparisons show that our method is remarkably superior to other existing SOTAs by 18.0 mAP at least.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">963.PointListNet: Deep Learning on 3D Point Lists</span><br>
                <span class="as">Fan, HeheandZhu, LinchaoandYang, YiandKankanhalli, Mohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PointListNet_Deep_Learning_on_3D_Point_Lists_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17692-17701.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时处理具有规则的一维列表（如自然语言）和不规则的三维集合（如点云）的数据？<br>
                    动机：有些数据既表现出规则的一维列表结构，又表现出不规则的三维集合结构，例如蛋白质和非编码RNA。<br>
                    方法：提出一种Transformer风格的PointListNet模型来处理这类数据。首先，使用基于距离的非参数注意力机制，因为在某些情况下，决定两个点（如氨基酸）相关性的主要因素是它们之间的距离，而非特征或类型。其次，与直接对输入执行简单线性变换并生成值而不显式建模相对关系的普通Transformer不同，我们的PointListNet将一维顺序和三维欧氏位移整合到值中。<br>
                    效果：在蛋白质折叠分类和酶反应分类任务上进行实验，结果表明提出的PointListNet的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks on regular 1D lists (e.g., natural languages) and irregular 3D sets (e.g., point clouds) have made tremendous achievements. The key to natural language processing is to model words and their regular order dependency in texts. For point cloud understanding, the challenge is to understand the geometry via irregular point coordinates, in which point-feeding orders do not matter. However, there are a few kinds of data that exhibit both regular 1D list and irregular 3D set structures, such as proteins and non-coding RNAs. In this paper, we refer to them as 3D point lists and propose a Transformer-style PointListNet to model them. First, PointListNet employs non-parametric distance-based attention because we find sometimes it is the distance, instead of the feature or type, that mainly determines how much two points, e.g., amino acids, are correlated in the micro world. Second, different from the vanilla Transformer that directly performs a simple linear transformation on inputs to generate values and does not explicitly model relative relations, our PointListNet integrates the 1D order and 3D Euclidean displacements into values. We conduct experiments on protein fold classification and enzyme reaction classification. Experimental results show the effectiveness of the proposed PointListNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">964.Spherical Transformer for LiDAR-Based 3D Recognition</span><br>
                <span class="as">Lai, XinandChen, YukangandLu, FanbinandLiu, JianhuiandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lai_Spherical_Transformer_for_LiDAR-Based_3D_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17545-17555.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地利用LiDAR点云数据进行三维识别，特别是在稀疏远距离点的识别上。<br>
                    动机：当前大多数方法在处理LiDAR点云数据时，没有充分考虑其分布特性，导致信息断开和感受野有限的问题，尤其是在稀疏远距离点的识别上。<br>
                    方法：提出SphereFormer模型，通过直接聚合密集近点的信息到稀疏远点，设计了径向窗口自注意力机制，将空间分割成多个不重叠的窄长窗口，解决了信息断开问题，平滑且显著地扩大了感受野。同时，为了适应窄长窗口，提出了指数分割产生细粒度的位置编码和动态特征选择来提高模型的表示能力。<br>
                    效果：在nuScenes和SemanticKITTI语义分割基准测试中，该方法分别以81.9%和74.8%的mIoU排名第一。在nuScenes对象检测基准测试中，取得了72.8%的NDS和68.5%的mAP，排名第三。代码已在GitHub上开源。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present SphereFormer to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1st on both nuScenes and SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU, respectively. Also, we achieve the 3rd place on nuScenes object detection benchmark with 72.8% NDS and 68.5% mAP. Code is available at https://github.com/dvlab-research/SphereFormer.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">965.VisFusion: Visibility-Aware Online 3D Scene Reconstruction From Videos</span><br>
                <span class="as">Gao, HuiyuandMao, WeiandLiu, Miaomiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_VisFusion_Visibility-Aware_Online_3D_Scene_Reconstruction_From_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17317-17326.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种可见性感知的在线3D场景重建方法，从单目视频中重建场景。<br>
                    动机：现有的重建方法在对每个体素的特征进行聚合时，没有考虑到其可见性，这影响了特征融合的效果。<br>
                    方法：我们提出了VisFusion方法，通过从每对图像中的投影特征计算相似度矩阵，显式推断出每个体素的可见性，从而改进了特征融合。此外，我们还提出了一种局部特征体积稀疏化的方法，以保留更多的细节信息。<br>
                    效果：实验结果表明，我们的方法在基准测试上取得了优越的性能，能够重建出更多场景细节。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose VisFusion, a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at: https://github.com/huiyu-gao/VisFusion</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">966.Feature Shrinkage Pyramid for Camouflaged Object Detection With Transformers</span><br>
                <span class="as">Huang, ZhouandDai, HangandXiang, Tian-ZhuandWang, ShuoandChen, Huai-XinandQin, JieandXiong, Huan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Feature_Shrinkage_Pyramid_for_Camouflaged_Object_Detection_With_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5557-5566.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉转换器在伪装目标检测中存在局部建模效果不佳和解码器特征聚合不足的问题。<br>
                    动机：为了解决这些问题，提出了一种基于转换器的新颖的特征收缩金字塔网络（FSPNet），通过逐步缩小来分层解码增强局部性的相邻转换器特征，以提高伪装目标检测的性能。<br>
                    方法：设计了一种非局部令牌增强模块（NL-TEM）和特征收缩解码器（FSD），前者利用非局部机制交互相邻的令牌，探索令牌内的基于图的高阶关系以增强转换器的局部表示；后者则通过逐层缩小的金字塔逐渐聚合相邻的转换器特征，尽可能多地累积难以察觉但有效的线索用于对象信息解码。<br>
                    效果：大量的定量和定性实验表明，所提出的模型在三个具有挑战性的COD基准数据集上显著优于24个现有竞争对手，并在六种广泛使用的评估指标下表现优秀。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers have recently shown strong global context modeling capabilities in camouflaged object detection. However, they suffer from two major limitations: less effective locality modeling and insufficient feature aggregation in decoders, which are not conducive to camouflaged object detection that explores subtle cues from indistinguishable backgrounds. To address these issues, in this paper, we propose a novel transformer-based Feature Shrinkage Pyramid Network (FSPNet), which aims to hierarchically decode locality-enhanced neighboring transformer features through progressive shrinking for camouflaged object detection. Specifically, we propose a non-local token enhancement module (NL-TEM) that employs the non-local mechanism to interact neighboring tokens and explore graph-based high-order relations within tokens to enhance local representations of transformers. Moreover, we design a feature shrinkage decoder (FSD) with adjacent interaction modules (AIM), which progressively aggregates adjacent transformer features through a layer-by-layer shrinkage pyramid to accumulate imperceptible but effective cues as much as possible for object information decoding. Extensive quantitative and qualitative experiments demonstrate that the proposed model significantly outperforms the existing 24 competitors on three challenging COD benchmark datasets under six widely-used evaluation metrics. Our code is publicly available at https://github.com/ZhouHuang23/FSPNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">967.MSF: Motion-Guided Sequential Fusion for Efficient 3D Object Detection From Point Cloud Sequences</span><br>
                <span class="as">He, ChenhangandLi, RuihuangandZhang, YabinandLi, ShuaiandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_MSF_Motion-Guided_Sequential_Fusion_for_Efficient_3D_Object_Detection_From_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5196-5205.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用点云序列准确检测3D对象？<br>
                    动机：目前的多帧探测器在处理点云序列时存在大量冗余计算，因为相邻帧之间高度相关。<br>
                    方法：提出一种运动引导的序列融合（MSF）方法，通过挖掘物体运动的连续性，提取有用的序列上下文进行当前帧的目标检测。首先在当前帧生成3D提议，并根据估计的速度将其传播到前一帧。然后从序列中汇总感兴趣的点，并将其编码为提议特征。进一步提出了一种新的双向特征聚合（BiFA）模块，以促进提议特征在各帧之间的交互。此外，通过体素采样技术优化了点云池化，使数百万个点能在几毫秒内处理完毕。<br>
                    效果：提出的MSF方法不仅比其他多帧探测器更高效，而且准确性领先，在Waymo开放数据集的LEVEL1和LEVEL2测试集上分别达到了83.12%和78.30%的mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud sequences are commonly used to accurately detect 3D objects in applications such as autonomous driving. Current top-performing multi-frame detectors mostly follow a Detect-and-Fuse framework, which extracts features from each frame of the sequence and fuses them to detect the objects in the current frame. However, this inevitably leads to redundant computation since adjacent frames are highly correlated. In this paper, we propose an efficient Motion-guided Sequential Fusion (MSF) method, which exploits the continuity of object motion to mine useful sequential contexts for object detection in the current frame. We first generate 3D proposals on the current frame and propagate them to preceding frames based on the estimated velocities. The points-of-interest are then pooled from the sequence and encoded as proposal features. A novel Bidirectional Feature Aggregation (BiFA) module is further proposed to facilitate the interactions of proposal features across frames. Besides, we optimize the point cloud pooling by a voxel-based sampling technique so that millions of points can be processed in several milliseconds. The proposed MSF method achieves not only better efficiency than other multi-frame detectors but also leading accuracy, with 83.12% and 78.30% mAP on the LEVEL1 and LEVEL2 test sets of Waymo Open Dataset, respectively. Codes can be found at https://github.com/skyhehe123/MSF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">968.Kernel Aware Resampler</span><br>
                <span class="as">Bernasconi, MichaelandDjelouah, AbdelazizandSalehi, FarnoodandGross, MarkusandSchroers, Christopher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bernasconi_Kernel_Aware_Resampler_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22347-22355.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习在图像超分辨率重建中的问题，如固定整数缩放因子和非整数缩放因子的处理。<br>
                    动机：现有的深度学习方法在处理图像超分辨率时，主要针对固定的整数缩放因子（如x2或x4），对于非整数缩放因子和模糊核的建模等问题尚未给出完善的解决方案。<br>
                    方法：本文提出了一个通用的图像重采样框架，该框架不仅解决了上述问题，还扩展了可能的变换集，从放大到一般的变换。关键的一点是在训练数据准备阶段忠实地模拟图像变形和采样率的变化。这使得我们可以局部表示隐含的图像退化，考虑到重建核、局部几何畸变和抗锯齿核。<br>
                    效果：通过使用这种空间变化的退化图作为我们重采样模型的条件，我们可以用同一个模型处理全局变换（如放大或旋转）和局部变化（如镜头畸变或去畸变）。此外，我们还实现了在更复杂的重采样设置（即盲图像重采样）中自动估计退化图。实验结果表明，通过预测应用于输入图像的内核，而不是直接预测颜色，可以达到最先进的结果。这使得我们的模型可以应用于训练期间未见过的不同类型数据，如法线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning based methods for super-resolution have become state-of-the-art and outperform traditional approaches by a significant margin. From the initial models designed for fixed integer scaling factors (e.g. x2 or x4), efforts were made to explore different directions such as modeling blur kernels or addressing non-integer scaling factors. However, existing works do not provide a sound framework to handle them jointly. In this paper we propose a framework for generic image resampling that not only addresses all the above mentioned issues but extends the sets of possible transforms from upscaling to generic transforms. A key aspect to unlock these capabilities is the faithful modeling of image warping and changes of the sampling rate during the training data preparation. This allows a localized representation of the implicit image degradation that takes into account the reconstruction kernel, the local geometric distortion and the anti-aliasing kernel. Using this spatially variant degradation map as conditioning for our resampling model, we can address with the same model both global transformations, such as upscaling or rotation, and locally varying transformations such lens distortion or undistortion. Another important contribution is the automatic estimation of the degradation map in this more complex resampling setting (i.e. blind image resampling). Finally, we show that state-of-the-art results can be achieved by predicting kernels to apply on the input image instead of direct color prediction. This renders our model applicable for different types of data not seen during the training such as normals.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">969.HypLiLoc: Towards Effective LiDAR Pose Regression With Hyperbolic Fusion</span><br>
                <span class="as">Wang, SijieandKang, QiyuandShe, RuiandWang, WeiandZhao, KaiandSong, YangandTay, WeePeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_HypLiLoc_Towards_Effective_LiDAR_Pose_Regression_With_Hyperbolic_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5176-5185.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高LiDAR重定位的精度和效率。<br>
                    动机：LiDAR重定位在机器人、自动驾驶和计算机视觉等领域起着关键作用，但传统的基于数据库的检索方法计算存储成本高且可能产生全局不准确的位姿估计，而直接回归全局位姿的方法则计算效率高但准确性不足。<br>
                    方法：提出HypLiLoc模型，使用两个分支的骨干网络分别提取3D特征和2D投影特征，并在欧几里得空间和双曲空间中考虑多模态特征融合以获取更有效的特征表示。<br>
                    效果：实验结果表明，HypLiLoc在户外和室内数据集上都取得了最先进的性能，同时通过框架设计的消融研究证明了多模态特征提取和多空间嵌入的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR relocalization plays a crucial role in many fields, including robotics, autonomous driving, and computer vision. LiDAR-based retrieval from a database typically incurs high computation storage costs and can lead to globally inaccurate pose estimations if the database is too sparse. On the other hand, pose regression methods take images or point clouds as inputs and directly regress global poses in an end-to-end manner. They do not perform database matching and are more computationally efficient than retrieval techniques. We propose HypLiLoc, a new model for LiDAR pose regression. We use two branched backbones to extract 3D features and 2D projection features, respectively. We consider multi-modal feature fusion in both Euclidean and hyperbolic spaces to obtain more effective feature representations. Experimental results indicate that HypLiLoc achieves state-of-the-art performance in both outdoor and indoor datasets. We also conduct extensive ablation studies on the framework design, which demonstrate the effectiveness of multi-modal feature extraction and multi-space embedding. Our code is released at: https://github.com/sijieaaa/HypLiLoc</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">970.Transformer-Based Unified Recognition of Two Hands Manipulating Objects</span><br>
                <span class="as">Cho, HoseongandKim, ChanwooandKim, JihyeonandLee, SeongyeongandIsmayilzada, ElkhanandBaek, Seungryul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4769-4778.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地理解从第一人称视角视频中的手-物体交互。<br>
                    动机：目前大多数方法基于卷积神经网络（CNN）特征和通过长短期记忆（LSTM）或图卷积网络（GCN）的时序编码，提供两只手、一个物体及其交互的统一理解，但效果有待提高。<br>
                    方法：提出一种基于Transformer的统一框架，将描绘两只手、一个物体及其交互的整个图像作为输入，从每一帧中估计两只手的姿势、一个物体的姿势和对象类型三种信息，然后根据估计的信息和编码两只手与物体之间交互的接触图，从整个视频中预测由手-物体交互定义的动作类别。<br>
                    效果：在H2O和FPHA基准数据集上进行实验，证明了该方法的优越性，实现了最先进的精度。消融研究进一步证明了每个提出的模块的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding the hand-object interactions from an egocentric video has received a great attention recently. So far, most approaches are based on the convolutional neural network (CNN) features combined with the temporal encoding via the long short-term memory (LSTM) or graph convolution network (GCN) to provide the unified understanding of two hands, an object and their interactions. In this paper, we propose the Transformer-based unified framework that provides better understanding of two hands manipulating objects. In our framework, we insert the whole image depicting two hands, an object and their interactions as input and jointly estimate 3 information from each frame: poses of two hands, pose of an object and object types. Afterwards, the action class defined by the hand-object interactions is predicted from the entire video based on the estimated information combined with the contact map that encodes the interaction between two hands and an object. Experiments are conducted on H2O and FPHA benchmark datasets and we demonstrated the superiority of our method achieving the state-of-the-art accuracy. Ablative studies further demonstrate the effectiveness of each proposed module.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">971.Efficient Map Sparsification Based on 2D and 3D Discretized Grids</span><br>
                <span class="as">Zhang, XiaoyuandLiu, Yun-Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_Map_Sparsification_Based_on_2D_and_3D_Discretized_Grids_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12470-12478.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行机器人自主导航中的地图稀疏化和定位。<br>
                    动机：随着地图规模的增大，传统的地图稀疏化方法需要更高的内存容量和计算量，且未考虑映射和查询序列的空间分布差异对定位性能的影响。<br>
                    方法：本文提出了一种高效的线性地图稀疏化方法，通过二维离散网格均匀选择地标，并引入基于三维离散网格的空间约束项以减小空间分布差异的影响。<br>
                    效果：实验证明，该方法在效率和定位性能上都优于现有方法。相关代码将在https://github.com/fishmarch/SLAM_Map_Compression上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Localization in a pre-built map is a basic technique for robot autonomous navigation. Existing mapping and localization methods commonly work well in small-scale environments. As a map grows larger, however, more memory is required and localization becomes inefficient. To solve these problems, map sparsification becomes a practical necessity to acquire a subset of the original map for localization. Previous map sparsification methods add a quadratic term in mixed-integer programming to enforce a uniform distribution of selected landmarks, which requires high memory capacity and heavy computation. In this paper, we formulate map sparsification in an efficient linear form and select uniformly distributed landmarks based on 2D discretized grids. Furthermore, to reduce the influence of different spatial distributions between the mapping and query sequences, which is not considered in previous methods, we also introduce a space constraint term based on 3D discretized grids. The exhaustive experiments in different datasets demonstrate the superiority of the proposed methods in both efficiency and localization performance. The relevant codes will be released at https://github.com/fishmarch/SLAM_Map_Compression.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">972.Generalizable Implicit Neural Representations via Instance Pattern Composers</span><br>
                <span class="as">Kim, ChiheonandLee, DoyupandKim, SaehoonandCho, MinsuandHan, Wook-Shin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Generalizable_Implicit_Neural_Representations_via_Instance_Pattern_Composers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11808-11817.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让基于坐标的多层感知机（MLP）学习到的数据实例通用表示能够泛化到未见过的数据实例。<br>
                    动机：尽管隐式神经表示（INRs）有了最新的进展，但基于坐标的MLP在学习和泛化数据实例通用表示上仍然面临挑战。<br>
                    方法：我们提出了一个简单而有效的框架，通过在早期MLP层中调制一小部分权重作为实例模式合成器，使基于坐标的MLP能够表示复杂的数据实例；其余的MLP权重则学习模式合成规则，以学习跨实例的通用表示。<br>
                    效果：广泛的实验表明，我们的方法在音频、图像和3D对象等多种领域都取得了高性能，同时消融研究验证了我们的权重调制。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite recent advances in implicit neural representations (INRs), it remains challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to learn a common representation across data instances and generalize it for unseen instances. In this work, we introduce a simple yet effective framework for generalizable INRs that enables a coordinate-based MLP to represent complex data instances by modulating only a small set of weights in an early MLP layer as an instance pattern composer; the remaining MLP weights learn pattern composition rules to learn common representations across instances. Our generalizable INR framework is fully compatible with existing meta-learning and hypernetworks in learning to predict the modulated weight for unseen instances. Extensive experiments demonstrate that our method achieves high performance on a wide range of domains such as an audio, image, and 3D object, while the ablation study validates our weight modulation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">973.3D Registration With Maximal Cliques</span><br>
                <span class="as">Zhang, XiyuandYang, JiaqiandZhang, ShikunandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17745-17754.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的基本问题，即3D点云配准（PCR），寻找最优姿态对齐点云对。<br>
                    动机：目前的3D点云配准方法存在准确度不高的问题，因此需要提出一种新的方法来提高配准的精度。<br>
                    方法：本文提出了一种基于最大团的3D配准方法（MAC）。该方法首先构建了一个兼容性图来呈现初始对应点的亲和关系，然后在图中搜索最大团，每个团代表一个共识集。接着，通过SVD算法为选定的团计算变换假设，并使用最佳假设进行配准。<br>
                    效果：在U3M、3DMatch、3DLoMatch和KITTI等数据集上的大量实验表明，MAC有效地提高了配准精度，优于各种最先进的方法，并提高了深度学习方法的性能。MAC与深度学习方法结合，在3DMatch和3DLoMatch上实现了95.7% / 78.9%的最佳注册召回率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As a fundamental problem in computer vision, 3D point cloud registration (PCR) aims to seek the optimal pose to align a point cloud pair. In this paper, we present a 3D registration method with maximal cliques (MAC). The key insight is to loosen the previous maximum clique constraint, and to mine more local consensus information in a graph for accurate pose hypotheses generation: 1) A compatibility graph is constructed to render the affinity relationship between initial correspondences. 2) We search for maximal cliques in the graph, each of which represents a consensus set. We perform node-guided clique selection then, where each node corresponds to the maximal clique with the greatest graph weight. 3) Transformation hypotheses are computed for the selected cliques by SVD algorithm and the best hypothesis is used to perform registration. Extensive experiments on U3M, 3DMatch, 3DLoMatch and KITTI demonstrate that MAC effectively increases registration accuracy, outperforms various state-of-the-art methods and boosts the performance of deep-learned methods. MAC combined with deep-learned methods achieves state-of-the-art registration recall of 95.7% / 78.9% on the 3DMatch / 3DLoMatch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">974.Efficient RGB-T Tracking via Cross-Modality Distillation</span><br>
                <span class="as">Zhang, TianluandGuo, HongyuanandJiao, QiangandZhang, QiangandHan, Jungong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5404-5413.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前大多数RGB-T跟踪器采用两流结构提取单模态RGB和热特征，并通过复杂的融合策略实现多模态特征融合，这需要大量的参数，限制了其在现实生活中的应用。<br>
                    动机：为了解决这一问题，提出了一种跨模态蒸馏框架，以缩小简单跟踪器和强大跟踪器之间的性能差距。<br>
                    方法：具体来说，提出了一个特定-通用特征蒸馏模块，将深层双流网络的模态通用信息和模态特定信息转化为浅层单流网络。此外，还提出了一种多路径选择蒸馏模块，通过使用多种路径指导简单的融合模块从精心设计的融合机制中学习更准确的多模态信息。<br>
                    效果：在三个RGB-T基准测试上进行了广泛的实验验证，该方法实现了最先进的性能，但消耗的计算资源却少得多。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most current RGB-T trackers adopt a two-stream structure to extract unimodal RGB and thermal features and complex fusion strategies to achieve multi-modal feature fusion, which require a huge number of parameters, thus hindering their real-life applications. On the other hand, a compact RGB-T tracker may be computationally efficient but encounter non-negligible performance degradation, due to the weakening of feature representation ability. To remedy this situation, a cross-modality distillation framework is presented to bridge the performance gap between a compact tracker and a powerful tracker. Specifically, a specific-common feature distillation module is proposed to transform the modality-common information as well as the modality-specific information from a deeper two-stream network to a shallower single-stream network. In addition, a multi-path selection distillation module is proposed to instruct a simple fusion module to learn more accurate multi-modal information from a well-designed fusion mechanism by using multiple paths. We validate the effectiveness of our method with extensive experiments on three RGB-T benchmarks, which achieves state-of-the-art performance but consumes much less computational resources.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">975.VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking</span><br>
                <span class="as">Chen, YukangandLiu, JianhuiandZhang, XiangyuandQi, XiaojuanandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21674-21683.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D物体检测中需要借助手工制作的代理（如锚点或中心）以及将2D框架转化为3D的问题，同时处理稀疏体素特征的密度化和密集预测头部的处理，这无疑增加了额外的计算成本。<br>
                    动机：本文提出了一种全新的方法VoxelNext，该方法直接基于稀疏体素特征进行物体预测，而无需依赖手工制作的代理。<br>
                    方法：我们的核心思想是直接根据稀疏体素特征进行物体的检测和跟踪，不依赖于手工制作的代理。我们的强稀疏卷积网络VoxelNeXt通过体素特征完全进行3D物体的检测和跟踪。这是一种优雅且高效的框架，无需进行稀疏到密集的转换或NMS后处理。<br>
                    效果：在nuScenes数据集上，我们的方法比其他主流探测器实现了更好的速度-准确性权衡。首次证明，全稀疏的体素表示法可以很好地用于LIDAR 3D物体检测和跟踪。在nuScenes、Waymo和Argoverse2基准上的大量实验验证了我们方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detectors usually rely on hand-crafted proxies, e.g., anchors or centers, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel features need to be densified and processed by dense prediction heads, which inevitably costs extra computation. In this paper, we instead propose VoxelNext for fully sparse 3D object detection. Our core insight is to predict objects directly based on sparse voxel features, without relying on hand-crafted proxies. Our strong sparse convolutional network VoxelNeXt detects and tracks 3D objects through voxel features entirely. It is an elegant and efficient framework, with no need for sparse-to-dense conversion or NMS post-processing. Our method achieves a better speed-accuracy trade-off than other mainframe detectors on the nuScenes dataset. For the first time, we show that a fully sparse voxel-based representation works decently for LIDAR 3D object detection and tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2 benchmarks validate the effectiveness of our approach. Without bells and whistles, our model outperforms all existing LIDAR methods on the nuScenes tracking test benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">976.AttentionShift: Iteratively Estimated Part-Based Attention Map for Pointly Supervised Instance Segmentation</span><br>
                <span class="as">Liao, MingxiangandGuo, ZonghaoandWang, YuzeandYuan, PengandFeng, BailanandWan, Fang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_AttentionShift_Iteratively_Estimated_Part-Based_Attention_Map_for_Pointly_Supervised_Instance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19519-19528.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过单一监督点解决实例分割中的语义偏差和错误分割问题。<br>
                    动机：现有的点监督实例分割方法由于只使用对象的一个点进行监督，导致对象部分之间的语义差异大，产生语义偏差和错误分割。<br>
                    方法：提出一种迭代分解实例注意力图并估计每个部分细粒度语义的AttentionShift方法。该方法包括两步：一是生成点监督注意力图的标记查询；二是通过在特征空间中的关键部位过滤来重新估计基于部位的关注图。这两个步骤反复执行，以优化空间和特征空间中的基于部位的关注图，覆盖整个对象范围。<br>
                    效果：在PASCAL VOC和MS COCO 2017数据集上的实验表明，AttentionShift分别将最先进的mAP@0.5提高了7.7%和4.8%，为使用视觉转换器的点监督实例分割建立了坚实的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pointly supervised instance segmentation (PSIS) learns to segment objects using a single point within the object extent as supervision. Challenged by the non-negligible semantic variance between object parts, however, the single supervision point causes semantic bias and false segmentation. In this study, we propose an AttentionShift method, to solve the semantic bias issue by iteratively decomposing the instance attention map to parts and estimating fine-grained semantics of each part. AttentionShift consists of two modules plugged on the vision transformer backbone: (i) token querying for pointly supervised attention map generation, and (ii) key-point shift, which re-estimates part-based attention maps by key-point filtering in the feature space. These two steps are iteratively performed so that the part-based attention maps are optimized spatially as well as in the feature space to cover full object extent. Experiments on PASCAL VOC and MS COCO 2017 datasets show that AttentionShift respectively improves the state-of-the-art of by 7.7% and 4.8% under mAP@0.5, setting a solid PSIS baseline using vision transformer. Code is enclosed in the supplementary material.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">977.Spatial-Frequency Mutual Learning for Face Super-Resolution</span><br>
                <span class="as">Wang, ChenyangandJiang, JunjunandZhong, ZhiweiandLiu, Xianming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Spatial-Frequency_Mutual_Learning_for_Face_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22356-22366.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决面部超分辨率（FSR）问题，即如何从低分辨率（LR）人脸图像重建高分辨率（HR）图像。<br>
                    动机：尽管深度学习在面部超分辨率技术中取得了重大突破，但现有的方法要么具有固定的感知野，要么无法保持面部结构，限制了FSR的性能。<br>
                    方法：本文提出了一种基于傅里叶变换的空间频率互相关网络（SFMNet），这是首个探索空间和频率域之间相关性的FSR方法。具体来说，SFMNet是一个双分支网络，包括一个空间分支和一个频率分支。频率分支利用傅里叶变换实现图像大小的感知野并捕获全局依赖性，而空间分支则提取局部依赖性。考虑到这两种依赖性是互补的，并且都有利于FSR，我们进一步开发了一个频率-空间交互模块（FSIB），该模块将互补的空间和频率信息进行相互融合，增强了模型的能力。<br>
                    效果：定量和定性的实验结果表明，该方法在恢复人脸图像方面优于最先进的FSR方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face super-resolution (FSR) aims to reconstruct high-resolution (HR) face images from the low-resolution (LR) ones. With the advent of deep learning, the FSR technique has achieved significant breakthroughs. However, existing FSR methods either have a fixed receptive field or fail to maintain facial structure, limiting the FSR performance. To circumvent this problem, Fourier transform is introduced, which can capture global facial structure information and achieve image-size receptive field. Relying on the Fourier transform, we devise a spatial-frequency mutual network (SFMNet) for FSR, which is the first FSR method to explore the correlations between spatial and frequency domains as far as we know. To be specific, our SFMNet is a two-branch network equipped with a spatial branch and a frequency branch. Benefiting from the property of Fourier transform, the frequency branch can achieve image-size receptive field and capture global dependency while the spatial branch can extract local dependency. Considering that these dependencies are complementary and both favorable for FSR, we further develop a frequency-spatial interaction block (FSIB) which mutually amalgamates the complementary spatial and frequency information to enhance the capability of the model. Quantitative and qualitative experimental results show that the proposed method outperforms state-of-the-art FSR methods in recovering face images. The implementation and model will be released at https://github.com/wcy-cs/SFMNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">978.Efficient Frequency Domain-Based Transformers for High-Quality Image Deblurring</span><br>
                <span class="as">Kong, LingshunandDong, JiangxinandGe, JianjunandLi, MingqiangandPan, Jinshan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Efficient_Frequency_Domain-Based_Transformers_for_High-Quality_Image_Deblurring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5886-5895.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用Transformers在频率域中进行高质量的图像去模糊。<br>
                    动机：受到卷积定理的启发，即两个信号在空间域中的相关性或卷积等价于它们在频率域中的逐元素乘积。<br>
                    方法：开发了一种基于频率域的高效自注意力求解器（FSAS），通过逐元素乘法操作而不是空间域中的矩阵乘法来估计缩放的点积注意力。同时，提出了一种简单而有效的判别性频率域前馈网络（DFFN），引入基于JPEG压缩算法的门控机制，以判别性地确定应保留哪些特征的低频和高频信息用于潜在清晰图像恢复。<br>
                    效果：实验结果表明，该方法优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present an effective and efficient method that explores the properties of Transformers in the frequency domain for high-quality image deblurring. Our method is motivated by the convolution theorem that the correlation or convolution of two signals in the spatial domain is equivalent to an element-wise product of them in the frequency domain. This inspires us to develop an efficient frequency domain-based self-attention solver (FSAS) to estimate the scaled dot-product attention by an element-wise product operation instead of the matrix multiplication in the spatial domain. In addition, we note that simply using the naive feed-forward network (FFN) in Transformers does not generate good deblurred results. To overcome this problem, we propose a simple yet effective discriminative frequency domain-based FFN (DFFN), where we introduce a gated mechanism in the FFN based on the Joint Photographic Experts Group (JPEG) compression algorithm to discriminatively determine which low- and high-frequency information of the features should be preserved for latent clear image restoration. We formulate the proposed FSAS and DFFN into an asymmetrical network based on an encoder and decoder architecture, where the FSAS is only used in the decoder module for better image deblurring. Experimental results show that the proposed method performs favorably against the state-of-the-art approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">979.B-Spline Texture Coefficients Estimator for Screen Content Image Super-Resolution</span><br>
                <span class="as">Pak, ByeonghyunandLee, JaewonandJin, KyongHwan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pak_B-Spline_Texture_Coefficients_Estimator_for_Screen_Content_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10062-10071.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确处理屏幕内容图像（SCIs）的边缘和纹理，以在显示设备的分辨率与SCIs不同时最小化内容的失真。<br>
                    动机：由于SCIs包含许多信息丰富的组件，如文本和图形，其像素分布与自然图像不同，因此需要适当处理边缘和纹理。<br>
                    方法：提出一种使用B-splines的隐式神经表示方法进行任意比例的屏幕内容图像超分辨率（SCI SR）。该方法提取B-splines的比例、平移和平滑参数，然后通过多层感知器（MLP）使用估计的B-splines恢复高分辨率SCI。<br>
                    效果：该方法在所有放大因子上均优于基于变压器的重建方法和隐式傅里叶表示方法，因为B-spline基具有正约束和平铺支持。此外，我们的SR结果被预训练的场景文本识别网络以最高置信度识别为正确的文本字母。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Screen content images (SCIs) include many informative components, e.g., texts and graphics. Such content creates sharp edges or homogeneous areas, making a pixel distribution of SCI different from the natural image. Therefore, we need to properly handle the edges and textures to minimize information distortion of the contents when a display device's resolution differs from SCIs. To achieve this goal, we propose an implicit neural representation using B-splines for screen content image super-resolution (SCI SR) with arbitrary scales. Our method extracts scaling, translating, and smoothing parameters of B-splines. The followed multi-layer perceptron (MLP) uses the estimated B-splines to recover high-resolution SCI. Our network outperforms both a transformer-based reconstruction and an implicit Fourier representation method in almost upscaling factor, thanks to the positive constraint and compact support of the B-spline basis. Moreover, our SR results are recognized as correct text letters with the highest confidence by a pre-trained scene text recognition network. Source code is available at https://github.com/ByeongHyunPak/btc.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">980.Towards End-to-End Generative Modeling of Long Videos With Memory-Efficient Bidirectional Transformers</span><br>
                <span class="as">Yoo, JaehoonandKim, SeminandLee, DoyupandKim, ChiheonandHong, Seunghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yoo_Towards_End-to-End_Generative_Modeling_of_Long_Videos_With_Memory-Efficient_Bidirectional_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22888-22897.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视频生成中长时依赖的学习效率和推断速度。<br>
                    动机：自回归转换器在视频生成中表现出色，但因自注意力的二次复杂度无法直接学习视频中的长时依赖，且推理速度慢、易产生误差传播。<br>
                    方法：提出一种内存高效的双向转换器（MeBT），通过将可观察的上下文标记投影到固定数量的潜在标记并通过交叉注意解码被遮蔽的标记，实现从部分观察到的补丁并行解码整个视频的空间-时间体积。<br>
                    效果：该方法实现了编码和解码的线性时间复杂度，并在生成中等到较长的视频方面在质量和速度上均显著优于自回归转换器。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Autoregressive transformers have shown remarkable success in video generation. However, the transformers are prohibited from directly learning the long-term dependency in videos due to the quadratic complexity of self-attention, and inherently suffering from slow inference time and error propagation due to the autoregressive process. In this paper, we propose Memory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of long-term dependency in videos and fast inference. Based on recent advances in bidirectional transformers, our method learns to decode the entire spatio-temporal volume of a video in parallel from partially observed patches. The proposed transformer achieves a linear time complexity in both encoding and decoding, by projecting observable context tokens into a fixed number of latent tokens and conditioning them to decode the masked tokens through the cross-attention. Empowered by linear complexity and bidirectional modeling, our method demonstrates significant improvement over the autoregressive Transformers for generating moderately long videos in both quality and speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">981.Masked Representation Learning for Domain Generalized Stereo Matching</span><br>
                <span class="as">Rao, ZhiboandXiong, BangshuandHe, MingyiandDai, YuchaoandHe, RenjieandShen, ZhelunandLi, Xing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rao_Masked_Representation_Learning_for_Domain_Generalized_Stereo_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5435-5444.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度立体匹配方法在跨领域性能上取得了显著成就，但在不同训练阶段的性能泛化波动性方面存在问题。<br>
                    动机：受掩码表示学习和多任务学习的启发，设计了一种简单有效的用于领域泛化的立体匹配的掩码表示。<br>
                    方法：首先，将遮蔽的左图像和完整的右图像作为输入模型。然后，在特征提取模块后添加一个轻量级且简单的解码器来恢复原始的左图像。最后，通过将两个任务（立体匹配和图像重建）作为伪多任务学习框架进行训练，促使模型学习结构信息并提高泛化性能。<br>
                    效果：实验结果表明，该方法可以很容易地插入到当前的多种立体匹配模型中以提高泛化性能；该方法可以减少在不同训练阶段的性能泛化波动性；发现当前的方法倾向于在不同的训练阶段中选择最好的结果作为泛化性能，但实际上无法通过地面真实值选择最好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, many deep stereo matching methods have begun to focus on cross-domain performance, achieving impressive achievements. However, these methods did not deal with the significant volatility of generalization performance among different training epochs. Inspired by masked representation learning and multi-task learning, this paper designs a simple and effective masked representation for domain generalized stereo matching. First, we feed the masked left and complete right images as input into the models. Then, we add a lightweight and simple decoder following the feature extraction module to recover the original left image. Finally, we train the models with two tasks (stereo matching and image reconstruction) as a pseudo-multi-task learning framework, promoting models to learn structure information and to improve generalization performance. We implement our method on two well-known architectures (CFNet and LacGwcNet) to demonstrate its effectiveness. Experimental results on multi-datasets show that: (1) our method can be easily plugged into the current various stereo matching models to improve generalization performance; (2) our method can reduce the significant volatility of generalization performance among different training epochs; (3) we find that the current methods prefer to choose the best results among different training epochs as generalization performance, but it is impossible to select the best performance by ground truth in practice.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">982.EqMotion: Equivariant Multi-Agent Motion Prediction With Invariant Interaction Reasoning</span><br>
                <span class="as">Xu, ChenxinandTan, RobbyT.andTan, YuhongandChen, SihengandWang, YuGuangandWang, XinchaoandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1410-1420.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何预测具有关系推理的代理运动，以实现在欧几里得几何变换下的动态等变和代理交互的不变性。<br>
                    动机：现有的方法忽视了动态等变和交互不变的属性，这对于许多应用来说是非常重要的。<br>
                    方法：提出了EqMotion模型，通过设计等变操作来学习欧几里得可转换特征，实现动态等变；通过提出不变的交互推理模块来实现更稳定的交互建模；并通过学习不变的模式特征来进一步提升网络表现力。<br>
                    效果：在粒子动力学、分子动力学、人体骨骼运动预测和行人轨迹预测四个场景中进行了实验，结果表明该方法不仅具有通用性，而且在所有任务上都取得了最先进的预测性能，提高了24.0/30.1/8.6/9.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning to predict agent motions with relationship reasoning is important for many applications. In motion prediction tasks, maintaining motion equivariance under Euclidean geometric transformations and invariance of agent interaction is a critical and fundamental principle. However, such equivariance and invariance properties are overlooked by most existing methods. To fill this gap, we propose EqMotion, an efficient equivariant motion prediction model with invariant interaction reasoning. To achieve motion equivariance, we propose an equivariant geometric feature learning module to learn a Euclidean transformable feature through dedicated designs of equivariant operations. To reason agent's interactions, we propose an invariant interaction reasoning module to achieve a more stable interaction modeling. To further promote more comprehensive motion features, we propose an invariant pattern feature learning module to learn an invariant pattern feature, which cooperates with the equivariant geometric feature to enhance network expressiveness. We conduct experiments for the proposed model on four distinct scenarios: particle dynamics, molecule dynamics, human skeleton motion prediction and pedestrian trajectory prediction. Experimental results show that our method is not only generally applicable, but also achieves state-of-the-art prediction performances on all the four tasks, improving by 24.0/30.1/8.6/9.2%. Code is available at https://github.com/MediaBrain-SJTU/EqMotion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">983.FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation</span><br>
                <span class="as">Shi, XiaoyuandHuang, ZhaoyangandLi, DasongandZhang, ManyuanandCheung, KaChunandSee, SimonandQin, HongweiandDai, JifengandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1599-1610.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将Transformer架构应用于光流估计并实现最先进的性能。<br>
                    动机：受最近在解锁Transformer编码视觉表示能力的掩蔽自动编码（MAE）预训练成功启发，提出掩蔽成本体积自动编码（MCVA）以通过新颖的MAE方案预训练成本-体积编码器来增强FlowFormer。<br>
                    方法：首先引入块共享屏蔽策略防止屏蔽信息泄漏，因为相邻源像素的成本图高度相关。其次，提出一种新的预文本重建任务，鼓励成本-体积编码器聚合长范围信息并确保预训练-微调一致性。我们还展示了如何在预训练期间修改FlowFormer架构以适应屏蔽。<br>
                    效果：使用MCVA预训练的我们提出的FlowFormer++在Sintel和KITTI-2015基准测试中均排名第一。具体来说，FlowFormer++在Sintel基准测试的清洁和最终通道上实现了1.07和1.94的平均端点误差（AEPE），比FlowFormer减少了7.76%和7.18%的错误。 FlowFormer++在KITTI-2015测试集上获得了4.52的F1-all，比FlowFormer提高了0.16。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>FlowFormer introduces a transformer architecture into optical flow estimation and achieves state-of-the-art performance. The core component of FlowFormer is the transformer-based cost-volume encoder. Inspired by recent success of masked autoencoding (MAE) pretraining in unleashing transformers' capacity of encoding visual representation, we propose Masked Cost Volume Autoencoding (MCVA) to enhance FlowFormer by pretraining the cost-volume encoder with a novel MAE scheme. Firstly, we introduce a block-sharing masking strategy to prevent masked information leakage, as the cost maps of neighboring source pixels are highly correlated. Secondly, we propose a novel pre-text reconstruction task, which encourages the cost-volume encoder to aggregate long-range information and ensures pretraining-finetuning consistency. We also show how to modify the FlowFormer architecture to accommodate masks during pretraining. Pretrained with MCVA, our proposed FlowFormer++ ranks 1st among published methods on both Sintel and KITTI-2015 benchmarks. Specifically, FlowFormer++ achieves 1.07 and 1.94 average end-point-error (AEPE) on the clean and final pass of Sintel benchmark, leading to 7.76% and 7.18% error reductions from FlowFormer. FlowFormer++ obtains 4.52 F1-all on the KITTI-2015 test set, improving FlowFormer by 0.16.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">984.STMixer: A One-Stage Sparse Action Detector</span><br>
                <span class="as">Wu, TaoandCao, MengqiandGao, ZitengandWu, GangshanandWang, Limin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_STMixer_A_One-Stage_Sparse_Action_Detector_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14720-14729.png><br>
            
            <span class="tt"><span class="t0">研究问题：传统的视频动作检测器通常采用两阶段流程，需要多阶段训练和推理，并且无法捕获边界框外的环境信息。<br>
                    动机：为了解决传统方法的局限性，本文提出了一种新颖的单阶段稀疏动作检测器STMixer。<br>
                    方法：STMixer基于两个核心设计。首先，我们提出了一种基于查询的自适应特征采样模块，使STMixer能够从整个时空域中挖掘一组判别性特征。其次，我们设计了一个双分支特征混合模块，使STMixer能够分别在空间和时间维度上动态关注并混合视频特征，以实现更好的特征解码。<br>
                    效果：通过将这两个设计与视频主干网络相结合，我们得到了一个高效且准确的动作检测器。在没有额外优化的情况下，STMixer在AVA、UCF101-24和JHMDB等数据集上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Traditional video action detectors typically adopt the two-stage pipeline, where a person detector is first employed to yield actor boxes and then 3D RoIAlign is used to extract actor-specific features for classification. This detection paradigm requires multi-stage training and inference and cannot capture context information outside the bounding box. Recently, a few query-based action detectors are proposed to predict action instances in an end-to-end manner. However, they still lack adaptability in feature sampling or decoding, thus suffering from the issue of inferior performance or slower convergence. In this paper, we propose a new one-stage sparse action detector, termed STMixer. STMixer is based on two core designs. First, we present a query-based adaptive feature sampling module, which endows our STMixer with the flexibility of mining a set of discriminative features from the entire spatiotemporal domain. Second, we devise a dual-branch feature mixing module, which allows our STMixer to dynamically attend to and mix video features along the spatial and the temporal dimension respectively for better feature decoding. Coupling these two designs with a video backbone yields an efficient and accurate action detector. Without bells and whistles, STMixer obtains the state-of-the-art results on the datasets of AVA, UCF101-24, and JHMDB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">985.HRDFuse: Monocular 360deg Depth Estimation by Collaboratively Learning Holistic-With-Regional Depth Distributions</span><br>
                <span class="as">Ai, HaoandCao, ZidongandCao, Yan-PeiandShan, YingandWang, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ai_HRDFuse_Monocular_360deg_Depth_Estimation_by_Collaboratively_Learning_Holistic-With-Regional_Depth_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13273-13282.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目360度图像深度估计是一个新兴的问题，由于其对场景的整体感知。<br>
                    动机：现有的方法如OmniFusion等，通过切线投影（TP）表示360度图像，并通过分块回归预测深度值，然后将这些深度值合并以获取等距极射投影（ERP）格式的深度图。但这些方法存在1) 合并大量分块的复杂过程；2) 直接回归每个像素的深度值，捕获的区域局部上下文信息较少的问题。<br>
                    方法：我们提出了一种新的框架HRDFuse，通过协同学习ERP的整体上下文信息和TP的区域结构信息，巧妙地结合了卷积神经网络（CNNs）和变压器的潜力。首先，我们提出了一个空间特征对齐（SFA）模块，该模块学习TP和ERP之间的特征相似性，以像素为单位将TP特征聚合到完整的ERP特征图中。其次，我们提出了一个协同深度分布分类（CDDC）模块，该模块学习捕捉ERP和TP深度分布的整体-区域直方图。因此，最终的深度值可以预测为直方图bin中心的线性组合。最后，我们自适应地组合来自ERP和TP的深度预测，以获得最终的深度图。<br>
                    效果：大量的实验表明，我们的方法预测出更平滑、更准确的深度结果，同时比最先进的方法取得了更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Depth estimation from a monocular 360 image is a burgeoning problem owing to its holistic sensing of a scene. Recently, some methods, e.g., OmniFusion, have applied the tangent projection (TP) to represent a 360 image and predicted depth values via patch-wise regressions, which are merged to get a depth map with equirectangular projection (ERP) format. However, these methods suffer from 1) non-trivial process of merging a large number of patches; 2) capturing less holistic-with-regional contextual information by directly regressing the depth value of each pixel. In this paper, we propose a novel framework, HRDFuse, that subtly combines the potential of convolutional neural networks (CNNs) and transformers by collaboratively learning the holistic contextual information from the ERP and the regional structural information from the TP. Firstly, we propose a spatial feature alignment (SFA) module that learns feature similarities between the TP and ERP to aggregate the TP features into a complete ERP feature map in a pixel-wise manner. Secondly, we propose a collaborative depth distribution classification (CDDC) module that learns the holistic-with-regional histograms capturing the ERP and TP depth distributions. As such, the final depth values can be predicted as a linear combination of histogram bin centers. Lastly, we adaptively combine the depth predictions from ERP and TP to obtain the final depth map. Extensive experiments show that our method predicts more smooth and accurate depth results while achieving favorably better results than the SOTA methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">986.PATS: Patch Area Transportation With Subdivision for Local Feature Matching</span><br>
                <span class="as">Ni, JunjieandLi, YijinandHuang, ZhaoyangandLi, HongshengandBao, HujunandCui, ZhaopengandZhang, Guofeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17776-17786.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何建立图像对之间的稀疏对应关系，特别是在图像对具有大尺度差异的情况下。<br>
                    动机：现有的无检测器方法在处理大尺度差异的图像对时效果不佳。<br>
                    方法：提出Patch Area Transportation with Subdivision（PATS）方法。将原始图像对分割成等大小的补丁，并逐步调整和细分它们为相同尺度的较小补丁。通过学习补丁区域传输来估计这些补丁之间的尺度差异。<br>
                    效果：PATS提高了匹配的准确性和覆盖率，并在下游任务如相对姿态估计、视觉定位和光流估计等方面表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Local feature matching aims at establishing sparse correspondences between a pair of images. Recently, detector-free methods present generally better performance but are not satisfactory in image pairs with large scale differences. In this paper, we propose Patch Area Transportation with Subdivision (PATS) to tackle this issue. Instead of building an expensive image pyramid, we start by splitting the original image pair into equal-sized patches and gradually resizing and subdividing them into smaller patches with the same scale. However, estimating scale differences between these patches is non-trivial since the scale differences are determined by both relative camera poses and scene structures, and thus spatially varying over image pairs. Moreover, it is hard to obtain the ground truth for real scenes. To this end, we propose patch area transportation, which enables learning scale differences in a self-supervised manner. In contrast to bipartite graph matching, which only handles one-to-one matching, our patch area transportation can deal with many-to-many relationships. PATS improves both matching accuracy and coverage, and shows superior performance in downstream tasks, such as relative pose estimation, visual localization, and optical flow estimation.The source code will be released to benefit the community.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">987.G-MSM: Unsupervised Multi-Shape Matching With Graph-Based Affinity Priors</span><br>
                <span class="as">Eisenberger, MarvinandToker, AysimandLeal-Taix\&#x27;e, LauraandCremers, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Eisenberger_G-MSM_Unsupervised_Multi-Shape_Matching_With_Graph-Based_Affinity_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22762-22772.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的无监督学习方法G-MSM，用于非刚性形状对应。<br>
                    动机：现有的方法将输入姿态集合视为无序样本集，而未明确对底层形状数据流形进行建模。<br>
                    方法：提出了一种自适应的多形状匹配架构，在给定的训练形状集合上以自监督的方式构建亲和力图。主要思想是通过在底层形状图中沿着最短路径传播映射来组合可能的成对对应关系。<br>
                    效果：在多个最新的形状对应基准测试中，包括具有拓扑噪声的真实世界3D扫描网格和具有挑战性的类别间对，展示了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present G-MSM (Graph-based Multi-Shape Matching), a novel unsupervised learning approach for non-rigid shape correspondence. Rather than treating a collection of input poses as an unordered set of samples, we explicitly model the underlying shape data manifold. To this end, we propose an adaptive multi-shape matching architecture that constructs an affinity graph on a given set of training shapes in a self-supervised manner. The key idea is to combine putative, pairwise correspondences by propagating maps along shortest paths in the underlying shape graph. During training, we enforce cycle-consistency between such optimal paths and the pairwise matches which enables our model to learn topology-aware shape priors. We explore different classes of shape graphs and recover specific settings, like template-based matching (star graph) or learnable ranking/sorting (TSP graph), as special cases in our framework. Finally, we demonstrate state-of-the-art performance on several recent shape correspondence benchmarks, including real-world 3D scan meshes with topological noise and challenging inter-class pairs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">988.Enhancing Deformable Local Features by Jointly Learning To Detect and Describe Keypoints</span><br>
                <span class="as">Potje, GuilhermeandCadar, FelipeandAraujo, Andr\&#x27;eandMartins, RenatoandNascimento, EricksonR.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Potje_Enhancing_Deformable_Local_Features_by_Jointly_Learning_To_Detect_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1306-1315.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中图像匹配和检索等重要任务，以及非刚性变形表面的匹配问题。<br>
                    动机：大多数方法假设图像只经历仿射变换，忽视了更复杂的非刚性变形效果。此外，针对非刚性对应关系的新方法仍然依赖于为刚性变换设计的关键点检测器，由于检测器的限制，这阻碍了性能的提高。<br>
                    方法：我们提出了DALF（Deformation-Aware Local Features），一种新颖的变形感知网络，用于联合检测和描述关键点，以处理匹配可变形表面的挑战。所有网络组件通过特征融合方法协同工作，增强了描述符的独特性和不变性。<br>
                    效果：使用真实变形物体的实验展示了我们方法的优势，与之前的最佳结果相比，我们的匹配得分提高了8%。我们的方法还提高了两个实际应用的性能：可变形物体检索和非刚性3D表面注册。我们的训练、推理和应用代码已在verlab.dcc.ufmg.br/descriptors/dalf_cvpr23上公开。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Local feature extraction is a standard approach in computer vision for tackling important tasks such as image matching and retrieval. The core assumption of most methods is that images undergo affine transformations, disregarding more complicated effects such as non-rigid deformations. Furthermore, incipient works tailored for non-rigid correspondence still rely on keypoint detectors designed for rigid transformations, hindering performance due to the limitations of the detector. We propose DALF (Deformation-Aware Local Features), a novel deformation-aware network for jointly detecting and describing keypoints, to handle the challenging problem of matching deformable surfaces. All network components work cooperatively through a feature fusion approach that enforces the descriptors' distinctiveness and invariance. Experiments using real deforming objects showcase the superiority of our method, where it delivers 8% improvement in matching scores compared to the previous best results. Our approach also enhances the performance of two real-world applications: deformable object retrieval and non-rigid 3D surface registration. Code for training, inference, and applications are publicly available at verlab.dcc.ufmg.br/descriptors/dalf_cvpr23.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">989.Neighborhood Attention Transformer</span><br>
                <span class="as">Hassani, AliandWalton, StevenandLi, JiachenandLi, ShenandShi, Humphrey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6185-6194.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一种高效且可扩展的滑动窗口注意力机制用于视觉处理？<br>
                    动机：现有的自注意力（SA）计算复杂度为二次，而局部化的自注意力（NA）将复杂度降低到线性，同时保持了对邻近像素的关注。<br>
                    方法：提出邻居注意力（NA），这是一种像素级的运算，将自注意力局限在最近的邻近像素上，因此其时间和空间复杂度都优于SA。通过滑动窗口模式，NA无需额外的像素位移即可扩大感受野，并保留了平移等变性。<br>
                    效果：开发的NATTEN包使NA比Swin的WSA快40%，内存使用减少25%。基于NA的新层级转换器设计NAT在图像分类和下游视觉性能上都有所提升。实验结果表明，NAT-Tiny在ImageNet上的top-1准确率达到83.2%，在MS-COCO上的mAP达到51.4%，在ADE20K上的mIoU达到48.4%，相比类似大小的Swin模型分别提高了1.9%、1.0%和2.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Neighborhood Attention (NA), the first efficient and scalable sliding window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding window attention, we open source our project and release our checkpoints.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">990.Trap Attention: Monocular Depth Estimation With Manual Traps</span><br>
                <span class="as">Ning, ChaoandGan, Hongping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ning_Trap_Attention_Monocular_Depth_Estimation_With_Manual_Traps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5033-5043.png><br>
            
            <span class="tt"><span class="t0">研究问题：从单张图像预测高质量的深度图是一项具有挑战性的任务，因为将2D场景投影到相应的3D场景的可能性是无限的。<br>
                    动机：最近的研究引入了多头注意力（MHA）模块来执行长距离交互，这在回归深度图中显示出显著的进展。然而，由于MHA的二次复杂度，这些方法无法利用MHA以适当的计算复杂度在高分辨率中计算深度特征。<br>
                    方法：本文提出了一种新颖的陷阱注意力机制，通过在扩展空间中为每个像素设置陷阱，并通过卷积窗口的特征保留率形成注意力机制，将二次计算复杂度转换为线性形式。然后构建了一个编码器-解码器陷阱深度估计网络，该网络引入了视觉转换器作为编码器，并使用陷阱注意力在解码器中从单张图像估计深度。<br>
                    效果：大量的实验结果表明，我们提出的网络在单目深度估计任务上优于现有的最先进的方法，在NYU Depth-v2和KITTI数据集上，参数数量显著减少。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting a high quality depth map from a single image is a challenging task, because it exists infinite possibility to project a 2D scene to the corresponding 3D scene. Recently, some studies introduced multi-head attention (MHA) modules to perform long-range interaction, which have shown significant progress in regressing the depth maps.The main functions of MHA can be loosely summarized to capture long-distance information and report the attention map by the relationship between pixels. However, due to the quadratic complexity of MHA, these methods can not leverage MHA to compute depth features in high resolution with an appropriate computational complexity. In this paper, we exploit a depth-wise convolution to obtain long-range information, and propose a novel trap attention, which sets some traps on the extended space for each pixel, and forms the attention mechanism by the feature retention ratio of convolution window, resulting in that the quadratic computational complexity can be converted to linear form. Then we build an encoder-decoder trap depth estimation network, which introduces a vision transformer as the encoder, and uses the trap attention to estimate the depth from single image in the decoder. Extensive experimental results demonstrate that our proposed network can outperform the state-of-the-art methods in monocular depth estimation on datasets NYU Depth-v2 and KITTI, with significantly reduced number of parameters. Code is available at: https://github.com/ICSResearch/TrapAttention.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">991.Rethinking Few-Shot Medical Segmentation: A Vector Quantization View</span><br>
                <span class="as">Huang, ShiqiandXu, TingfaandShen, NingandMu, FengandLi, Jianan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Few-Shot_Medical_Segmentation_A_Vector_Quantization_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3072-3081.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的少数镜头医学分割网络在原型数量和性能之间存在正相关关系，但缺乏对特征点聚类和未见过任务的适应的关注。<br>
                    动机：观察到这一现象后，我们提出了一种学习向量量化（VQ）机制，包括网格格式VQ（GFVQ）、自组织VQ（SOVQ）和残差导向VQ（ROVQ）。<br>
                    方法：GFVQ通过在空间范围内平均方格生成原型矩阵，均匀量化局部细节；SOVQ将特征点自适应地分配给不同的局部类别，并在全局视图中创建一个新的表示空间，其中可学习的局部原型得到更新；ROVQ引入残差信息微调上述学习的局部原型，无需重新训练，有利于提高与训练任务无关的泛化性能。<br>
                    效果：我们的VQ框架在腹部、心脏和前列腺MRI数据集上取得了最先进的性能，并期望这项工作将引发对当前少数镜头医学分割模型设计的重新思考。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The existing few-shot medical segmentation networks share the same practice that the more prototypes, the better performance. This phenomenon can be theoretically interpreted in Vector Quantization (VQ) view: the more prototypes, the more clusters are separated from pixel-wise feature points distributed over the full space. However, as we further think about few-shot segmentation with this perspective, it is found that the clusterization of feature points and the adaptation to unseen tasks have not received enough attention. Motivated by the observation, we propose a learning VQ mechanism consisting of grid-format VQ (GFVQ), self-organized VQ (SOVQ) and residual oriented VQ (ROVQ). To be specific, GFVQ generates the prototype matrix by averaging square grids over the spatial extent, which uniformly quantizes the local details; SOVQ adaptively assigns the feature points to different local classes and creates a new representation space where the learnable local prototypes are updated with a global view; ROVQ introduces residual information to fine-tune the aforementioned learned local prototypes without re-training, which benefits the generalization performance for the irrelevance to the training task. We empirically show that our VQ framework yields the state-of-the-art performance over abdomen, cardiac and prostate MRI datasets and expect this work will provoke a rethink of the current few-shot medical segmentation model design. Our code will soon be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">992.ARO-Net: Learning Implicit Fields From Anchored Radial Observations</span><br>
                <span class="as">Wang, YizhiandHuang, ZeyuandShamir, ArielandHuang, HuiandZhang, HaoandHu, Ruizhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ARO-Net_Learning_Implicit_Fields_From_Anchored_Radial_Observations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3572-3581.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的形状编码方法，用于学习3D形状的隐式场表示，该方法对形状变化具有显著的通用性和泛化性。<br>
                    动机：通过从一组被称为锚点的视点进行部分观察来推理形状，我们的目标是开发一种通用且统一的 shape 表示方法。<br>
                    方法：我们采用固定的一系列锚点，并通过斐波那契采样来设计一个坐标基础的深度神经网络，以预测空间中查询点的占用值。与使用全局形状特征的先前神经隐含模型不同，我们的 shape 编码器在上下文、特定于查询的特征上操作。<br>
                    效果：我们在稀疏点云的表面重建上展示了我们网络的质量与通用性，并在新颖和未见过的物体类别上进行了测试，“一形”训练，并与最先进的神经和经典方法进行了重建和细分比较。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce anchored radial observations (ARO), a novel shape encoding for learning implicit field representation of 3D shapes that is category-agnostic and generalizable amid significant shape variations. The main idea behind our work is to reason about shapes through partial observations from a set of viewpoints, called anchors. We develop a general and unified shape representation by employing a fixed set of anchors, via Fibonacci sampling, and designing a coordinate-based deep neural network to predict the occupancy value of a query point in space. Differently from prior neural implicit models that use global shape feature, our shape encoder operates on contextual, query-specific features. To predict point occupancy, locally observed shape information from the perspective of the anchors surrounding the input query point are encoded and aggregated through an attention module, before implicit decoding is performed. We demonstrate the quality and generality of our network, coined ARO-Net, on surface reconstruction from sparse point clouds, with tests on novel and unseen object categories, "one-shape" training, and comparisons to state-of-the-art neural and classical methods for reconstruction and tessellation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">993.Learnable Skeleton-Aware 3D Point Cloud Sampling</span><br>
                <span class="as">Wen, ChengandYu, BaoshengandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Learnable_Skeleton-Aware_3D_Point_Cloud_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17671-17681.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行大规模点云分析，特别是在采样过程中保持对象几何和拓扑信息。<br>
                    动机：现有的任务特定采样方法通常无法明确探索物体的几何形状，因此需要一种新方法来改进这个问题。<br>
                    方法：提出一种新的骨架感知学习采样方法，通过学习物体骨架作为先验知识来保留采样过程中的对象几何和拓扑信息。具体来说，我们首先在无监督的方式下通过中轴线变换定义学习类别无关的物体骨架，然后利用这个骨架评估局部特征大小的直方图作为先验知识，从概率的角度进行骨架感知采样。此外，我们还通过探索重参数化技巧使得包含任务网络的骨架感知采样流程可以进行端到端的训练。<br>
                    效果：在点云分类、检索和重建这三个常见下游任务上的大量实验表明，我们的方法可以有效地进行大规模点云分析。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud sampling is crucial for efficient large-scale point cloud analysis, where learning-to-sample methods have recently received increasing attention from the community for jointly training with downstream tasks. However, the above-mentioned task-specific sampling methods usually fail to explore the geometries of objects in an explicit manner. In this paper, we introduce a new skeleton-aware learning-to-sample method by learning object skeletons as the prior knowledge to preserve the object geometry and topology information during sampling. Specifically, without labor-intensive annotations per object category, we first learn category-agnostic object skeletons via the medial axis transform definition in an unsupervised manner. With object skeleton, we then evaluate the histogram of the local feature size as the prior knowledge to formulate skeleton-aware sampling from a probabilistic perspective. Additionally, the proposed skeleton-aware sampling pipeline with the task network is thus end-to-end trainable by exploring the reparameterization trick. Extensive experiments on three popular downstream tasks, point cloud classification, retrieval, and reconstruction, demonstrate the effectiveness of the proposed method for efficient point cloud analysis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">994.Rotation-Invariant Transformer for Point Cloud Matching</span><br>
                <span class="as">Yu, HaoandQin, ZhengandHou, JiandSaleh, MahdiandLi, DongshengandBusam, BenjaminandIlic, Slobodan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Rotation-Invariant_Transformer_for_Point_Cloud_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5384-5393.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习点云匹配器通过数据增强获取旋转不变性，但面对罕见旋转时表现不稳定。<br>
                    动机：为了解决这一问题，我们提出了一种名为RoITr的旋转不变Transformer，以应对点云匹配任务中的姿态变化。<br>
                    方法：我们从局部层面引入了一种嵌入了基于点对特征（PPF）坐标的注意力机制，构建了一个新颖的注意力基础编码器-解码器架构。我们还提出了一个全局变换器，其旋转不变的跨帧空间意识是通过自我注意力机制学习的，这显著提高了特征的独特性，并使模型在低重叠情况下具有鲁棒性。<br>
                    效果：我们在刚性和非刚性公共基准上进行了实验，RoITr在所有低重叠场景中都大大超过了所有最先进的模型。特别是在具有挑战性的3DLoMatch基准上放大旋转时，RoITr在内联比和注册召回率方面至少分别超过现有方法13%和5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The intrinsic rotation invariance lies at the core of matching point clouds with handcrafted descriptors. However, it is widely despised by recent deep matchers that obtain the rotation invariance extrinsically via data augmentation. As the finite number of augmented rotations can never span the continuous SO(3) space, these methods usually show instability when facing rotations that are rarely seen. To this end, we introduce RoITr, a Rotation-Invariant Transformer to cope with the pose variations in the point cloud matching task. We contribute both on the local and global levels. Starting from the local level, we introduce an attention mechanism embedded with Point Pair Feature (PPF)-based coordinates to describe the pose-invariant geometry, upon which a novel attention-based encoder-decoder architecture is constructed. We further propose a global transformer with rotation-invariant cross-frame spatial awareness learned by the self-attention mechanism, which significantly improves the feature distinctiveness and makes the model robust with respect to the low overlap. Experiments are conducted on both the rigid and non-rigid public benchmarks, where RoITr outperforms all the state-of-the-art models by a considerable margin in the low-overlapping scenarios. Especially when the rotations are enlarged on the challenging 3DLoMatch benchmark, RoITr surpasses the existing methods by at least 13 and 5 percentage points in terms of Inlier Ratio and Registration Recall, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">995.ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection</span><br>
                <span class="as">Park, JeeseungandPark, Jin-WooandLee, Jong-Seok</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17152-17162.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高人体与物体交互（HOI）检测的性能。<br>
                    动机：虽然两阶段HOI检测器在训练和推理效率上有优势，但由于旧的骨干网络和缺乏对人体在交互分类器中的HOI感知过程的考虑，其性能低于一阶段方法。<br>
                    方法：提出一种基于视觉变压器的姿态条件自我循环图（ViPLO）来解决这些问题。首先，提出了一种适用于视觉变压器骨干的新型特征提取方法，称为带重叠区域的掩蔽（MOA）模块。其次，设计了一种带有姿态条件自我循环结构图，该图使用人类关节的局部特征更新人类节点编码，使分类器能够专注于特定的人体关节以有效识别交互类型。<br>
                    效果：实验结果表明，ViPLO在两个公共基准测试中取得了最先进的结果，特别是在HICO-DET数据集上获得了+2.07 mAP的性能增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-Object Interaction (HOI) detection, which localizes and infers relationships between human and objects, plays an important role in scene understanding. Although two-stage HOI detectors have advantages of high efficiency in training and inference, they suffer from lower performance than one-stage methods due to the old backbone networks and the lack of considerations for the HOI perception process of humans in the interaction classifiers. In this paper, we propose Vision Transformer based Pose-Conditioned Self-Loop Graph (ViPLO) to resolve these problems. First, we propose a novel feature extraction method suitable for the Vision Transformer backbone, called masking with overlapped area (MOA) module. The MOA module utilizes the overlapped area between each patch and the given region in the attention function, which addresses the quantization problem when using the Vision Transformer backbone. In addition, we design a graph with a pose-conditioned self-loop structure, which updates the human node encoding with local features of human joints. This allows the classifier to focus on specific human joints to effectively identify the type of interaction, which is motivated by the human perception process for HOI. As a result, ViPLO achieves the state-of-the-art results on two public benchmarks, especially obtaining a +2.07 mAP performance gain on the HICO-DET dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">996.Improving Table Structure Recognition With Visual-Alignment Sequential Coordinate Modeling</span><br>
                <span class="as">Huang, YongshuaiandLu, NingandChen, DapengandLi, YiboandXie, ZechengandZhu, ShenggaoandGao, LiangcaiandPeng, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Improving_Table_Structure_Recognition_With_Visual-Alignment_Sequential_Coordinate_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11134-11143.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地从无结构的表格图像中提取逻辑和物理结构。<br>
                    动机：现有的端到端图像到文本方法在预测物理结构（单元格的边界框）时，由于缺乏局部视觉信息，常常产生不精确的边界框。<br>
                    方法：提出了一种名为VAST的端到端序列建模框架进行表格结构识别。该模型包含一个由逻辑结构解码器表示的非空单元格触发的新型坐标序列解码器。在坐标序列解码器中，我们将边界框坐标建模为语言序列，并按顺序解码左、上、右和底部坐标以利用坐标间的依赖关系。此外，还提出了一种辅助的视觉对齐损失来强制非空单元格的逻辑表示包含更多的局部视觉细节，从而生成更好的单元格边界框。<br>
                    效果：大量实验证明，该方法在逻辑结构和物理结构识别上都取得了最先进的结果。消融研究也验证了提出的坐标序列解码器和视觉对齐损失是该方法成功的关键。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, as the logical representation lacks the local visual information, the previous methods often produce imprecise bounding boxes. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">997.WIRE: Wavelet Implicit Neural Representations</span><br>
                <span class="as">Saragadam, VishwanathandLeJeune, DanielandTan, JasperandBalakrishnan, GuhaandVeeraraghavan, AshokandBaraniuk, RichardG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18507-18516.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高隐式神经表示（INRs）的准确性和鲁棒性，同时避免其对信号噪声、参数变化等的敏感性。<br>
                    动机：当前的INRs虽然设计得具有较高的准确性，但同时也存在鲁棒性差的问题。<br>
                    方法：受谐波分析的启发，开发了一种新的、高精度且鲁棒的INR——小波隐式神经表示（WIRE）。它使用复Gabor小波作为激活函数，该函数在空间-频率上具有最佳的集中性，并具有优秀的图像表示偏差。<br>
                    效果：通过广泛的实验（图像去噪、图像修复、超分辨率、计算机断层扫描重建、图像过拟合以及使用神经辐射场的新视图合成），证明WIRE在INR的准确性、训练时间和鲁棒性方面定义了新的最先进水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representations (INRs) have recently advanced numerous vision-related areas. INR performance depends strongly on the choice of activation function employed in its MLP network. A wide range of nonlinearities have been explored, but, unfortunately, current INRs designed to have high accuracy also suffer from poor robustness (to signal noise, parameter variation, etc.). Inspired by harmonic analysis, we develop a new, highly accurate and robust INR that does not exhibit this tradeoff. Our Wavelet Implicit neural REpresentation (WIRE) uses as its activation function the complex Gabor wavelet that is well-known to be optimally concentrated in space--frequency and to have excellent biases for representing images. A wide range of experiments (image denoising, image inpainting, super-resolution, computed tomography reconstruction, image overfitting, and novel view synthesis with neural radiance fields) demonstrate that WIRE defines the new state of the art in INR accuracy, training time, and robustness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">998.Bi-Directional Feature Fusion Generative Adversarial Network for Ultra-High Resolution Pathological Image Virtual Re-Staining</span><br>
                <span class="as">Sun, KexinandChen, ZhinengandWang, GongweiandLiu, JunandYe, XiongjunandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Bi-Directional_Feature_Fusion_Generative_Adversarial_Network_for_Ultra-High_Resolution_Pathological_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3904-3913.png><br>
            
            <span class="tt"><span class="t0">研究问题：病理图像的高分辨率导致传统的虚拟重染色方法在生成大尺寸图像时存在颜色、亮度和对比度的差异，即“方格效应”。<br>
                    动机：由于病理检查的成本高昂，使得虚拟重染病理图像具有实际意义。然而，由于病理图像的超高分辨率，传统的虚拟重染方法必须将WSI图像分割成小块进行模型训练和推理，这导致全局信息的缺失，从而在合并重染的小块以生成更大尺寸的图像时出现明显的颜色、亮度和对比度差异。<br>
                    方法：为了消除“方格效应”，我们设计了一种具有全局分支和局部分支的双向特征融合生成对抗网络（BFF-GAN）。它通过全局和局部特征的融合以及块状注意力来学习块之间的连接。<br>
                    效果：我们在私有数据集RCC和公共数据集ANHIR上进行了实验。结果显示，我们的模型取得了有竞争力的性能，并且能够生成极其真实的图像，即使是经验丰富的病理学家也难以分辨，这意味着它具有重大的临床意义。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The cost of pathological examination makes virtual re-staining of pathological images meaningful. However, due to the ultra-high resolution of pathological images, traditional virtual re-staining methods have to divide a WSI image into patches for model training and inference. Such a limitation leads to the lack of global information, resulting in observable differences in color, brightness and contrast when the re-stained patches are merged to generate an image of larger size. We summarize this issue as the square effect. Some existing methods try to solve this issue through overlapping between patches or simple post-processing. But the former one is not that effective, while the latter one requires carefully tuning. In order to eliminate the square effect, we design a bi-directional feature fusion generative adversarial network (BFF-GAN) with a global branch and a local branch. It learns the inter-patch connections through the fusion of global and local features plus patch-wise attention. We perform experiments on both the private dataset RCC and the public dataset ANHIR. The results show that our model achieves competitive performance and is able to generate extremely real images that are deceptive even for experienced pathologists, which means it is of great clinical significance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">999.Feature Representation Learning With Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition</span><br>
                <span class="as">Zhai, ZhijunandZhao, JianhuiandLong, ChengjiangandXu, WenjuandHe, ShuangjiangandZhao, Huijuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhai_Feature_Representation_Learning_With_Adaptive_Displacement_Generation_and_Transformer_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22086-22095.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地识别微表情，这是一项具有挑战性的任务，因为微表情短暂且强度低，难以被识别。<br>
                    动机：由于微表情在非语言交流中的重要性，开发一种能够有效识别微表情的方法是必要的。<br>
                    方法：提出了一种新的框架——特征表示学习与自适应位移生成和变压器融合（FRL-DGT）。该框架使用卷积位移生成模块（DGM）进行自我监督学习以提取动态特征，然后通过精心设计的变压器融合机制（包括基于变压器的局部融合模块、全局融合模块和全脸融合模块）从DGM的输出中提取多级信息特征，用于最终的微表情预测。<br>
                    效果：广泛的实验和坚实的leave-one-subject-out（LOSO）评估结果强有力地证明了提出的FRL-DGT优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Micro-expressions are spontaneous, rapid and subtle facial movements that can neither be forged nor suppressed. They are very important nonverbal communication clues, but are transient and of low intensity thus difficult to recognize. Recently deep learning based methods have been developed for micro-expression recognition using feature extraction and fusion techniques, however, targeted feature learning and efficient feature fusion still lack further study according to micro-expression characteristics. To address these issues, we propose a novel framework Feature Representation Learning with adaptive Displacement Generation and Transformer fusion (FRL-DGT), in which a convolutional Displacement Generation Module (DGM) with self-supervised learning is used to extract dynamic feature targeted to the subsequent ME recognition task, and a well-designed Transformer fusion mechanism composed of the Transformer-based local fusion module, global fusion module, and full-face fusion module is applied to extract the multi-level informative feature from the output of the DGM for the final micro-expression prediction. Extensive experiments with solid leave-one-subject-out (LOSO) evaluation results have strongly demonstrated the superiority of our proposed FRL-DGT to state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1000.ViewNet: A Novel Projection-Based Backbone With View Pooling for Few-Shot Point Cloud Classification</span><br>
                <span class="as">Chen, JiajingandYang, MinminandVelipasalar, Senem</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViewNet_A_Novel_Projection-Based_Backbone_With_View_Pooling_for_Few-Shot_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17652-17660.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管已经提出了许多针对3D点云相关任务的方法，但少样本学习（FSL）在3D点云中的应用仍然是一个未充分探索的领域。<br>
                    动机：现有的FSL方法主要采用基于点的模型作为其主干网络，然而，我们通过大量的实验和分析发现，这种方法存在一些问题，如大量点的特征被丢弃，对遮挡敏感等。<br>
                    方法：为了解决这些问题，我们提出了一种新的基于投影和二维卷积神经网络的主干网络——ViewNet。该方法首先将3D点云投影到六个不同的视图上，以缓解丢失点的问题。同时，我们还提出了视图池化方法，将不同的投影平面组合成五组，并在每组上执行最大池化操作，以生成更具描述性和区分性的特征。<br>
                    效果：我们在ModelNet40、ScanObjectNN和ModelNet40-C数据集上进行了实验，并与其他最先进的基线方法进行了比较，结果显示我们的方法始终优于其他方法。此外，与ResNet等传统的图像分类主干网络相比，我们的ViewNet能够从点云的多个视图中提取出更多的区分性特征。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although different approaches have been proposed for 3D point cloud-related tasks, few-shot learning (FSL) of 3D point clouds still remains under-explored. In FSL, unlike traditional supervised learning, the classes of training and test data do not overlap, and a model needs to recognize unseen classes from only a few samples. Existing FSL methods for 3D point clouds employ point-based models as their backbone. Yet, based on our extensive experiments and analysis, we first show that using a point-based backbone is not the most suitable FSL approach, since (i) a large number of points' features are discarded by the max pooling operation used in 3D point-based backbones, decreasing the ability of representing shape information; (ii)point-based backbones are sensitive to occlusion. To address these issues, we propose employing a projection- and 2D Convolutional Neural Network-based backbone, referred to as the ViewNet, for FSL from 3D point clouds. Our approach first projects a 3D point cloud onto six different views to alleviate the issue of missing points. Also, to generate more descriptive and distinguishing features, we propose View Pooling, which combines different projected plane combinations into five groups and performs max-pooling on each of them. The experiments performed on the ModelNet40, ScanObjectNN and ModelNet40-C datasets, with cross validation, show that our method consistently outperforms the state-of-the-art baselines. Moreover, compared to traditional image classification backbones, such as ResNet, the proposed ViewNet can extract more distinguishing features from multiple views of a point cloud. We also show that ViewNet can be used as a backbone with different FSL heads and provides improved performance compared to traditionally used backbones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1001.HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning</span><br>
                <span class="as">Kuo, Chia-WenandKira, Zsolt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kuo_HAAV_Hierarchical_Aggregation_of_Augmented_Views_for_Image_Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11039-11049.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用异构编码来提高图像描述的性能？<br>
                    动机：随着更先进的编码方式的可用性和整合，如何高效地利用这些异构编码成为了一个自然的问题。<br>
                    方法：本文提出了一种新的图像描述模型，该模型将编码视为输入图像的增强视图，并使用共享编码器独立地对每个视图进行编码。然后，通过一种新颖的方式在编码视图之间引入对比损失，以提高它们的表示质量和模型的数据效率。<br>
                    效果：实验结果表明，与现有技术相比，该方法在MS-COCO和Flickr30k数据集上分别提高了+5.6%和+12.9%的CIDEr性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A great deal of progress has been made in image captioning, driven by research into how to encode the image using pre-trained models. This includes visual encodings (e.g. image grid features or detected objects) and more recently textual encodings (e.g. image tags or text descriptions of image regions). As more advanced encodings are available and incorporated, it is natural to ask: how to efficiently and effectively leverage the heterogeneous set of encodings? In this paper, we propose to regard the encodings as augmented views of the input image. The image captioning model encodes each view independently with a shared encoder efficiently, and a contrastive loss is incorporated across the encoded views in a novel way to improve their representation quality and the model's data efficiency. Our proposed hierarchical decoder then adaptively weighs the encoded views according to their effectiveness for caption generation by first aggregating within each view at the token level, and then across views at the view level. We demonstrate significant performance improvements of +5.6% CIDEr on MS-COCO and +12.9% CIDEr on Flickr30k compared to state of the arts,</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1002.End-to-End 3D Dense Captioning With Vote2Cap-DETR</span><br>
                <span class="as">Chen, SijinandZhu, HongyuanandChen, XinandLei, YinjieandYu, GangandChen, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11124-11133.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D密集描述任务，即生成与对象区域相关联的多个描述。<br>
                    动机：现有的方法遵循复杂的"检测然后描述"流程，并配备了许多手工制作的组件。然而，在面对不同场景中杂乱的对象空间和类别分布时，这些手工制作的组件可能会产生次优的性能。<br>
                    方法：本文提出了一种简单而有效的基于最近流行的DEtection TRansformer (DETR)的变压器框架Vote2Cap-DETR。相比于先前的方法，我们的框架有几个吸引人的优点：1）不依赖于大量的手工制作的组件，我们的方法基于一个完整的变压器编码器-解码器架构，具有可学习的投票查询驱动的对象解码器和一个以集合预测方式生成密集描述的标题解码器。2）与两阶段方案相比，我们的方法可以在一阶段进行检测和描述。3）没有花哨的东西，我们在两个常用的数据集ScanRefer和Nr3D上进行了广泛的实验，证明我们的Vote2Cap-DETR在CIDEr@0.5IoU上分别超过了当前最先进的11.13%和7.11%。<br>
                    效果：实验结果表明，我们的Vote2Cap-DETR在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D dense captioning aims to generate multiple captions localized with their associated object regions. Existing methods follow a sophisticated "detect-then-describe" pipeline equipped with numerous hand-crafted components. However, these hand-crafted components would yield suboptimal performance given cluttered object spatial and class distributions among different scenes. In this paper, we propose a simple-yet-effective transformer framework Vote2Cap-DETR based on recent popular DEtection TRansformer (DETR). Compared with prior arts, our framework has several appealing advantages: 1) Without resorting to numerous hand-crafted components, our method is based on a full transformer encoder-decoder architecture with a learnable vote query driven object decoder, and a caption decoder that produces the dense captions in a set-prediction manner. 2) In contrast to the two-stage scheme, our method can perform detection and captioning in one-stage. 3) Without bells and whistles, extensive experiments on two commonly used datasets, ScanRefer and Nr3D, demonstrate that our Vote2Cap-DETR surpasses current state-of-the-arts by 11.13% and 7.11% in CIDEr@0.5IoU, respectively. Codes will be released soon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1003.Optimization-Inspired Cross-Attention Transformer for Compressive Sensing</span><br>
                <span class="as">Song, JiechongandMou, ChongandWang, ShiqiandMa, SiweiandZhang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Optimization-Inspired_Cross-Attention_Transformer_for_Compressive_Sensing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6174-6184.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度展开网络在提高图像质量时，往往需要大量的参数，且在迭代过程中存在特征信息损失的问题。<br>
                    动机：通过将优化求解器与深度神经网络结合，设计出具有良好解释性和高性能的深度展开网络。<br>
                    方法：提出一种基于优化启发的交叉注意力转换器（OCT）模块作为迭代过程，构建了一种轻量级的OCT基础展开框架（OCTUF）。并设计了一种新的双交叉注意力（Dual-CA）子模块，包括惯性供应交叉注意力（ISCA）块和投影引导交叉注意力（PGCA）块。<br>
                    效果：实验证明，相比于最先进的方法，OCTUF在训练复杂度更低的情况下，取得了优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>By integrating certain optimization solvers with deep neural networks, deep unfolding network (DUN) with good interpretability and high performance has attracted growing attention in compressive sensing (CS). However, existing DUNs often improve the visual quality at the price of a large number of parameters and have the problem of feature information loss during iteration. In this paper, we propose an Optimization-inspired Cross-attention Transformer (OCT) module as an iterative process, leading to a lightweight OCT-based Unfolding Framework (OCTUF) for image CS. Specifically, we design a novel Dual Cross Attention (Dual-CA) sub-module, which consists of an Inertia-Supplied Cross Attention (ISCA) block and a Projection-Guided Cross Attention (PGCA) block. ISCA block introduces multi-channel inertia forces and increases the memory effect by a cross attention mechanism between adjacent iterations. And, PGCA block achieves an enhanced information interaction, which introduces the inertia force into the gradient descent step through a cross attention block. Extensive CS experiments manifest that our OCTUF achieves superior performance compared to state-of-the-art methods while training lower complexity. Codes are available at https://github.com/songjiechong/OCTUF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1004.TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization</span><br>
                <span class="as">Guillaro, FabrizioandCozzolino, DavideandSud, AvneeshandDufour, NicholasandVerdoliva, Luisa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20606-20615.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种适用于各种图像处理方式的取证框架。<br>
                    动机：现有的取证方法无法有效应对从传统低成本假图片到基于深度学习的最新处理方式的图像篡改。<br>
                    方法：通过结合RGB图像和学习到的对噪声敏感的指纹（通过仅在真实数据上进行自我监督训练来嵌入与相机内部和外部处理相关的伪影）的转换器基础融合架构提取高级和低级痕迹。<br>
                    效果：该方法能够可靠地检测并定位各种局部篡改，并在几个数据集上的实验中表现出色，优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper we present TruFor, a forensic framework that can be applied to a large variety of image manipulation methods, from classic cheapfakes to more recent manipulations based on deep learning. We rely on the extraction of both high-level and low-level traces through a transformer-based fusion architecture that combines the RGB image and a learned noise-sensitive fingerprint. The latter learns to embed the artifacts related to the camera internal and external processing by training only on real data in a self-supervised manner. Forgeries are detected as deviations from the expected regular pattern that characterizes each pristine image. Looking for anomalies makes the approach able to robustly detect a variety of local manipulations, ensuring generalization. In addition to a pixel-level localization map and a whole-image integrity score, our approach outputs a reliability map that highlights areas where localization predictions may be error-prone. This is particularly important in forensic applications in order to reduce false alarms and allow for a large scale analysis. Extensive experiments on several datasets show that our method is able to reliably detect and localize both cheapfakes and deepfakes manipulations outperforming state-of-the-art works. Code is publicly available at https://grip-unina.github.io/TruFor/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1005.Topology-Guided Multi-Class Cell Context Generation for Digital Pathology</span><br>
                <span class="as">Abousamra, ShahiraandGupta, RajarsiandKurc, TahsinandSamaras, DimitrisandSaltz, JoelandChen, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Abousamra_Topology-Guided_Multi-Class_Cell_Context_Generation_for_Digital_Pathology_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3323-3333.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用细胞的空间上下文进行细胞分类、癌症诊断和预后。<br>
                    动机：细胞形成不同的混合物、谱系、簇和孔洞，要学习模型这种复杂的细胞结构模式具有挑战性。<br>
                    方法：引入空间统计和拓扑数据分析的数学工具，将结构性描述符整合到深度生成模型中作为条件输入和可微损失。<br>
                    效果：首次生成高质量的多类细胞布局，证明富含拓扑结构的细胞布局可用于数据增强并提高下游任务（如细胞分类）的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In digital pathology, the spatial context of cells is important for cell classification, cancer diagnosis and prognosis. To model such complex cell context, however, is challenging. Cells form different mixtures, lineages, clusters and holes. To model such structural patterns in a learnable fashion, we introduce several mathematical tools from spatial statistics and topological data analysis. We incorporate such structural descriptors into a deep generative model as both conditional inputs and a differentiable loss. This way, we are able to generate high quality multi-class cell layouts for the first time. We show that the topology-rich cell layouts can be used for data augmentation and improve the performance of downstream tasks such as cell classification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1006.Learning Steerable Function for Efficient Image Resampling</span><br>
                <span class="as">Li, JiachengandChen, ChangandHuang, WeiandLang, ZhiqiangandSong, FenglongandYan, YouliangandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_Steerable_Function_for_Efficient_Image_Resampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5866-5875.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高图像重采样的效率和连续性？<br>
                    动机：现有的深度学习网络在重采样性能上取得了显著进步，但效率和连续性仍是问题。<br>
                    方法：提出一种学习重采样函数（LeRF）的新方法，利用深度学习网络学习的结构化先验知识和插值方法的局部连续假设。通过空间变化的可定向重采样函数对输入图像像素进行赋值，并使用神经网络预测确定这些重采样函数方向的超参数。<br>
                    效果：实验表明，该方法运行速度快，能适应任意变换，且在性能上优于插值方法，例如，在Manga109数据集上的x2上采样任务中，比双三次插值提高了3dB的PSNR。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image resampling is a basic technique that is widely employed in daily applications. Existing deep neural networks (DNNs) have made impressive progress in resampling performance. Yet these methods are still not the perfect substitute for interpolation, due to the issues of efficiency and continuous resampling. In this work, we propose a novel method of Learning Resampling Function (termed LeRF), which takes advantage of both the structural priors learned by DNNs and the locally continuous assumption of interpolation methods. Specifically, LeRF assigns spatially-varying steerable resampling functions to input image pixels and learns to predict the hyper-parameters that determine the orientations of these resampling functions with a neural network. To achieve highly efficient inference, we adopt look-up tables (LUTs) to accelerate the inference of the learned neural network. Furthermore, we design a directional ensemble strategy and edge-sensitive indexing patterns to better capture local structures. Extensive experiments show that our method runs as fast as interpolation, generalizes well to arbitrary transformations, and outperforms interpolation significantly, e.g., up to 3dB PSNR gain over bicubic for x2 upsampling on Manga109.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1007.TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers</span><br>
                <span class="as">Zhang, ChengandLiu, HaiandDeng, YongjianandXie, BochenandLi, Youfu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8897-8906.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何应对头部姿态估计中极端的头部姿态随机性和严重的遮挡问题。<br>
                    动机：现有的方法无法处理头部姿态估计中的极端随机性和严重遮挡问题。<br>
                    方法：提出了一种基于Transformer架构的新型关键少数关系感知方法，通过设计方向令牌来明确编码基本方向区域，并设计了一种新的令牌引导多损失函数来指导方向令牌学习所需的区域相似性和关系。<br>
                    效果：在三个具有挑战性的基准HPE数据集上进行评估，实验表明该方法比现有方法表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Head pose estimation (HPE) has been widely used in the fields of human machine interaction, self-driving, and attention estimation. However, existing methods cannot deal with extreme head pose randomness and serious occlusions. To address these challenges, we identify three cues from head images, namely, neighborhood similarities, significant facial changes, and critical minority relationships. To leverage the observed findings, we propose a novel critical minority relationship-aware method based on the Transformer architecture in which the facial part relationships can be learned. Specifically, we design several orientation tokens to explicitly encode the basic orientation regions. Meanwhile, a novel token guide multi-loss function is designed to guide the orientation tokens as they learn the desired regional similarities and relationships. We evaluate the proposed method on three challenging benchmark HPE datasets. Experiments show that our method achieves better performance compared with state-of-the-art methods. Our code is publicly available at https://github.com/zc2023/TokenHPE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1008.RIFormer: Keep Your Vision Backbone Effective but Removing Token Mixer</span><br>
                <span class="as">Wang, JiahaoandZhang, SongyangandLiu, YongandWu, TaiqiangandYang, YujiuandLiu, XihuiandChen, KaiandLuo, PingandLin, Dahua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RIFormer_Keep_Your_Vision_Backbone_Effective_but_Removing_Token_Mixer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14443-14452.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在移除视觉Transformer中的基本构建模块中的标记混合器的同时，保持视觉主干模型的有效性。<br>
                    动机：标记混合器作为视觉变换器的自注意力机制，用于在不同的空间标记之间进行信息交流，但计算成本和延迟较高。直接移除它们会导致模型结构不完整，从而带来显著的准确性下降。<br>
                    方法：首先基于重参数化思想开发了一个RepIdentityFormer，以研究无标记混合器模型架构。然后探索了改进的学习范式，打破了简单无标记混合器主干的局限性，并将经验实践总结为5条准则。通过采用提出的优化策略，我们能够构建一个非常简单的视觉主干模型，同时在推理过程中具有高效率。大量的实验和消融分析还表明，网络架构的归纳偏置可以与适当的优化策略结合到简单的网络结构中。<br>
                    效果：实验结果表明，在移除标记混合器的同时，使用优化策略可以在保持模型有效性的同时提高模型的效率。我们希望这项工作能成为优化驱动的有效网络设计的探索起点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies how to keep a vision backbone effective while removing token mixers in its basic building blocks. Token mixers, as self-attention for vision transformers (ViTs), are intended to perform information communication between different spatial tokens but suffer from considerable computational cost and latency. However, directly removing them will lead to an incomplete model structure prior, and thus brings a significant accuracy drop. To this end, we first develop an RepIdentityFormer base on the re-parameterizing idea, to study the token mixer free model architecture. And we then explore the improved learning paradigm to break the limitation of simple token mixer free backbone, and summarize the empirical practice into 5 guidelines. Equipped with the proposed optimization strategy, we are able to build an extremely simple vision backbone with encouraging performance, while enjoying the high efficiency during inference. Extensive experiments and ablative analysis also demonstrate that the inductive bias of network architecture, can be incorporated into simple network structure with appropriate optimization strategy. We hope this work can serve as a starting point for the exploration of optimization-driven efficient network design.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1009.Context-Based Trit-Plane Coding for Progressive Image Compression</span><br>
                <span class="as">Jeon, SeungminandChoi, KwangPyoandPark, YoungoandKim, Chang-Su</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Context-Based_Trit-Plane_Coding_for_Progressive_Image_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14348-14357.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用自动回归上下文模型实现深度渐进式图像压缩。<br>
                    动机：目前的三进制平面编码虽然可以实现深度渐进式图像压缩，但不能使用自动回归上下文模型。<br>
                    方法：提出基于上下文的三进制平面编码（CTC）算法，通过开发基于上下文的率降低模块和基于上下文的失真降低模块，以及设计解码器再训练方案，实现更紧凑的渐进式压缩。<br>
                    效果：实验表明，CTC在Kodak无损数据集上的BD-rate比基线三进制平面编码器提高了14.84%，而时间复杂度仅略有增加。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Trit-plane coding enables deep progressive image compression, but it cannot use autoregressive context models. In this paper, we propose the context-based trit-plane coding (CTC) algorithm to achieve progressive compression more compactly. First, we develop the context-based rate reduction module to estimate trit probabilities of latent elements accurately and thus encode the trit-planes compactly. Second, we develop the context-based distortion reduction module to refine partial latent tensors from the trit-planes and improve the reconstructed image quality. Third, we propose a retraining scheme for the decoder to attain better rate-distortion tradeoffs. Extensive experiments show that CTC outperforms the baseline trit-plane codec significantly, e.g. by -14.84% in BD-rate on the Kodak lossless dataset, while increasing the time complexity only marginally. The source codes are available at https://github.com/seungminjeon-github/CTC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1010.Self-Supervised Learning for Multimodal Non-Rigid 3D Shape Matching</span><br>
                <span class="as">Cao, DongliangandBernard, Florian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Self-Supervised_Learning_for_Multimodal_Non-Rigid_3D_Shape_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17735-17744.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高3D形状匹配的质量，特别是在处理点云和网格这两种不同表示形式时。<br>
                    动机：虽然点云是原始真实世界3D数据（如激光扫描仪）的常见表示，但网格编码丰富且富有表现力的结构信息，但其创建通常需要某种形式的（通常是手动的）整理。反过来，纯粹依赖点云的方法无法满足利用额外拓扑结构的网格方法的匹配质量。<br>
                    方法：我们引入了一种自我监督的多模态学习方法，该方法结合了基于网格的功能地图正则化和耦合网格和点云数据的对比损失。我们的形态匹配方法允许获取三角形网格、完整点云和部分观测点云的模内对应关系，以及这些数据模态之间的对应关系。<br>
                    效果：我们在几个具有挑战性的基准数据集上展示了我们的方法实现了最先进的结果，甚至与最近的有监督方法相比也是如此，并且我们的方法达到了前所未有的跨数据集泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The matching of 3D shapes has been extensively studied for shapes represented as surface meshes, as well as for shapes represented as point clouds. While point clouds are a common representation of raw real-world 3D data (e.g. from laser scanners), meshes encode rich and expressive topological information, but their creation typically requires some form of (often manual) curation. In turn, methods that purely rely on point clouds are unable to meet the matching quality of mesh-based methods that utilise the additional topological structure. In this work we close this gap by introducing a self-supervised multimodal learning strategy that combines mesh-based functional map regularisation with a contrastive loss that couples mesh and point cloud data. Our shape matching approach allows to obtain intramodal correspondences for triangle meshes, complete point clouds, and partially observed point clouds, as well as correspondences across these data modalities. We demonstrate that our method achieves state-of-the-art results on several challenging benchmark datasets even in comparison to recent supervised methods, and that our method reaches previously unseen cross-dataset generalisation ability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1011.Recurrent Vision Transformers for Object Detection With Event Cameras</span><br>
                <span class="as">Gehrig, MathiasandScaramuzza, Davide</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13884-13893.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的对象检测方法，利用事件相机和循环视觉转换器（RVTs）作为新的视觉主干。<br>
                    动机：事件相机具有亚毫秒级的延迟、高动态范围和对运动模糊的强大鲁棒性等独特属性，为低延迟的对象检测和跟踪提供了巨大的潜力。<br>
                    方法：通过重新设计循环视觉主干，实现了在保持相似性能的同时将推理时间减少6倍。具体来说，我们采用了三个关键概念：一是可以视为条件位置嵌入的卷积先验；二是用于空间特征交互的局部和扩张全局自注意力；三是用于最小化延迟同时保留时间信息的递归时间特征聚合。<br>
                    效果：实验结果表明，RVTs可以在事件驱动的对象检测上达到最先进的性能，实现Gen1汽车数据集上的mAP为47.2%。同时，RVTs具有快速的推理能力（在T4 GPU上小于12ms）和良好的参数效率（比现有技术少5倍）。这项研究为超越事件驱动视觉的有效设计选择提供了新的见解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Recurrent Vision Transformers (RVTs), a novel backbone for object detection with event cameras. Event cameras provide visual information with sub-millisecond latency at a high-dynamic range and with strong robustness against motion blur. These unique properties offer great potential for low-latency object detection and tracking in time-critical scenarios. Prior work in event-based vision has achieved outstanding detection performance but at the cost of substantial inference time, typically beyond 40 milliseconds. By revisiting the high-level design of recurrent vision backbones, we reduce inference time by a factor of 6 while retaining similar performance. To achieve this, we explore a multi-stage design that utilizes three key concepts in each stage: First, a convolutional prior that can be regarded as a conditional positional embedding. Second, local- and dilated global self-attention for spatial feature interaction. Third, recurrent temporal feature aggregation to minimize latency while retaining temporal information. RVTs can be trained from scratch to reach state-of-the-art performance on event-based object detection - achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time, RVTs offer fast inference (<12 ms on a T4 GPU) and favorable parameter efficiency (5 times fewer than prior art). Our study brings new insights into effective design choices that can be fruitful for research beyond event-based vision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1012.METransformer: Radiology Report Generation by Transformer With Multiple Learnable Expert Tokens</span><br>
                <span class="as">Wang, ZhanyuandLiu, LingqiaoandWang, LeiandZhou, Luping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11558-11567.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用多专家联合诊断机制提升现有单专家框架在临床场景中的诊断效果。<br>
                    动机：多专家咨询在复杂病例的诊断中具有显著优势，因此探索一种“多专家联合诊断”机制以改进现有文献中的“单专家”框架。<br>
                    方法：提出METransformer，该方法采用基于变压器的主干实现这一想法。关键设计是在变压器编码器和解码器中引入多个可学习的“专家”令牌。在编码器中，每个专家令牌与视觉令牌和其他专家令牌交互，学习关注图像的不同区域以进行图像表示。通过最小化它们重叠的正交损失来鼓励这些专家令牌捕获互补信息。在解码器中，每个被关注的专家令牌引导输入单词和视觉令牌之间的交叉注意力，从而影响生成的报告。进一步开发了一个基于指标的专家投票策略来生成最终报告。<br>
                    效果：实验结果表明，所提出的模型在两个广泛使用的基准测试上表现出有希望的性能。最后但并非最不重要的是，该框架级别的创新使我们的工作能够整合现有“单专家”模型的进步，进一步提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In clinical scenarios, multi-specialist consultation could significantly benefit the diagnosis, especially for intricate cases. This inspires us to explore a "multi-expert joint diagnosis" mechanism to upgrade the existing "single expert" framework commonly seen in the current literature. To this end, we propose METransformer, a method to realize this idea with a transformer-based backbone. The key design of our method is the introduction of multiple learnable "expert" tokens into both the transformer encoder and decoder. In the encoder, each expert token interacts with both vision tokens and other expert tokens to learn to attend different image regions for image representation. These expert tokens are encouraged to capture complementary information by an orthogonal loss that minimizes their overlap. In the decoder, each attended expert token guides the cross-attention between input words and visual tokens, thus influencing the generated report. A metrics-based expert voting strategy is further developed to generate the final report. By the multi-experts concept, our model enjoys the merits of an ensemble-based approach but through a manner that is computationally more efficient and supports more sophisticated interactions among experts. Experimental results demonstrate the promising performance of our proposed model on two widely used benchmarks. Last but not least, the framework-level innovation makes our work ready to incorporate advances on existing "single-expert" models to further improve its performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1013.Omni Aggregation Networks for Lightweight Image Super-Resolution</span><br>
                <span class="as">Wang, HangandChen, XuanhongandNi, BingbingandLiu, YutianandLiu, Jinfan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Omni_Aggregation_Networks_for_Lightweight_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22378-22387.png><br>
            
            <span class="tt"><span class="t0">研究问题：ViT框架在图像超分辨率方面取得了巨大进步，但其单研究问题：ViT框架在图像超分辨率方面取得了巨大进步，但其单维自注意力模型和同质聚合方案限制了其有效感受野（ERF），无法包含更全面的来自空间和通道维度的交互。<br>
                    动机：为了解决这些缺点，本文提出了一种新的Omni-SR架构，包括两个增强组件。<br>
                    方法：首先，基于密集交互原理提出了Omni Self-Attention（OSA）范式，可以同时从空间和通道维度对像素交互进行建模，挖掘跨omni轴（即空间和通道）的潜在相关性。与主流的窗口划分策略结合，OSA可以在具有强大计算预算的情况下实现优越的性能。其次，提出了一种多尺度交互方案，以减轻浅层模型中次优ERF（即过早饱和）的问题，促进局部传播和中/全局尺度的交互，形成一个omni-scale聚合构建块。<br>
                    效果：大量实验证明，Omni-SR在轻量级超分辨率基准测试中实现了创纪录的性能（例如，仅使用792K参数时，Urban100 x4达到26.95dB）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While lightweight ViT framework has made tremendous progress in image super-resolution, its uni-dimensional self-attention modeling, as well as homogeneous aggregation scheme, limit its effective receptive field (ERF) to include more comprehensive interactions from both spatial and channel dimensions. To tackle these drawbacks, this work proposes two enhanced components under a new Omni-SR architecture. First, an Omni Self-Attention (OSA) paradigm is proposed based on dense interaction principle, which can simultaneously model pixel-interaction from both spatial and channel dimensions, mining the potential correlations across omni-axis (i.e., spatial and channel). Coupling with mainstream window partitioning strategies, OSA can achieve superior performance with compelling computational budgets. Second, a multi-scale interaction scheme is proposed to mitigate sub-optimal ERF (i.e., premature saturation) in shallow models, which facilitates local propagation and meso-/global-scale interactions, rendering a omni-scale aggregation building block. Extensive experiments demonstrate that Omni-SR achieves record-high performance on lightweight super-resolution benchmarks (e.g., 26.95dB@Urban100 x4 with only 792K parameters). Our code is available at https://github.com/Francis0625/Omni-SR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1014.Correlational Image Modeling for Self-Supervised Visual Pre-Training</span><br>
                <span class="as">Li, WeiandXie, JiahaoandLoy, ChenChange</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Correlational_Image_Modeling_for_Self-Supervised_Visual_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15105-15115.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在介绍一种新的、有效的自我监督视觉预训练方法——关联图像建模（CIM）。<br>
                    动机：目前的预训练模型在视觉任务上的表现仍有提升空间，作者提出通过自我监督的方式对图像进行预训练。<br>
                    方法：CIM采用一种简单的预训练任务：从输入图像中随机裁剪出图像区域（样例），并预测样例与上下文之间的关联映射。设计了三个关键部分使得关联图像建模成为一个有意义且不平凡的自我监督任务。首先，为了生成有用的样例-上下文对，考虑以不同的尺度、形状、旋转和变换来裁剪图像区域。其次，采用引导学习和目标网络的自举学习框架。在预训练过程中，前者将样例作为输入，后者将上下文进行转换。最后，通过一个简单的交叉注意力模块来对输出的关联映射进行建模，其中上下文作为查询，样例提供值和键。<br>
                    效果：实验结果表明，CIM在自我监督和迁移基准测试上的表现与当前最先进的技术相当甚至更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Correlational Image Modeling (CIM), a novel but surprisingly effective approach to self-supervised visual pre-training. Our CIM performs a simple pretext task: we randomly crop image regions (exemplar) from an input image (context) and predict correlation maps between the exemplars and the context. Three key designs enable correlational image modeling as a nontrivial and meaningful self-supervisory task. First, to generate useful exemplar-context pairs, we consider cropping image regions with various scales, shapes, rotations, and transformations. Second, we employ a bootstrap learning framework that involves online and target networks. During pre-training, the former takes exemplars as inputs while the latter converts the context. Third, we model the output correlation maps via a simple cross-attention block, within which the context serves as queries and the exemplars offer values and keys. We show that CIM performs on par or better than the current state of the art on self-supervised and transfer benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1015.Self-Supervised Implicit Glyph Attention for Text Recognition</span><br>
                <span class="as">Guan, TongkunandGu, ChaochenandTu, JingzhengandYang, XueandFeng, QiandZhao, YudiandShen, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_Self-Supervised_Implicit_Glyph_Attention_for_Text_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15285-15294.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高场景文本识别（STR）中的注意力机制效果。<br>
                    动机：目前的注意力机制，无论是隐式注意力还是监督注意力，都存在一些问题，如隐式注意力可能提取到粗略的或错误的注意力区域，而监督注意力需要大量的字符级边界框注释且类别特定。<br>
                    方法：提出一种新的注意力机制——自我监督隐式字形注意力（SIGA）。SIGA通过联合自我监督的文本分割和隐式注意力对齐来描绘文本图像的字形结构，以此作为改进注意力正确性的监督，而无需额外的字符级注释。<br>
                    效果：实验结果表明，SIGA在注意力正确性和最终识别性能上都显著优于之前的注意力基础STR方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The attention mechanism has become the de facto module in scene text recognition (STR) methods, due to its capability of extracting character-level representations. These methods can be summarized into implicit attention based and supervised attention based, depended on how the attention is computed, i.e., implicit attention and supervised attention are learned from sequence-level text annotations and character-level bounding box annotations, respectively. Implicit attention, as it may extract coarse or even incorrect spatial regions as character attention, is prone to suffering from an alignment-drifted issue. Supervised attention can alleviate the above issue, but it is category-specific, which requires extra laborious character-level bounding box annotations and would be memory-intensive when the number of character categories is large. To address the aforementioned issues, we propose a novel attention mechanism for STR, self-supervised implicit glyph attention (SIGA). SIGA delineates the glyph structures of text images by jointly self-supervised text segmentation and implicit attention alignment, which serve as the supervision to improve attention correctness without extra character-level annotations. Experimental results demonstrate that SIGA performs consistently and significantly better than previous attention-based STR methods, in terms of both attention correctness and final recognition performance on publicly available context benchmarks and our contributed contextless benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1016.ACL-SPC: Adaptive Closed-Loop System for Self-Supervised Point Cloud Completion</span><br>
                <span class="as">Hong, SangminandYavartanoo, MohsenandNeshatavar, ReyhanehandLee, KyoungMu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_ACL-SPC_Adaptive_Closed-Loop_System_for_Self-Supervised_Point_Cloud_Completion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9435-9444.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决点云补全问题，即如何从深度传感器获取的部分点云中填充缺失部分并生成完整的点云。<br>
                    动机：虽然在合成点云补全任务上，有监督方法取得了很大进展，但由于合成数据集和真实世界数据集之间的领域差距或对先验信息的需求，这些方法很难应用于真实世界场景。<br>
                    方法：为克服这些限制，我们提出了一种新的自我监督框架ACL-SPC用于点云补全，以在同一数据上进行训练和测试。ACL-SPC采用自适应闭环（ACL）系统，将单个部分输入尝试输出完整的点云，该系统强制输出对于输入的变化相同。<br>
                    效果：我们在各种数据集上评估我们的ACL-SPC，证明它可以成功地学习完成部分点云，这是第一个自我监督方案。结果显示，我们的方法与无监督方法相当，并在真实世界数据集上比在合成数据集上训练的有监督方法表现更好。大量实验证明了自我监督学习的必要性以及我们提出的方法在真实世界点云补全任务上的有效性。代码可在此链接公开获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud completion addresses filling in the missing parts of a partial point cloud obtained from depth sensors and generating a complete point cloud. Although there has been steep progress in the supervised methods on the synthetic point cloud completion task, it is hardly applicable in real-world scenarios due to the domain gap between the synthetic and real-world datasets or the requirement of prior information. To overcome these limitations, we propose a novel self-supervised framework ACL-SPC for point cloud completion to train and test on the same data. ACL-SPC takes a single partial input and attempts to output the complete point cloud using an adaptive closed-loop (ACL) system that enforces the output same for the variation of an input. We evaluate our ACL-SPC on various datasets to prove that it can successfully learn to complete a partial point cloud as the first self-supervised scheme. Results show that our method is comparable with unsupervised methods and achieves superior performance on the real-world dataset compared to the supervised methods trained on the synthetic dataset. Extensive experiments justify the necessity of self-supervised learning and the effectiveness of our proposed method for the real-world point cloud completion task. The code is publicly available from this link.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1017.Focus on Details: Online Multi-Object Tracking With Diverse Fine-Grained Representation</span><br>
                <span class="as">Ren, HaoandHan, ShoudongandDing, HuilinandZhang, ZiwenandWang, HongweiandWang, Faquan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Focus_on_Details_Online_Multi-Object_Tracking_With_Diverse_Fine-Grained_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11289-11298.png><br>
            
            <span class="tt"><span class="t0">研究问题：在多目标跟踪（MOT）中，如何提取具有区分性的特征表示以保持每个目标的唯一标识符。<br>
                    动机：现有的MOT方法主要通过边界框区域或中心点的特征进行身份嵌入，但当目标被遮挡时，这些粗粒度的全局表示变得不可靠。<br>
                    方法：提出了一种多样化的细粒度表示方法，从全局和局部两个角度全面描述目标的外观。为了有效缓解由无序的上下文信息聚合引起的语义错位，提出了流对齐FPN（FAFPN）进行多尺度特征对齐聚合。此外，还提出了多头部分掩码生成器（MPMG）基于对齐后的特征图提取细粒度表示。<br>
                    效果：实验结果表明，该方法在MOT17和MOT20测试集上取得了最先进的性能。即使在目标外观极其相似的DanceTrack上，该方法也比ByteTrack在HOTA和IDF1上分别提高了5.0%和5.6%。大量实验证明，多样化的细粒度表示使Re-ID在MOT中再次表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Discriminative representation is essential to keep a unique identifier for each target in Multiple object tracking (MOT). Some recent MOT methods extract features of the bounding box region or the center point as identity embeddings. However, when targets are occluded, these coarse-grained global representations become unreliable. To this end, we propose exploring diverse fine-grained representation, which describes appearance comprehensively from global and local perspectives. This fine-grained representation requires high feature resolution and precise semantic information. To effectively alleviate the semantic misalignment caused by indiscriminate contextual information aggregation, Flow Alignment FPN (FAFPN) is proposed for multi-scale feature alignment aggregation. It generates semantic flow among feature maps from different resolutions to transform their pixel positions. Furthermore, we present a Multi-head Part Mask Generator (MPMG) to extract fine-grained representation based on the aligned feature maps. Multiple parallel branches of MPMG allow it to focus on different parts of targets to generate local masks without label supervision. The diverse details in target masks facilitate fine-grained representation. Eventually, benefiting from a Shuffle-Group Sampling (SGS) training strategy with positive and negative samples balanced, we achieve state-of-the-art performance on MOT17 and MOT20 test sets. Even on DanceTrack, where the appearance of targets is extremely similar, our method significantly outperforms ByteTrack by 5.0% on HOTA and 5.6% on IDF1. Extensive experiments have proved that diverse fine-grained representation makes Re-ID great again in MOT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1018.Structure Aggregation for Cross-Spectral Stereo Image Guided Denoising</span><br>
                <span class="as">Sheng, ZehuaandYu, ZhuandLiu, XiongweiandCao, Si-YuanandLiu, YuqiandShen, Hui-LiangandZhang, Huaqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sheng_Structure_Aggregation_for_Cross-Spectral_Stereo_Image_Guided_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13997-14006.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从噪声观测中获取具有显著结构的清晰图像？<br>
                    动机：当前去噪研究中的普遍做法是利用高信噪比的额外指导图像，但目前的跨光谱立体匹配方法无法完全保证像素级的对齐精度，且很少考虑噪声污染的情况。<br>
                    方法：首次提出一种用于跨光谱立体图像的引导去噪框架。不通过传统的立体匹配来对齐输入图像，而是从指导图像中聚合结构以估计噪声目标图像的清洁结构图，然后使用空间可变的线性表示模型回归最终的去噪结果。基于此，设计了一个名为SANet的神经网络来完成整个引导去噪过程。<br>
                    效果：实验结果表明，我们的SANet能够有效地将结构从未对齐的指导图像转移到恢复结果，并在各种立体图像数据集上优于最先进的去噪器。此外，我们结构聚合策略也显示出处理其他未对齐的引导恢复任务（如超分辨率和去模糊）的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To obtain clean images with salient structures from noisy observations, a growing trend in current denoising studies is to seek the help of additional guidance images with high signal-to-noise ratios, which are often acquired in different spectral bands such as near infrared. Although previous guided denoising methods basically require the input images to be well-aligned, a more common way to capture the paired noisy target and guidance images is to exploit a stereo camera system. However, current studies on cross-spectral stereo matching cannot fully guarantee the pixel-level registration accuracy, and rarely consider the case of noise contamination. In this work, for the first time, we propose a guided denoising framework for cross-spectral stereo images. Instead of aligning the input images via conventional stereo matching, we aggregate structures from the guidance image to estimate a clean structure map for the noisy target image, which is then used to regress the final denoising result with a spatially variant linear representation model. Based on this, we design a neural network, called as SANet, to complete the entire guided denoising process. Experimental results show that, our SANet can effectively transfer structures from an unaligned guidance image to the restoration result, and outperforms state-of-the-art denoisers on various stereo image datasets. Besides, our structure aggregation strategy also shows its potential to handle other unaligned guided restoration tasks such as super-resolution and deblurring. The source code is available at https://github.com/lustrouselixir/SANet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1019.One-Stage 3D Whole-Body Mesh Recovery With Component Aware Transformer</span><br>
                <span class="as">Lin, JingandZeng, AilingandWang, HaoqianandZhang, LeiandLi, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21159-21168.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图片中估计3D人体、面部和手部的参数。<br>
                    动机：由于分辨率问题，即面部和手部通常位于极小的区域，使用单一网络执行此任务具有挑战性。<br>
                    方法：提出了一种名为OSX的一阶段全身网格恢复管道，无需为每个部分单独的网络。设计了一个由全局身体编码器和局部面部/手部解码器组成的组件感知变压器（CAT）。<br>
                    效果：实验结果表明，OSX的效果显著，整个流程简单有效，无需任何手动后处理，自然避免了不合理的预测。同时构建了一个大规模的上半身数据集（UBody），包含在各种真实生活场景中部分可见身体的人员，以弥合基本任务和下游应用之间的差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Whole-body mesh recovery aims to estimate the 3D human body, face, and hands parameters from a single image. It is challenging to perform this task with a single network due to resolution issues, i.e., the face and hands are usually located in extremely small regions. Existing works usually detect hands and faces, enlarge their resolution to feed in a specific network to predict the parameter, and finally fuse the results. While this copy-paste pipeline can capture the fine-grained details of the face and hands, the connections between different parts cannot be easily recovered in late fusion, leading to implausible 3D rotation and unnatural pose. In this work, we propose a one-stage pipeline for expressive whole-body mesh recovery, named OSX, without separate networks for each part. Specifically, we design a Component Aware Transformer (CAT) composed of a global body encoder and a local face/hand decoder. The encoder predicts the body parameters and provides a high-quality feature map for the decoder, which performs a feature-level upsample-crop scheme to extract high-resolution part-specific features and adopt keypoint-guided deformable attention to estimate hand and face precisely. The whole pipeline is simple yet effective without any manual post-processing and naturally avoids implausible prediction. Comprehensive experiments demonstrate the effectiveness of OSX. Lastly, we build a large-scale Upper-Body dataset (UBody) with high-quality 2D and 3D whole-body annotations. It contains persons with partially visible bodies in diverse real-life scenarios to bridge the gap between the basic task and downstream applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1020.Masked Jigsaw Puzzle: A Versatile Position Embedding for Vision Transformers</span><br>
                <span class="as">Ren, BinandLiu, YahuiandSong, YueandBi, WeiandCucchiara, RitaandSebe, NicuandWang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Masked_Jigsaw_Puzzle_A_Versatile_Position_Embedding_for_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20382-20391.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器中位置嵌入（PEs）可能导致的隐私泄露问题。<br>
                    动机：尽管位置嵌入在提升视觉转换器性能上起着关键作用，但其可能暴露输入图像块的空间信息，从而引发隐私泄露问题。<br>
                    方法：提出一种被遮蔽的拼图位置嵌入（MJP）方法。首先通过分块随机拼图洗牌算法对选定的图像块进行混洗，并对其对应的位置嵌入进行遮蔽。对于未被遮蔽的图像块，其位置嵌入保持原样，但其空间关系通过密集绝对定位回归器得到加强。<br>
                    效果：实验结果显示，1) 位置嵌入明确编码了二维空间关系，并在梯度反转攻击下导致严重的隐私泄露问题；2) 使用简单混洗的图像块训练视觉转换器可以缓解此问题，但这会损害准确性；3) 在一定混洗比例下，提出的MJP不仅提升了大型数据集（如ImageNet-1K和ImageNet-C, -A/O）的性能和鲁棒性，而且在典型的梯度攻击下也大幅提高了隐私保护能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Position Embeddings (PEs), an arguably indispensable component in Vision Transformers (ViTs), have been shown to improve the performance of ViTs on many vision tasks. However, PEs have a potentially high risk of privacy leakage since the spatial information of the input patches is exposed. This caveat naturally raises a series of interesting questions about the impact of PEs on accuracy, privacy, prediction consistency, etc. To tackle these issues, we propose a Masked Jigsaw Puzzle (MJP) position embedding method. In particular, MJP first shuffles the selected patches via our block-wise random jigsaw puzzle shuffle algorithm, and their corresponding PEs are occluded. Meanwhile, for the non-occluded patches, the PEs remain the original ones but their spatial relation is strengthened via our dense absolute localization regressor. The experimental results reveal that 1) PEs explicitly encode the 2D spatial relationship and lead to severe privacy leakage problems under gradient inversion attack; 2) Training ViTs with the naively shuffled patches can alleviate the problem, but it harms the accuracy; 3) Under a certain shuffle ratio, the proposed MJP not only boosts the performance and robustness on large-scale datasets (i.e., ImageNet-1K and ImageNet-C, -A/O) but also improves the privacy preservation ability under typical gradient attacks by a large margin. The source code and trained models are available at https://github.com/yhlleo/MJP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1021.Robust Multiview Point Cloud Registration With Reliable Pose Graph Initialization and History Reweighting</span><br>
                <span class="as">Wang, HaipingandLiu, YuanandDong, ZhenandGuo, YulanandLiu, Yu-ShenandWang, WenpingandYang, Bisheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Robust_Multiview_Point_Cloud_Registration_With_Reliable_Pose_Graph_Initialization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9506-9515.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决点云多视角注册的问题。<br>
                    动机：现有的多视角注册方法依赖于详尽的配对注册来构建密集连接的姿态图，并在姿态图上应用迭代加权最小二乘法（IRLS）来计算扫描姿态，但构建密集连接的图耗时且包含许多异常边，使得后续的IRLS难以找到正确的姿态。<br>
                    方法：首先提出使用神经网络来估计扫描对之间的重叠，从而构建稀疏但可靠的姿态图。然后设计了一种新的历史重加权函数在IRLS方案中，该函数对图中的异常边具有很强的鲁棒性。<br>
                    效果：与现有的多视角注册方法相比，该方法在3DMatch数据集上的注册召回率提高了11%，在ScanNet数据集上的注册误差降低了13%，同时减少了70%所需的配对注册。通过全面的消融研究证明了我们设计的有效性。源代码可在https://github.com/WHU-USI3DV/SGHR获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a new method for the multiview registration of point cloud. Previous multiview registration methods rely on exhaustive pairwise registration to construct a densely-connected pose graph and apply Iteratively Reweighted Least Square (IRLS) on the pose graph to compute the scan poses. However, constructing a densely-connected graph is time-consuming and contains lots of outlier edges, which makes the subsequent IRLS struggle to find correct poses. To address the above problems, we first propose to use a neural network to estimate the overlap between scan pairs, which enables us to construct a sparse but reliable pose graph. Then, we design a novel history reweighting function in the IRLS scheme, which has strong robustness to outlier edges on the graph. In comparison with existing multiview registration methods, our method achieves 11% higher registration recall on the 3DMatch dataset and  13% lower registration errors on the ScanNet dataset while reducing  70% required pairwise registrations. Comprehensive ablation studies are conducted to demonstrate the effectiveness of our designs. The source code is available at https://github.com/WHU-USI3DV/SGHR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1022.PointCMP: Contrastive Mask Prediction for Self-Supervised Learning on Point Cloud Videos</span><br>
                <span class="as">Shen, ZhiqiangandSheng, XiaoxiaoandWang, LongguangandGuo, YulanandLiu, QiongandZhou, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_PointCMP_Contrastive_Mask_Prediction_for_Self-Supervised_Learning_on_Point_Cloud_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1212-1222.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用无标签数据从点云视频中提取高质量的表示。<br>
                    动机：由于高标注成本，点云视频的自监督学习具有吸引力。<br>
                    方法：提出一种对比掩码预测（PointCMP）框架进行点云视频的自监督学习。具体来说，PointCMP采用双分支结构同时学习局部和全局时空信息，并在该双分支结构上开发基于互相似性的增强模块，在特征层面合成困难样本。通过屏蔽主要标记和擦除主要通道，生成困难样本以促进学习具有更好区分能力和泛化性能的表示。<br>
                    效果：大量实验表明，PointCMP在基准数据集上实现了最先进的性能，并优于现有的全监督对应物。转移学习结果展示了所学表示在不同数据集和任务上的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning can extract representations of good quality from solely unlabeled data, which is appealing for point cloud videos due to their high labelling cost. In this paper, we propose a contrastive mask prediction (PointCMP) framework for self-supervised learning on point cloud videos. Specifically, our PointCMP employs a two-branch structure to achieve simultaneous learning of both local and global spatio-temporal information. On top of this two-branch structure, a mutual similarity based augmentation module is developed to synthesize hard samples at the feature level. By masking dominant tokens and erasing principal channels, we generate hard samples to facilitate learning representations with better discrimination and generalization performance. Extensive experiments show that our PointCMP achieves the state-of-the-art performance on benchmark datasets and outperforms existing full-supervised counterparts. Transfer learning results demonstrate the superiority of the learned representations across different datasets and tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1023.Multimodal Industrial Anomaly Detection via Hybrid Fusion</span><br>
                <span class="as">Wang, YueandPeng, JinlongandZhang, JiangningandYi, RanandWang, YabiaoandWang, Chengjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multimodal_Industrial_Anomaly_Detection_via_Hybrid_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8032-8041.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行基于3D点云和RGB图像的多模态工业异常检测。<br>
                    动机：现有的多模态工业异常检测方法直接连接多模态特征，导致特征之间存在强烈的干扰，影响检测性能。<br>
                    方法：提出一种新颖的多模态异常检测方法Multi-3D-Memory (M3DM)，采用混合融合方案：首先设计了一种无监督的特征融合方法，通过补丁对比学习来鼓励不同模态特征之间的交互；其次，使用具有多个记忆库的决策层融合，避免信息丢失，并添加新的分类器做出最终决定。进一步提出了一种点特征对齐操作，以更好地对齐点云和RGB特征。<br>
                    效果：大量实验表明，我们的多模态工业异常检测模型在MVTec-3D AD数据集上的检测精度和分割精度均优于现有最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>2D-based Industrial Anomaly Detection has been widely discussed, however, multimodal industrial anomaly detection based on 3D point clouds and RGB images still has many untouched fields. Existing multimodal industrial anomaly detection methods directly concatenate the multimodal features, which leads to a strong disturbance between features and harms the detection performance. In this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly detection method with hybrid fusion scheme: firstly, we design an unsupervised feature fusion with patch-wise contrastive learning to encourage the interaction of different modal features; secondly, we use a decision layer fusion with multiple memory banks to avoid loss of information and additional novelty classifiers to make the final decision. We further propose a point feature alignment operation to better align the point cloud and RGB features. Extensive experiments show that our multimodal industrial anomaly detection model outperforms the state-of-the-art (SOTA) methods on both detection and segmentation precision on MVTec-3D AD dataset. Code at github.com/nomewang/M3DM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1024.BEV@DC: Bird&#x27;s-Eye View Assisted Training for Depth Completion</span><br>
                <span class="as">Zhou, WendingandYan, XuandLiao, YinghongandLin, YuankaiandHuang, JinandZhao, GangmingandCui, ShuguangandLi, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_BEVDC_Birds-Eye_View_Assisted_Training_for_Depth_Completion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9233-9242.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高自动驾驶中图像引导的深度补全效果。<br>
                    动机：现有的方法利用LiDAR的空间几何约束来增强图像引导的深度补全，但效率低下且泛化能力差。<br>
                    方法：提出BEV@DC模型，通过在训练阶段充分利用具有丰富几何细节的LiDAR，并在推理阶段采用仅以图像（RGB和深度）为输入的增强深度补全方式。具体地，将几何感知的LiDAR特征投影到统一的BEV空间，与RGB特征结合进行BEV补全。通过引入新提出的点体素空间传播网络（PV-SPN），该辅助分支通过3D密集监督和特征一致性为原始图像分支提供强大指导。<br>
                    效果：实验结果表明，该方法在仅有图像输入的情况下取得了显著改进，在一些基准测试中达到了最先进的水平，例如在具有挑战性的KITTI深度补全基准测试中排名第一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Depth completion plays a crucial role in autonomous driving, in which cameras and LiDARs are two complementary sensors. Recent approaches attempt to exploit spatial geometric constraints hidden in LiDARs to enhance image-guided depth completion. However, only low efficiency and poor generalization can be achieved. In this paper, we propose BEV@DC, a more efficient and powerful multi-modal training scheme, to boost the performance of image-guided depth completion. In practice, the proposed BEV@DC model comprehensively takes advantage of LiDARs with rich geometric details in training, employing an enhanced depth completion manner in inference, which takes only images (RGB and depth) as input. Specifically, the geometric-aware LiDAR features are projected onto a unified BEV space, combining with RGB features to perform BEV completion. By equipping a newly proposed point-voxel spatial propagation network (PV-SPN), this auxiliary branch introduces strong guidance to the original image branches via 3D dense supervision and feature consistency. As a result, our baseline model demonstrates significant improvements with the sole image inputs. Concretely, it achieves state-of-the-art on several benchmarks, e.g., ranking Top-1 on the challenging KITTI depth completion benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1025.LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation</span><br>
                <span class="as">Wang, SongandLi, WentongandLiu, WenyuandLiu, XiaoluandZhu, Jianke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LiDAR2Map_In_Defense_of_LiDAR-Based_Semantic_Map_Construction_Using_Online_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5186-5195.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用激光雷达在鸟瞰图（BEV）中有效地构建语义地图。<br>
                    动机：与摄像头图像相比，激光雷达提供了准确的3D观察，可以自然地将捕获的3D特征投影到BEV空间。然而，普通的基于激光雷达的BEV特征通常包含许多不确定的噪声，其中空间特征几乎没有纹理和语义线索。<br>
                    方法：提出了一种有效的基于激光雷达的方法来构建语义地图。具体来说，引入了一个BEV金字塔特征解码器，用于学习用于语义地图构建的稳健多尺度BEV特征，这大大提高了基于激光雷达的方法的准确性。为了缓解激光雷达数据缺乏语义线索的问题，提出了一种在线摄像头到激光雷达的蒸馏方案，以促进从图像到点云的语义学习。<br>
                    效果：在具有挑战性的nuScenes数据集上的实验结果表明，我们提出的LiDAR2Map在语义地图构建方面非常有效，比之前的基于激光雷达的方法提高了27.9% mIoU，甚至比最先进的基于摄像头的方法表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic map construction under bird's-eye view (BEV) plays an essential role in autonomous driving. In contrast to camera image, LiDAR provides the accurate 3D observations to project the captured 3D features onto BEV space inherently. However, the vanilla LiDAR-based BEV feature often contains many indefinite noises, where the spatial features have little texture and semantic cues. In this paper, we propose an effective LiDAR-based method to build semantic map. Specifically, we introduce a BEV pyramid feature decoder that learns the robust multi-scale BEV features for semantic map construction, which greatly boosts the accuracy of the LiDAR-based method. To mitigate the defects caused by lacking semantic cues in LiDAR data, we present an online Camera-to-LiDAR distillation scheme to facilitate the semantic learning from image to point cloud. Our distillation scheme consists of feature-level and logit-level distillation to absorb the semantic information from camera in BEV. The experimental results on challenging nuScenes dataset demonstrate the efficacy of our proposed LiDAR2Map on semantic map construction, which significantly outperforms the previous LiDAR-based methods over 27.9% mIoU and even performs better than the state-of-the-art camera-based approaches. Source code is available at: https://github.com/songw-zju/LiDAR2Map.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1026.PSVT: End-to-End Multi-Person 3D Pose and Shape Estimation With Progressive Video Transformers</span><br>
                <span class="as">Qiu, ZhongweiandYang, QianshengandWang, JianandFeng, HaochengandHan, JunyuandDing, ErruiandXu, ChangandFu, DongmeiandWang, Jingdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiu_PSVT_End-to-End_Multi-Person_3D_Pose_and_Shape_Estimation_With_Progressive_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21254-21263.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多人体视频3D人体姿态和形状估计方法通常采用两阶段策略，首先在每帧中检测出人体实例，然后使用时间模型进行单人体姿态和形状估计。然而，空间实例之间的全局时空上下文无法被捕获。<br>
                    动机：为了解决上述问题，我们提出了一种新的端到端的多人体3D姿态和形状估计框架，称为PSVT。<br>
                    方法：在PSVT中，我们首先使用一个时空编码器（STE）来捕获空间对象之间的全局特征依赖关系。然后，我们使用时空姿态解码器（STPD）和形状解码器（STSD）分别捕获姿态查询和特征标记、形状查询和特征标记之间的全局依赖关系。为了处理随着时间推移的对象变化，我们采用了一种新颖的渐进解码方案，在每一帧更新姿态和形状查询。此外，我们还提出了一种新的姿态引导注意力（PGA）机制，以更好地预测形状参数。这两个组件加强了PSVT的解码器，提高了性能。<br>
                    效果：我们在四个数据集上进行了广泛的实验，结果表明PSVT达到了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods of multi-person video 3D human Pose and Shape Estimation (PSE) typically adopt a two-stage strategy, which first detects human instances in each frame and then performs single-person PSE with temporal model. However, the global spatio-temporal context among spatial instances can not be captured. In this paper, we propose a new end-to-end multi-person 3D Pose and Shape estimation framework with progressive Video Transformer, termed PSVT. In PSVT, a spatio-temporal encoder (STE) captures the global feature dependencies among spatial objects. Then, spatio-temporal pose decoder (STPD) and shape decoder (STSD) capture the global dependencies between pose queries and feature tokens, shape queries and feature tokens, respectively. To handle the variances of objects as time proceeds, a novel scheme of progressive decoding is used to update pose and shape queries at each frame. Besides, we propose a novel pose-guided attention (PGA) for shape decoder to better predict shape parameters. The two components strengthen the decoder of PSVT to improve performance. Extensive experiments on the four datasets show that PSVT achieves stage-of-the-art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1027.VoxFormer: Sparse Voxel Transformer for Camera-Based 3D Semantic Scene Completion</span><br>
                <span class="as">Li, YimingandYu, ZhidingandChoy, ChristopherandXiao, ChaoweiandAlvarez, JoseM.andFidler, SanjaandFeng, ChenandAnandkumar, Anima</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9087-9098.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使AI系统像人类一样，通过2D图像就能想象出被遮挡物体和场景的完整3D几何形状？<br>
                    动机：这种能力对于识别和理解至关重要，但目前的AI系统还无法实现。<br>
                    方法：提出了VoxFormer，一种基于Transformer的语义场景补全框架，该框架从深度估计的稀疏可见和占用体素查询开始，然后通过密集化阶段从稀疏体素生成密集3D体素。<br>
                    效果：在SemanticKITTI上的实验表明，VoxFormer优于现有技术，在几何形状和语义方面的相对改进分别为20.0%和18.1%，并且在训练过程中减少了GPU内存的使用量至不到16GB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans can easily imagine the complete 3D geometry of occluded objects and scenes. This appealing ability is vital for recognition and understanding. To enable such capability in AI systems, we propose VoxFormer, a Transformer-based semantic scene completion framework that can output complete 3D volumetric semantics from only 2D images. Our framework adopts a two-stage design where we start from a sparse set of visible and occupied voxel queries from depth estimation, followed by a densification stage that generates dense 3D voxels from the sparse ones. A key idea of this design is that the visual features on 2D images correspond only to the visible scene structures rather than the occluded or empty spaces. Therefore, starting with the featurization and prediction of the visible structures is more reliable. Once we obtain the set of sparse queries, we apply a masked autoencoder design to propagate the information to all the voxels by self-attention. Experiments on SemanticKITTI show that VoxFormer outperforms the state of the art with a relative improvement of 20.0% in geometry and 18.1% in semantics and reduces GPU memory during training to less than 16GB. Our code is available on https://github.com/NVlabs/VoxFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1028.NeuDA: Neural Deformable Anchor for High-Fidelity Implicit Surface Reconstruction</span><br>
                <span class="as">Cai, BowenandHuang, JinchiandJia, RongfeiandLv, ChengfeiandFu, Huan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_NeuDA_Neural_Deformable_Anchor_for_High-Fidelity_Implicit_Surface_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8476-8485.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在3D空间中预测和渲染表面时，先前的方法如IDR和NeuS忽视了空间上下文，可能无法捕捉到小孔和结构等尖锐的局部拓扑结构的问题。<br>
                    动机：为了缓解这个问题，我们提出了一种灵活的神经隐式表示方法，利用分层体素网格进行高保真表面重建，即神经可变形锚点（NeuDA）。<br>
                    方法：NeuDA保持了分层锚点网格，其中每个顶点都存储一个3D位置（或锚点），而不是直接嵌入（或特征）。我们优化了锚点网格，使得不同的局部几何结构可以被自适应地编码。此外，我们还探讨了频率编码策略，并引入了一种简单的分层位置编码方法，以便灵活地利用高频和低频几何和外观的特性。<br>
                    效果：在DTU和BlendedMVS数据集上的实验表明，NeuDA可以生成有前景的网格表面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies implicit surface reconstruction leveraging differentiable ray casting. Previous works such as IDR and NeuS overlook the spatial context in 3D space when predicting and rendering the surface, thereby may fail to capture sharp local topologies such as small holes and structures. To mitigate the limitation, we propose a flexible neural implicit representation leveraging hierarchical voxel grids, namely Neural Deformable Anchor (NeuDA), for high-fidelity surface reconstruction. NeuDA maintains the hierarchical anchor grids where each vertex stores a 3d position (or anchor) instead of the direct embedding (or feature). We optimize the anchor grids such that different local geometry structures can be adaptively encoded. Besides, we dig into the frequency encoding strategies and introduce a simple hierarchical positional encoding method for the hierarchical anchor structure to flexibly exploited the properties of high-frequency and low-frequency geometry and appearance. Experiments on both the DTU and BlendedMVS datasets demonstrate that NeuDA can produce promising mesh surfaces.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1029.DINER: Disorder-Invariant Implicit Neural Representation</span><br>
                <span class="as">Xie, ShaowenandZhu, HaoandLiu, ZhenandZhang, QiandZhou, YouandCao, XunandMa, Zhan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_DINER_Disorder-Invariant_Implicit_Neural_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6143-6152.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有隐含神经表示（INR）在网络训练中频谱偏差的问题。<br>
                    动机：INR的容量受到网络训练中频谱偏差的限制，影响了其解决逆问题的能力。<br>
                    方法：通过在传统INR主干上增加哈希表，提出无序不变的隐含神经表示（DINER）。对于具有相同属性直方图但不同排列顺序的离散信号，哈希表可以将坐标映射到相同的分布，从而改善后续INR网络对映射信号的建模，显著减轻频谱偏差。<br>
                    效果：实验表明，DINER可以广泛应用于不同的INR主干（MLP和SIREN）和各种任务（图像/视频表示、相位检索和折射率恢复），并在质量和速度上都优于最先进的算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representation (INR) characterizes the attributes of a signal as a function of corresponding coordinates which emerges as a sharp weapon for solving inverse problems. However, the capacity of INR is limited by the spectral bias in the network training. In this paper, we find that such a frequency-related problem could be largely solved by re-arranging the coordinates of the input signal, for which we propose the disorder-invariant implicit neural representation (DINER) by augmenting a hash-table to a traditional INR backbone. Given discrete signals sharing the same histogram of attributes and different arrangement orders, the hash-table could project the coordinates into the same distribution for which the mapped signal can be better modeled using the subsequent INR network, leading to significantly alleviated spectral bias. Experiments not only reveal the generalization of the DINER for different INR backbones (MLP vs. SIREN) and various tasks (image/video representation, phase retrieval, and refractive index recovery) but also show the superiority over the state-of-the-art algorithms both in quality and speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1030.Deep Graph-Based Spatial Consistency for Robust Non-Rigid Point Cloud Registration</span><br>
                <span class="as">Qin, ZhengandYu, HaoandWang, ChangjianandPeng, YuxingandXu, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Deep_Graph-Based_Spatial_Consistency_for_Robust_Non-Rigid_Point_Cloud_Registration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5394-5403.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决非刚性点云配准中的异常对应关系剪枝问题。<br>
                    动机：在刚性配准中，空间一致性被广泛用于区分异常和正常对应关系，但在非刚性情况下不再适用，因此对非刚性配准的异常剔除问题尚未得到充分研究。<br>
                    方法：本文提出基于图的空间一致性网络（GraphSCNet）来过滤非刚性配准时的异常对应关系。该方法基于非刚性变形通常局部刚硬或局部形状保持的事实，首先设计了一种局部空间一致性度量，仅评估点云变形图中节点附近对应关系的 spatial compatibility。然后设计了一个基于注意力的非刚性对应关系嵌入模块，从局部空间一致性中学习稳健的非刚性对应关系表示。<br>
                    效果：尽管方法简单，但GraphSCNet有效提高了潜在对应关系的质量，并在三个具有挑战性的基准测试上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of outlier correspondence pruning for non-rigid point cloud registration. In rigid registration, spatial consistency has been a commonly used criterion to discriminate outliers from inliers. It measures the compatibility of two correspondences by the discrepancy between the respective distances in two point clouds. However, spatial consistency no longer holds in non-rigid cases and outlier rejection for non-rigid registration has not been well studied. In this work, we propose Graph-based Spatial Consistency Network (GraphSCNet) to filter outliers for non-rigid registration. Our method is based on the fact that non-rigid deformations are usually locally rigid, or local shape preserving. We first design a local spatial consistency measure over the deformation graph of the point cloud, which evaluates the spatial compatibility only between the correspondences in the vicinity of a graph node. An attention-based non-rigid correspondence embedding module is then devised to learn a robust representation of non-rigid correspondences from local spatial consistency. Despite its simplicity, GraphSCNet effectively improves the quality of the putative correspondences and attains state-of-the-art performance on three challenging benchmarks. Our code and models are available at https://github.com/qinzheng93/GraphSCNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1031.Slide-Transformer: Hierarchical Vision Transformer With Local Self-Attention</span><br>
                <span class="as">Pan, XuranandYe, TianzhuandXia, ZhuofanandSong, ShijiandHuang, Gao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2082-2091.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自注意力方法在减少计算复杂度的同时可能会影响局部特征学习，且依赖于一些手工设计。<br>
                    动机：为了解决上述问题，本文提出了一种新的局部注意力模块——滑窗注意力（Slide Attention）。<br>
                    方法：滑窗注意力模块首先从行的角度重新解释了基于列的Im2Col函数，并使用深度卷积作为有效的替代。在此基础上，提出了一种基于重参数化技术的变形移位模块，该模块进一步放松了固定的关键/值位置，使其能够在局部区域内适应变形的特征。<br>
                    效果：实验表明，滑窗注意力模块适用于各种先进的视觉转换模型，并与各种硬件设备兼容，在综合基准测试中实现了持续的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1032.Neural Intrinsic Embedding for Non-Rigid Point Cloud Matching</span><br>
                <span class="as">Jiang, PuhuaandSun, MingzeandHuang, Ruqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Neural_Intrinsic_Embedding_for_Non-Rigid_Point_Cloud_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21835-21845.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接在变形形状的点云样本之间建立对应关系。<br>
                    动机：由于点云作为原始的3D数据表示，缺乏底层对象的内在结构信息，这给直接建立对应关系带来了巨大挑战。<br>
                    方法：提出神经内在嵌入（NIE）方法，将每个顶点嵌入到一个高维空间中，以尊重内在结构。基于NIE，进一步提出了一种弱监督学习框架用于非刚性点云配准。<br>
                    效果：实验结果表明，该框架的表现与或甚至优于需要更多监督和/或更多结构几何输入的最先进的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As a primitive 3D data representation, point clouds are prevailing in 3D sensing, yet short of intrinsic structural information of the underlying objects. Such discrepancy poses great challenges in directly establishing correspondences between point clouds sampled from deformable shapes. In light of this, we propose Neural Intrinsic Embedding (NIE) to embed each vertex into a high-dimensional space in a way that respects the intrinsic structure. Based upon NIE, we further present a weakly-supervised learning framework for non-rigid point cloud registration. Unlike the prior works, we do not require expansive and sensitive off-line basis construction (e.g., eigen-decomposition of Laplacians), nor do we require ground-truth correspondence labels for supervision. We empirically show that our framework performs on par with or even better than the state-of-the-art baselines, which generally require more supervision and/or more structural geometric input.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1033.SHS-Net: Learning Signed Hyper Surfaces for Oriented Normal Estimation of Point Clouds</span><br>
                <span class="as">Li, QingandFeng, HuifangandShi, KanleandGao, YueandFang, YiandLiu, Yu-ShenandHan, Zhizhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SHS-Net_Learning_Signed_Hyper_Surfaces_for_Oriented_Normal_Estimation_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13591-13600.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法，称为SHS-Net，通过学习有符号超表面进行点云的定向法线估计。<br>
                    动机：现有的方法通常通过两阶段流程（无向法线估计和法线定向）来估计定向法线，并且每一步都由单独的算法实现。然而，这些方法对参数设置敏感，导致在具有噪声、密度变化和复杂几何形状的点云上结果不佳。<br>
                    方法：我们引入了有符号超表面（SHS），这是一种由多层感知器（MLP）层参数化的模型，用于端到端地从点云中学习估计定向法线。有符号超表面是在高维特征空间中隐式学习的，其中局部和全局信息被聚合。具体来说，我们引入了一个片编码模块和一个形状编码模块，分别将3D点云编码为局部潜在代码和全局潜在代码。然后，提出了一个注意力加权的法线预测模块作为解码器，该模块将局部和全局潜在代码作为输入，预测定向法线。<br>
                    效果：实验结果表明，我们的SHS-Net在常用的基准测试上，无论是无向还是定向法线估计，都优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel method called SHS-Net for oriented normal estimation of point clouds by learning signed hyper surfaces, which can accurately predict normals with global consistent orientation from various point clouds. Almost all existing methods estimate oriented normals through a two-stage pipeline, i.e., unoriented normal estimation and normal orientation, and each step is implemented by a separate algorithm. However, previous methods are sensitive to parameter settings, resulting in poor results from point clouds with noise, density variations and complex geometries. In this work, we introduce signed hyper surfaces (SHS), which are parameterized by multi-layer perceptron (MLP) layers, to learn to estimate oriented normals from point clouds in an end-to-end manner. The signed hyper surfaces are implicitly learned in a high-dimensional feature space where the local and global information is aggregated. Specifically, we introduce a patch encoding module and a shape encoding module to encode a 3D point cloud into a local latent code and a global latent code, respectively. Then, an attention-weighted normal prediction module is proposed as a decoder, which takes the local and global latent codes as input to predict oriented normals. Experimental results show that our SHS-Net outperforms the state-of-the-art methods in both unoriented and oriented normal estimation on the widely used benchmarks. The code, data and pretrained models are available at https://github.com/LeoQLi/SHS-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1034.Think Twice Before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</span><br>
                <span class="as">Jia, XiaosongandWu, PenghaoandChen, LiandXie, JiangweiandHe, ConghuiandYan, JunchiandLi, Hongyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21983-21994.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自动驾驶方法通常采用解耦的编码器-解码器模型，其中编码器从原始传感器数据中提取隐藏特征，解码器输出自我车辆的未来轨迹或行动。这种模式下，编码器无法获取自我代理的预期行为，使得寻找安全关键区域和推断未来情况的任务全部落在解码器上。<br>
                    动机：为了解决上述问题，本文提出了两个原则：充分利用编码器的容量；增加解码器的容量。具体来说，我们首先根据编码器的特征预测出粗略的未来位置和行动，然后在这个位置和行动的基础上想象未来的环境，检查如果我们按照预测的行动行驶会产生什么后果。<br>
                    方法：我们还检索预测坐标周围的编码器特征，以获取关于安全关键区域的精细信息。最后，基于预测的未来和检索到的关键特征，我们通过预测其与地面实况的偏移量来细化粗略的位置和行动。<br>
                    效果：我们在CARLA模拟器上进行实验，在闭环基准测试中取得了最先进的性能。广泛的消融研究表明了每个提出的模块的有效性。代码和模型可以在https://github.com/opendrivelab/ThinkTwice 获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>End-to-end autonomous driving has made impressive progress in recent years. Existing methods usually adopt the decoupled encoder-decoder paradigm, where the encoder extracts hidden features from raw sensor data, and the decoder outputs the ego-vehicle's future trajectories or actions. Under such a paradigm, the encoder does not have access to the intended behavior of the ego agent, leaving the burden of finding out safety-critical regions from the massive receptive field and inferring about future situations to the decoder. Even worse, the decoder is usually composed of several simple multi-layer perceptrons (MLP) or GRUs while the encoder is delicately designed (e.g., a combination of heavy ResNets or Transformer). Such an imbalanced resource-task division hampers the learning process. In this work, we aim to alleviate the aforementioned problem by two principles: (1) fully utilizing the capacity of the encoder; (2) increasing the capacity of the decoder. Concretely, we first predict a coarse-grained future position and action based on the encoder features. Then, conditioned on the position and action, the future scene is imagined to check the ramification if we drive accordingly. We also retrieve the encoder features around the predicted coordinate to obtain fine-grained information about the safety-critical region. Finally, based on the predicted future and the retrieved salient feature, we refine the coarse-grained position and action by predicting its offset from ground-truth. The above refinement module could be stacked in a cascaded fashion, which extends the capacity of the decoder with spatial-temporal prior knowledge about the conditioned future. We conduct experiments on the CARLA simulator and achieve state-of-the-art performance in closed-loop benchmarks. Extensive ablation studies demonstrate the effectiveness of each proposed module. Code and models are available at https://github.com/opendrivelab/ThinkTwice.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1035.DSVT: Dynamic Sparse Voxel Transformer With Rotated Sets</span><br>
                <span class="as">Wang, HaiyangandShi, ChenandShi, ShaoshuaiandLei, MengandWang, SenandHe, DiandSchiele, BerntandWang, Liwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13520-13529.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种高效且易于部署的3D骨干网络来处理稀疏点云是3D感知中的基本问题。<br>
                    动机：相比于定制化的稀疏卷积，Transformers中的注意机制更适合灵活地建模长距离关系，并且更容易在现实世界的应用中进行部署。然而，由于点云的稀疏特性，将标准的Transformer应用于稀疏点是具有挑战性的。<br>
                    方法：本文提出了动态稀疏体素变换器（DSVT），这是一种基于窗口的单步体素Transformer骨干网络，用于户外3D感知。为了有效地并行处理稀疏点，我们提出了动态稀疏窗口注意力，该方法根据每个窗口的稀疏性对一系列局部区域进行分区，然后以全并行的方式计算所有区域的特征。为了实现跨集连接，我们设计了一种旋转的集分区策略，该策略在连续的自我注意层之间交替使用两种分区配置。为了支持有效的降采样和更好地编码几何信息，我们还提出了一种基于注意力的稀疏点的3D池化模块，该模块无需使用任何定制的CUDA操作即可实现强大的性能和易于部署。<br>
                    效果：我们的模型在广泛的3D感知任务上取得了最先进的性能。更重要的是，DSVT可以轻松地通过TensorRT进行部署，实现实时推理速度（27Hz）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Designing an efficient yet deployment-friendly 3D backbone to handle sparse point clouds is a fundamental problem in 3D perception. Compared with the customized sparse convolution, the attention mechanism in Transformers is more appropriate for flexibly modeling long-range relationships and is easier to be deployed in real-world applications. However, due to the sparse characteristics of point clouds, it is non-trivial to apply a standard transformer on sparse points. In this paper, we present Dynamic Sparse Voxel Transformer (DSVT), a single-stride window-based voxel Transformer backbone for outdoor 3D perception. In order to efficiently process sparse points in parallel, we propose Dynamic Sparse Window Attention, which partitions a series of local regions in each window according to its sparsity and then computes the features of all regions in a fully parallel manner. To allow the cross-set connection, we design a rotated set partitioning strategy that alternates between two partitioning configurations in consecutive self-attention layers. To support effective downsampling and better encode geometric information, we also propose an attention-style 3D pooling module on sparse points, which is powerful and deployment-friendly without utilizing any customized CUDA operations. Our model achieves state-of-the-art performance with a broad range of 3D perception tasks. More importantly, DSVT can be easily deployed by TensorRT with real-time inference speed (27Hz). Code will be available at https://github.com/Haiyang-W/DSVT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1036.Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation With Cross-Scale Distortion Awareness</span><br>
                <span class="as">Shen, ZhijieandZheng, ZishuoandLin, ChunyuandNie, LangandLiao, KangandZheng, ShuaiandZhao, Yao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Disentangling_Orthogonal_Planes_for_Indoor_Panoramic_Room_Layout_Estimation_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17337-17345.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的室内布局估计方案主要关注从垂直压缩的一维序列中恢复布局，但压缩过程混淆了不同平面的语义，导致性能较差且解释性模糊。<br>
                    动机：为了解决这个问题，我们提出了一种通过预先分割复杂场景中的正交（垂直和水平）平面来解耦一维表示的方法，以明确捕捉室内布局估计的几何线索。<br>
                    方法：我们设计了一种软翻转融合策略来协助预分割，并提出了特征组装机制来有效地整合浅层和深层特征，同时考虑畸变分布。此外，我们还利用三元注意力重构解耦序列以弥补预分割中的潜在错误。<br>
                    效果：在四个流行的基准测试上进行的实验表明，我们的方法优于现有的最先进解决方案，尤其是在3DIoU指标上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Based on the Manhattan World assumption, most existing indoor layout estimation schemes focus on recovering layouts from vertically compressed 1D sequences. However, the compression procedure confuses the semantics of different planes, yielding inferior performance with ambiguous interpretability. To address this issue, we propose to disentangle this 1D representation by pre-segmenting orthogonal (vertical and horizontal) planes from a complex scene, explicitly capturing the geometric cues for indoor layout estimation. Considering the symmetry between the floor boundary and ceiling boundary, we also design a soft-flipping fusion strategy to assist the pre-segmentation. Besides, we present a feature assembling mechanism to effectively integrate shallow and deep features with distortion distribution awareness. To compensate for the potential errors in pre-segmentation, we further leverage triple attention to reconstruct the disentangled sequences for better performance. Experiments on four popular benchmarks demonstrate our superiority over existing SoTA solutions, especially on the 3DIoU metric. The code is available at https://github.com/zhijieshen-bjtu/DOPNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1037.PEAL: Prior-Embedded Explicit Attention Learning for Low-Overlap Point Cloud Registration</span><br>
                <span class="as">Yu, JunleandRen, LuweiandZhang, YuandZhou, WenhuiandLin, LiliandDai, Guojun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17702-17711.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高低重叠点云注册的性能。<br>
                    动机：在几何空间中，全局依赖性可能模糊不清，缺乏显著性，特别是在室内低重叠场景中，与大量非重叠点的依赖关系引入了模糊性。<br>
                    方法：提出了一种基于先验知识的显式注意力学习模型（PEAL），通过将先验知识纳入学习过程，将点分为两部分，一部分是位于假设重叠区域的点，另一部分是位于假设非重叠区域的点，然后PEAL显式地学习了与假设重叠点之间的单向注意力。<br>
                    效果：该方法在具有挑战性的3DLoMatch基准上提高了6%以上的注册召回率，并在特征匹配召回率、内联比率和注册召回率方面在3DMatch和3DLoMatch上都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning distinctive point-wise features is critical for low-overlap point cloud registration. Recently, it has achieved huge success in incorporating Transformer into point cloud feature representation, which usually adopts a self-attention module to learn intra-point-cloud features first, then utilizes a cross-attention module to perform feature exchange between input point clouds. Self-attention is computed by capturing the global dependency in geometric space. However, this global dependency can be ambiguous and lacks distinctiveness, especially in indoor low-overlap scenarios, as which the dependence with an extensive range of non-overlapping points introduces ambiguity. To address this issue, we present PEAL, a Prior-embedded Explicit Attention Learning model. By incorporating prior knowledge into the learning process, the points are divided into two parts. One includes points lying in the putative overlapping region and the other includes points lying in the putative non-overlapping region. Then PEAL explicitly learns one-way attention with the putative overlapping points. This simplistic design attains surprising performance, significantly relieving the aforementioned feature ambiguity. Our method improves the Registration Recall by 6+% on the challenging 3DLoMatch benchmark and achieves state-of-the-art performance on Feature Matching Recall, Inlier Ratio, and Registration Recall on both 3DMatch and 3DLoMatch. Code will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1038.GeoVLN: Learning Geometry-Enhanced Visual Representation With Slot Attention for Vision-and-Language Navigation</span><br>
                <span class="as">Huo, JingyangandSun, QiangandJiang, BoyanandLin, HaitaoandFu, Yanwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huo_GeoVLN_Learning_Geometry-Enhanced_Visual_Representation_With_Slot_Attention_for_Vision-and-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23212-23221.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的解决Room-to-Room VLN问题的方法仅利用RGB图像，没有考虑候选视图周围的局部上下文，缺乏足够的周围环境视觉线索。<br>
                    动机：自然语言包含复杂的语义信息，因此其与视觉输入的相关性很难仅通过交叉注意力进行建模。<br>
                    方法：我们提出了GeoVLN，它基于插槽注意力学习几何增强的视觉表示，以实现稳健的视觉和语言导航。我们将RGB图像与Omnidata预测的相应深度图和法线图相结合作为视觉输入。<br>
                    效果：我们引入了一个两阶段模块，结合局部插槽注意力和CLIP模型从这种输入中产生几何增强的表示。我们使用V&L BERT学习一个融合语言和视觉信息的交流模态表示。此外，设计了一种新的多路注意力模块，鼓励不同的输入指令短语从视觉输入中提取最相关的特征。广泛的实验证明了我们新设计的模块的有效性，并展示了所提出方法的强大性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing works solving Room-to-Room VLN problem only utilize RGB images and do not consider local context around candidate views, which lack sufficient visual cues about surrounding environment. Moreover, natural language contains complex semantic information thus its correlations with visual inputs are hard to model merely with cross attention. In this paper, we propose GeoVLN, which learns Geometry-enhanced visual representation based on slot attention for robust Visual-and-Language Navigation. The RGB images are compensated with the corresponding depth maps and normal maps predicted by Omnidata as visual inputs. Technically, we introduce a two-stage module that combine local slot attention and CLIP model to produce geometry-enhanced representation from such input. We employ V&L BERT to learn a cross-modal representation that incorporate both language and vision informations. Additionally, a novel multiway attention module is designed, encouraging different phrases of input instruction to exploit the most related features from visual input. Extensive experiments demonstrate the effectiveness of our newly designed modules and show the compelling performance of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1039.Progressive Neighbor Consistency Mining for Correspondence Pruning</span><br>
                <span class="as">Liu, XinandYang, Jufeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Neighbor_Consistency_Mining_for_Correspondence_Pruning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9527-9537.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在特征匹配任务中，如何从初始对应关系中识别出正确的对应关系。<br>
                    动机：由于错误对应关系的分布极其不规则，因此难以确保在坐标和特征空间中寻找的邻居始终一致。<br>
                    方法：提出一种新颖的全局图空间来基于加权的全局图搜索一致的邻居，以明确探索对应关系之间的长程依赖性。此外，根据不同的邻居搜索空间逐步构建三种邻居嵌入，并设计一个邻居一致性块来提取邻居上下文并按顺序探索它们的交互。最终，开发了一个名为Neighbor Consistency Mining Network（NCMNet）的网络，用于准确恢复相机姿态和识别内联。<br>
                    效果：实验结果表明，NCMNet在具有挑战性的户外和室内匹配场景上的性能明显优于最先进的竞争对手。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of correspondence pruning is to recognize correct correspondences (inliers) from initial ones, with applications to various feature matching based tasks. Seeking neighbors in the coordinate and feature spaces is a common strategy in many previous methods. However, it is difficult to ensure that these neighbors are always consistent, since the distribution of false correspondences is extremely irregular. For addressing this problem, we propose a novel global-graph space to search for consistent neighbors based on a weighted global graph that can explicitly explore long-range dependencies among correspondences. On top of that, we progressively construct three neighbor embeddings according to different neighbor search spaces, and design a Neighbor Consistency block to extract neighbor context and explore their interactions sequentially. In the end, we develop a Neighbor Consistency Mining Network (NCMNet) for accurately recovering camera poses and identifying inliers. Experimental results indicate that our NCMNet achieves a significant performance advantage over state-of-the-art competitors on challenging outdoor and indoor matching scenes. The source code can be found at https://github.com/xinliu29/NCMNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1040.From Node Interaction To Hop Interaction: New Effective and Scalable Graph Learning Paradigm</span><br>
                <span class="as">Chen, JieandLi, ZilongandZhu, YinandZhang, JunpingandPu, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_From_Node_Interaction_To_Hop_Interaction_New_Effective_and_Scalable_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7876-7885.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图神经网络（GNNs）在大规模工业应用中存在扩展性和过平滑问题。<br>
                    动机：解决GNNs的扩展性问题和过平滑问题，提高节点的判别能力。<br>
                    方法：提出一种新颖的跳跃交互范式，将节点间的交互目标转化为每个节点内的预处理多跳特征，设计了易于利用现有GNN实现跳跃交互的HopGNN框架，并提出了带有自监督学习目标的多任务学习策略来增强HopGNN。<br>
                    效果：在12个不同领域、规模和图形平滑度的基准数据集上进行大量实验，结果显示该方法在保持高可扩展性和效率的同时，取得了优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing Graph Neural Networks (GNNs) follow the message-passing mechanism that conducts information interaction among nodes iteratively. While considerable progress has been made, such node interaction paradigms still have the following limitation. First, the scalability limitation precludes the broad application of GNNs in large-scale industrial settings since the node interaction among rapidly expanding neighbors incurs high computation and memory costs. Second, the over-smoothing problem restricts the discrimination ability of nodes, i.e., node representations of different classes will converge to indistinguishable after repeated node interactions. In this work, we propose a novel hop interaction paradigm to address these limitations simultaneously. The core idea is to convert the interaction target among nodes to pre-processed multi-hop features inside each node. We design a simple yet effective HopGNN framework that can easily utilize existing GNNs to achieve hop interaction. Furthermore, we propose a multi-task learning strategy with a self-supervised learning objective to enhance HopGNN. We conduct extensive experiments on 12 benchmark datasets in a wide range of domains, scales, and smoothness of graphs. Experimental results show that our methods achieve superior performance while maintaining high scalability and efficiency. The code is at https://github.com/JC-202/HopGNN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1041.Understanding and Improving Features Learned in Deep Functional Maps</span><br>
                <span class="as">Attaiki, SouhaibandOvsjanikov, Maks</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Attaiki_Understanding_and_Improving_Features_Learned_in_Deep_Functional_Maps_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1316-1326.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度功能映射在非刚性3D形状对应任务中是一种成功的范例，但学习并存储在这些函数中的信息的具体性质尚未完全理解。<br>
                    动机：主要问题是这些特征除了在解决功能映射矩阵时的纯代数作用外，是否可以用于其他目标。<br>
                    方法：本文表明，在某些温和条件下，深度功能映射方法中学习的特征可以用作点状描述符，因此可以直接比较不同形状，甚至在测试时间无需解决功能映射。<br>
                    效果：基于我们的研究，我们提出了对标准深度功能映射流程的有效修改，这促进了学习到的特征的结构属性，显著提高了匹配结果。我们还证明，以前使用外在架构进行深度功能映射特征提取的失败尝试可以通过简单的架构改变来补救，这推动了我们分析所建议的理论特性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep functional maps have recently emerged as a successful paradigm for non-rigid 3D shape correspondence tasks. An essential step in this pipeline consists in learning feature functions that are used as constraints to solve for a functional map inside the network. However, the precise nature of the information learned and stored in these functions is not yet well understood. Specifically, a major question is whether these features can be used for any other objective, apart from their purely algebraic role, in solving for functional map matrices. In this paper, we show that under some mild conditions, the features learned within deep functional map approaches can be used as point-wise descriptors and thus are directly comparable across different shapes, even without the necessity of solving for a functional map at test time. Furthermore, informed by our analysis, we propose effective modifications to the standard deep functional map pipeline, which promotes structural properties of learned features, significantly improving the matching results. Finally, we demonstrate that previously unsuccessful attempts at using extrinsic architectures for deep functional map feature extraction can be remedied via simple architectural changes, which promote the theoretical properties suggested by our analysis. We thus bridge the gap between intrinsic and extrinsic surface-based learning, suggesting the necessary and sufficient conditions for successful shape matching. Our code is available at https://github.com/pvnieo/clover.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1042.High-Frequency Stereo Matching Network</span><br>
                <span class="as">Zhao, HaoliangandZhou, HuizhouandZhang, YongjunandChen, JieandYang, YitongandZhao, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_High-Frequency_Stereo_Matching_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1327-1336.png><br>
            
            <span class="tt"><span class="t0">研究问题：在双目立体匹配领域，迭代方法如RAFT-Stereo和CREStereo取得了显著进展，但这些方法在迭代过程中丢失信息，难以生成充分利用高频信息的详细差异图。<br>
                    动机：为了解决数据耦合问题并允许包含细微细节的特征在迭代中传递，我们提出了Decouple模块。同时，为了进一步捕捉高频细节，我们提出了Normalization Refinement模块。<br>
                    方法：我们的方法包括Decouple模块、Normalization Refinement模块以及引入通道自注意力机制的多尺度多阶段特征提取器。<br>
                    效果：我们的方法（DLNR）在Middlebury排行榜上排名第一，比第二名高出13.04%。在KITTI-2015基准测试中，我们的方法和D1-fg也达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In the field of binocular stereo matching, remarkable progress has been made by iterative methods like RAFT-Stereo and CREStereo. However, most of these methods lose information during the iterative process, making it difficult to generate more detailed difference maps that take full advantage of high-frequency information. We propose the Decouple module to alleviate the problem of data coupling and allow features containing subtle details to transfer across the iterations which proves to alleviate the problem significantly in the ablations. To further capture high-frequency details, we propose a Normalization Refinement module that unifies the disparities as a proportion of the disparities over the width of the image, which address the problem of module failure in cross-domain scenarios. Further, with the above improvements, the ResNet-like feature extractor that has not been changed for years becomes a bottleneck. Towards this end, we proposed a multi-scale and multi-stage feature extractor that introduces the channel-wise self-attention mechanism which greatly addresses this bottleneck. Our method (DLNR) ranks 1st on the Middlebury leaderboard, significantly outperforming the next best method by 13.04%. Our method also achieves SOTA performance on the KITTI-2015 benchmark for D1-fg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1043.Spatial-Then-Temporal Self-Supervised Learning for Video Correspondence</span><br>
                <span class="as">Li, RuiandLiu, Dong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spatial-Then-Temporal_Self-Supervised_Learning_for_Video_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2279-2288.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有视频分析中，对空间和时间线索协同利用不足的问题。<br>
                    动机：现有的视频分析方法主要集中在空间分辨特征或时间重复特征上，对于空间和时间线索的协同利用关注不够。<br>
                    方法：提出一种新颖的空间-然后-时间自我监督学习方法。首先通过对比学习从无标签图像中提取空间特征，然后通过重建学习利用无标签视频中的时间线索增强这些特征。设计全局关联蒸馏损失确保学习过程中不忘记空间线索，设计局部关联蒸馏损失对抗可能破坏重建的时间不连续性。<br>
                    效果：实验结果表明，该方法在一系列基于对应关系的视频分析任务上优于最先进的自我监督学习方法。消融研究验证了两步设计和蒸馏损失的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In low-level video analyses, effective representations are important to derive the correspondences between video frames. These representations have been learned in a self-supervised fashion from unlabeled images/videos, using carefully designed pretext tasks in some recent studies. However, the previous work concentrates on either spatial-discriminative features or temporal-repetitive features, with little attention to the synergy between spatial and temporal cues. To address this issue, we propose a novel spatial-then-temporal self-supervised learning method. Specifically, we firstly extract spatial features from unlabeled images via contrastive learning, and secondly enhance the features by exploiting the temporal cues in unlabeled videos via reconstructive learning. In the second step, we design a global correlation distillation loss to ensure the learning not to forget the spatial cues, and we design a local correlation distillation loss to combat the temporal discontinuity that harms the reconstruction. The proposed method outperforms the state-of-the-art self-supervised methods, as established by the experimental results on a series of correspondence-based video analysis tasks. Also, we performed ablation studies to verify the effectiveness of the two-step design as well as the distillation losses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1044.Super-Resolution Neural Operator</span><br>
                <span class="as">Wei, MinandZhang, Xuesong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Super-Resolution_Neural_Operator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18247-18256.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种深度操作学习框架——超分辨率神经操作器（SRNO），用于从低分辨率图像重建高分辨率图像。<br>
                    动机：现有的超分辨率方法通常需要固定网格大小，限制了其处理任意尺度的图像的能力。<br>
                    方法：SRNO将低分辨率和高分辨率图像对视为连续函数，通过嵌入低分辨率输入到更高维度的潜在表示空间中，并迭代地使用核积分机制近似隐式图像函数，最后进行降维生成目标坐标的RGB表示。<br>
                    效果：SRNO通过在每一层使用高效的伽辽金型注意力实现核积分，并在多层注意力架构中实现了动态潜在基更新，从而在准确性和运行时间上优于现有的连续超分辨率方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Super-resolution Neural Operator (SRNO), a deep operator learning framework that can resolve high-resolution (HR) images at arbitrary scales from the low-resolution (LR) counterparts. Treating the LR-HR image pairs as continuous functions approximated with different grid sizes, SRNO learns the mapping between the corresponding function spaces. From the perspective of approximation theory, SRNO first embeds the LR input into a higher-dimensional latent representation space, trying to capture sufficient basis functions, and then iteratively approximates the implicit image function with a kernel integral mechanism, followed by a final dimensionality reduction step to generate the RGB representation at the target coordinates. The key characteristics distinguishing SRNO from prior continuous SR works are: 1) the kernel integral in each layer is efficiently implemented via the Galerkin-type attention, which possesses non-local properties in the spatial domain and therefore benefits the grid-free continuum; and 2) the multilayer attention architecture allows for the dynamic latent basis update, which is crucial for SR problems to "hallucinate" high-frequency information from the LR image. Experiments show that SRNO outperforms existing continuous SR methods in terms of both accuracy and running time. Our code is at https://github.com/2y7c3/Super-Resolution-Neural-Operator.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1045.LP-DIF: Learning Local Pattern-Specific Deep Implicit Function for 3D Objects and Scenes</span><br>
                <span class="as">Wang, MengandLiu, Yu-ShenandGao, YueandShi, KanleandFang, YiandHan, Zhizhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LP-DIF_Learning_Local_Pattern-Specific_Deep_Implicit_Function_for_3D_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21856-21865.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地捕捉3D形状的几何细节。<br>
                    动机：现有的主流方法通过将3D形状划分为局部区域并使用共享几何相似性的单个解码器学习每个局部区域的局部潜在代码来捕获几何细节，但这种方法在处理所有区域时存在难度，并且对不同局部区域的多样性和不平衡分布的处理不佳。<br>
                    方法：提出一种新的局部模式特定隐式函数（LP-DIF）方法，该方法使用多个解码器分别关注具有某种模式的局部区域群组，并通过核密度估计器为每个模式特定的解码器引入区域再权重模块以动态地在学习过程中重新加权区域，从而简化了学习精细几何细节的过程。<br>
                    效果：实验证明，LP-DIF可以恢复更多的几何细节，从而提高3D重建的质量，并在性能上超过了先前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep Implicit Function (DIF) has gained much popularity as an efficient 3D shape representation. To capture geometry details, current mainstream methods divide 3D shapes into local regions and then learn each one with a local latent code via a decoder, where the decoder shares the geometric similarities among different local regions. Although such local methods can capture more local details, a large diversity of different local regions increases the difficulty of learning an implicit function when treating all regions equally using only a single decoder. In addition, these local regions often exhibit imbalanced distributions, where certain regions have significantly fewer observations. This leads that fine geometry details could not be preserved well. To solve this problem, we propose a novel Local Pattern-specific Implicit Function, named LP-DIF, for representing a shape with some clusters of local regions and multiple decoders, where each decoder only focuses on one cluster of local regions which share a certain pattern. Specifically, we first extract local codes for all regions, and then cluster them into multiple groups in the latent space, where similar regions sharing a common pattern fall into one group. After that, we train multiple decoders for mining local patterns of different groups, which simplifies learning of fine geometric details by reducing the diversity of local regions seen by each decoder. To further alleviate the data-imbalance problem, we introduce a region re-weighting module to each pattern-specific decoder by kernel density estimator, which dynamically re-weights the regions during learning. Our LP-DIF can restore more geometry details, and thus improve the quality of 3D reconstruction. Experiments demonstrate that our method can achieve the state-of-the-art performance over previous methods. Code is available at https://github.com/gtyxyz/lpdif.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1046.PeakConv: Learning Peak Receptive Field for Radar Semantic Segmentation</span><br>
                <span class="as">Zhang, LiwenandZhang, XinyanandZhang, YouchengandGuo, YufeiandChen, YuanpeiandHuang, XuhuiandMa, Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PeakConv_Learning_Peak_Receptive_Field_for_Radar_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17577-17586.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用现代机器学习技术进行雷达场景理解，特别是雷达语义分割。<br>
                    动机：现有的卷积操作对雷达信号的解读并不特异，因此需要一种针对雷达信号特性的新方法。<br>
                    方法：提出峰值卷积操作（PeakConv），将卷积的感知域定义为峰值感知域，并以此在端到端网络中学习物体特征。<br>
                    效果：通过在编码器中引入PeakConv层，我们的雷达语义分割网络在多视角真实测量数据集上的表现优于其他最新方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The modern machine learning-based technologies have shown considerable potential in automatic radar scene understanding. Among these efforts, radar semantic segmentation (RSS) can provide more refined and detailed information including the moving objects and background clutters within the effective receptive field of the radar. Motivated by the success of convolutional networks in various visual computing tasks, these networks have also been introduced to solve RSS task. However, neither the regular convolution operation nor the modified ones are specific to interpret radar signals. The receptive fields of existing convolutions are defined by the object presentation in optical signals, but these two signals have different perception mechanisms. In classic radar signal processing, the object signature is detected according to a local peak response, i.e., CFAR detection. Inspired by this idea, we redefine the receptive field of the convolution operation as the peak receptive field (PRF) and propose the peak convolution operation (PeakConv) to learn the object signatures in an end-to-end network. By incorporating the proposed PeakConv layers into the encoders, our RSS network can achieve better segmentation results compared with other SoTA methods on a multi-view real-measured dataset collected from an FMCW radar. Our code for PeakConv is available at https://github.com/zlw9161/PKC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1047.Parallel Diffusion Models of Operator and Image for Blind Inverse Problems</span><br>
                <span class="as">Chung, HyungjinandKim, JeongsolandKim, SehuiandYe, JongChul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Parallel_Diffusion_Models_of_Operator_and_Image_for_Blind_Inverse_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6059-6069.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型在已知前向算子（非盲）的情况下，已在逆问题求解中表现出了最先进的性能，但其在解决盲逆问题方面的适用性尚未得到探索。<br>
                    动机：通过为前向算子构建另一种扩散先验，我们能够解决一类盲逆问题。<br>
                    方法：具体来说，通过并行反向扩散并利用中间阶段的梯度进行引导，可以同时优化前向算子的参数和图像，使得两者都在并行反向扩散过程结束时被联合估计。<br>
                    效果：我们在两个具有代表性的问题上展示了该方法的有效性——盲去模糊和通过湍流成像，结果显示我们的方法产生了最先进的性能，并且在我们知道函数形式的情况下，该方法可以灵活地应用于一般的盲逆问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion model-based inverse problem solvers have demonstrated state-of-the-art performance in cases where the forward operator is known (i.e. non-blind). However, the applicability of the method to blind inverse problems has yet to be explored. In this work, we show that we can indeed solve a family of blind inverse problems by constructing another diffusion prior for the forward operator. Specifically, parallel reverse diffusion guided by gradients from the intermediate stages enables joint optimization of both the forward operator parameters as well as the image, such that both are jointly estimated at the end of the parallel reverse diffusion procedure. We show the efficacy of our method on two representative tasks --- blind deblurring, and imaging through turbulence --- and show that our method yields state-of-the-art performance, while also being flexible to be applicable to general blind inverse problems when we know the functional forms. Code available: https://github.com/BlindDPS/blind-dps</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1048.Local-Guided Global: Paired Similarity Representation for Visual Reinforcement Learning</span><br>
                <span class="as">Choi, HyesongandLee, HunsangandSong, WonilandJeon, SangryulandSohn, KwanghoonandMin, Dongbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Local-Guided_Global_Paired_Similarity_Representation_for_Visual_Reinforcement_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15072-15082.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉强化学习方法主要关注从原始像素中提取高级特征，忽视了连续堆叠的帧中的局部空间结构。<br>
                    动机：本文提出了一种新的无监督学习方式，称为自我监督的成对相似性表示学习（PSRL），以有效地编码空间结构。<br>
                    方法：首先，使用编码器分别生成输入帧的潜在体积，然后利用这些潜在体积捕获局部空间结构的方差，即多个帧之间的对应关系图。然后，在全局预测模块中尝试学习未来状态表示的全局语义表示，其中使用动作向量作为媒介进行预测。<br>
                    效果：在Atari游戏和DeepMind控制套件的复杂任务上进行的实验结果表明，通过提出的方法学习结构化表示，可以显著提高强化学习方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent vision-based reinforcement learning (RL) methods have found extracting high-level features from raw pixels with self-supervised learning to be effective in learning policies. However, these methods focus on learning global representations of images, and disregard local spatial structures present in the consecutively stacked frames. In this paper, we propose a novel approach, termed self-supervised Paired Similarity Representation Learning (PSRL) for effectively encoding spatial structures in an unsupervised manner. Given the input frames, the latent volumes are first generated individually using an encoder, and they are used to capture the variance in terms of local spatial structures, i.e., correspondence maps among multiple frames. This enables for providing plenty of fine-grained samples for training the encoder of deep RL. We further attempt to learn the global semantic representations in the global prediction module that predicts future state representations using action vector as a medium. The proposed method imposes similarity constraints on the three latent volumes; transformed query representations by estimated pixel-wise correspondence, predicted query representations from the global prediction model, and target representations of future state, guiding global prediction with locality-inherent volume. Experimental results on complex tasks in Atari Games and DeepMind Control Suite demonstrate that the RL methods are significantly boosted by the proposed self-supervised learning of structured representations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1049.LargeKernel3D: Scaling Up Kernels in 3D Sparse CNNs</span><br>
                <span class="as">Chen, YukangandLiu, JianhuiandZhang, XiangyuandQi, XiaojuanandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_LargeKernel3D_Scaling_Up_Kernels_in_3D_Sparse_CNNs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13488-13498.png><br>
            
            <span class="tt"><span class="t0">研究问题：直接在3D CNNs上应用大型卷积核时遇到了严重困难，2D上的成功模块设计在3D网络上效果惊人地差。<br>
                    动机：解决这个关键挑战，提出空间分区卷积及其大核模块。<br>
                    方法：避免直接使用大型卷积核的优化和效率问题，通过空间分区卷积和大核模块进行改进。<br>
                    效果：提出的LargeKernel3D网络在语义分割和对象检测的3D任务上取得了显著改进，并在nuScenes激光雷达排行榜上排名第一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advance in 2D CNNs has revealed that large kernels are important. However, when directly applying large convolutional kernels in 3D CNNs, severe difficulties are met, where those successful module designs in 2D become surprisingly ineffective on 3D networks, including the popular depth-wise convolution. To address this vital challenge, we instead propose the spatial-wise partition convolution and its large-kernel module. As a result, it avoids the optimization and efficiency issues of naive 3D large kernels. Our large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance further boosts to 74.2% NDS with a simple multi-modal fusion. In addition, LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object detection. For the first time, we show that large kernels are feasible and essential for 3D visual tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1050.Long Range Pooling for 3D Large-Scale Scene Understanding</span><br>
                <span class="as">Li, Xiang-LiandGuo, Meng-HaoandMu, Tai-JiangandMartin, RalphR.andHu, Shi-Min</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Long_Range_Pooling_for_3D_Large-Scale_Scene_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10300-10311.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在分析并探索视觉转换器和大型卷积核设计在卷积神经网络（CNNs）中成功的关键因素。<br>
                    动机：通过借鉴视觉转换器的成功以及大型卷积核设计，作者认为更大的感受野和更强的非线性操作是实现大规模三维场景理解的两个关键因素。<br>
                    方法：作者提出了一种简单而有效的长距离池化（LRP）模块，使用膨胀最大池化来提供网络的自适应大感受野。基于LRP，作者还展示了一个用于三维理解的完整网络架构，LRPNet。<br>
                    效果：消融研究表明，LRP模块在减少计算的同时，比大型卷积核实现了更好的结果，这归功于其非线性特性。此外，LRPNet在各种基准测试中表现优越，证明了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inspired by the success of recent vision transformers and large kernel design in convolutional neural networks (CNNs), in this paper, we analyze and explore essential reasons for their success. We claim two factors that are critical for 3D large-scale scene understanding: a larger receptive field and operations with greater non-linearity. The former is responsible for providing long range contexts and the latter can enhance the capacity of the network. To achieve the above properties, we propose a simple yet effective long range pooling (LRP) module using dilation max pooling, which provides a network with a large adaptive receptive field. LRP has few parameters, and can be readily added to current CNNs. Also, based on LRP, we present an entire network architecture, LRPNet, for 3D understanding. Ablation studies are presented to support our claims, and show that the LRP module achieves better results than large kernel convolution yet with reduced computation, due to its non-linearity. We also demonstrate the superiority of LRPNet on various benchmarks: LRPNet performs the best on ScanNet and surpasses other CNN-based methods on S3DIS and Matterport3D. Code will be avalible at https://github.com/li-xl/LRPNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1051.TriVol: Point Cloud Rendering via Triple Volumes</span><br>
                <span class="as">Hu, TaoandXu, XiaogangandChu, RuihangandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_TriVol_Point_Cloud_Rendering_via_Triple_Volumes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20732-20741.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于学习的点云渲染方法在提取连续和判别性的3D特征时面临挑战，导致渲染的图像中出现伪影。<br>
                    动机：为了解决这一问题，本文提出了一种密集而轻量级的3D表示方法TriVol，可以与NeRF结合，从点云中渲染出照片级真实的图像。<br>
                    方法：TriVol由三部分组成，每个部分都从输入的点云中编码。这种表示法有两个优点：一是融合了不同尺度的各自领域，从而提取局部和非局部特征进行判别性表示；二是由于体积大大减小，因此我们的3D解码器可以高效地推断，允许我们增加3D空间的分辨率以渲染更多的点细节。<br>
                    效果：通过在不同场景/物体上进行广泛的实验，并与当前的方法进行比较，证明了我们的框架的有效性。此外，我们的框架具有良好的泛化能力，可以在不进行微调的情况下渲染一类场景或物体。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing learning-based methods for point cloud rendering adopt various 3D representations and feature querying mechanisms to alleviate the sparsity problem of point clouds. However, artifacts still appear in the rendered images, due to the challenges in extracting continuous and discriminative 3D features from point clouds. In this paper, we present a dense while lightweight 3D representation, named TriVol, that can be combined with NeRF to render photo-realistic images from point clouds. Our TriVol consists of triple slim volumes, each of which is encoded from the input point cloud. Our representation has two advantages. First, it fuses the respective fields at different scales and thus extracts local and non-local features for discriminative representation. Second, since the volume size is greatly reduced, our 3D decoder can be efficiently inferred, allowing us to increase the resolution of the 3D space to render more point details. Extensive experiments on different benchmarks with varying kinds of scenes/objects demonstrate our framework's effectiveness compared with current approaches. Moreover, our framework has excellent generalization ability to render a category of scenes or objects without fine-tuning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1052.(ML)\${\textasciicircum</span><br>
                <span class="as">Liu, ZimingandGuo, SongandLu, XiaochengandGuo, JingcaiandZhang, JieweiandZeng, YueandHuo, Fushuo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_ML2P-Encoder_On_Exploration_of_Channel-Class_Correlation_for_Multi-Label_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23859-23868.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多标签零样本学习（MLZSL）中的问题，即现有的方法通常在空间类别相关性上进行视觉语义映射，这可能会消耗大量的计算资源，并且无法捕捉到精细的类别特定语义。<br>
                    动机：作者观察到不同的通道对于类别的敏感性通常是不同的，这种内在的通道-类别关联性为更准确和和谐的类别特征表示提供了可能。<br>
                    方法：本文提出了一种轻量而高效的基于多层感知器的编码器（ML^2P-Encoder），用于提取和保留通道级的语义。我们将生成的特征图重新组织成几个组，每个组都可以独立地用（ML^2P-Encoder）进行训练。此外，我们还设计了一个全局的组间注意力模块，以建立不同类别之间的多标签特定类关系，最终形成了一种新的通道-类别关联MLZSL框架（C^3-MLZSL）。<br>
                    效果：在包括NUS-WIDE和Open-Images-V4在内的大规模MLZSL基准测试中，我们的模型在性能上优于其他代表性的最先进的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies usually approach multi-label zero-shot learning (MLZSL) with visual-semantic mapping on spatial-class correlation, which can be computationally costly, and worse still, fails to capture fine-grained class-specific semantics. We observe that different channels may usually have different sensitivities on classes, which can correspond to specific semantics. Such an intrinsic channel-class correlation suggests a potential alternative for the more accurate and class-harmonious feature representations. In this paper, our interest is to fully explore the power of channel-class correlation as the unique base for MLZSL. Specifically, we propose a light yet efficient Multi-Label Multi-Layer Perceptron-based Encoder, dubbed (ML)^2P-Encoder, to extract and preserve channel-wise semantics. We reorganize the generated feature maps into several groups, of which each of them can be trained independently with (ML)^2P-Encoder. On top of that, a global group-wise attention module is further designed to build the multi-label specific class relationships among different classes, which eventually fulfills a novel Channel-Class Correlation MLZSL framework (C^3-MLZSL). Extensive experiments on large-scale MLZSL benchmarks including NUS-WIDE and Open-Images-V4 demonstrate the superiority of our model against other representative state-of-the-art models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1053.MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction</span><br>
                <span class="as">Wang, CongyiandZhu, FeidaandWen, Shilei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MeMaHand_Exploiting_Mesh-Mano_Interaction_for_Single_Image_Two-Hand_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/564-573.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决手部重建任务，提出一种从单张RGB图像同时重建两只手的网格和估计MANO参数的方法。<br>
                    动机：现有的手部重建方法通常采用参数化或非参数化的3D手模型，但两者各有优缺点。为了充分利用两种手表示的优点，本文提出了一种新的方法。<br>
                    方法：本文提出了Mesh-Mano交互模块（MMIB），该模块将网格顶点位置和MANO参数作为两种查询标记。MMIB由一个图残差块和一个配备不同非对称注意力掩码的两个变压器编码器组成，以分别建模手内和手间的注意力。此外，还引入了网格对齐细化模块来进一步提高网格与图像的对齐。<br>
                    效果：在InterHand2.6M基准测试中，该方法在各种手部重建任务上取得了优于现有方法的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representations consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders are equipped with different asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement module to further enhance the mesh-image alignment. Extensive experiments on the InterHand2.6M benchmark demonstrate promising results over the state-of-the-art hand reconstruction methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1054.Asymmetric Feature Fusion for Image Retrieval</span><br>
                <span class="as">Wu, HuiandWang, MinandZhou, WengangandLu, ZhenboandLi, Houqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Asymmetric_Feature_Fusion_for_Image_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11082-11092.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有非对称检索系统中存在的检索效率与非对称准确性之间的两难问题。<br>
                    动机：由于轻量级查询模型的容量较低，现有的方法在检索效率和非对称准确性之间存在困境。<br>
                    方法：本文提出了一种非对称特征融合（AFF）范式，通过仅在图库侧考虑不同特征的互补性来改进现有的非对称检索系统。具体来说，它首先将每个图库图像嵌入到各种特征中，例如局部特征和全局特征。然后引入动态混合器将这些特征聚合为一个紧凑的嵌入以进行有效搜索。在查询侧，只部署了一个用于特征提取的轻量级模型。查询模型和动态混合器通过共享一个动量更新的分类器进行联合训练。值得注意的是，所提出的方法在不增加查询侧任何额外开销的情况下提高了非对称检索的准确性。<br>
                    效果：通过对各种地标检索数据集的大量实验，证明了我们的范式的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In asymmetric retrieval systems, models with different capacities are deployed on platforms with different computational and storage resources. Despite the great progress, existing approaches still suffer from a dilemma between retrieval efficiency and asymmetric accuracy due to the low capacity of the lightweight query model. In this work, we propose an Asymmetric Feature Fusion (AFF) paradigm, which advances existing asymmetric retrieval systems by considering the complementarity among different features just at the gallery side. Specifically, it first embeds each gallery image into various features, e.g., local features and global features. Then, a dynamic mixer is introduced to aggregate these features into a compact embedding for efficient search. On the query side, only a single lightweight model is deployed for feature extraction. The query model and dynamic mixer are jointly trained by sharing a momentum-updated classifier. Notably, the proposed paradigm boosts the accuracy of asymmetric retrieval without introducing any extra overhead to the query side. Exhaustive experiments on various landmark retrieval datasets demonstrate the superiority of our paradigm.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1055.Context-Aware Pretraining for Efficient Blind Image Decomposition</span><br>
                <span class="as">Wang, ChaoandZheng, ZhedongandQuan, RuijieandSun, YifanandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Context-Aware_Pretraining_for_Efficient_Blind_Image_Decomposition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18186-18195.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决盲图像分解（BID）中同时去除多种类型的退化而不预先知道噪声类型的问题。<br>
                    动机：现有的方法通常需要大量的数据监督，使其在现实世界的场景中不可行。此外，传统的范式通常关注挖掘叠加图像的异常模式以分离噪声，这实际上与主要图像恢复任务相冲突。<br>
                    方法：我们提出了一种高效且简化的范式，称为上下文感知预训练（CP），并设计了两个预训练任务：混合图像分离和掩蔽图像重建。这种范式减少了标注需求，并明确促进了上下文感知特征学习。我们还引入了一个上下文感知预训练网络（CPNet）。<br>
                    效果：广泛的实验表明，我们的方法在各种BID任务上取得了有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study Blind Image Decomposition (BID), which is to uniformly remove multiple types of degradation at once without foreknowing the noise type. There remain two practical challenges: (1) Existing methods typically require massive data supervision, making them infeasible to real-world scenarios. (2) The conventional paradigm usually focuses on mining the abnormal pattern of a superimposed image to separate the noise, which de facto conflicts with the primary image restoration task. Therefore, such a pipeline compromises repairing efficiency and authenticity. In an attempt to solve the two challenges in one go, we propose an efficient and simplified paradigm, called Context-aware Pretraining (CP), with two pretext tasks: mixed image separation and masked image reconstruction. Such a paradigm reduces the annotation demands and explicitly facilitates context-aware feature learning. Assuming the restoration process follows a structure-to-texture manner, we also introduce a Context-aware Pretrained network (CPNet). In particular, CPNet contains two transformer-based parallel encoders, one information fusion module, and one multi-head prediction module. The information fusion module explicitly utilizes the mutual correlation in the spatial-channel dimension, while the multi-head prediction module facilitates texture-guided appearance flow. Moreover, a new sampling loss along with an attribute label constraint is also deployed to make use of the spatial context, leading to high-fidelity image restoration. Extensive experiments on both real and synthetic benchmarks show that our method achieves competitive performance for various BID tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1056.3D Line Mapping Revisited</span><br>
                <span class="as">Liu, ShaohuiandYu, YifanandPautrat, R\&#x27;emiandPollefeys, MarcandLarsson, Viktor</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_3D_Line_Mapping_Revisited_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21445-21455.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前基于线的重建方法远落后于基于点的重建方法。<br>
                    动机：线段可以简洁地编码高层场景布局，且在城市景观和室内场景中普遍存在，但目前的线重建方法却无法有效利用这一优势。<br>
                    方法：本文提出了LIMAP，一个用于从多视图图像创建3D线地图的库。通过重新审视线三角测量的退化问题，精心设计的评分和轨迹构建，以及利用线重合、平行和正交等结构先验，实现了高效稳健的3D线地图创建。<br>
                    效果：实验表明，LIMAP显著优于现有的3D线地图创建方法。此外，该方法还能恢复线与点/消失点之间的3D关联图。在视觉定位和光束法平差两个示例应用中，整合线和点的方法取得了最佳效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In contrast to sparse keypoints, a handful of line segments can concisely encode the high-level scene layout, as they often delineate the main structural elements. In addition to offering strong geometric cues, they are also omnipresent in urban landscapes and indoor scenes. Despite their apparent advantages, current line-based reconstruction methods are far behind their point-based counterparts. In this paper we aim to close the gap by introducing LIMAP, a library for 3D line mapping that robustly and efficiently creates 3D line maps from multi-view imagery. This is achieved through revisiting the degeneracy problem of line triangulation, carefully crafted scoring and track building, and exploiting structural priors such as line coincidence, parallelism, and orthogonality. Our code integrates seamlessly with existing point-based Structure-from-Motion methods and can leverage their 3D points to further improve the line reconstruction. Furthermore, as a byproduct, the method is able to recover 3D association graphs between lines and points / vanishing points (VPs). In thorough experiments, we show that LIMAP significantly outperforms existing approaches for 3D line mapping. Our robust 3D line maps also open up new research directions. We show two example applications: visual localization and bundle adjustment, where integrating lines alongside points yields the best results. Code is available at https://github.com/cvg/limap.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1057.Self-Supervised Pre-Training With Masked Shape Prediction for 3D Scene Understanding</span><br>
                <span class="as">Jiang, LiandYang, ZetongandShi, ShaoshuaiandGolyanik, VladislavandDai, DengxinandSchiele, Bernt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Self-Supervised_Pre-Training_With_Masked_Shape_Prediction_for_3D_Scene_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1168-1178.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索在3D场景理解中应用屏蔽信号建模的新方法。<br>
                    动机：尽管屏蔽信号建模已在语言和二维图像的自监督预训练中取得了显著进展，但在3D场景理解中的应用尚未得到充分探索。<br>
                    方法：本文提出了一种新的框架——屏蔽形状预测（MSP），用于在3D场景中进行屏蔽信号建模。MSP使用关键的3D语义线索，即几何形状，作为屏蔽点的预测目标。同时，提出了包含显式形状上下文和隐式深度形状特征的上下文增强形状目标，以便于在形状预测中利用上下文线索。此外，MSP的预训练架构经过精心设计，以减轻点坐标导致的屏蔽形状泄漏。<br>
                    效果：在多个室内外数据集上的3D理解任务的实验表明，MSP在学习良好的特征表示以持续提升下游性能方面具有有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked signal modeling has greatly advanced self-supervised pre-training for language and 2D images. However, it is still not fully explored in 3D scene understanding. Thus, this paper introduces Masked Shape Prediction (MSP), a new framework to conduct masked signal modeling in 3D scenes. MSP uses the essential 3D semantic cue, i.e., geometric shape, as the prediction target for masked points. The context-enhanced shape target consisting of explicit shape context and implicit deep shape feature is proposed to facilitate exploiting contextual cues in shape prediction. Meanwhile, the pre-training architecture in MSP is carefully designed to alleviate the masked shape leakage from point coordinates. Experiments on multiple 3D understanding tasks on both indoor and outdoor datasets demonstrate the effectiveness of MSP in learning good feature representations to consistently boost downstream performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1058.Efficient and Explicit Modelling of Image Hierarchies for Image Restoration</span><br>
                <span class="as">Li, YaweiandFan, YuchenandXiang, XiaoyuandDemandolx, DenisandRanjan, RakeshandTimofte, RaduandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_and_Explicit_Modelling_of_Image_Hierarchies_for_Image_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18278-18289.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种机制，以有效地和明确地在全局、区域和局部范围内对图像进行恢复。<br>
                    动机：通过对自然图像的交叉尺度相似性和各向异性图像特征的分析，作者受到启发，提出了锚定带状自注意力机制，以实现自注意力的空间和时间复杂度与超出区域范围的建模能力之间的良好平衡。<br>
                    方法：作者提出了一种新的网络架构GRL，通过锚定带状自注意力、窗口自注意力和通道注意力增强卷积，明确地在全局、区域和局部范围内对图像层次进行建模。<br>
                    效果：所提出的网络被应用于7种图像恢复类型，包括真实和合成设置。对于其中几种类型，该方法达到了新的最先进的水平。代码将在https://github.com/ofsoundof/GRL-Image-Restoration.git上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The aim of this paper is to propose a mechanism to efficiently and explicitly model image hierarchies in the global, regional, and local range for image restoration. To achieve that, we start by analyzing two important properties of natural images including cross-scale similarity and anisotropic image features. Inspired by that, we propose the anchored stripe self-attention which achieves a good balance between the space and time complexity of self-attention and the modelling capacity beyond the regional range. Then we propose a new network architecture dubbed GRL to explicitly model image hierarchies in the Global, Regional, and Local range via anchored stripe self-attention, window self-attention, and channel attention enhanced convolution. Finally, the proposed network is applied to 7 image restoration types, covering both real and synthetic settings. The proposed method sets the new state-of-the-art for several of those. Code will be available at https://github.com/ofsoundof/GRL-Image-Restoration.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1059.Progressive Random Convolutions for Single Domain Generalization</span><br>
                <span class="as">Choi, SeokeonandDas, DebasmitandChoi, SunghaandYang, SeunghanandPark, HyunsinandYun, Sungrack</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Progressive_Random_Convolutions_for_Single_Domain_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10312-10322.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决单领域泛化问题，即如何训练一个具有单一源领域的模型，使其能良好地执行任意未见过的目标领域任务。<br>
                    动机：现有的基于随机卷积（RandConv）的图像增强方法虽然简单且轻量级，但其生成的图像随着内核大小的增加容易失去语义，且缺乏单个卷积操作的内在多样性。<br>
                    方法：为解决这个问题，本文提出了一种渐进式随机卷积（Pro-RandConv）方法，该方法通过递归堆叠小内核大小的随机卷积层，而不是增大内核大小。这种渐进式方法不仅可以减少远离理论感受野中心的像素的影响，从而减轻语义失真的影响，而且可以通过逐渐增加风格多样性来创建更有效的虚拟领域。此外，我们还将基本的随机卷积层开发为包含形变偏移和仿射变换的随机卷积块，以支持纹理和对比度多样化，这两者也都是随机初始化的。<br>
                    效果：在无需复杂生成器或对抗性学习的情况下，我们证明了我们的这种简单而有效的增强策略在单领域泛化基准测试上优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single domain generalization aims to train a generalizable model with only one source domain to perform well on arbitrary unseen target domains. Image augmentation based on Random Convolutions (RandConv), consisting of one convolution layer randomly initialized for each mini-batch, enables the model to learn generalizable visual representations by distorting local textures despite its simple and lightweight structure. However, RandConv has structural limitations in that the generated image easily loses semantics as the kernel size increases, and lacks the inherent diversity of a single convolution operation. To solve the problem, we propose a Progressive Random Convolution (Pro-RandConv) method that recursively stacks random convolution layers with a small kernel size instead of increasing the kernel size. This progressive approach can not only mitigate semantic distortions by reducing the influence of pixels away from the center in the theoretical receptive field, but also create more effective virtual domains by gradually increasing the style diversity. In addition, we develop a basic random convolution layer into a random convolution block including deformable offsets and affine transformation to support texture and contrast diversification, both of which are also randomly initialized. Without complex generators or adversarial learning, we demonstrate that our simple yet effective augmentation strategy outperforms state-of-the-art methods on single domain generalization benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1060.OPE-SR: Orthogonal Position Encoding for Designing a Parameter-Free Upsampling Module in Arbitrary-Scale Image Super-Resolution</span><br>
                <span class="as">Song, GaochaoandSun, QianandZhang, LuoandSu, RanandShi, JianfengandHe, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_OPE-SR_Orthogonal_Position_Encoding_for_Designing_a_Parameter-Free_Upsampling_Module_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10009-10020.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决任意尺度图像超分辨率问题，通过引入正交位置编码（OPE）和OPE-Upscale模块来改进现有的隐式神经表示（INR）方法。<br>
                    动机：目前的任意尺度图像超分辨率方法主要依赖隐式神经表示（INR），但其需要大量的训练参数且计算效率低下。因此，本文提出了一种不需要训练参数的OPE-Upscale模块，以提高超分辨率的效率和性能。<br>
                    方法：本文提出了一种新的任意尺度图像超分辨率方法，该方法使用正交位置编码（OPE）和OPE-Upscale模块来替代现有的INR-based上采样模块。OPE-Upscale模块直接执行线性组合操作，无需任何训练参数，从而实现了连续的图像重建和任意尺度的图像重建。<br>
                    效果：实验结果表明，新的方法在任意尺度图像超分辨率任务上取得了与现有方法相当的结果，同时具有更高的计算效率和更少的内存消耗。此外，我们还验证了OPE对应于一组正交基，从而证实了我们设计原则的正确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Arbitrary-scale image super-resolution (SR) is often tackled using the implicit neural representation (INR) approach, which relies on a position encoding scheme to improve its representation ability. In this paper, we introduce orthogonal position encoding (OPE), an extension of position encoding, and an OPE-Upscale module to replace the INR-based upsampling module for arbitrary-scale image super-resolution. Our OPE-Upscale module takes 2D coordinates and latent code as inputs, just like INR, but does not require any training parameters. This parameter-free feature allows the OPE-Upscale module to directly perform linear combination operations, resulting in continuous image reconstruction and achieving arbitrary-scale image reconstruction. As a concise SR framework, our method is computationally efficient and consumes less memory than state-of-the-art methods, as confirmed by extensive experiments and evaluations. In addition, our method achieves comparable results with state-of-the-art methods in arbitrary-scale image super-resolution. Lastly, we show that OPE corresponds to a set of orthogonal basis, validating our design principle.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1061.Implicit Surface Contrastive Clustering for LiDAR Point Clouds</span><br>
                <span class="as">Zhang, ZaiweiandBai, MinandLi, Erran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Implicit_Surface_Contrastive_Clustering_for_LiDAR_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21716-21725.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模无标签数据集进行自我监督预训练，以改善计算机视觉任务的性能。<br>
                    动机：尽管这种方法在许多计算机视觉任务中取得了巨大的成功，但由于户外LiDAR点云的复杂性和范围广泛，这种技术尚未广泛应用于户外LiDAR点云感知。<br>
                    方法：本文提出了一种新的自我监督预训练方法ISCC，其核心是针对LiDAR点云设计的两种新的预训练任务。第一个任务通过对比学习将场景中的局部点群排序为一组全局一致且具有语义意义的簇，从而学习语义信息。第二个任务通过隐式表面重建来推理场景各个部分的精确表面，以学习几何结构。<br>
                    效果：实验结果表明，该方法在现实世界的LiDAR场景中的3D对象检测和语义分割的迁移学习性能上非常有效。我们还设计了一个无监督的语义分组任务，以展示我们的方法学习的高度语义有意义的特征。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised pretraining on large unlabeled datasets has shown tremendous success on improving the task performance of many computer vision tasks. However, such techniques have not been widely used for outdoor LiDAR point cloud perception due to its scene complexity and wide range. This prevents impactful application from 2D pretraining frameworks. In this paper, we propose ISCC, a new self-supervised pretraining method, core of which are two pretext tasks newly designed for LiDAR point clouds. The first task focuses on learning semantic information by sorting local groups of points in the scene into a globally consistent set of semantically meaningful clusters using contrastive learning. This is augmented with a second task which reasons about precise surfaces of various parts of the scene through implicit surface reconstruction to learn geometric structures. We demonstrate their effectiveness on transfer learning performance on 3D object detection and semantic segmentation in real world LiDAR scenes. We further design an unsupervised semantic grouping task to showcase the highly semantically meaningful features learned by our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1062.Learning Compact Representations for LiDAR Completion and Generation</span><br>
                <span class="as">Xiong, YuwenandMa, Wei-ChiuandWang, JingkangandUrtasun, Raquel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Learning_Compact_Representations_for_LiDAR_Completion_and_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1074-1083.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用低成本的稀疏LiDAR数据生成高精度的三维世界模型？<br>
                    动机：现有的密集LiDAR设备昂贵，而低光束LiDAR捕获的点云通常稀疏。<br>
                    方法：提出UltraLiDAR数据驱动框架，通过将稀疏点云的表示与密集点云对齐，实现点云的稠密化，并学习一个离散码本以生成多样、真实的LiDAR点云。<br>
                    效果：实验证明，使用UltraLiDAR可以显著提高感知系统的性能，且生成的点云比现有技术更真实，人类参与者在A/B测试中超过98.5%的时间更喜欢其结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR provides accurate geometric measurements of the 3D world. Unfortunately, dense LiDARs are very expensive and the point clouds captured by low-beam LiDAR are often sparse. To address these issues, we present UltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR generation, and LiDAR manipulation. The crux of UltraLiDAR is a compact, discrete representation that encodes the point cloud's geometric structure, is robust to noise, and is easy to manipulate. We show that by aligning the representation of a sparse point cloud to that of a dense point cloud, we can densify the sparse point clouds as if they were captured by a real high-density LiDAR, drastically reducing the cost. Furthermore, by learning a prior over the discrete codebook, we can generate diverse, realistic LiDAR point clouds for self-driving. We evaluate the effectiveness of UltraLiDAR on sparse-to-dense LiDAR completion and LiDAR generation. Experiments show that densifying real-world point clouds with our approach can significantly improve the performance of downstream perception systems. Compared to prior art on LiDAR generation, our approach generates much more realistic point clouds. According to A/B test, over 98.5% of the time human participants prefer our results over those of previous methods. Please refer to project page https://waabi.ai/research/ultralidar/ for more information.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1063.Improving Graph Representation for Point Cloud Segmentation via Attentive Filtering</span><br>
                <span class="as">Zhang, NanandPan, ZhiyiandLi, ThomasH.andGao, WeiandLi, Ge</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Improving_Graph_Representation_for_Point_Cloud_Segmentation_via_Attentive_Filtering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1244-1254.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何结合图卷积和自注意力机制，提高点云分割的性能。<br>
                    动机：虽然自注意力网络在点云分割上表现优秀，但图卷积在捕捉局部几何信息上有更强的能力且计算成本更低。<br>
                    方法：提出一种混合架构设计，构建了具有注意力过滤的图卷积网络（AF-GCN）。该网络采用图卷积来聚合浅层编码器阶段的局部特征，深层阶段则使用一种名为图注意力滤波器（GAF）的自注意力类似模块来更好地建模远程邻居的长程上下文。此外，为了进一步改善点云分割的图形表示，还引入了一种用于图卷积的空间特征投影（SFP）模块以处理非结构化点云的空间变化。最后，引入了一种图共享的降采样和上采样策略，以充分利用点云处理中的图形结构。<br>
                    效果：在S3DIS、ScanNetV2、Toronto-3D和ShapeNetPart等多个数据集上进行大量实验，结果显示AF-GCN取得了有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, self-attention networks achieve impressive performance in point cloud segmentation due to their superiority in modeling long-range dependencies. However, compared to self-attention mechanism, we find graph convolutions show a stronger ability in capturing local geometry information with less computational cost. In this paper, we employ a hybrid architecture design to construct our Graph Convolution Network with Attentive Filtering (AF-GCN), which takes advantage of both graph convolution and self-attention mechanism. We adopt graph convolutions to aggregate local features in the shallow encoder stages, while in the deeper stages, we propose a self-attention-like module named Graph Attentive Filter (GAF) to better model long-range contexts from distant neighbors. Besides, to further improve graph representation for point cloud segmentation, we employ a Spatial Feature Projection (SFP) module for graph convolutions which helps to handle spatial variations of unstructured point clouds. Finally, a graph-shared down-sampling and up-sampling strategy is introduced to make full use of the graph structures in point cloud processing. We conduct extensive experiments on multiple datasets including S3DIS, ScanNetV2, Toronto-3D, and ShapeNetPart. Experimental results show our AF-GCN obtains competitive performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1064.Activating More Pixels in Image Super-Resolution Transformer</span><br>
                <span class="as">Chen, XiangyuandWang, XintaoandZhou, JiantaoandQiao, YuandDong, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22367-22377.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的Transformer模型在低层次视觉任务中表现出色，但通过分析发现其只能利用有限的输入信息空间。<br>
                    动机：为了充分利用Transformer的潜力，提高重建质量，提出一种新型的混合注意力Transformer（HAT）。<br>
                    方法：HAT结合了通道注意力和基于窗口的自我注意力机制，以利用全局统计信息和强大的局部拟合能力。同时，引入了重叠交叉注意力模块，以增强相邻窗口特征之间的交互。<br>
                    效果：实验表明，所提出的模块有效，且通过预训练策略进一步优化模型性能。与现有技术相比，HAT的性能提高了1dB以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages of being able to utilize global statistics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to exploit the potential of the model for further improvement. Extensive experiments show the effectiveness of the proposed modules, and we further scale up the model to demonstrate that the performance of this task can be greatly improved. Our overall method significantly outperforms the state-of-the-art methods by more than 1dB. Codes and models are available at https://github.com/XPixelGroup/HAT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1065.BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks</span><br>
                <span class="as">Chi, XiaoweiandLiu, JiamingandLu, MingandZhang, RongyuandWang, ZhaoqingandGuo, YandongandZhang, Shanghang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_BEV-SAN_Accurate_BEV_3D_Object_Detection_via_Slice_Attention_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17461-17470.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行三维物体检测的鸟瞰图（BEV）特征构建。<br>
                    动机：现有的方法通过将多视角相机特征聚合到展平的网格中来构建BEV特征，但这种方法未能强调不同高度的有信息量的特征。<br>
                    方法：本文提出了一种名为BEV切片注意力网络（BEV-SAN）的新方法，该方法首先沿着高度维度进行采样以构建全局和局部BEV切片，然后从相机特征中聚合BEV切片的特征并使用注意力机制进行合并，最后通过变换器融合合并后的局部和全局BEV特征以生成任务头部的最终特征图。<br>
                    效果：实验结果表明，与均匀采样相比，BEV-SAN能够确定更多有信息量的高度，从而有效地进行三维物体检测的鸟瞰图特征构建。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Bird's-Eye-View (BEV) 3D Object Detection is a crucial multi-view technique for autonomous driving systems. Recently, plenty of works are proposed, following a similar paradigm consisting of three essential components, i.e., camera feature extraction, BEV feature construction, and task heads. Among the three components, BEV feature construction is BEV-specific compared with 2D tasks. Existing methods aggregate the multi-view camera features to the flattened grid in order to construct the BEV feature. However, flattening the BEV space along the height dimension fails to emphasize the informative features of different heights. For example, the barrier is located at a low height while the truck is located at a high height. In this paper, we propose a novel method named BEV Slice Attention Network (BEV-SAN) for exploiting the intrinsic characteristics of different heights. Instead of flattening the BEV space, we first sample along the height dimension to build the global and local BEV slices. Then, the features of BEV slices are aggregated from the camera features and merged by the attention mechanism. Finally, we fuse the merged local and global BEV features by a transformer to generate the final feature map for task heads. The purpose of local BEV slices is to emphasize informative heights. In order to find them, we further propose a LiDAR-guided sampling strategy to leverage the statistical distribution of LiDAR to determine the heights of local slices. Compared with uniform sampling, LiDAR-guided sampling can determine more informative heights. We conduct detailed experiments to demonstrate the effectiveness of BEV-SAN. Code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1066.NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization</span><br>
                <span class="as">Tang, ShitaoandTang, SicongandTagliasacchi, AndreaandTan, PingandFurukawa, Yasutaka</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_NeuMap_Neural_Coordinate_Mapping_by_Auto-Transdecoder_for_Camera_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/929-939.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行相机定位？<br>
                    动机：现有的特征匹配方法需要大量的存储空间，而压缩则会影响性能；坐标回归方法虽然可以压缩数据，但鲁棒性较差。<br>
                    方法：提出了一种名为NeuMap的端到端神经映射方法，将整个场景编码为潜在代码的网格，然后使用基于Transformer的自动解码器对查询像素的3D坐标进行回归。<br>
                    效果：在五个基准测试中，NeuMap显著优于其他坐标回归方法，并在保持网络权重固定的情况下，快速优化新场景的代码，同时实现与特征匹配方法相当的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings. The codes are available at https://github.com/Tangshitao/NeuMap.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1067.AShapeFormer: Semantics-Guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers</span><br>
                <span class="as">Li, ZechuanandYu, HongshanandYang, ZhengengandChen, TongjiaandAkhtar, Naveed</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_AShapeFormer_Semantics-Guided_Object-Level_Active_Shape_Encoding_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1012-1021.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的3D物体检测技术主要通过聚合预测的物体中心点特征来计算候选点，但这种方法忽略了物体级别的形状信息，导致3D物体检测效果不佳。<br>
                    动机：为了解决这个问题，本文提出了一种名为AShapeFormer的语义引导的物体级别形状编码模块，用于改善3D物体检测的效果。<br>
                    方法：AShapeFormer是一个即插即用的模块，利用多头注意力机制来编码物体的形状信息。同时，还提出了形状令牌和物体场景位置编码，以确保形状信息被充分利用。此外，还引入了一个语义指导子模块，以更好地感知物体形状。<br>
                    效果：在流行的SUN RGB-D和ScanNetV2数据集上进行的大量实验表明，使用AShapeFormer增强的模型能够显著提高检测性能，最高可提升8.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection techniques commonly follow a pipeline that aggregates predicted object central point features to compute candidate points. However, these candidate points contain only positional information, largely ignoring the object-level shape information. This eventually leads to sub-optimal 3D object detection. In this work, we propose AShapeFormer, a semantics-guided object-level shape encoding module for 3D object detection. This is a plug-n-play module that leverages multi-head attention to encode object shape information. We also propose shape tokens and object-scene positional encoding to ensure that the shape information is fully exploited. Moreover, we introduce a semantic guidance sub-module to sample more foreground points and suppress the influence of background points for a better object shape perception. We demonstrate a straightforward enhancement of multiple existing methods with our AShapeFormer. Through extensive experiments on the popular SUN RGB-D and ScanNetV2 dataset, we show that our enhanced models are able to outperform the baselines by a considerable absolute margin of up to 8.1%. Code will be available at https://github.com/ZechuanLi/AShapeFormer</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1068.Adaptive Spot-Guided Transformer for Consistent Local Feature Matching</span><br>
                <span class="as">Yu, JiahuanandChang, JiahaoandHe, JianfengandZhang, TianzhuandYu, JiyangandWu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Adaptive_Spot-Guided_Transformer_for_Consistent_Local_Feature_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21898-21908.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决局部特征匹配中的问题，如保持局部一致性和处理大规模变化。<br>
                    动机：尽管现有的无检测器方法利用Transformer架构取得了显著的性能，但很少有工作考虑保持局部一致性，并且大多数方法在处理大规模变化时表现不佳。<br>
                    方法：为此，我们提出了一种自适应点引导的Transformer（ASTR）用于局部特征匹配，该模型在一个统一的粗到细的架构中联合建模局部一致性和规模变化。<br>
                    效果：我们的ASTR具有几个优点。首先，我们设计了一个点引导的聚合模块，以避免在特征聚合过程中干扰无关区域。其次，我们设计了一个自适应缩放模块，根据精细阶段的计算深度信息调整网格的大小。我们在五个标准基准上进行的大量实验结果表明，我们的ASTR优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Local feature matching aims at finding correspondences between a pair of images. Although current detector-free methods leverage Transformer architecture to obtain an impressive performance, few works consider maintaining local consistency. Meanwhile, most methods struggle with large scale variations. To deal with the above issues, we propose Adaptive Spot-Guided Transformer (ASTR) for local feature matching, which jointly models the local consistency and scale variations in a unified coarse-to-fine architecture. The proposed ASTR enjoys several merits. First, we design a spot-guided aggregation module to avoid interfering with irrelevant areas during feature aggregation. Second, we design an adaptive scaling module to adjust the size of grids according to the calculated depth information at fine stage. Extensive experimental results on five standard benchmarks demonstrate that our ASTR performs favorably against state-of-the-art methods.Our code will be released on https://astr2023.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1069.Heat Diffusion Based Multi-Scale and Geometric Structure-Aware Transformer for Mesh Segmentation</span><br>
                <span class="as">Wong, Chi-Chong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wong_Heat_Diffusion_Based_Multi-Scale_and_Geometric_Structure-Aware_Transformer_for_Mesh_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4413-4422.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将Transformer模型从自然语言处理应用到3D网格处理，特别是在三角形网格分割等3D形状分析任务中。<br>
                    动机：Transformer模型的输入置换不变性使其成为3D网格处理的理想候选模型，但如何提取网格数据的多尺度信息和捕获网格数据的形状判别特征是两个主要挑战。<br>
                    方法：提出了一种基于热扩散的方法来解决这些问题，设计了一种新的Transformer模型MeshFormer，该模型将热扩散方法整合到多头自注意力操作中，以自适应地捕获局部邻域到全局上下文的特征，并应用了一种新颖的基于热核签名的结构编码来嵌入网格实例的内在几何结构。<br>
                    效果：在三角形网格分割等任务上的大量实验验证了MeshFormer模型的有效性，并在当前最先进的方法上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Triangle mesh segmentation is an important task in 3D shape analysis, especially in applications such as digital humans and AR/VR. Transformer model is inherently permutation-invariant to input, which makes it a suitable candidate model for 3D mesh processing. However, two main challenges involved in adapting Transformer from natural languages to 3D mesh are yet to be solved, such as i) extracting the multi-scale information of mesh data in an adaptive manner; ii) capturing geometric structures of mesh data as the discriminative characteristics of the shape. Current point based Transformer models fail to tackle such challenges and thus provide inferior performance for discretized surface segmentation. In this work, heat diffusion based method is exploited to tackle these problems. A novel Transformer model called MeshFormer is proposed, which i) integrates Heat Diffusion method into Multi-head Self-Attention operation (HDMSA) to adaptively capture the features from local neighborhood to global contexts; ii) applies a novel Heat Kernel Signature based Structure Encoding (HKSSE) to embed the intrinsic geometric structures of mesh instances into Transformer for structure-aware processing. Extensive experiments on triangle mesh segmentation validate the effectiveness of the proposed MeshFormer model and show significant improvements over current state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1070.Paired-Point Lifting for Enhanced Privacy-Preserving Visual Localization</span><br>
                <span class="as">Lee, ChunghwanandKim, JaihoonandYun, ChanhyukandHong, JeHyeong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Paired-Point_Lifting_for_Enhanced_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17266-17275.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过视觉定位从已知场景的输入图像中恢复相机姿态，这是许多视觉和机器人系统的基础。<br>
                    动机：虽然许多算法使用通过结构从运动（SfM）获得的稀疏3D点云进行定位，但最近的研究表明，这种方法可能会泄露场景的高保真外观，引发隐私问题。<br>
                    方法：我们提出了一种名为配对点提升（PPL）的替代轻量级策略来构建3D线云。PPL将3D点分割成对，并将每对连接起来形成3D线，而不是像以前的方法那样为每个3D点绘制一条随机定向的线。<br>
                    效果：实验结果表明，PPL在不牺牲定位精度的情况下有效地隐藏了场景细节，提高了对隐私攻击的保护能力，解锁了3D线云的真正潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual localization refers to the process of recovering camera pose from input image relative to a known scene, forming a cornerstone of numerous vision and robotics systems. While many algorithms utilize sparse 3D point cloud of the scene obtained via structure-from-motion (SfM) for localization, recent studies have raised privacy concerns by successfully revealing high-fidelity appearance of the scene from such sparse 3D representation. One prominent approach for bypassing this attack was to lift 3D points to randomly oriented 3D lines thereby hiding scene geometry, but latest work have shown such random line cloud has a critical statistical flaw that can be exploited to break through protection. In this work, we present an alternative lightweight strategy called Paired-Point Lifting (PPL) for constructing 3D line clouds. Instead of drawing one randomly oriented line per 3D point, PPL splits 3D points into pairs and joins each pair to form 3D lines. This seemingly simple strategy yields 3 benefits, i) new ambiguity in feature selection, ii) increased line cloud sparsity, and iii) non-trivial distribution of 3D lines, all of which contributes to enhanced protection against privacy attacks. Extensive experimental results demonstrate the strength of PPL in concealing scene details without compromising localization accuracy, unlocking the true potential of 3D line clouds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1071.Depth Estimation From Camera Image and mmWave Radar Point Cloud</span><br>
                <span class="as">Singh, AkashDeepandBa, YunhaoandSarker, AnkurandZhang, HowardandKadambi, AchutaandSoatto, StefanoandSrivastava, ManiandWong, Alex</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Depth_Estimation_From_Camera_Image_and_mmWave_Radar_Point_Cloud_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9275-9285.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从相机图像和稀疏有噪声的雷达点云中推断密集深度？<br>
                    动机：毫米波雷达点云形成的原理及其带来的挑战，包括模糊的海拔高度、有噪声的深度和方位组件在投影到图像上时会产生错误的位置，而现有的工作忽视了这些在相机-雷达融合中的细微差别。<br>
                    方法：设计一种网络，将每个雷达点映射到其在图像平面上可能投影到的可能表面。与现有工作不同，我们不是将原始雷达点云处理为错误的深度图，而是独立查询每个原始点，将其与图像中可能的像素关联起来，从而生成半密集的雷达深度图。为了融合雷达深度和图像，我们提出了一种门控融合方案，该方案考虑了对应关系的信心分数，以便我们选择性地结合雷达和相机嵌入来生成密集的深度图。<br>
                    效果：我们在NuScenes基准测试中测试了我们的方法，结果显示，相比于最佳方法，我们的均方根误差降低了9.1%，平均绝对误差降低了10.3%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a method for inferring dense depth from a camera image and a sparse noisy radar point cloud. We first describe the mechanics behind mmWave radar point cloud formation and the challenges that it poses, i.e. ambiguous elevation and noisy depth and azimuth components that yields incorrect positions when projected onto the image, and how existing works have overlooked these nuances in camera-radar fusion. Our approach is motivated by these mechanics, leading to the design of a network that maps each radar point to the possible surfaces that it may project onto in the image plane. Unlike existing works, we do not process the raw radar point cloud as an erroneous depth map, but query each raw point independently to associate it with likely pixels in the image -- yielding a semi-dense radar depth map. To fuse radar depth with an image, we propose a gated fusion scheme that accounts for the confidence scores of the correspondence so that we selectively combine radar and camera embeddings to yield a dense depth map. We test our method on the NuScenes benchmark and show a 10.3% improvement in mean absolute error and a 9.1% improvement in root-mean-square error over the best method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1072.Prototypical Residual Networks for Anomaly Detection and Localization</span><br>
                <span class="as">Zhang, HuiandWu, ZuxuanandWang, ZhengandChen, ZhinengandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prototypical_Residual_Networks_for_Anomaly_Detection_and_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16281-16291.png><br>
            
            <span class="tt"><span class="t0">研究问题：工业制造中广泛使用异常检测和定位，但现有监督模型容易过拟合少数异常样本，且异常难以察觉和定位。<br>
                    动机：为了解决这些问题，我们提出了一个名为原型残差网络（PRN）的框架，通过学习异常和正常模式之间不同尺度和大小的残差特征来准确重建异常区域的分割图。<br>
                    方法：PRN主要由两部分构成：一是多尺度原型，明确表示异常到正常模式的残差特征；二是多尺寸自注意力机制，实现可变大小的异常特征学习。此外，我们还提出了多种考虑可见和不可见外观变化的异常生成策略，以扩大和多样化异常。<br>
                    效果：在具有挑战性和广泛应用的MVTec AD基准测试集上进行的大量实验表明，PRN优于当前最先进的无监督和有监督方法。我们在另外三个数据集上也取得了最新的结果，证明了PRN的有效性和泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Anomaly detection and localization are widely used in industrial manufacturing for its efficiency and effectiveness. Anomalies are rare and hard to collect and supervised models easily over-fit to these seen anomalies with a handful of abnormal samples, producing unsatisfactory performance. On the other hand, anomalies are typically subtle, hard to discern, and of various appearance, making it difficult to detect anomalies and let alone locate anomalous regions. To address these issues, we propose a framework called Prototypical Residual Network (PRN), which learns feature residuals of varying scales and sizes between anomalous and normal patterns to accurately reconstruct the segmentation maps of anomalous regions. PRN mainly consists of two parts: multi-scale prototypes that explicitly represent the residual features of anomalies to normal patterns; a multi-size self-attention mechanism that enables variable-sized anomalous feature learning. Besides, we present a variety of anomaly generation strategies that consider both seen and unseen appearance variance to enlarge and diversify anomalies. Extensive experiments on the challenging and widely used MVTec AD benchmark show that PRN outperforms current state-of-the-art unsupervised and supervised methods. We further report SOTA results on three additional datasets to demonstrate the effectiveness and generalizability of PRN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1073.Vector Quantization With Self-Attention for Quality-Independent Representation Learning</span><br>
                <span class="as">Yang, ZhouandDong, WeishengandLi, XinandHuang, MengluanandSun, YulinandShi, Guangming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vector_Quantization_With_Self-Attention_for_Quality-Independent_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24438-24448.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络模型对低质量图像的识别鲁棒性。<br>
                    动机：由于训练和测试数据之间的潜在分布偏移，深度神经网络的鲁棒性引起了广泛关注。<br>
                    方法：通过引入离散向量量化（VQ）来消除识别模型中的冗余，具体做法是首先在网络中添加一个码本模块以量化深层特征，然后将它们连接起来并设计一个自我注意模块以增强表示。在训练过程中，我们强制将来自清洁和损坏图像的特征量化到相同的离散嵌入空间中，以便学习质量无关的特征表示，从而提高低质量图像的识别鲁棒性。<br>
                    效果：定性和定量的实验结果表明，该方法有效地实现了这一目标，在ImageNet-C上使用ResNet50作为主干的网络达到了新的最先进的结果43.1% mCE。在其他鲁棒性基准数据集上，如ImageNet-R，该方法的准确率也提高了近2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, the robustness of deep neural networks has drawn extensive attention due to the potential distribution shift between training and testing data (e.g., deep models trained on high-quality images are sensitive to corruption during testing). Many researchers attempt to make the model learn invariant representations from multiple corrupted data through data augmentation or image-pair-based feature distillation to improve the robustness. Inspired by sparse representation in image restoration, we opt to address this issue by learning image-quality-independent feature representation in a simple plug-and-play manner, that is, to introduce discrete vector quantization (VQ) to remove redundancy in recognition models. Specifically, we first add a codebook module to the network to quantize deep features. Then we concatenate them and design a self-attention module to enhance the representation. During training, we enforce the quantization of features from clean and corrupted images in the same discrete embedding space so that an invariant quality-independent feature representation can be learned to improve the recognition robustness of low-quality images. Qualitative and quantitative experimental results show that our method achieved this goal effectively, leading to a new state-of-the-art result of 43.1% mCE on ImageNet-C with ResNet50 as the backbone. On other robustness benchmark datasets, such as ImageNet-R, our method also has an accuracy improvement of almost 2%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1074.DeepSolo: Let Transformer Decoder With Explicit Points Solo for Text Spotting</span><br>
                <span class="as">Ye, MaoyuanandZhang, JingandZhao, ShanshanandLiu, JuhuaandLiu, TongliangandDu, BoandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19348-19357.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决端到端文本识别中场景文本检测和识别的整合问题，以及这两个子任务之间的关系处理。<br>
                    动机：尽管基于Transformer的方法消除了启发式后处理，但它们仍然受到子任务协同作用的影响，并且训练效率较低。<br>
                    方法：本文提出了DeepSolo，一种简单的DETR类似基线，让单个具有显式点的解码器同时进行文本检测和识别。具体来说，对于每个文本实例，我们将字符序列表示为有序点，并用可学习的显式点查询对其进行建模。通过单个解码器后，点查询已经编码了必要的文本语义和位置，因此可以通过非常简单的并行预测头进一步解码为文本的中心线、边界、脚本和置信度。此外，我们还引入了一种文本匹配标准来提供更准确的监督信号，从而实现更高效的训练。<br>
                    效果：定量实验表明，DeepSolo优于先前最先进的方法，并实现了更好的训练效率。此外，DeepSolo也与线条注释兼容，其所需的标注成本远低于多边形。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. In this paper, we present DeepSolo, a simple DETR-like baseline that lets a single Decoder with Explicit Points Solo for text detection and recognition simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Besides, we also introduce a text-matching criterion to deliver more accurate supervisory signals, thus enabling more efficient training. Quantitative experiments on public benchmarks demonstrate that DeepSolo outperforms previous state-of-the-art methods and achieves better training efficiency. In addition, DeepSolo is also compatible with line annotations, which require much less annotation cost than polygons. The code is available at https://github.com/ViTAE-Transformer/DeepSolo.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1075.TINC: Tree-Structured Implicit Neural Compression</span><br>
                <span class="as">Yang, Runzhao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_TINC_Tree-Structured_Implicit_Neural_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18517-18526.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地压缩和表示复杂的、具有多样性的数据。<br>
                    动机：隐式神经表示（INR）虽然能够以少量参数高精度地描述目标场景，但其频谱覆盖有限，且在复杂多样的数据中去除冗余信息困难。<br>
                    方法：提出一种树形结构的隐式神经压缩（TINC）方法，对局部区域进行紧凑表示，并按空间距离在层次结构中提取这些局部表示的共享特征。具体来说，使用多层感知器拟合分区的局部区域，并将这些MLPs组织成树状结构以根据空间距离共享参数。这种参数共享方案不仅确保了相邻区域之间的连续性，而且联合消除了局部和非局部冗余。<br>
                    效果：实验表明，TINC提高了INR的压缩保真度，并在商业工具和其他深度学习方法上显示出了令人印象深刻的压缩能力。此外，该方法具有很高的灵活性，可以针对不同的数据和参数设置进行调整。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representation (INR) can describe the target scenes with high fidelity using a small number of parameters, and is emerging as a promising data compression technique. However, limited spectrum coverage is intrinsic to INR, and it is non-trivial to remove redundancy in diverse complex data effectively. Preliminary studies can only exploit either global or local correlation in the target data and thus of limited performance. In this paper, we propose a Tree-structured Implicit Neural Compression (TINC) to conduct compact representation for local regions and extract the shared features of these local representations in a hierarchical manner. Specifically, we use Multi-Layer Perceptrons (MLPs) to fit the partitioned local regions, and these MLPs are organized in tree structure to share parameters according to the spatial distance. The parameter sharing scheme not only ensures the continuity between adjacent regions, but also jointly removes the local and non-local redundancy. Extensive experiments show that TINC improves the compression fidelity of INR, and has shown impressive compression capabilities over commercial tools and other deep learning based methods. Besides, the approach is of high flexibility and can be tailored for different data and parameter settings. The source code can be found at https://github.com/RichealYoung/TINC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1076.NerVE: Neural Volumetric Edges for Parametric Curve Extraction From Point Cloud</span><br>
                <span class="as">Zhu, XiangyuandDu, DongandChen, WeikaiandZhao, ZhiyouandNie, YinyuandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_NerVE_Neural_Volumetric_Edges_for_Parametric_Curve_Extraction_From_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13601-13610.png><br>
            
            <span class="tt"><span class="t0">研究问题：从点云中提取参数化边缘曲线是3D视觉和几何处理的基本问题。<br>
                    动机：现有的方法主要依赖于关键点检测，这是一个有挑战性的过程，往往会产生噪声输出，使得后续的边缘提取容易出错。<br>
                    方法：我们提出了一种新的神经体积边缘表示方法NerVE，通过体积学习框架可以很容易地学习到这种表示。NerVE可以被无缝转换为一种通用的分段线性（PWL）曲线表示，从而实现对所有类型的自由形式曲线的统一学习策略。<br>
                    效果：我们在具有挑战性的ABC数据集上评估了我们的方法，结果显示基于NerVE的简单网络已经能够大大超过先前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Extracting parametric edge curves from point clouds is a fundamental problem in 3D vision and geometry processing. Existing approaches mainly rely on keypoint detection, a challenging procedure that tends to generate noisy output, making the subsequent edge extraction error-prone. To address this issue, we propose to directly detect structured edges to circumvent the limitations of the previous point-wise methods. We achieve this goal by presenting NerVE, a novel neural volumetric edge representation that can be easily learned through a volumetric learning framework. NerVE can be seamlessly converted to a versatile piece-wise linear (PWL) curve representation, enabling a unified strategy for learning all types of free-form curves. Furthermore, as NerVE encodes rich structural information, we show that edge extraction based on NerVE can be reduced to a simple graph search problem. After converting NerVE to the PWL representation, parametric curves can be obtained via off-the-shelf spline fitting algorithms. We evaluate our method on the challenging ABC dataset. We show that a simple network based on NerVE can already outperform the previous state-of-the-art methods by a great margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1077.Generalized Relation Modeling for Transformer Tracking</span><br>
                <span class="as">Gao, ShenyuanandZhou, ChunluanandZhang, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Generalized_Relation_Modeling_for_Transformer_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18686-18695.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的单流跟踪器在搜索区域内的所有部分都让模板进行交互，可能导致目标和背景混淆。<br>
                    动机：为了解决这个问题，我们提出了一种基于自适应令牌分割的广义关系建模方法。<br>
                    方法：该方法是Transformer跟踪中基于注意力的关系建模的通用形式，通过选择适当的搜索令牌与模板令牌进行交互，继承前两种流管线的优点，实现更灵活的关系建模。引入了注意力掩蔽策略和Gumbel-Softmax技术来促进令牌分割模块的并行计算和端到端学习。<br>
                    效果：大量实验表明，我们的方法优于两流和单流管线，并在六个具有挑战性的基准测试上实现了最先进的性能，同时保持了实时运行速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compared with previous two-stream trackers, the recent one-stream tracking pipeline, which allows earlier interaction between the template and search region, has achieved a remarkable performance gain. However, existing one-stream trackers always let the template interact with all parts inside the search region throughout all the encoder layers. This could potentially lead to target-background confusion when the extracted feature representations are not sufficiently discriminative. To alleviate this issue, we propose a generalized relation modeling method based on adaptive token division. The proposed method is a generalized formulation of attention-based relation modeling for Transformer tracking, which inherits the merits of both previous two-stream and one-stream pipelines whilst enabling more flexible relation modeling by selecting appropriate search tokens to interact with template tokens. An attention masking strategy and the Gumbel-Softmax technique are introduced to facilitate the parallel computation and end-to-end learning of the token division module. Extensive experiments show that our method is superior to the two-stream and one-stream pipelines and achieves state-of-the-art performance on six challenging benchmarks with a real-time running speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1078.SmartAssign: Learning a Smart Knowledge Assignment Strategy for Deraining and Desnowing</span><br>
                <span class="as">Wang, YinglongandMa, ChaoandLiu, Jianzhuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_SmartAssign_Learning_a_Smart_Knowledge_Assignment_Strategy_for_Deraining_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3677-3686.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有方法主要处理单一天气类型，但不同天气条件之间的深层联系通常被忽视。<br>
                    动机：如果正确使用，这些联系可以生成互补的表示，以弥补不足的训练数据，获得积极的性能提升和更好的泛化能力。<br>
                    方法：本文专注于研究密切相关的雨和雪在深层表示层面的联系。提出了一种名为SmartAssign的智能知识分配策略，以优化地将从两个任务中学到的知识分配给一个特定的任务。<br>
                    效果：广泛的实验证明，提出的SmartAssign有效地探索了雨和雪之间的有效联系，明显提高了除雨和除雪的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing methods mainly handle single weather types. However, the connections of different weather conditions at deep representation level are usually ignored. These connections, if used properly, can generate complementary representations for each other to make up insufficient training data, obtaining positive performance gains and better generalization. In this paper, we focus on the very correlated rain and snow to explore their connections at deep representation level. Because sub-optimal connections may cause negative effect, another issue is that if rain and snow are handled in a multi-task learning way, how to find an optimal connection strategy to simultaneously improve deraining and desnowing performance. To build desired connection, we propose a smart knowledge assignment strategy, called SmartAssign, to optimally assign the knowledge learned from both tasks to a specific one. In order to further enhance the accuracy of knowledge assignment, we propose a novel knowledge contrast mechanism, so that the knowledge assigned to different tasks preserves better uniqueness. The inherited inductive biases usually limit the modelling ability of CNNs, we introduce a novel transformer block to constitute the backbone of our network to effectively combine long-range context dependency and local image details. Extensive experiments on seven benchmark datasets verify that proposed SmartAssign explores effective connection between rain and snow, and improves the performances of both deraining and desnowing apparently. The implementation code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/SmartAssign.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1079.Regularize Implicit Neural Representation by Itself</span><br>
                <span class="as">Li, ZheminandWang, HongxiaandMeng, Deyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Regularize_Implicit_Neural_Representation_by_Itself_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10280-10288.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种名为隐式神经表示正则化器（INRR）的正则化器，以提高隐式神经表示（INR）的泛化能力。<br>
                    动机：尽管INR是一种可以表示不受网格分辨率限制的信号细节的全连接网络，但其泛化能力仍有待提高，特别是在非均匀采样数据上。<br>
                    方法：提出的INRR基于学习到的狄利克雷能量（DE），该能量度量矩阵行/列之间的相似性。通过将DE参数化为微小的INR，进一步整合了拉普拉斯矩阵的平滑度。<br>
                    效果：通过精心设计的数值实验，论文还揭示了一系列从INRR派生的性质，包括类似于收敛轨迹和多尺度相似性的动量方法。此外，所提出的方法还可以提高其他信号表示方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a regularizer called Implicit Neural Representation Regularizer (INRR) to improve the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can represent signals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further integrated by parameterizing DE with a tiny INR. INRR improves the generalization of INR in signal representation by perfectly integrating the signal's self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could improve the performance of other signal representation methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1080.DropKey for Vision Transformer</span><br>
                <span class="as">Li, BonanandHu, YinhanandNie, XuechengandHan, CongyingandJiang, XiangjianandGuo, TiandeandLiu, Luoqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DropKey_for_Vision_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22700-22709.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文关注并改进了视觉转换器的自我注意力层的丢弃技术，这是一个被先前的工作所忽视的重要问题。<br>
                    动机：不同于文献中丢弃注意力权重的做法，我们提出在计算注意力矩阵之前提前进行丢弃操作，并将键作为丢弃单位，从而提出了一种新颖的丢弃-然后-softmax方案。<br>
                    方法：我们提出了三个核心问题的解决方案：首先，在自我注意力层中丢弃什么？其次，如何安排连续层的丢弃比率？最后，是否需要像CNN那样执行结构化的丢弃操作？<br>
                    效果：实验结果表明，提出的DropKey方法通过将键视为丢弃单位并使用递减的丢弃比率计划，可以有效地提高各种ViT架构和视觉任务的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we focus on analyzing and improving the dropout technique for self-attention layers of Vision Transformer, which is important while surprisingly ignored by prior works. In particular, we conduct researches on three core questions: First, what to drop in self-attention layers? Different from dropping attention weights in literature, we propose to move dropout operations forward ahead of attention matrix calculation and set the Key as the dropout unit, yielding a novel dropout-before-softmax scheme. We theoretically verify that this scheme helps keep both regularization and probability features of attention weights, alleviating the overfittings problem to specific patterns and enhancing the model to globally capture vital information; Second, how to schedule the drop ratio in consecutive layers? In contrast to exploit a constant drop ratio for all layers, we present a new decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. We experimentally validate the proposed schedule can avoid overfittings in low-level features and missing in high-level semantics, thus improving the robustness and stableness of model training; Third, whether need to perform structured dropout operation as CNN? We attempt patch-based block-version of dropout operation and find that this useful trick for CNN is not essential for ViT. Given exploration on the above three questions, we present the novel DropKey method that regards Key as the drop unit and exploits decreasing schedule for drop ratio, improving ViTs in a general way. Comprehensive experiments demonstrate the effectiveness of DropKey for various ViT architectures, e.g. T2T, VOLO, CeiT and DeiT, as well as for various vision tasks, e.g., image classification, object detection, human-object interaction detection and human body shape recovery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1081.Meta Architecture for Point Cloud Analysis</span><br>
                <span class="as">Lin, HaojiaandZheng, XiawuandLi, LijiangandChao, FeiandWang, ShanshanandWang, YanandTian, YonghongandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Meta_Architecture_for_Point_Cloud_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17682-17691.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D点云分析领域缺乏统一解释框架的问题，以便于进行系统比较、对比和分析。<br>
                    动机：目前3D点云分析网络架构多样，但缺乏统一的解释框架，使得系统性的比较、对比和分析具有挑战性，限制了该领域的健康发展。<br>
                    方法：本文提出一个名为PointMeta的统一解释框架，可以适用于流行的3D点云分析方法。通过这个框架，可以进行公平的比较，快速验证任何从比较中总结出的实证观察或假设。同时，PointMeta也使我们可以跨越不同的组件进行思考，重新审视流行方法的共同信念和关键设计决策。<br>
                    效果：基于前两个分析的学习，我们通过在现有方法上进行简单的调整，得出了一个基本构建模块——PointMetaBase。大量实验表明，它在效率和效果上都表现出强大的性能，并在具有挑战性的基准测试上超过了先前最先进的方法。特别是在S3DIS数据集上，PointMetaBase仅使用2%/11%/13%的计算成本，就比先前最先进的方法高出0.7%/1.4%/2.1% mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in 3D point cloud analysis bring a diverse set of network architectures to the field. However, the lack of a unified framework to interpret those networks makes any systematic comparison, contrast, or analysis challenging, and practically limits healthy development of the field. In this paper, we take the initiative to explore and propose a unified framework called PointMeta, to which the popular 3D point cloud analysis approaches could fit. This brings three benefits. First, it allows us to compare different approaches in a fair manner, and use quick experiments to verify any empirical observations or assumptions summarized from the comparison. Second, the big picture brought by PointMeta enables us to think across different components, and revisit common beliefs and key design decisions made by the popular approaches. Third, based on the learnings from the previous two analyses, by doing simple tweaks on the existing approaches, we are able to derive a basic building block, termed PointMetaBase. It shows very strong performance in efficiency and effectiveness through extensive experiments on challenging benchmarks, and thus verifies the necessity and benefits of high-level interpretation, contrast, and comparison like PointMeta. In particular, PointMetaBase surpasses the previous state-of-the-art method by 0.7%/1.4/%2.1% mIoU with only 2%/11%/13% of the computation cost on the S3DIS datasets. Codes are available in the supplementary materials.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1082.FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer</span><br>
                <span class="as">Liu, ZhijianandYang, XinyuandTang, HaotianandYang, ShangandHan, Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1200-1211.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高3D点云转换器的处理效率，以适应资源有限、延迟敏感的应用。<br>
                    动机：现有的3D点云转换器在准确性上已经达到顶级水平，但其延迟性却比稀疏卷积模型慢3倍，这阻碍了其在自动驾驶等资源有限、延迟敏感的应用中的使用。<br>
                    方法：本文提出了FlatFormer，通过将空间邻近性转化为更好的计算规则性来缩小这种延迟差距。首先，我们通过基于窗口的排序对点云进行展平，并将点分为大小相等的组，而不是形状相等的窗口。然后，我们在组内应用自我注意力以提取局部特征，交替改变排序轴以从不同方向收集特征，并移动窗口以在不同组之间交换特征。<br>
                    效果：FlatFormer在Waymo开放数据集上实现了最先进的精度，比基于变压器的SST快4.6倍，比稀疏卷积的CenterPoint快1.4倍。这是第一个在边缘GPU上实现实时性能的点云转换器，其速度比稀疏卷积方法更快，同时在大尺度基准测试中实现了同等甚至更高的精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformer, as an alternative to CNN, has been proven effective in many modalities (e.g., texts and images). For 3D point cloud transformers, existing efforts focus primarily on pushing their accuracy to the state-of-the-art level. However, their latency lags behind sparse convolution-based models (3x slower), hindering their usage in resource-constrained, latency-sensitive applications (such as autonomous driving). This inefficiency comes from point clouds' sparse and irregular nature, whereas transformers are designed for dense, regular workloads. This paper presents FlatFormer to close this latency gap by trading spatial proximity for better computational regularity. We first flatten the point cloud with window-based sorting and partition points into groups of equal sizes rather than windows of equal shapes. This effectively avoids expensive structuring and padding overheads. We then apply self-attention within groups to extract local features, alternate sorting axis to gather features from different directions, and shift windows to exchange features across groups. FlatFormer delivers state-of-the-art accuracy on Waymo Open Dataset with 4.6x speedup over (transformer-based) SST and 1.4x speedup over (sparse convolutional) CenterPoint. This is the first point cloud transformer that achieves real-time performance on edge GPUs and is faster than sparse convolutional methods while achieving on-par or even superior accuracy on large-scale benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1083.Dynamic Graph Learning With Content-Guided Spatial-Frequency Relation Reasoning for Deepfake Detection</span><br>
                <span class="as">Wang, YuanandYu, KunandChen, ChenandHu, XiyuanandPeng, Silong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7278-7287.png><br>
            
            <span class="tt"><span class="t0">研究问题：随着人脸合成技术的出现，如何开发强大的人脸伪造检测方法成为了一个突出的问题。<br>
                    动机：由于安全考虑，现有的一些方法试图结合辅助的频率感知信息和CNN主干网络来发现伪造的线索。然而，由于与图像内容的信息交互不足，提取的频率特征在空间上不相关，难以在越来越真实的伪造类型上进行泛化。<br>
                    方法：为了解决这个问题，我们提出了一种空间-频率动态图方法，通过动态图学习在空间和频率域中利用关系感知的特征。为此，我们引入了三个精心设计的组件：1）内容引导的自适应频率提取模块，用于挖掘内容自适应的伪造频率线索；2）多领域注意力图学习模块，利用多尺度注意力图丰富空间-频率上下文特征；3）动态图空间-频率特征融合网络，探索空间和频率特征的高阶关系。<br>
                    效果：我们在几个基准数据集上进行了广泛的实验，结果表明我们提出的方法持续地大幅度超过了最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the springing up of face synthesis techniques, it is prominent in need to develop powerful face forgery detection methods due to security concerns. Some existing methods attempt to employ auxiliary frequency-aware information combined with CNN backbones to discover the forged clues. Due to the inadequate information interaction with image content, the extracted frequency features are thus spatially irrelavant, struggling to generalize well on increasingly realistic counterfeit types. To address this issue, we propose a Spatial-Frequency Dynamic Graph method to exploit the relation-aware features in spatial and frequency domains via dynamic graph learning. To this end, we introduce three well-designed components: 1) Content-guided Adaptive Frequency Extraction module to mine the content-adaptive forged frequency clues. 2) Multiple Domains Attention Map Learning module to enrich the spatial-frequency contextual features with multiscale attention maps. 3) Dynamic Graph Spatial-Frequency Feature Fusion Network to explore the high-order relation of spatial and frequency features. Extensive experiments on several benchmark show that our proposed method sustainedly exceeds the state-of-the-arts by a considerable margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1084.Learning Anchor Transformations for 3D Garment Animation</span><br>
                <span class="as">Zhao, FangandLi, ZekunandHuang, ShaoliandWeng, JunwuandZhou, TianfeiandXie, Guo-SenandWang, JueandShan, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Anchor_Transformations_for_3D_Garment_Animation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/491-500.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种基于锚点的变形模型，用于从身体运动序列预测3D服装动画。<br>
                    动机：现有的3D服装动画预测方法在处理宽松服装时效果不佳，因此需要一种新的方法来提高预测的准确性和稳定性。<br>
                    方法：本文提出了一种名为AnchorDEF的基于锚点的变形模型，该模型通过刚性变换和额外的非线性位移来变形服装网格模板。在网格表面引入一组锚点来指导刚性变换矩阵的学习。一旦找到锚点变换，就可以在正则空间中回归服装模板的每个顶点的非线性位移，从而降低变形空间学习的难度。<br>
                    效果：定性和定量实验表明，AnchorDEF在不同类型的服装上实现了最先进的性能，特别是在预测宽松服装的运动变形方面表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes an anchor-based deformation model, namely AnchorDEF, to predict 3D garment animation from a body motion sequence. It deforms a garment mesh template by a mixture of rigid transformations with extra nonlinear displacements. A set of anchors around the mesh surface is introduced to guide the learning of rigid transformation matrices. Once the anchor transformations are found, per-vertex nonlinear displacements of the garment template can be regressed in a canonical space, which reduces the complexity of deformation space learning. By explicitly constraining the transformed anchors to satisfy the consistencies of position, normal and direction, the physical meaning of learned anchor transformations in space is guaranteed for better generalization. Furthermore, an adaptive anchor updating is proposed to optimize the anchor position by being aware of local mesh topology for learning representative anchor transformations. Qualitative and quantitative experiments on different types of garments demonstrate that AnchorDEF achieves the state-of-the-art performance on 3D garment deformation prediction in motion, especially for loose-fitting garments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1085.Tree Instance Segmentation With Temporal Contour Graph</span><br>
                <span class="as">Firoze, AdnanandWingren, CameronandYeh, RaymondA.andBenes, BedrichandAliaga, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2193-2202.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对密集的自相似树进行实例分割和计数。<br>
                    动机：现有的方法无法有效处理紧密排列的树木，需要一种新方法来提高分割和计数的准确性。<br>
                    方法：利用顶部视角RGB图像序列，首先对图像序列进行初始过度分割，并将结构特征聚合到带有时间信息的轮廓图中。然后，使用图卷积网络及其固有的局部消息传递能力，将相邻的树冠补丁合并为最终的树冠集。<br>
                    效果：该方法在所有先前的方法中表现优越，即使在树木紧密排列的情况下，也能实现高精度的实例分割和计数。同时，提供了适用于后续基准测试和评估的不同海拔和叶条件的各种森林图像序列数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel approach to perform instance segmentation, and counting, for densely packed self-similar trees using a top-view RGB image sequence. We propose a solution that leverages pixel content, shape, and self-occlusion. First, we perform an initial over-segmentation of the image sequence and aggregate structural characteristics into a contour graph with temporal information incorporated. Second, using a graph convolutional network and its inherent local messaging passing abilities, we merge adjacent tree crown patches into a final set of tree crowns. Per various studies and comparisons, our method is superior to all prior methods and results in high-accuracy instance segmentation and counting, despite the trees being tightly packed. Finally, we provide various forest image sequence datasets suitable for subsequent benchmarking and evaluation captured at different altitudes and leaf conditions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1086.Grad-PU: Arbitrary-Scale Point Cloud Upsampling via Gradient Descent With Learned Distance Functions</span><br>
                <span class="as">He, YunandTang, DanhangandZhang, YindaandXue, XiangyangandFu, Yanwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Grad-PU_Arbitrary-Scale_Point_Cloud_Upsampling_via_Gradient_Descent_With_Learned_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5354-5363.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的点云上采样方法存在固定上采样率和预测3D坐标困难导致的异常值或收缩伪影两个关键问题。<br>
                    动机：为了解决这些问题，我们提出了一种新的精确点云上采样框架，该框架支持任意的上采样率。<br>
                    方法：我们的方法首先根据给定的上采样率对低分辨率点云进行插值，然后通过一个训练模型来估计当前点云与高分辨率目标之间的差异，从而指导迭代优化过程来细化插值点的位置。<br>
                    效果：我们在基准测试和下游任务上的大量定量和定性结果表明，我们的方法实现了最先进的准确性和效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing point cloud upsampling methods have roughly three steps: feature extraction, feature expansion and 3D coordinate prediction. However, they usually suffer from two critical issues: (1) fixed upsampling rate after one-time training, since the feature expansion unit is customized for each upsampling rate; (2) outliers or shrinkage artifact caused by the difficulty of precisely predicting 3D coordinates or residuals of upsampled points. To adress them, we propose a new framework for accurate point cloud upsampling that supports arbitrary upsampling rates. Our method first interpolates the low-res point cloud according to a given upsampling rate. And then refine the positions of the interpolated points with an iterative optimization process, guided by a trained model estimating the difference between the current point cloud and the high-res target. Extensive quantitative and qualitative results on benchmarks and downstream tasks demonstrate that our method achieves the state-of-the-art accuracy and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1087.InternImage: Exploring Large-Scale Vision Foundation Models With Deformable Convolutions</span><br>
                <span class="as">Wang, WenhaiandDai, JifengandChen, ZheandHuang, ZhenhangandLi, ZhiqiandZhu, XizhouandHu, XiaoweiandLu, TongandLu, LeweiandLi, HongshengandWang, XiaogangandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14408-14419.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1088.MED-VT: Multiscale Encoder-Decoder Video Transformer With Application To Object Segmentation</span><br>
                <span class="as">Karim, RezaulandZhao, HeandWildes, RichardP.andSiam, Mennatullah</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_MED-VT_Multiscale_Encoder-Decoder_Video_Transformer_With_Application_To_Object_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6323-6333.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索一种统一的多尺度编码器-解码器转换器，专注于视频中的密集预测任务。<br>
                    动机：目前的多尺度处理仅限于编码器或解码器，而本研究提出了一种统一的多尺度编码器-解码器转换器，能够同时提取空间-时间特征和进行高级别的语义检测。<br>
                    方法：通过在编码器和解码器中都使用多尺度表示，实现了隐式提取空间-时间特征以及编码和解码的时序一致性。此外，还提出了一种转导学习方案，通过多对多的标签传播来提供时序一致的预测。<br>
                    效果：在自动视频对象分割和演员/动作分割等任务上，该模型在多个基准测试中优于最先进的方法，且无需使用光流。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multiscale video transformers have been explored in a wide variety of vision tasks. To date, however, the multiscale processing has been confined to the encoder or decoder alone. We present a unified multiscale encoder-decoder transformer that is focused on dense prediction tasks in videos. Multiscale representation at both encoder and decoder yields key benefits of implicit extraction of spatiotemporal features (i.e. without reliance on input optical flow) as well as temporal consistency at encoding and coarse-to-fine detection for high-level (e.g. object) semantics to guide precise localization at decoding. Moreover, we propose a transductive learning scheme through many-to-many label propagation to provide temporally consistent predictions.We showcase our Multiscale Encoder-Decoder Video Transformer (MED-VT) on Automatic Video Object Segmentation (AVOS) and actor/action segmentation, where we outperform state-of-the-art approaches on multiple benchmarks using only raw images, without using optical flow.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1089.Spatially Adaptive Self-Supervised Learning for Real-World Image Denoising</span><br>
                <span class="as">Li, JunyiandZhang, ZhiluandLiu, XiaoyuandFeng, ChaoyuandWang, XiaotaoandLei, LeiandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spatially_Adaptive_Self-Supervised_Learning_for_Real-World_Image_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9914-9924.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像去噪方法主要针对空间独立的噪声，对于真实世界中具有空间相关性噪声的sRGB图像效果不佳。<br>
                    动机：为了解决这一问题，本文提出了一种新的视角，即寻求对真实世界的sRGB图像进行空间自适应监督的去噪方法。<br>
                    方法：具体来说，我们考虑了噪声图像中平坦区域和纹理区域各自的特点，并分别对它们进行监督。对于平坦区域，我们可以安全地从非相邻像素中获取监督，以排除受噪声相关像素影响的可能。同时，我们将盲点网络扩展到盲邻域网络（BNN），为平坦区域提供监督。对于纹理区域，其监督必须与相邻像素的内容紧密相关。因此，我们提出了一种局部感知网络（LAN）来满足这一需求，而LAN本身则由BNN的输出有选择地进行监督。通过结合这两种监督方式，我们可以训练出性能良好的去噪网络（如U-Net）。<br>
                    效果：大量实验表明，我们的方法在真实世界的sRGB照片上优于最先进的自监督图像去噪方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Significant progress has been made in self-supervised image denoising (SSID) in the recent few years. However, most methods focus on dealing with spatially independent noise, and they have little practicality on real-world sRGB images with spatially correlated noise. Although pixel-shuffle downsampling has been suggested for breaking the noise correlation, it breaks the original information of images, which limits the denoising performance. In this paper, we propose a novel perspective to solve this problem, i.e., seeking for spatially adaptive supervision for real-world sRGB image denoising. Specifically, we take into account the respective characteristics of flat and textured regions in noisy images, and construct supervisions for them separately. For flat areas, the supervision can be safely derived from non-adjacent pixels, which are much far from the current pixel for excluding the influence of the noise-correlated ones. And we extend the blind-spot network to a blind-neighborhood network (BNN) for providing supervision on flat areas. For textured regions, the supervision has to be closely related to the content of adjacent pixels. And we present a locally aware network (LAN) to meet the requirement, while LAN itself is selectively supervised with the output of BNN. Combining these two supervisions, a denoising network (e.g., U-Net) can be well-trained. Extensive experiments show that our method performs favorably against state-of-the-art SSID methods on real-world sRGB photographs. The code is available at https://github.com/nagejacob/SpatiallyAdaptiveSSID.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1090.Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis</span><br>
                <span class="as">Xu, XiuweiandWang, ZiweiandZhou, JieandLu, Jiwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Binarizing_Sparse_Convolutional_Networks_for_Efficient_Point_Cloud_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5313-5322.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行点云分析。<br>
                    动机：现有的稀疏卷积操作在进行量化时会产生较大的误差，导致性能下降。<br>
                    方法：提出一种名为BSC-Net的二值稀疏卷积网络，通过寻找激活稀疏卷积的最佳位置来缓解量化误差，无需增加计算复杂度。<br>
                    效果：在ScanNet和NYU Depth v2数据集上的实验结果表明，BSC-Net在高效的点云分析上取得了显著的改进，且优于最先进的网络二值化方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose binary sparse convolutional networks called BSC-Net for efficient point cloud analysis. We empirically observe that sparse convolution operation causes larger quantization errors than standard convolution. However, conventional network quantization methods directly binarize the weights and activations in sparse convolution, resulting in performance drop due to the significant quantization loss. On the contrary, we search the optimal subset of convolution operation that activates the sparse convolution at various locations for quantization error alleviation, and the performance gap between real-valued and binary sparse convolutional networks is closed without complexity overhead. Specifically, we first present the shifted sparse convolution that fuses the information in the receptive field for the active sites that match the pre-defined positions. Then we employ the differentiable search strategies to discover the optimal opsitions for active site matching in the shifted sparse convolution, and the quantization errors are significantly alleviated for efficient point cloud analysis. For fair evaluation of the proposed method, we empirically select the recently advances that are beneficial for sparse convolution network binarization to construct a strong baseline. The experimental results on ScanNet and NYU Depth v2 show that our BSC-Net achieves significant improvement upon our srtong baseline and outperforms the state-of-the-art network binarization methods by a remarkable margin without additional computation overhead for binarizing sparse convolutional networks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1091.Quantum-Inspired Spectral-Spatial Pyramid Network for Hyperspectral Image Classification</span><br>
                <span class="as">Zhang, JieandZhang, YongshanandZhou, Yicong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Quantum-Inspired_Spectral-Spatial_Pyramid_Network_for_Hyperspectral_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9925-9934.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用量子理论设计一种新的深度学习模型，用于高光谱图像（HSI）的特征提取和分类。<br>
                    动机：现有的深度学习模型在处理高光谱图像时，通常采用传统的学习模式。而量子计算机作为新兴的机器，虽然在嘈杂的中型量子（NISQ）时代中受到限制，但其量子理论为设计深度学习模型提供了新的范式。<br>
                    方法：受量子电路（QC）模型的启发，我们提出了一种量子启发的光谱-空间网络（QSSN）用于HSI特征提取。该网络由一个相位预测模块（PPM）和一个测量类似的融合模块（MFM）组成，灵感来自量子理论，用于动态融合光谱和空间信息。具体来说，QSSN使用量子表示来表示一个HSI立方体，并使用MFM提取联合光谱-空间特征。<br>
                    效果：通过将QSSN作为构建块，我们提出了一种端到端的量子启发的光谱-空间金字塔网络（QSSPN）用于HSI特征提取和分类。在该金字塔框架中，QSSPN通过级联QSSN块逐步学习特征表示，并使用softmax分类器进行分类。这是首次尝试在HSI处理模型设计中引入量子理论。我们在三个HSI数据集上进行了大量实验，以验证所提出的QSSPN框架优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Hyperspectral image (HSI) classification aims at assigning a unique label for every pixel to identify categories of different land covers. Existing deep learning models for HSIs are usually performed in a traditional learning paradigm. Being emerging machines, quantum computers are limited in the noisy intermediate-scale quantum (NISQ) era. The quantum theory offers a new paradigm for designing deep learning models. Motivated by the quantum circuit (QC) model, we propose a quantum-inspired spectral-spatial network (QSSN) for HSI feature extraction. The proposed QSSN consists of a phase-prediction module (PPM) and a measurement-like fusion module (MFM) inspired from quantum theory to dynamically fuse spectral and spatial information. Specifically, QSSN uses a quantum representation to represent an HSI cuboid and extracts joint spectral-spatial features using MFM. An HSI cuboid and its phases predicted by PPM are used in the quantum representation. Using QSSN as the building block, we propose an end-to-end quantum-inspired spectral-spatial pyramid network (QSSPN) for HSI feature extraction and classification. In this pyramid framework, QSSPN progressively learns feature representations by cascading QSSN blocks and performs classification with a softmax classifier. It is the first attempt to introduce quantum theory in HSI processing model design. Substantial experiments are conducted on three HSI datasets to verify the superiority of the proposed QSSPN framework over the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1092.DETRs With Hybrid Matching</span><br>
                <span class="as">Jia, DingandYuan, YuhuiandHe, HaodiandWu, XiaopeiandYu, HaojunandLin, WeihongandSun, LeiandZhang, ChaoandHu, Han</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19702-19712.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决目标检测中需要手动设计非最大抑制（NMS）去除重复检测的问题。<br>
                    动机：现有的DETR模型在训练过程中，正样本的查询被分配为少数，一对一匹配大大减少了正样本的训练效率。<br>
                    方法：提出一种基于混合匹配策略的方法，将原始的一对一匹配分支与辅助的一对多匹配分支结合进行训练。<br>
                    效果：实验表明，这种混合策略可以显著提高准确性。在推理阶段，仅使用原始的一对一匹配分支，从而保持了DETR的端到端优势和相同的推理效率。该方法被称为H-DETR，并在各种视觉任务上对一系列代表性的DETR方法进行了一致的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to broader vision tasks. However, we note that there are few queries assigned as positive samples and the one-to-one set matching significantly reduces the training efficacy of positive samples. We propose a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with an auxiliary one-to-many matching branch during training. Our hybrid strategy has been shown to significantly improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named H-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including Deformable-DETR, PETRv2, PETR, and TransTrack, among others.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1093.A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization</span><br>
                <span class="as">He, YijiaandXu, BoandOuyang, ZhanpengandLi, Hongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_A_Rotation-Translation-Decoupled_Solution_for_Robust_and_Efficient_Visual-Inertial_Initialization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/739-748.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的视觉惯性里程计（VIO）初始化方法，该方法将旋转和平移估计解耦，以实现更高的效率和更好的鲁棒性。<br>
                    动机：现有的松散耦合的VIO初始化方法在视觉结构运动恢复（SfM）的稳定性上表现不佳，而那些紧密耦合的方法往往忽略了闭型解决方案中的陀螺仪偏差，导致准确性有限。此外，上述两类方法都存在计算成本高的问题，因为需要同时重建3D点云。<br>
                    方法：我们的新型方法充分利用了惯性和视觉测量数据进行旋转和平移初始化。首先，设计了一种仅旋转的解决方案用于陀螺仪偏差估计，该方案紧密地结合了陀螺仪和相机观测。其次，使用线性平移约束在全球范围内最优地解决了初始速度和重力向量，无需重建3D点云。<br>
                    效果：大量实验证明，我们的方法比最先进的方法快872倍（基于10帧集），并且表现出显著的更高鲁棒性和准确性。源代码可在https://github.com/boxuLibrary/drt-vio-init获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel visual-inertial odometry (VIO) initialization method, which decouples rotation and translation estimation, and achieves higher efficiency and better robustness. Existing loosely-coupled VIO-initialization methods suffer from poor stability of visual structure-from-motion (SfM), whereas those tightly-coupled methods often ignore the gyroscope bias in the closed-form solution, resulting in limited accuracy. Moreover, the aforementioned two classes of methods are computationally expensive, because 3D point clouds need to be reconstructed simultaneously. In contrast, our new method fully combines inertial and visual measurements for both rotational and translational initialization. First, a rotation-only solution is designed for gyroscope bias estimation, which tightly couples the gyroscope and camera observations. Second, the initial velocity and gravity vector are solved with linear translation constraints in a globally optimal fashion and without reconstructing 3D point clouds. Extensive experiments have demonstrated that our method is 8 72 times faster (w.r.t. a 10-frame set) than the state-of-the-art methods, and also presents significantly higher robustness and accuracy. The source code is available at https://github.com/boxuLibrary/drt-vio-init.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1094.Multi-Modal Gait Recognition via Effective Spatial-Temporal Feature Fusion</span><br>
                <span class="as">Cui, YufengandKang, Yimei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17949-17957.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过融合和聚合骨架和剪影的空间-时间信息，获得更鲁棒和全面的步态识别表示。<br>
                    动机：现有的基于剪影和骨架的步态识别方法受到服装遮挡的影响，且缺乏身体形状信息。<br>
                    方法：提出一种基于变压器的步态识别框架MMGaitFormer，包括空间融合模块（SFM）和时间融合模块（TFM），用于有效融合和聚合两种模态的空间-时间信息。<br>
                    效果：实验证明，MMGaitFormer在流行的步态数据集上取得了最先进的性能，对于最具挑战性的“CL”条件，该方法实现了94.8%的rank-1准确率，大幅超过了最先进的单模态方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gait recognition is a biometric technology that identifies people by their walking patterns. The silhouettes-based method and the skeletons-based method are the two most popular approaches. However, the silhouette data are easily affected by clothing occlusion, and the skeleton data lack body shape information. To obtain a more robust and comprehensive gait representation for recognition, we propose a transformer-based gait recognition framework called MMGaitFormer, which effectively fuses and aggregates the spatial-temporal information from the skeletons and silhouettes. Specifically, a Spatial Fusion Module (SFM) and a Temporal Fusion Module (TFM) are proposed for effective spatial-level and temporal-level feature fusion, respectively. The SFM performs fine-grained body parts spatial fusion and guides the alignment of each part of the silhouette and each joint of the skeleton through the attention mechanism. The TFM performs temporal modeling through Cycle Position Embedding (CPE) and fuses temporal information of two modalities. Experiments demonstrate that our MMGaitFormer achieves state-of-the-art performance on popular gait datasets. For the most challenging "CL" (i.e., walking in different clothes) condition in CASIA-B, our method achieves a rank-1 accuracy of 94.8%, which outperforms the state-of-the-art single-modal methods by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1095.Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution</span><br>
                <span class="as">Chen, Hao-WeiandXu, Yu-SyuanandHong, Min-FongandTsai, Yi-MinandKuo, Hsien-KaiandLee, Chun-Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Cascaded_Local_Implicit_Transformer_for_Arbitrary-Scale_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18257-18267.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用注意力机制和频率编码技术，通过局部隐式图像函数来表示任意分辨率的图像。<br>
                    动机：当前的神经网络表征在表示任意分辨率的图像方面表现出了强大的潜力。<br>
                    方法：提出了局部隐式变换器（LIT），将注意力机制和频率编码技术整合到局部隐式图像函数中，设计了一个跨尺度局部注意模块来有效聚合局部特征，以及一个局部频率编码模块来结合位置编码和傅里叶域信息以构建高分辨率图像。为了进一步提高代表性，我们提出了级联LIT（CLIT），利用多尺度特征和累积训练策略逐渐增加训练的上采样因子。<br>
                    效果：大量的实验验证了这些组件的有效性，并分析了各种训练策略的变化。定性和定量的结果表明，LIT和CLIT在任意超分辨率任务上都取得了良好的效果，超过了先前的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representation demonstrates promising ability in representing images with arbitrary resolutions recently. In this paper, we present Local Implicit Transformer (LIT) that integrates attention mechanism and frequency encoding technique into local implicit image function. We design a cross-scale local attention block to effectively aggregate local features and a local frequency encoding block to combine positional encoding with Fourier domain information for constructing high-resolution (HR) images. To further improve representative power, we propose Cascaded LIT (CLIT) exploiting multi-scale features along with cumulative training strategy that gradually increase the upsampling factors for training. We have performed extensive experiments to validate the effectiveness of these components and analyze the variants of the training strategy. The qualitative and quantitative results demonstrated that LIT and CLIT achieve favorable results and outperform the previous works within arbitrary super-resolution tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1096.Transformer Scale Gate for Semantic Segmentation</span><br>
                <span class="as">Shi, HengcanandHayat, MunawarandCai, Jianfei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Transformer_Scale_Gate_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3051-3060.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地编码多尺度的上下文信息以提高语义分割的准确性。<br>
                    动机：现有的基于变压器的分割模型在结合不同尺度的特征时，没有进行任何选择，可能会导致次优尺度的特征降低分割结果的质量。<br>
                    方法：提出了一种简单而有效的模块——变压器尺度门（TSG），以优化地结合多尺度特征。TSG利用了变压器中自我和交叉注意力的特性进行尺度选择。TSG是一个高度灵活的即插即用模块，可以很容易地集成到任何基于编码器-解码器层次结构的视觉变压器架构中。<br>
                    效果：在Pascal Context、ADE20K和Cityscapes数据集上的大量实验表明，我们的特征选择策略能够持续获得收益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Effectively encoding multi-scale contextual information is crucial for accurate semantic segmentation. Most of the existing transformer-based segmentation models combine features across scales without any selection, where features on sub-optimal scales may degrade segmentation outcomes. Leveraging from the inherent properties of Vision Transformers, we propose a simple yet effective module, Transformer Scale Gate (TSG), to optimally combine multi-scale features. TSG exploits cues in self and cross attentions in Vision Transformers for the scale selection. TSG is a highly flexible plug-and-play module, and can easily be incorporated with any encoder-decoder-based hierarchical vision Transformer architecture. Extensive experiments on the Pascal Context, ADE20K and Cityscapes datasets demonstrate that our feature selection strategy achieves consistent gains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1097.PMatch: Paired Masked Image Modeling for Dense Geometric Matching</span><br>
                <span class="as">Zhu, ShengjieandLiu, Xiaoming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_PMatch_Paired_Masked_Image_Modeling_for_Dense_Geometric_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21909-21918.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练大规模文本语料库和知识图谱，利用外部知识增强语言表示模型的性能。<br>
                    动机：目前的预训练语言模型缺乏对结构化知识的利用，而知识图谱中的有信息量的实体可以提升语言理解能力。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，构建了ERNIE模型，该模型能同时捕捉词汇、句法和知识信息。<br>
                    效果：实验结果显示，ERNIE在各种知识驱动任务上表现优秀，且在其他常见的NLP任务上与BERT模型性能相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dense geometric matching determines the dense pixel-wise correspondence between a source and support image corresponding to the same 3D structure. Prior works employ an encoder of transformer blocks to correlate the two-frame features. However, existing monocular pretraining tasks, e.g., image classification, and masked image modeling (MIM), can not pretrain the cross-frame module, yielding less optimal performance. To resolve this, we reformulate the MIM from reconstructing a single masked image to reconstructing a pair of masked images, enabling the pretraining of transformer module. Additionally, we incorporate a decoder into pretraining for improved upsampling results. Further, to be robust to the textureless area, we propose a novel cross-frame global matching module (CFGM). Since the most textureless area is planar surfaces, we propose a homography loss to further regularize its learning. Combined together, we achieve the State-of-The-Art (SoTA) performance on geometric matching. Codes and models are available at https://github.com/ShngJZ/PMatch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1098.Teaching Matters: Investigating the Role of Supervision in Vision Transformers</span><br>
                <span class="as">Walmer, MatthewandSuri, SakshamandGupta, KamalandShrivastava, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Walmer_Teaching_Matters_Investigating_the_Role_of_Supervision_in_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7486-7496.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在探索视觉转换器（ViTs）在不同学习范式下的行为。<br>
                    动机：近年来，视觉转换器在许多应用中取得了显著的普及，但其在不同监督学习方法下的行为尚未得到充分探索。<br>
                    方法：通过比较不同监督学习方法训练的视觉转换器，分析其注意力、表示和下游性能的差异。<br>
                    效果：研究发现，视觉转换器具有高度的灵活性，能够根据训练方法的不同以不同的顺序处理局部和全局信息。对比性自监督学习方法学习的特征与显式监督学习方法的特征具有竞争力，甚至在某些部分任务上更优。此外，重建模型的表示与对比性自监督模型的表示存在显著相似性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) have gained significant popularity in recent years and have proliferated into many applications. However, their behavior under different learning paradigms is not well explored. We compare ViTs trained through different methods of supervision, and show that they learn a diverse range of behaviors in terms of their attention, representations, and downstream performance. We also discover ViT behaviors that are consistent across supervision, including the emergence of Offset Local Attention Heads. These are self-attention heads that attend to a token adjacent to the current token with a fixed directional offset, a phenomenon that to the best of our knowledge has not been highlighted in any prior work. Our analysis shows that ViTs are highly flexible and learn to process local and global information in different orders depending on their training method. We find that contrastive self-supervised methods learn features that are competitive with explicitly supervised features, and they can even be superior for part-level tasks. We also find that the representations of reconstruction-based models show non-trivial similarity to contrastive self-supervised models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1099.Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers</span><br>
                <span class="as">Long, SifanandZhao, ZhenandPi, JiminandWang, ShengshengandWang, Jingdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_Beyond_Attentive_Tokens_Incorporating_Token_Importance_and_Diversity_for_Efficient_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10334-10343.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器在各种视觉任务上取得了显著改进，但其二次项的标记交互会大大降低计算效率。<br>
                    动机：现有的剪枝方法主要关注保留局部注意力标记的重要性，但完全忽略了全局标记的多样性。<br>
                    方法：我们提出了一种有效的标记解耦和合并方法，该方法可以同时考虑标记的重要性和多样性进行标记剪枝。根据类标记注意力，我们将注意力和非注意力标记分离。<br>
                    效果：尽管方法简单，但我们的方法在模型复杂度和分类准确性之间取得了良好的平衡。在DeiT-S上，我们的方法将FLOPs减少了35%，仅使准确率下降了0.2%。值得注意的是，由于保持了标记的多样性，我们的方法甚至可以在将DeiT-T的FLOPs减少40%后，将其准确率提高了0.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers have achieved significant improvements on various vision tasks but their quadratic interactions between tokens significantly reduce computational efficiency. Many pruning methods have been proposed to remove redundant tokens for efficient vision transformers recently. However, existing studies mainly focus on the token importance to preserve local attentive tokens but completely ignore the global token diversity. In this paper, we emphasize the cruciality of diverse global semantics and propose an efficient token decoupling and merging method that can jointly consider the token importance and diversity for token pruning. According to the class token attention, we decouple the attentive and inattentive tokens. In addition to preserve the most discriminative local tokens, we merge similar inattentive tokens and match homogeneous attentive tokens to maximize the token diversity. Despite its simplicity, our method obtains a promising trade-off between model complexity and classification accuracy. On DeiT-S, our method reduces the FLOPs by 35% with only a 0.2% accuracy drop. Notably, benefiting from maintaining the token diversity, our method can even improve the accuracy of DeiT-T by 0.1% after reducing its FLOPs by 40%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1100.AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation</span><br>
                <span class="as">Li, ZhenandZhu, Zuo-LiangandHan, Ling-HaoandHou, QibinandGuo, Chun-LeandCheng, Ming-Ming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_AMT_All-Pairs_Multi-Field_Transforms_for_Efficient_Frame_Interpolation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9801-9810.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的网络架构——全对多域变换（AMT），用于视频帧插值。<br>
                    动机：现有的视频帧插值方法在处理大运动和遮挡区域时存在困难，而基于卷积的方法在准确性和效率上与基于Transformer的方法相比有优势。<br>
                    方法：首先，为所有像素对构建双向关联量，并使用预测的双边流来检索相关性以更新流和插值内容特征。其次，从一对更新的粗流中导出多组细粒度流场，分别对输入帧进行反向扭曲。这两种设计的结合使得模型能够生成面向任务的流，并在视频帧插值中降低大运动和遮挡区域的建模难度。<br>
                    效果：实验结果表明，该模型在各种基准测试中实现了最先进的性能，并且在准确性和效率上都优于基于Transformer的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present All-Pairs Multi-Field Transforms (AMT), a new network architecture for video frame interpolation. It is based on two essential designs. First, we build bidirectional correlation volumes for all pairs of pixels and use the predicted bilateral flows to retrieve correlations for updating both flows and the interpolated content feature. Second, we derive multiple groups of fine-grained flow fields from one pair of updated coarse flows for performing backward warping on the input frames separately. Combining these two designs enables us to generate promising task-oriented flows and reduce the difficulties in modeling large motions and handling occluded areas during frame interpolation. These qualities promote our model to achieve state-of-the-art performance on various benchmarks with high efficiency. Moreover, our convolution-based model competes favorably compared to Transformer-based models in terms of accuracy and efficiency. Our code is available at https://github.com/MCG-NKU/AMT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1101.Deep Discriminative Spatial and Temporal Network for Efficient Video Deblurring</span><br>
                <span class="as">Pan, JinshanandXu, BomingandDong, JiangxinandGe, JianjunandTang, Jinhui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Deep_Discriminative_Spatial_and_Temporal_Network_for_Efficient_Video_Deblurring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22191-22200.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地探索视频去模糊中的空间和时间信息。<br>
                    动机：与现有的直接对齐相邻帧而不加区分的方法不同，我们开发了一种深度判别性空间和时间网络，以促进空间和时间特征的探索，从而更好地进行视频去模糊。<br>
                    方法：我们首先开发了一个通道门控动态网络来自适应地探索空间信息。然后，为了获取有用的时间特征以恢复潜在清晰帧，我们开发了一个简单的但有效的判别性时间特征融合模块。此外，为了利用来自远距离帧的信息，我们开发了一种基于小波的特征传播方法，该方法将判别性时间特征融合模块作为基本单元，有效地从远距离帧传播主要结构，以实现更好的视频去模糊。<br>
                    效果：我们的实验表明，所提出的方法不需要额外的对齐方法，并且在准确性和模型复杂度方面优于现有的最佳方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>How to effectively explore spatial and temporal information is important for video deblurring. In contrast to existing methods that directly align adjacent frames without discrimination, we develop a deep discriminative spatial and temporal network to facilitate the spatial and temporal feature exploration for better video deblurring. We first develop a channel-wise gated dynamic network to adaptively explore the spatial information. As adjacent frames usually contain different contents, directly stacking features of adjacent frames without discrimination may affect the latent clear frame restoration. Therefore, we develop a simple yet effective discriminative temporal feature fusion module to obtain useful temporal features for latent frame restoration. Moreover, to utilize the information from long-range frames, we develop a wavelet-based feature propagation method that takes the discriminative temporal feature fusion module as the basic unit to effectively propagate main structures from long-range frames for better video deblurring. We show that the proposed method does not require additional alignment methods and performs favorably against state-of-the-art ones on benchmark datasets in terms of accuracy and model complexity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1102.Deep Arbitrary-Scale Image Super-Resolution via Scale-Equivariance Pursuit</span><br>
                <span class="as">Wang, XiaohangandChen, XuanhongandNi, BingbingandWang, HangandTong, ZhengyanandLiu, Yutian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Deep_Arbitrary-Scale_Image_Super-Resolution_via_Scale-Equivariance_Pursuit_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1786-1795.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用变换器框架中的规模等变模块来提高任意尺度的图像超分辨率（ASISR）性能，特别是在高上采样率图像外推中。<br>
                    动机：观察到规模等变处理模块在任意尺度图像超分辨率任务中的关键作用，受此启发，提出了两个新的规模等变模块。<br>
                    方法：设计了一个名为“自适应特征提取器”的插件模块，该模块在频率扩展编码中注入显式规模信息，从而实现表示学习的规模适应。在上采样阶段，引入了一种可学习的神经插值上采样算子，该算子同时以双边方式编码相对距离（即规模感知）信息和特征相似性（即从训练数据中学到的先验知识）。<br>
                    效果：实验结果表明，所提出的操作符和学习框架提供了出色的规模等变能力，比之前的SOTA在任意尺度的SR上都有更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability of scale-equivariance processing blocks plays a central role in arbitrary-scale image super-resolution tasks. Inspired by this crucial observation, this work proposes two novel scale-equivariant modules within a transformer-style framework to enhance arbitrary-scale image super-resolution (ASISR) performance, especially in high upsampling rate image extrapolation. In the feature extraction phase, we design a plug-in module called Adaptive Feature Extractor, which injects explicit scale information in frequency-expanded encoding, thus achieving scale-adaption in representation learning. In the upsampling phase, a learnable Neural Kriging upsampling operator is introduced, which simultaneously encodes both relative distance (i.e., scale-aware) information as well as feature similarity (i.e., with priori learned from training data) in a bilateral manner, providing scale-encoded spatial feature fusion. The above operators are easily plugged into multiple stages of a SR network, and a recent emerging pre-training strategy is also adopted to impulse the model's performance further. Extensive experimental results have demonstrated the outstanding scale-equivariance capability offered by the proposed operators and our learning framework, with much better results than previous SOTAs at arbitrary scales for SR. Our code is available at https://github.com/neuralchen/EQSR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1103.OmniAL: A Unified CNN Framework for Unsupervised Anomaly Localization</span><br>
                <span class="as">Zhao, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_OmniAL_A_Unified_CNN_Framework_for_Unsupervised_Anomaly_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3924-3933.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行无监督异常定位和检测，特别是在工业制造过程中由于缺乏异常样本的情况下。<br>
                    动机：现有的无监督工业异常检测方法通过为许多不同类别训练单独的模型来实现高性能，但这种方法的模型存储和训练时间成本高，且一模型-N-类别的设置会导致现有方法的性能下降。<br>
                    方法：本文提出了一种名为OmniAL的统一CNN框架进行无监督异常定位，通过改进异常合成、重建和定位来解决这个问题。该方法使用提出的面板引导的合成异常数据训练模型，而不是直接使用正常数据，以防止模型学习到相同的重建。同时，通过使用提出的Dilated Channel and Spatial Attention (DCSA)块增加多类分布的异常重建误差。为了更好地定位异常区域，它在重建和定位子网络之间使用了提出的DiffNeck来探索多级差异。<br>
                    效果：在15类MVTecAD和12类VisA数据集上的实验验证了OmniAL的优势，超越了统一模型的最新技术。在15类-MVTecAD/12类-VisA上，其单一统一模型实现了97.2/87.8的图像AUROC，98.3/96.6的像素AUROC和73.4/41.7的像素AP用于异常检测和定位。此外，首次尝试对无监督异常定位和检测方法在不同级别的对抗攻击下的鲁棒性进行了全面研究。实验结果表明，OmniAL具有优越的性能和良好的应用前景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised anomaly localization and detection is crucial for industrial manufacturing processes due to the lack of anomalous samples. Recent unsupervised advances on industrial anomaly detection achieve high performance by training separate models for many different categories. The model storage and training time cost of this paradigm is high. Moreover, the setting of one-model-N-classes leads to fearful degradation of existing methods. In this paper, we propose a unified CNN framework for unsupervised anomaly localization, named OmniAL. This method conquers aforementioned problems by improving anomaly synthesis, reconstruction and localization. To prevent the model learning identical reconstruction, it trains the model with proposed panel-guided synthetic anomaly data rather than directly using normal data. It increases anomaly reconstruction error for multi-class distribution by using a network that is equipped with proposed Dilated Channel and Spatial Attention (DCSA) blocks. To better localize the anomaly regions, it employs proposed DiffNeck between reconstruction and localization sub-networks to explore multi-level differences. Experiments on 15-class MVTecAD and 12-class VisA datasets verify the advantage of proposed OmniAL that surpasses the state-of-the-art of unified models. On 15-class-MVTecAD/12-class-VisA, its single unified model achieves 97.2/87.8 image-AUROC, 98.3/96.6 pixel-AUROC and 73.4/41.7 pixel-AP for anomaly detection and localization respectively. Besides that, we make the first attempt to conduct a comprehensive study on the robustness of unsupervised anomaly localization and detection methods against different level adversarial attacks. Experiential results show OmniAL has good application prospects for its superior performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1104.Recurrent Homography Estimation Using Homography-Guided Image Warping and Focus Transformer</span><br>
                <span class="as">Cao, Si-YuanandZhang, RunminandLuo, LunandYu, BeinanandSheng, ZehuaandLi, JunweiandShen, Hui-Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9833-9842.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用循环同构估计框架和焦点变压器，通过使用同构图引导的图像变形和关注机制，提高特征一致性和注意力集中性。<br>
                    动机：为了解决以往方法在处理具有挑战性的跨分辨率和跨模态数据集时准确性不足的问题，同时实现参数效率。<br>
                    方法：提出了一种名为RHWF的循环同构估计框架，该框架将同构图引导的图像变形和焦点变压器适当地吸收到循环框架中，以逐步增强特征一致性，并通过全局->非局部->局部的方式聚合内部-外部对应关系。<br>
                    效果：实验结果表明，RHWF在各种数据集上的准确性都名列前茅，包括具有挑战性的跨分辨率和跨模态数据集。与先前最先进的LocalTrans和IHN方法相比，RHWF在MSCOCO数据集上将平均角误差降低了约70%和38.1%，同时节省了86.5%和24.6%的参数成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose the Recurrent homography estimation framework using Homography-guided image Warping and Focus transformer (FocusFormer), named RHWF. Both being appropriately absorbed into the recurrent framework, the homography-guided image warping progressively enhances the feature consistency and the attention-focusing mechanism in FocusFormer aggregates the intra-inter correspondence in a global->nonlocal->local manner. Thanks to the above strategies, RHWF ranks top in accuracy on a variety of datasets, including the challenging cross-resolution and cross-modal ones. Meanwhile, benefiting from the recurrent framework, RHWF achieves parameter efficiency despite the transformer architecture. Compared to previous state-of-the-art approaches LocalTrans and IHN, RHWF reduces the mean average corner error (MACE) by about 70% and 38.1% on the MSCOCO dataset, while saving the parameter costs by 86.5% and 24.6%. Similar to the previous works, RHWF can also be arranged in 1-scale for efficiency and 2-scale for accuracy, with the 1-scale RHWF already outperforming most of the previous methods. Source code is available at https://github.com/imdumpl78/RHWF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1105.DLBD: A Self-Supervised Direct-Learned Binary Descriptor</span><br>
                <span class="as">Xiao, BinandHu, YangandLiu, BoandBi, XiuliandLi, WeishengandGao, Xinbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_DLBD_A_Self-Supervised_Direct-Learned_Binary_Descriptor_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15846-15855.png><br>
            
            <span class="tt"><span class="t0">研究问题：学习型二值描述符的二值化过程尚未得到很好的解决，因为二值化阻碍了梯度反向传播。<br>
                    动机：现有的学习型二值描述符首先学习实数值输出，然后通过其提出的二值化过程转换为二值描述符。由于它们的二值化过程不是网络的一部分，因此学习型二值描述符无法充分利用深度学习的进步。<br>
                    方法：我们提出了一种模型无关的插件二值转换层（BTL），使网络直接生成二值描述符。然后，我们提出了第一个自我监督、直接学习的二值描述符，称为DLBD。此外，我们还提出了超宽温度比例交叉熵损失来调整学习的描述符在更大范围内的分布。<br>
                    效果：实验表明，我们提出的BTL可以替代以前的二值化过程。我们提出的DLBD在不同的任务上优于最先进的技术，如图像检索和分类。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For learning-based binary descriptors, the binarization process has not been well addressed. The reason is that the binarization blocks gradient back-propagation. Existing learning-based binary descriptors learn real-valued output, and then it is converted to binary descriptors by their proposed binarization processes. Since their binarization processes are not a component of the network, the learning-based binary descriptor cannot fully utilize the advances of deep learning. To solve this issue, we propose a model-agnostic plugin binary transformation layer (BTL), making the network directly generate binary descriptors. Then, we present the first self-supervised, direct-learned binary descriptor, dubbed DLBD. Furthermore, we propose ultra-wide temperature-scaled cross-entropy loss to adjust the distribution of learned descriptors in a larger range. Experiments demonstrate that the proposed BTL can substitute the previous binarization process. Our proposed DLBD outperforms SOTA on different tasks such as image retrieval and classification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1106.AutoFocusFormer: Image Segmentation off the Grid</span><br>
                <span class="as">Ziwen, ChenandPatnaik, KaushikandZhai, ShuangfeiandWan, AlvinandRen, ZhileandSchwing, AlexanderG.andColburn, AlexandFuxin, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ziwen_AutoFocusFormer_Image_Segmentation_off_the_Grid_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18227-18236.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善卷积神经网络在处理高不平衡内容密度的实际世界图像时，对小物体信息丢失的问题。<br>
                    动机：现有的连续网格下采样策略在处理图像时，会忽视小物体的信息，导致分割等任务效果下降。<br>
                    方法：提出一种自适应下采样的局部注意力变压器图像识别模型AutoFocusFormer (AFF)，通过学习保留最重要的像素来保留小物体信息。<br>
                    效果：实验证明，AutoFocusFormer (AFF)在类似规模的基线模型上有了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real world images often have highly imbalanced content density. Some areas are very uniform, e.g., large patches of blue sky, while other areas are scattered with many small objects. Yet, the commonly used successive grid downsampling strategy in convolutional deep networks treats all areas equally. Hence, small objects are represented in very few spatial locations, leading to worse results in tasks such as segmentation. Intuitively, retaining more pixels representing small objects during downsampling helps to preserve important information. To achieve this, we propose AutoFocusFormer (AFF), a local-attention transformer image recognition backbone, which performs adaptive downsampling by learning to retain the most important pixels for the task. Since adaptive downsampling generates a set of pixels irregularly distributed on the image plane, we abandon the classic grid structure. Instead, we develop a novel point-based local attention block, facilitated by a balanced clustering module and a learnable neighborhood merging module, which yields representations for our point-based versions of state-of-the-art segmentation heads. Experiments show that our AutoFocusFormer (AFF) improves significantly over baseline models of similar sizes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1107.CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion</span><br>
                <span class="as">Zhao, ZixiangandBai, HaowenandZhang, JiangsheandZhang, YulunandXu, ShuangandLin, ZudiandTimofte, RaduandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5906-5916.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态图像融合旨在生成保持不同模态优点（如功能亮点和详细纹理）的融合图像。<br>
                    动机：为了解决跨模态特征建模和分解理想模态特定和模态共享特征的挑战，我们提出了一种新的相关性驱动的特征分解融合网络。<br>
                    方法：首先，CDDFuse使用Restormer块提取跨模态浅层特征。然后，引入一个具有Lite Transformer（LT）块的双重分支Transformer-CNN特征提取器，利用长范围注意力处理低频全局特征，以及关注提取高频局部信息的Invertible Neural Networks（INN）块。进一步提出一种相关性驱动的损失函数，使低频特征基于嵌入信息相关，而高频特征不相关。最后，LT基全局融合和INN基局部融合层输出融合图像。<br>
                    效果：大量实验证明，我们的CDDFuse在多种融合任务中取得了良好的效果，包括红外可见光图像融合和医学图像融合。我们还表明，CDDFuse可以在统一的基准测试中提高下游红外可见光语义分割和对象检测的性能。代码可在https://github.com/Zhaozixiang1228/MMIF-CDDFuse获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-modality (MM) image fusion aims to render fused images that maintain the merits of different modalities, e.g., functional highlight and detailed textures. To tackle the challenge in modeling cross-modality features and decomposing desirable modality-specific and modality-shared features, we propose a novel Correlation-Driven feature Decomposition Fusion (CDDFuse) network. Firstly, CDDFuse uses Restormer blocks to extract cross-modality shallow features. We then introduce a dual-branch Transformer-CNN feature extractor with Lite Transformer (LT) blocks leveraging long-range attention to handle low-frequency global features and Invertible Neural Networks (INN) blocks focusing on extracting high-frequency local information. A correlation-driven loss is further proposed to make the low-frequency features correlated while the high-frequency features uncorrelated based on the embedded information. Then, the LT-based global fusion and INN-based local fusion layers output the fused image. Extensive experiments demonstrate that our CDDFuse achieves promising results in multiple fusion tasks, including infrared-visible image fusion and medical image fusion. We also show that CDDFuse can boost the performance in downstream infrared-visible semantic segmentation and object detection in a unified benchmark. The code is available at https://github.com/Zhaozixiang1228/MMIF-CDDFuse.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1108.HGNet: Learning Hierarchical Geometry From Points, Edges, and Surfaces</span><br>
                <span class="as">Yao, TingandLi, YehaoandPan, YingweiandMei, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_HGNet_Learning_Hierarchical_Geometry_From_Points_Edges_and_Surfaces_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21846-21855.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将无结构的点集解析为局部几何结构，以理解和表示点云。<br>
                    动机：为了深入分析点云，需要设计一种能够从点、边、面（三角形）到超表面（相邻的表面）的层次几何模型的深度架构。<br>
                    方法：本文提出了一种新的分层几何网络（HGNet），该网络以自顶向下的方式整合了从超表面、表面、边到点的层次几何结构，用于学习点云表示。具体来说，首先在每两个相邻的点之间构建边缘。然后通过边缘到点的聚合来学习点级表示，即将所有连接的边缘聚合到锚点上。接下来，由于每两个相邻的边缘组成一个面，因此通过在所有邻居表面上进行表面到边的聚合，得到每个锚边缘的边缘级表示。此外，通过将所有超表面转换为锚表面，实现表面级表示，即超表面到表面的聚合。最后，设计了一个Transformer结构，将所有点级、边级和表面级特征统一为整体的点云表示。<br>
                    效果：在四个点云分析数据集上的大量实验表明，HGNet在3D对象分类和部分/语义分割任务上具有优越性。更值得注意的是，HGNet在ScanObjectNN上的总体准确率达到了89.2%，比PointNeXt-S提高了1.5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Parsing an unstructured point set into constituent local geometry structures (e.g., edges or surfaces) would be helpful for understanding and representing point clouds. This motivates us to design a deep architecture to model the hierarchical geometry from points, edges, surfaces (triangles), to super-surfaces (adjacent surfaces) for the thorough analysis of point clouds. In this paper, we present a novel Hierarchical Geometry Network (HGNet) that integrates such hierarchical geometry structures from super-surfaces, surfaces, edges, to points in a top-down manner for learning point cloud representations. Technically, we first construct the edges between every two neighbor points. A point-level representation is learnt with edge-to-point aggregation, i.e., aggregating all connected edges into the anchor point. Next, as every two neighbor edges compose a surface, we obtain the edge-level representation of each anchor edge via surface-to-edge aggregation over all neighbor surfaces. Furthermore, the surface-level representation is achieved through super-surface-to-surface aggregation by transforming all super-surfaces into the anchor surface. A Transformer structure is finally devised to unify all the point-level, edge-level, and surface-level features into the holistic point cloud representations. Extensive experiments on four point cloud analysis datasets demonstrate the superiority of HGNet for 3D object classification and part/semantic segmentation tasks. More remarkably, HGNet achieves the overall accuracy of 89.2% on ScanObjectNN, improving PointNeXt-S by 1.5%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1109.PointVector: A Vector Representation in Point Cloud Analysis</span><br>
                <span class="as">Deng, XinandZhang, WenYuandDing, QingandZhang, XinMing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_PointVector_A_Vector_Representation_in_Point_Cloud_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9455-9465.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地提取局部特征，提高点云分析的性能。<br>
                    动机：尽管点云分析中的基于点的方法和诸如PointNeXt的简洁多层感知器结构已经显示出与卷积和变压器结构的竞争力，但标准多层感知器在提取局部特征方面的能力有限。<br>
                    方法：我们提出了一种矢量导向的点集抽象，通过更高维度的向量聚合相邻的特征。为了便于网络优化，我们构建了一个基于3D向量旋转的独立角度的标量到向量的转换。最后，我们开发了一种遵循PointNeXt结构的PointVector模型。<br>
                    效果：我们的实验结果表明，PointVector在S3DIS Area 5上实现了72.3% mIOU的最佳性能，在S3DIS（6折交叉验证）上实现了78.4% mIOU的最佳性能，而其模型参数仅为PointNeXt的58%。我们希望我们的研究有助于探索简洁有效的特征表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In point cloud analysis, point-based methods have rapidly developed in recent years. These methods have recently focused on concise MLP structures, such as PointNeXt, which have demonstrated competitiveness with Convolutional and Transformer structures. However, standard MLPs are limited in their ability to extract local features effectively. To address this limitation, we propose a Vector-oriented Point Set Abstraction that can aggregate neighboring features through higher-dimensional vectors. To facilitate network optimization, we construct a transformation from scalar to vector using independent angles based on 3D vector rotations. Finally, we develop a PointVector model that follows the structure of PointNeXt. Our experimental results demonstrate that PointVector achieves state-of-the-art performance 72.3% mIOU on the S3DIS Area 5 and 78.4% mIOU on the S3DIS (6-fold cross-validation) with only 58% model parameters of PointNeXt. We hope our work will help the exploration of concise and effective feature representations. The code will be released soon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1110.BASiS: Batch Aligned Spectral Embedding Space</span><br>
                <span class="as">Streicher, OrandCohen, IdoandGilboa, Guy</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Streicher_BASiS_Batch_Aligned_Spectral_Embedding_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10396-10405.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计具有谱图特性的深度网络构建模块。<br>
                    动机：谱图理论提供了强大的算法，可以用于设计最优的图结构或获取数据的正交低维嵌入。<br>
                    方法：提出一种直接学习图谱特征空间的方法，并设计了一种稳定的对齐机制来处理批次和图度量的变化。<br>
                    效果：实验证明，该方法在NMI、ACC、Grassman距离、正交性和分类准确率方面优于现有技术，且学习过程更稳定。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph is a highly generic and diverse representation, suitable for almost any data processing problem. Spectral graph theory has been shown to provide powerful algorithms, backed by solid linear algebra theory. It thus can be extremely instrumental to design deep network building blocks with spectral graph characteristics. For instance, such a network allows the design of optimal graphs for certain tasks or obtaining a canonical orthogonal low-dimensional embedding of the data. Recent attempts to solve this problem were based on minimizing Rayleigh-quotient type losses. We propose a different approach of directly learning the graph's eigensapce. A severe problem of the direct approach, applied in batch-learning, is the inconsistent mapping of features to eigenspace coordinates in different batches. We analyze the degrees of freedom of learning this task using batches and propose a stable alignment mechanism that can work both with batch changes and with graph-metric changes. We show that our learnt spectral embedding is better in terms of NMI, ACC, Grassman distnace, orthogonality and classification accuracy, compared to SOTA. In addition, the learning is more stable.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1111.Recognizing Rigid Patterns of Unlabeled Point Clouds by Complete and Continuous Isometry Invariants With No False Negatives and No False Positives</span><br>
                <span class="as">Widdowson, DanielandKurlin, Vitaliy</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Widdowson_Recognizing_Rigid_Patterns_of_Unlabeled_Point_Clouds_by_Complete_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1275-1284.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地表示和比较刚性结构如汽车或其他固体对象的点云数据。<br>
                    动机：由于噪声和运动的影响，现有的刚性模式比较方法存在误报和漏报的问题，需要寻找一种在数据扰动下连续的不变性度量。<br>
                    方法：提出了一种新的基于欧几里得空间的无标签点云的连续和完整的不变性度量方法。<br>
                    效果：该方法可以在固定维度下以多项式时间计算新指标，有效解决了现有方法的问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rigid structures such as cars or any other solid objects are often represented by finite clouds of unlabeled points. The most natural equivalence on these point clouds is rigid motion or isometry maintaining all inter-point distances. Rigid patterns of point clouds can be reliably compared only by complete isometry invariants that can also be called equivariant descriptors without false negatives (isometric clouds having different descriptions) and without false positives (non-isometric clouds with the same description). Noise and motion in data motivate a search for invariants that are continuous under perturbations of points in a suitable metric. We propose the first continuous and complete invariant of unlabeled clouds in any Euclidean space. For a fixed dimension, the new metric for this invariant is computable in a polynomial time in the number of points.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1112.N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution</span><br>
                <span class="as">Choi, HaramandLee, JeongminandYang, Jihoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_N-Gram_in_Swin_Transformers_for_Efficient_Lightweight_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2071-2081.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的预训练语言模型缺乏对丰富的结构化知识的利用，以及S研究问题：现有的预训练语言模型缺乏对丰富的结构化知识的利用，以及Swin Transformer在重建高分辨率图像时由于受限的感知场而忽略大范围区域的问题。<br>
                    动机：为了解决这些问题，本文提出了将N-Gram上下文引入到低层视觉的Transformer中，并使用滑动窗口自注意力来扩大可见区域以恢复退化的像素。<br>
                    方法：通过定义N-Gram为Swin中的相邻局部窗口，并将其与滑动窗口自注意力相结合，扩展了可见区域以恢复退化的像素。同时，我们还提出了一种高效的SR网络NGswin，该网络具有SCDP瓶颈，可以处理分层编码器的多尺度输出。<br>
                    效果：实验结果表明，NGswin在保持高效结构的同时，与先前的方法相比具有竞争力的性能。此外，我们还改进了其他基于Swin的SR方法，构建了一个增强的模型SwinIR-NG，该模型优于当前最佳的轻量级SR方法，并取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While some studies have proven that Swin Transformer (Swin) with window self-attention (WSA) is suitable for single image super-resolution (SR), the plain WSA ignores the broad regions when reconstructing high-resolution images due to a limited receptive field. In addition, many deep learning SR methods suffer from intensive computations. To address these problems, we introduce the N-Gram context to the low-level vision with Transformers for the first time. We define N-Gram as neighboring local windows in Swin, which differs from text analysis that views N-Gram as consecutive characters or words. N-Grams interact with each other by sliding-WSA, expanding the regions seen to restore degraded pixels. Using the N-Gram context, we propose NGswin, an efficient SR network with SCDP bottleneck taking multi-scale outputs of the hierarchical encoder. Experimental results show that NGswin achieves competitive performance while maintaining an efficient structure when compared with previous leading methods. Moreover, we also improve other Swin-based SR methods with the N-Gram context, thereby building an enhanced model: SwinIR-NG. Our improved SwinIR-NG outperforms the current best lightweight SR approaches and establishes state-of-the-art results. Codes are available at https://github.com/rami0205/NGramSwin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1113.Virtual Sparse Convolution for Multimodal 3D Object Detection</span><br>
                <span class="as">Wu, HaiandWen, ChengluandShi, ShaoshuaiandLi, XinandWang, Cheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Virtual_Sparse_Convolution_for_Multimodal_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21653-21662.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地融合RGB图像和LiDAR数据进行3D物体检测。<br>
                    动机：目前的虚拟/伪点基3D物体检测方法在生成的虚拟点非常密集，导致检测过程中冗余计算量大，且由不准确的深度补全引入的噪声会显著降低检测精度。<br>
                    方法：提出了一种名为VirConvNet的新骨干网络，该网络基于新的运算符VirConv（虚拟稀疏卷积）。VirConv包含两个关键设计：(1) StVD（随机体素丢弃）和 (2) NRConv（抗噪子流形卷积）。StVD通过丢弃大量附近的冗余体素来缓解计算问题。NRConv通过在2D图像和3D LiDAR空间中编码体素特征来解决噪声问题。<br>
                    效果：在KITTI汽车3D检测测试排行榜上，我们的VirConv-L实现了85%的AP，运行速度快达56ms。我们的VirConv-T和VirConv-S分别达到了86.3%和87.2%的高精确度，目前分别排名第二和第一。代码可在https://github.com/hailanyi/VirConv获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, virtual/pseudo-point-based 3D object detection that seamlessly fuses RGB images and LiDAR data by depth completion has gained great attention. However, virtual points generated from an image are very dense, introducing a huge amount of redundant computation during detection. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection precision. This paper proposes a fast yet effective backbone, termed VirConvNet, based on a new operator VirConv (Virtual Sparse Convolution), for virtual-point-based 3D object detection. The VirConv consists of two key designs: (1) StVD (Stochastic Voxel Discard) and (2) NRConv (Noise-Resistant Submanifold Convolution). The StVD alleviates the computation problem by discarding large amounts of nearby redundant voxels. The NRConv tackles the noise problem by encoding voxel features in both 2D image and 3D LiDAR space. By integrating our VirConv, we first develop an efficient pipeline VirConv-L based on an early fusion design. Then, we build a high-precision pipeline VirConv-T based on a transformed refinement scheme. Finally, we develop a semi-supervised pipeline VirConv-S based on a pseudo-label framework. On the KITTI car 3D detection test leaderboard, our VirConv-L achieves 85% AP with a fast running speed of 56ms. Our VirConv-T and VirConv-S attains a high-precision of 86.3% and 87.2% AP, and currently rank 2nd and 1st, respectively. The code is available at https://github.com/hailanyi/VirConv.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1114.ALTO: Alternating Latent Topologies for Implicit 3D Reconstruction</span><br>
                <span class="as">Wang, ZhenandZhou, ShijieandPark, JeongJoonandPaschalidou, DespoinaandYou, SuyaandWetzstein, GordonandGuibas, LeonidasandKadambi, Achuta</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ALTO_Alternating_Latent_Topologies_for_Implicit_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/259-270.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从有噪声的点云中高保真地重建隐含的3D表面。<br>
                    动机：现有的方法在恢复细节方面存在困难，点状潜在向量和网格潜在向量都有各自的优缺点。<br>
                    方法：提出交替潜在拓扑（ALTO）的方法，通过在几何表示之间进行交替，最终得到易于解码的潜在向量。<br>
                    效果：实验证明，ALTO不仅在性能上超过了最先进的方法，而且在运行时间上提高了3-10倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work introduces alternating latent topologies (ALTO) for high-fidelity reconstruction of implicit 3D surfaces from noisy point clouds. Previous work identifies that the spatial arrangement of latent encodings is important to recover detail. One school of thought is to encode a latent vector for each point (point latents). Another school of thought is to project point latents into a grid (grid latents) which could be a voxel grid or triplane grid. Each school of thought has tradeoffs. Grid latents are coarse and lose high-frequency detail. In contrast, point latents preserve detail. However, point latents are more difficult to decode into a surface, and quality and runtime suffer. In this paper, we propose ALTO to sequentially alternate between geometric representations, before converging to an easy-to-decode latent. We find that this preserves spatial expressiveness and makes decoding lightweight. We validate ALTO on implicit 3D recovery and observe not only a performance improvement over the state-of-the-art, but a runtime improvement of 3-10x. Anonymized source code at https://visual.ee.ucla.edu/alto.htm/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1115.MSMDFusion: Fusing LiDAR and Camera at Multiple Scales With Multi-Depth Seeds for 3D Object Detection</span><br>
                <span class="as">Jiao, YangandJie, ZequnandChen, ShaoxiangandChen, JingjingandMa, LinandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21643-21652.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地融合激光雷达和相机信息，以实现自动驾驶系统中的精确可靠的3D物体检测。<br>
                    动机：由于两种截然不同的模态（即激光雷达和相机）的多粒度几何和语义特征难以结合，因此将它们融合起来进行准确的3D物体检测在自动驾驶系统中至关重要。<br>
                    方法：提出了一种新的框架，该框架更好地利用了深度信息，并在体素空间中实现了激光雷达和相机之间的细粒度跨模态交互。这个框架包括两个重要的组件：一是使用多深度未投影（MDU）方法提高每个交互级别的提升点的深度质量；二是应用门控模态感知卷积（GMA-Conv）块，以细粒度的方式调整涉及相机模态的体素，然后将多模态特征聚合到一个统一的空间中。<br>
                    效果：在nuScenes测试基准上，该方法（简称为MSMDFusion）无需使用测试时增强和集成技术，就在3D物体检测和跟踪任务上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fusing LiDAR and camera information is essential for accurate and reliable 3D object detection in autonomous driving systems. This is challenging due to the difficulty of combining multi-granularity geometric and semantic features from two drastically different modalities. Recent approaches aim at exploring the semantic densities of camera features through lifting points in 2D camera images (referred to as "seeds") into 3D space, and then incorporate 2D semantics via cross-modal interaction or fusion techniques. However, depth information is under-investigated in these approaches when lifting points into 3D space, thus 2D semantics can not be reliably fused with 3D points. Moreover, their multi-modal fusion strategy, which is implemented as concatenation or attention, either can not effectively fuse 2D and 3D information or is unable to perform fine-grained interactions in the voxel space. To this end, we propose a novel framework with better utilization of the depth information and fine-grained cross-modal interaction between LiDAR and camera, which consists of two important components. First, a Multi-Depth Unprojection (MDU) method is used to enhance the depth quality of the lifted points at each interaction level. Second, a Gated Modality-Aware Convolution (GMA-Conv) block is applied to modulate voxels involved with the camera modality in a fine-grained manner and then aggregate multi-modal features into a unified space. Together they provide the detection head with more comprehensive features from LiDAR and camera. On the nuScenes test benchmark, our proposed method, abbreviated as MSMDFusion, achieves state-of-the-art results on both 3D object detection and tracking tasks without using test-time-augmentation and ensemble techniques. The code is available at https://github.com/SxJyJay/MSMDFusion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1116.Toward Stable, Interpretable, and Lightweight Hyperspectral Super-Resolution</span><br>
                <span class="as">Guo, Wen-jinandXie, WeiyingandJiang, KaiandLi, YunsongandLei, JieandFang, Leyuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Toward_Stable_Interpretable_and_Lightweight_Hyperspectral_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22272-22281.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的高光谱图像超分辨率（HSI-SR）方法在未知场景下性能不稳定，且计算消耗大。<br>
                    动机：开发一种稳定、可解释且轻量级的HSI-SR新协调优化框架。<br>
                    方法：创建了一种新的概率框架下的融合和退化估计之间的正循环。利用估计的退化作为指导进行退化感知的HSI-SR。<br>
                    效果：实验证明该方法优于现有技术，例如在CAVE数据集上实现了2.3 dB的PSNR提升，模型大小减少了120倍，计算量减少了4300倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For real applications, existing HSI-SR methods are mostly not only limited to unstable performance under unknown scenarios but also suffer from high computation consumption. In this paper, we develop a new coordination optimization framework for stable, interpretable, and lightweight HSI-SR. Specifically, we create a positive cycle between fusion and degradation estimation under a new probabilistic framework. The estimated degradation is applied to fusion as guidance for a degradation-aware HSI-SR. Under the framework, we establish an explicit degradation estimation method to tackle the indeterminacy and unstable performance driven by black-box simulation in previous methods. Considering the interpretability in fusion, we integrate spectral mixing prior to the fusion process, which can be easily realized by a tiny autoencoder, leading to a dramatic release of the computation burden. We then develop a partial fine-tune strategy in inference to reduce the computation cost further. Comprehensive experiments demonstrate the superiority of our method against state-of-the-art under synthetic and real datasets. For instance, we achieve a 2.3 dB promotion on PSNR with 120x model size reduction and 4300x FLOPs reduction under the CAVE dataset. Code is available in https://github.com/WenjinGuo/DAEM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1117.R2Former: Unified Retrieval and Reranking Transformer for Place Recognition</span><br>
                <span class="as">Zhu, SijieandYang, LinjieandChen, ChenandShah, MubarakandShen, XiaohuiandWang, Heng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_R2Former_Unified_Retrieval_and_Reranking_Transformer_for_Place_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19370-19380.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉地点识别（VPR）中的问题，即如何通过匹配查询图像与参考数据库中的图像来估计查询图像的位置。<br>
                    动机：传统的VPR方法通常采用聚合的CNN特征进行全局检索，并使用基于RANSAC的几何验证进行重排。然而，RANSAC只使用几何信息，忽略了其他可能对重排有用的信息，如局部特征相关性和注意力值。<br>
                    方法：本文提出了一个统一的地点识别框架，该框架使用一种新的变压器模型R2Former处理检索和重排。提出的重排模块考虑了特征相关性、注意力值和xy坐标，并学习确定图像对是否来自同一位置。整个流程是端到端可训练的，重排模块也可以单独应用于其他CNN或变压器主干作为通用组件。<br>
                    效果：实验结果表明，R2Former在主要VPR数据集上显著优于最先进的方法，同时具有更小的推理时间和内存消耗。它在未参与的MSLS挑战集上也取得了最先进的成果，可以作为现实世界大规模应用的简单而强大的解决方案。实验还表明，视觉变压器令牌在某些情况下比CNN局部特征更好。代码已在https://github.com/Jeff-Zilence/R2Former上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual Place Recognition (VPR) estimates the location of query images by matching them with images in a reference database. Conventional methods generally adopt aggregated CNN features for global retrieval and RANSAC-based geometric verification for reranking. However, RANSAC only employs geometric information but ignores other possible information that could be useful for reranking, e.g. local feature correlations, and attention values. In this paper, we propose a unified place recognition framework that handles both retrieval and reranking with a novel transformer model, named R2Former. The proposed reranking module takes feature correlation, attention value, and xy coordinates into account, and learns to determine whether the image pair is from the same location. The whole pipeline is end-to-end trainable and the reranking module alone can also be adopted on other CNN or transformer backbones as a generic component. Remarkably, R2Former significantly outperforms state-of-the-art methods on major VPR datasets with much less inference time and memory consumption. It also achieves the state-of-the-art on the hold-out MSLS challenge set and could serve as a simple yet strong solution for real-world large-scale applications. Experiments also show vision transformer tokens are comparable and sometimes better than CNN local features on local matching. The code is released at https://github.com/Jeff-Zilence/R2Former.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1118.CompletionFormer: Depth Completion With Convolutions and Vision Transformers</span><br>
                <span class="as">Zhang, YouminandGuo, XiandaandPoggi, MatteoandZhu, ZhengandHuang, GuanandMattoccia, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_CompletionFormer_Depth_Completion_With_Convolutions_and_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18527-18536.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练大规模文本语料库和知识图谱，利用外部知识增强语言表示模型的性能。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，本文提出利用知识图谱中的有信息量的实体来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出一种能够同时充分利用词汇、句法和知识信息的增强的语言表示模型ERNIE。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given sparse depths and the corresponding RGB images, depth completion aims at spatially propagating the sparse measurements throughout the whole image to get a dense depth prediction. Despite the tremendous progress of deep-learning-based depth completion methods, the locality of the convolutional layer or graph model makes it hard for the network to model the long-range relationship between pixels. While recent fully Transformer-based architecture has reported encouraging results with the global receptive field, the performance and efficiency gaps to the well-developed CNN models still exist because of its deteriorative local feature details. This paper proposes a joint convolutional attention and Transformer block (JCAT), which deeply couples the convolutional attention layer and Vision Transformer into one block, as the basic unit to construct our depth completion model in a pyramidal structure. This hybrid architecture naturally benefits both the local connectivity of convolutions and the global context of the Transformer in one single model. As a result, our CompletionFormer outperforms state-of-the-art CNNs-based methods on the outdoor KITTI Depth Completion benchmark and indoor NYUv2 dataset, achieving significantly higher efficiency (nearly 1/3 FLOPs) compared to pure Transformer-based methods. Especially when the captured depth is highly sparse, the performance gap with other methods gets much larger.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1119.Comprehensive and Delicate: An Efficient Transformer for Image Restoration</span><br>
                <span class="as">Zhao, HaiyuandGou, YuanbiaoandLi, BoyunandPeng, DezhongandLv, JianchengandPeng, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Comprehensive_and_Delicate_An_Efficient_Transformer_for_Image_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14122-14132.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的高效图像恢复Transformer，以解决现有方法在捕获像素间全局依赖性方面的局限性。<br>
                    动机：现有的图像恢复Transformer虽然取得了一定的成功，但由于其局部注意力机制，无法充分捕捉像素间的全局依赖关系。<br>
                    方法：本文提出了一种先捕获超像素级全局依赖性，再将其转移到每个像素的粗到细的框架。具体来说，通过两个神经模块——压缩注意力神经模块（CA）和双自适应神经模块（DA）实现。CA采用特征聚合、注意力计算和特征恢复来有效捕获超像素级的全局依赖性；DA则采用新颖的双向结构，将超像素级的全局性自适应地封装到像素中。<br>
                    效果：由于采用了这两个神经模块，本文的方法在性能上与SwinIR相当，但计算量仅为其6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers have shown promising performance in image restoration, which usually conduct window- or channel-based attention to avoid intensive computations. Although the promising performance has been achieved, they go against the biggest success factor of Transformers to a certain extent by capturing the local instead of global dependency among pixels. In this paper, we propose a novel efficient image restoration Transformer that first captures the superpixel-wise global dependency, and then transfers it into each pixel. Such a coarse-to-fine paradigm is implemented through two neural blocks, i.e., condensed attention neural block (CA) and dual adaptive neural block (DA). In brief, CA employs feature aggregation, attention computation, and feature recovery to efficiently capture the global dependency at the superpixel level. To embrace the pixel-wise global dependency, DA takes a novel dual-way structure to adaptively encapsulate the globality from superpixels into pixels. Thanks to the two neural blocks, our method achieves comparable performance while taking only  6% FLOPs compared with SwinIR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1120.Camouflaged Object Detection With Feature Decomposition and Edge Reconstruction</span><br>
                <span class="as">He, ChunmingandLi, KaiandZhang, YachaoandTang, LongxiangandZhang, YulunandGuo, ZhenhuaandLi, Xiu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22046-22055.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在复杂背景中识别伪装物体的问题。<br>
                    动机：由于伪装物体与背景的相似性以及模糊的边界，使得伪装物体检测成为一项挑战。<br>
                    方法：提出了一种名为FEDER的模型，该模型通过使用可学习的小波将特征分解成不同的频带，然后专注于最有价值的频带来挖掘区分前景和背景的微妙线索。同时，设计了一个受微分方程启发的边缘重建模块来生成精确的边缘。<br>
                    效果：实验表明，FEDER模型在性能上显著优于现有方法，且计算和存储成本更低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Camouflaged object detection (COD) aims to address the tough issue of identifying camouflaged objects visually blended into the surrounding backgrounds. COD is a challenging task due to the intrinsic similarity of camouflaged objects with the background, as well as their ambiguous boundaries. Existing approaches to this problem have developed various techniques to mimic the human visual system. Albeit effective in many cases, these methods still struggle when camouflaged objects are so deceptive to the vision system. In this paper, we propose the FEature Decomposition and Edge Reconstruction (FEDER) model for COD. The FEDER model addresses the intrinsic similarity of foreground and background by decomposing the features into different frequency bands using learnable wavelets. It then focuses on the most informative bands to mine subtle cues that differentiate foreground and background. To achieve this, a frequency attention module and a guidance-based feature aggregation module are developed. To combat the ambiguous boundary problem, we propose to learn an auxiliary edge reconstruction task alongside the COD task. We design an ordinary differential equation-inspired edge reconstruction module that generates exact edges. By learning the auxiliary task in conjunction with the COD task, the FEDER model can generate precise prediction maps with accurate object boundaries. Experiments show that our FEDER model significantly outperforms state-of-the-art methods with cheaper computational and memory costs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1121.ALOFT: A Lightweight MLP-Like Architecture With Dynamic Low-Frequency Transform for Domain Generalization</span><br>
                <span class="as">Guo, JintaoandWang, NaandQi, LeiandShi, Yinghuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_ALOFT_A_Lightweight_MLP-Like_Architecture_With_Dynamic_Low-Frequency_Transform_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24132-24141.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个模型，使其能利用多个源领域数据对未见的目标领域进行泛化。<br>
                    动机：现有的大部分泛化方法基于卷积神经网络（CNN），但卷积核的局部运算使模型过于关注局部表示（如纹理），这导致模型更容易过拟合源领域，影响其泛化能力。<br>
                    方法：受最近轻量级多层感知机（MLP）方法的启发，我们首先分析了CNN和MLP在领域泛化中的区别，发现MLP方法具有更好的泛化能力，因为它们可以更好地捕捉全局表示（如结构）。然后，基于最近的轻量级MLP方法，我们得到了一个强大的基线，它优于大多数最先进的CNN方法。该基线可以使用滤波器来抑制频率空间中的结构无关信息。此外，我们还提出了动态低频谱变换（ALOFT），可以在保留全局结构特征的同时扰动局部纹理特征，从而使滤波器能够充分去除结构无关信息。<br>
                    效果：在四个基准测试上的大量实验表明，与最先进的CNN-based DG方法相比，我们的方法可以在少量参数的情况下实现显著的性能提升。我们的代码可在https://github.com/lingeringlight/ALOFT/获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most start-of-the-art CNN-based methods. The baseline can learn global structure representations with a filter to suppress structure-irrelevant information in the frequency space. Moreover, we propose a dynAmic LOw-Frequency spectrum Transform (ALOFT) that can perturb local texture features while preserving global structure features, thus enabling the filter to remove structure-irrelevant information sufficiently. Extensive experiments on four benchmarks have demonstrated that our method can achieve great performance improvement with a small number of parameters compared to SOTA CNN-based DG methods. Our code is available at https://github.com/lingeringlight/ALOFT/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1122.NLOST: Non-Line-of-Sight Imaging With Transformer</span><br>
                <span class="as">Li, YueandPeng, JiayongandYe, JuntianandZhang, YueyiandXu, FeihuandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_NLOST_Non-Line-of-Sight_Imaging_With_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13313-13322.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从非视距（NLOS）测量中重建复杂的3D场景。<br>
                    动机：现有的方法在处理复杂场景的非视距成像重建上存在挑战，需要提高性能。<br>
                    方法：提出一种基于变压器的神经网络NLOST，通过物理先验辅助提取浅层特征，设计两种空间-时间自注意力编码器和空间-时间跨注意力解码器来探索局部和全局相关性，最后融合深层和浅层特征重建隐藏场景的3D体积。<br>
                    效果：实验结果表明，该方法在合成数据和不同非视距成像系统捕获的真实世界数据上都优于现有解决方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Time-resolved non-line-of-sight (NLOS) imaging is based on the multi-bounce indirect reflections from the hidden objects for 3D sensing. Reconstruction from NLOS measurements remains challenging especially for complicated scenes. To boost the performance, we present NLOST, the first transformer-based neural network for NLOS reconstruction. Specifically, after extracting the shallow features with the assistance of physics-based priors, we design two spatial-temporal self attention encoders to explore both local and global correlations within 3D NLOS data by splitting or downsampling the features into different scales, respectively. Then, we design a spatial-temporal cross attention decoder to integrate local and global features in the token space of transformer, resulting in deep features with high representation capabilities. Finally, deep and shallow features are fused to reconstruct the 3D volume of hidden scenes. Extensive experimental results demonstrate the superior performance of the proposed method over existing solutions on both synthetic data and real-world data captured by different NLOS imaging systems.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1123.MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling With Informative-Preserved Reconstruction and Self-Distilled Consistency</span><br>
                <span class="as">Xu, MingyeandXu, MutianandHe, TongandOuyang, WanliandWang, YaliandHan, XiaoguangandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MM-3DScene_3D_Scene_Understanding_by_Customizing_Masked_Modeling_With_Informative-Preserved_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4380-4390.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将MM应用于大规模的3D场景，解决数据稀疏和场景复杂性的问题。<br>
                    动机：传统的随机遮蔽方式在恢复3D场景的遮蔽区域时存在很大的模糊性，因此需要探索新的策略。<br>
                    方法：提出一种新颖的信息保留重建方法，通过局部统计数据发现并保留具有代表性的结构化点，以增强预文本遮蔽任务对3D场景理解的效果。<br>
                    效果：通过结合信息保留重建和一致性自我蒸馏的方法，实验结果在一系列下游任务中得到了一致的改进，证明了该方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Modeling (MM) has demonstrated widespread success in various vision challenges, by reconstructing masked visual patches. Yet, applying MM for large-scale 3D scenes remains an open problem due to the data sparsity and scene complexity. The conventional random masking paradigm used in 2D images often causes a high risk of ambiguity when recovering the masked region of 3D scenes. To this end, we propose a novel informative-preserved reconstruction, which explores local statistics to discover and preserve the representative structured points, effectively enhancing the pretext masking task for 3D scene understanding. Integrated with a progressive reconstruction manner, our method can concentrate on modeling regional geometry and enjoy less ambiguity for masked reconstruction. Besides, such scenes with progressive masking ratios can also serve to self-distill their intrinsic spatial consistency, requiring to learn the consistent representations from unmasked areas. By elegantly combining informative-preserved reconstruction on masked areas and consistency self-distillation from unmasked areas, a unified framework called MM-3DScene is yielded. We conduct comprehensive experiments on a host of downstream tasks. The consistent improvement (e.g., +6.1% mAP@0.5 on object detection and +2.2% mIoU on semantic segmentation) demonstrates the superiority of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1124.PointClustering: Unsupervised Point Cloud Pre-Training Using Transformation Invariance in Clustering</span><br>
                <span class="as">Long, FuchenandYao, TingandQiu, ZhaofanandLi, LusongandMei, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_PointClustering_Unsupervised_Point_Cloud_Pre-Training_Using_Transformation_Invariance_in_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21824-21834.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用不同数据转换的不变性进行无监督表示学习。<br>
                    动机：现有的预训练模型缺乏对点云数据的充分利用，而点云数据的几何特性和语义在常见转换中不会改变。<br>
                    方法：提出一种新的无监督表示学习方法PointClustering，该方法通过变换不变性进行点云预训练。PointClustering将预训练任务设定为深度聚类，并将变换不变性作为归纳偏置，认为常见的点云转换不会改变其几何特性和语义。<br>
                    效果：实验证明，PointClustering在六个基准测试上表现优秀，无论是分类还是分割等下游任务。更值得注意的是，使用Transformer主干网络时，PointClustering在ModelNet40上达到了94.5%的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Feature invariance under different data transformations, i.e., transformation invariance, can be regarded as a type of self-supervision for representation learning. In this paper, we present PointClustering, a new unsupervised representation learning scheme that leverages transformation invariance for point cloud pre-training. PointClustering formulates the pretext task as deep clustering and employs transformation invariance as an inductive bias, following the philosophy that common point cloud transformation will not change the geometric properties and semantics. Technically, PointClustering iteratively optimizes the feature clusters and backbone, and delves into the transformation invariance as learning regularization from two perspectives: point level and instance level. Point-level invariance learning maintains local geometric properties through gathering point features of one instance across transformations, while instance-level invariance learning further measures clusters over the entire dataset to explore semantics of instances. Our PointClustering is architecture-agnostic and readily applicable to MLP-based, CNN-based and Transformer-based backbones. We empirically demonstrate that the models pre-learnt on the ScanNet dataset by PointClustering provide superior performances on six benchmarks, across downstream tasks of classification and segmentation. More remarkably, PointClustering achieves an accuracy of 94.5% on ModelNet40 with Transformer backbone. Source code is available at https://github.com/FuchenUSTC/PointClustering.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1125.CiaoSR: Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution</span><br>
                <span class="as">Cao, JiezhangandWang, QinandXian, YongqinandLi, YaweiandNi, BingbingandPi, ZhimingandZhang, KaiandZhang, YulunandTimofte, RaduandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_CiaoSR_Continuous_Implicit_Attention-in-Attention_Network_for_Arbitrary-Scale_Image_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1796-1807.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过学习连续的图像表示来改进图像超分辨率（SR）？<br>
                    动机：现有的方法主要依赖于局部特征的集成，忽视了视觉特征的相似性，且其感受野有限，无法集成大范围的重要信息。<br>
                    方法：提出了一种称为CiaoSR的连续隐式注意力网络，设计了一个隐式注意力网络来学习附近局部特征的集成权重，并在其中嵌入了尺度感知的注意力以利用额外的非局部信息。<br>
                    效果：在基准数据集上的大量实验表明，CiaoSR显著优于具有相同主干网络的现有单图像SR方法，并在任意尺度SR任务上实现了最先进的性能。该方法在真实世界的SR设置中也显示出有效性，更重要的是，CiaoSR可以灵活地集成到任何主干网络中以提高SR性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning continuous image representations is recently gaining popularity for image super-resolution (SR) because of its ability to reconstruct high-resolution images with arbitrary scales from low-resolution inputs. Existing methods mostly ensemble nearby features to predict the new pixel at any queried coordinate in the SR image. Such a local ensemble suffers from some limitations: i) it has no learnable parameters and it neglects the similarity of the visual features; ii) it has a limited receptive field and cannot ensemble relevant features in a large field which are important in an image. To address these issues, this paper proposes a continuous implicit attention-in-attention network, called CiaoSR. We explicitly design an implicit attention network to learn the ensemble weights for the nearby local features. Furthermore, we embed a scale-aware attention in this implicit attention network to exploit additional non-local information. Extensive experiments on benchmark datasets demonstrate CiaoSR significantly outperforms the existing single image SR methods with the same backbone. In addition, CiaoSR also achieves the state-of-the-art performance on the arbitrary-scale SR task. The effectiveness of the method is also demonstrated on the real-world SR setting. More importantly, CiaoSR can be flexibly integrated into any backbone to improve the SR performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1126.Directional Connectivity-Based Segmentation of Medical Images</span><br>
                <span class="as">Yang, ZiyunandFarsiu, Sina</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Directional_Connectivity-Based_Segmentation_of_Medical_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11525-11535.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过深度学习网络实现生物标记物分割的解剖一致性。<br>
                    动机：解剖一致性在许多医学图像分析任务中至关重要，而现有的连接性建模方法忽略了潜在空间中的丰富通道方向信息。<br>
                    方法：提出一种方向连接性建模方案，通过解耦、跟踪和利用网络中的方向信息来增强特征表示。<br>
                    效果：实验证明，该方法在各种公共医学图像分割基准上的效果优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Anatomical consistency in biomarker segmentation is crucial for many medical image analysis tasks. A promising paradigm for achieving anatomically consistent segmentation via deep networks is incorporating pixel connectivity, a basic concept in digital topology, to model inter-pixel relationships. However, previous works on connectivity modeling have ignored the rich channel-wise directional information in the latent space. In this work, we demonstrate that effective disentanglement of directional sub-space from the shared latent space can significantly enhance the feature representation in the connectivity-based network. To this end, we propose a directional connectivity modeling scheme for segmentation that decouples, tracks, and utilizes the directional information across the network. Experiments on various public medical image segmentation benchmarks show the effectiveness of our model as compared to the state-of-the-art methods. Code is available at https://github.com/Zyun-Y/DconnNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1127.Implicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization</span><br>
                <span class="as">Dong, ShichaoandWang, JinandJi, RenheandLiang, JiajunandFan, HaoqiangandGe, Zheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3994-4004.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在分析深度伪造检测任务中二元分类器的泛化能力。<br>
                    动机：发现深度伪造检测的泛化能力受到图像上意外学习到的身份表示的影响，即“隐含身份泄露”现象。<br>
                    方法：提出了一种名为ID-unaware Deepfake Detection Model的方法来减少这种现象的影响。<br>
                    效果：实验结果表明，该方法在数据集内和跨数据集评估中均优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we analyse the generalization ability of binary classifiers for the task of deepfake detection. We find that the stumbling block to their generalization is caused by the unexpected learned identity representation on images. Termed as the Implicit Identity Leakage, this phenomenon has been qualitatively and quantitatively verified among various DNNs. Furthermore, based on such understanding, we propose a simple yet effective method named the ID-unaware Deepfake Detection Model to reduce the influence of this phenomenon. Extensive experimental results demonstrate that our method outperforms the state-of-the-art in both in-dataset and cross-dataset evaluation. The code is available at https://github.com/megvii-research/CADDM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1128.DNF: Decouple and Feedback Network for Seeing in the Dark</span><br>
                <span class="as">Jin, XinandHan, Ling-HaoandLi, ZhenandGuo, Chun-LeandChai, ZhiandLi, Chongyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_DNF_Decouple_and_Feedback_Network_for_Seeing_in_the_Dark_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18135-18144.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何充分利用RAW数据的特性进行低光图像增强，并解决现有架构在单阶段和多阶段方法中的局限性。<br>
                    动机：尽管RAW数据具有巨大的潜力用于低光图像增强，但现有的架构限制了其性能。<br>
                    方法：提出了一种去耦与反馈（DNF）框架，通过将特定领域的子任务解耦，并充分利用RAW和sRGB领域的独特属性，以及通过反馈机制在各阶段之间传递特征，避免由于图像级数据流导致的信息丢失。<br>
                    效果：该方法成功地解决了RAW数据基础的低光图像增强的内在限制，并在Sony和Fuji的SID子集上实现了0.97dB和1.30dB的PSNR改进，大大超过了先前最先进的方法，且仅使用了19%的参数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The exclusive properties of RAW data have shown great potential for low-light image enhancement. Nevertheless, the performance is bottlenecked by the inherent limitations of existing architectures in both single-stage and multi-stage methods. Mixed mapping across two different domains, noise-to-clean and RAW-to-sRGB, misleads the single-stage methods due to the domain ambiguity. The multi-stage methods propagate the information merely through the resulting image of each stage, neglecting the abundant features in the lossy image-level dataflow. In this paper, we probe a generalized solution to these bottlenecks and propose a Decouple aNd Feedback framework, abbreviated as DNF. To mitigate the domain ambiguity, domainspecific subtasks are decoupled, along with fully utilizing the unique properties in RAW and sRGB domains. The feature propagation across stages with a feedback mechanism avoids the information loss caused by image-level dataflow. The two key insights of our method resolve the inherent limitations of RAW data-based low-light image enhancement satisfactorily, empowering our method to outperform the previous state-of-the-art method by a large margin with only 19% parameters, achieving 0.97dB and 1.30dB PSNR improvements on the Sony and Fuji subsets of SID.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1129.Deformable Mesh Transformer for 3D Human Mesh Recovery</span><br>
                <span class="as">Yoshiyasu, Yusuke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshiyasu_Deformable_Mesh_Transformer_for_3D_Human_Mesh_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17006-17015.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的基于顶点的单目3D人体网格恢复方法，即变形网格变换器（DeFormer）。<br>
                    动机：以前的技术在处理高分辨率图像特征图和密集网格模型时，计算成本较高。<br>
                    方法：通过在配备有高效体网格驱动注意力模块的变压器解码器内形成网格对齐反馈循环，迭代地将身体网格模型拟合到输入图像中。具体包括1）身体稀疏自注意力和2）可变形网格交叉注意力。<br>
                    效果：实验结果表明，DeFormer在Human3.6M和3DPW基准测试上取得了最先进的性能。消融研究也表明，DeFormer模型设计能有效利用多尺度特征图。代码可在https://github.com/yusukey03012/DeFormer获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Deformable mesh transFormer (DeFormer), a novel vertex-based approach to monocular 3D human mesh recovery. DeFormer iteratively fits a body mesh model to an input image via a mesh alignment feedback loop formed within a transformer decoder that is equipped with efficient body mesh driven attention modules: 1) body sparse self-attention and 2) deformable mesh cross attention. As a result, DeFormer can effectively exploit high-resolution image feature maps and a dense mesh model which were computationally expensive to deal with in previous approaches using the standard transformer attention. Experimental results show that DeFormer achieves state-of-the-art performances on the Human3.6M and 3DPW benchmarks. Ablation study is also conducted to show the effectiveness of the DeFormer model designs for leveraging multi-scale feature maps. Code is available at https://github.com/yusukey03012/DeFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1130.HS-Pose: Hybrid Scope Feature Extraction for Category-Level Object Pose Estimation</span><br>
                <span class="as">Zheng, LinfangandWang, ChenandSun, YinghanandDasgupta, EshaandChen, HuaandLeonardis, Ale\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_HS-Pose_Hybrid_Scope_Feature_Extraction_for_Category-Level_Object_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17163-17173.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文关注类别级物体姿态估计问题，由于大的内部类别形状变化，此问题具有挑战性。<br>
                    动机：3D图卷积（3D-GC）方法广泛用于提取局部几何特征，但对复杂形状的物体有限制，且对噪声敏感。此外，3D-GC的比例和平移不变性限制了对象大小和平移信息的感知。<br>
                    方法：本文提出了一种简单的网络结构——HS层，它将3D-GC扩展到点云数据中，以提取用于类别级物体姿态估计任务的混合范围潜在特征。提出的HS层：1)能够感知局部-全局几何结构和全局信息；2)对噪声具有鲁棒性；3)可以编码大小和平移信息。<br>
                    效果：实验表明，在基线方法（GPV-Pose）上简单地用提出的HS层替换3D-GC层，性能显著提高，5d2cm度量提高了14.5%，IoU75提高了10.3%。该方法在REAL275数据集上比最先进的方法高出8.3%（5d2cm），6.9%（IoU75），并且运行实时（50 FPS）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we focus on the problem of category-level object pose estimation, which is challenging due to the large intra-category shape variation. 3D graph convolution (3D-GC) based methods have been widely used to extract local geometric features, but they have limitations for complex shaped objects and are sensitive to noise. Moreover, the scale and translation invariant properties of 3D-GC restrict the perception of an object's size and translation information. In this paper, we propose a simple network structure, the HS-layer, which extends 3D-GC to extract hybrid scope latent features from point cloud data for category-level object pose estimation tasks. The proposed HS-layer: 1) is able to perceive local-global geometric structure and global information, 2) is robust to noise, and 3) can encode size and translation information. Our experiments show that the simple replacement of the 3D-GC layer with the proposed HS-layer on the baseline method (GPV-Pose) achieves a significant improvement, with the performance increased by 14.5% on 5d2cm metric and 10.3% on IoU75. Our method outperforms the state-of-the-art methods by a large margin (8.3% on 5d2cm, 6.9% on IoU75) on REAL275 dataset and runs in real-time (50 FPS).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1131.Parts2Words: Learning Joint Embedding of Point Clouds and Texts by Bidirectional Matching Between Parts and Words</span><br>
                <span class="as">Tang, ChuanandYang, XiandWu, BojianandHan, ZhizhongandChang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Parts2Words_Learning_Joint_Embedding_of_Point_Clouds_and_Texts_by_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6884-6893.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决形状-文本匹配问题，即如何更好地理解3D形状。<br>
                    动机：现有的方法主要通过将3D形状表示为多个2D视图来处理，但由于视图数量有限，自我遮挡造成的结构模糊性使得这种方法效果不佳。<br>
                    方法：本文提出直接将3D形状表示为点云，并通过在优化的特征空间中学习点云和文本的联合嵌入来实现形状和文本的双向匹配。具体来说，首先将点云分割成部分，然后利用最优传输方法在优化的特征空间中匹配部分和单词，其中每个部分由其内所有点的 features 聚合而成，每个单词则由其上下文信息抽象而来。<br>
                    效果：实验结果表明，该方法在Text2Shape数据集上的多模态检索任务上取得了显著优于现有技术的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Shape-Text matching is an important task of high-level shape understanding. Current methods mainly represent a 3D shape as multiple 2D rendered views, which obviously can not be understood well due to the structural ambiguity caused by self-occlusion in the limited number of views. To resolve this issue, we directly represent 3D shapes as point clouds, and propose to learn joint embedding of point clouds and texts by bidirectional matching between parts from shapes and words from texts. Specifically, we first segment the point clouds into parts, and then leverage optimal transport method to match parts and words in an optimized feature space, where each part is represented by aggregating features of all points within it and each word is abstracted by its contextual information. We optimize the feature space in order to enlarge the similarities between the paired training samples, while simultaneously maximizing the margin between the unpaired ones. Experiments demonstrate that our method achieves a significant improvement in accuracy over the SOTAs on multi-modal retrieval tasks under the Text2Shape dataset. Codes are available at https://github.com/JLUtangchuan/Parts2Words.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1132.How Can Objects Help Action Recognition?</span><br>
                <span class="as">Zhou, XingyiandArnab, AnuragandSun, ChenandSchmid, Cordelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2353-2362.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用对象的知识来设计更好的视频模型，即处理更少的令牌并提高识别准确性。<br>
                    动机：目前的最先进的视频模型将所有的视频标记作为长序列的空间-时间令牌进行处理，但并没有明确地对对象及其在视频中的交互进行建模。<br>
                    方法：提出了一种对象引导的令牌采样策略和一种对象感知的注意力模块。前者使我们能够保留一小部分输入令牌，同时对准确性的影响最小；后者则通过将对象信息融入特征表示中，提高了整体的准确性。<br>
                    效果：我们的结果框架在使用较少的令牌时，其性能优于强大的基线。具体来说，我们在SomethingElse、Something-something v2和Epic-Kitchens上分别与基线的30%、40%和60%的输入令牌相匹配。当我们使用我们的模型处理与基线相同数量的令牌时，我们在这些数据集上提高了0.6到4.2个百分点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current state-of-the-art video models process a video clip as a long sequence of spatio-temporal tokens. However, they do not explicitly model objects, their interactions across the video, and instead process all the tokens in the video. In this paper, we investigate how we can use knowledge of objects to design better video models, namely to process fewer tokens and to improve recognition accuracy. This is in contrast to prior works which either drop tokens at the cost of accuracy, or increase accuracy whilst also increasing the computation required. First, we propose an object-guided token sampling strategy that enables us to retain a small fraction of the input tokens with minimal impact on accuracy. And second, we propose an object-aware attention module that enriches our feature representation with object information and improves overall accuracy. Our resulting framework achieves better performance when using fewer tokens than strong baselines. In particular, we match our baseline with 30%, 40%, and 60% of the input tokens on SomethingElse, Something-something v2, and Epic-Kitchens, respectively. When we use our model to process the same number of tokens as our baseline, we improve by 0.6 to 4.2 points on these datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1133.Efficient Hierarchical Entropy Model for Learned Point Cloud Compression</span><br>
                <span class="as">Song, RuiandFu, ChunyangandLiu, ShanandLi, Ge</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Efficient_Hierarchical_Entropy_Model_for_Learned_Point_Cloud_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14368-14377.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地去除点云压缩中的冗余信息。<br>
                    动机：点云压缩中存在大量冗余信息，准确学习熵模型是去除冗余的关键。现有的基于八叉树的自回归熵模型虽然有效，但计算复杂度高，不适用于实际应用。<br>
                    方法：提出一种分层注意力结构和分组上下文结构来提高注意力模型的效率和解决自回归导致的串行解码问题。<br>
                    效果：实验证明，提出的熵模型在率-失真性能和解码延迟方面优于现有最先进的大规模自回归熵模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning an accurate entropy model is a fundamental way to remove the redundancy in point cloud compression. Recently, the octree-based auto-regressive entropy model which adopts the self-attention mechanism to explore dependencies in a large-scale context is proved to be promising. However, heavy global attention computations and auto-regressive contexts are inefficient for practical applications. To improve the efficiency of the attention model, we propose a hierarchical attention structure that has a linear complexity to the context scale and maintains the global receptive field. Furthermore, we present a grouped context structure to address the serial decoding issue caused by the auto-regression while preserving the compression performance. Experiments demonstrate that the proposed entropy model achieves superior rate-distortion performance and significant decoding latency reduction compared with the state-of-the-art large-scale auto-regressive entropy model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1134.DKM: Dense Kernelized Feature Matching for Geometry Estimation</span><br>
                <span class="as">Edstedt, JohanandAthanasiadis, IoannisandWadenb\&quot;ack, M\r{a</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17765-17775.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中的特征匹配问题，即在两个3D场景图像之间找到对应关系。<br>
                    动机：尽管稀疏方法在估计两视图几何方面的效果通常优于密集方法，但作者提出一种新的密集方法，该方法在所有几何估计任务上都超越了稀疏和半稀疏方法。<br>
                    方法：首先，作者提出了一种核回归全局匹配器；其次，通过堆叠特征图和深度卷积内核进行扭曲优化；最后，通过一致的深度和平衡采样方法为密集置信图学习密集置信。<br>
                    效果：实验证明，提出的密集方法“密集核化特征匹配”在多个几何估计基准测试中创造了新的最先进水平。特别是在MegaDepth-1500上，与之前最好的稀疏方法和密集方法相比，分别提高了+4.9和+8.9 AUC@5。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Feature matching is a challenging computer vision task that involves finding correspondences between two images of a 3D scene. In this paper we consider the dense approach instead of the more common sparse paradigm, thus striving to find all correspondences. Perhaps counter-intuitively, dense methods have previously shown inferior performance to their sparse and semi-sparse counterparts for estimation of two-view geometry. This changes with our novel dense method, which outperforms both dense and sparse methods on geometry estimation. The novelty is threefold: First, we propose a kernel regression global matcher. Secondly, we propose warp refinement through stacked feature maps and depthwise convolution kernels. Thirdly, we propose learning dense confidence through consistent depth and a balanced sampling approach for dense confidence maps. Through extensive experiments we confirm that our proposed dense method, Dense Kernelized Feature Matching, sets a new state-of-the-art on multiple geometry estimation benchmarks. In particular, we achieve an improvement on MegaDepth-1500 of +4.9 and +8.9 AUC@5 compared to the best previous sparse method and dense method respectively. Our code is provided at the following repository: https://github.com/Parskatt/DKM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1135.Image Cropping With Spatial-Aware Feature and Rank Consistency</span><br>
                <span class="as">Wang, ChaoandNiu, LiandZhang, BoandZhang, Liqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_Cropping_With_Spatial-Aware_Feature_and_Rank_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10052-10061.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过图像裁剪找到视觉上吸引人的图像部分，同时捕捉到裁剪区域与美学元素（如显著对象、语义边缘）的空间关系。<br>
                    动机：尽管先前的方法在这方面取得了重大进展，但他们在捕捉裁剪区域与美学元素之间的空间关系方面表现较弱。此外，由于标注数据的高成本，未标注数据的潜在价值仍有待挖掘。<br>
                    方法：我们提出了一种空间感知特征来编码候选裁剪区域与美学元素之间的空间关系，方法是将裁剪掩膜和选择性聚合的特征图进行连接，然后输入到一个轻量级的编码器中。为了解决第二个问题，我们在标记图像上训练了一个成对排名分类器，并将这种知识转移到未标记的图像上，以强制保持排名一致性。<br>
                    效果：在基准数据集上的实验结果表明，我们提出的方法在性能上优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image cropping aims to find visually appealing crops in an image. Despite the great progress made by previous methods, they are weak in capturing the spatial relationship between crops and aesthetic elements (e.g., salient objects, semantic edges). Besides, due to the high annotation cost of labeled data, the potential of unlabeled data awaits to be excavated. To address the first issue, we propose spatial-aware feature to encode the spatial relationship between candidate crops and aesthetic elements, by feeding the concatenation of crop mask and selectively aggregated feature maps to a light-weighted encoder. To address the second issue, we train a pair-wise ranking classifier on labeled images and transfer such knowledge to unlabeled images to enforce rank consistency. Experimental results on the benchmark datasets show that our proposed method performs favorably against state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1136.SVGformer: Representation Learning for Continuous Vector Graphics Using Transformers</span><br>
                <span class="as">Cao, DefuandWang, ZhaowenandEchevarria, JoseandLiu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10093-10102.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地理解和生成数据，特别是在矢量图形数据中。<br>
                    动机：现有的深度学习方法在处理矢量图形数据时，往往需要量化SVG参数，无法直接利用其几何特性，导致下游任务效果不佳。<br>
                    方法：提出一种基于变压器的表示学习模型SVGformer，该模型直接在连续输入值上操作，操纵SVG的几何信息以编码轮廓细节和长距离依赖关系。<br>
                    效果：通过在矢量字体和图标数据集上的大量实验，证明该模型能够捕获高质量的表示信息，并在下游任务上显著超越先前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Advances in representation learning have led to great success in understanding and generating data in various domains. However, in modeling vector graphics data, the pure data-driven approach often yields unsatisfactory results in downstream tasks as existing deep learning methods often require the quantization of SVG parameters and cannot exploit the geometric properties explicitly. In this paper, we propose a transformer-based representation learning model (SVGformer) that directly operates on continuous input values and manipulates the geometric information of SVG to encode outline details and long-distance dependencies. SVGfomer can be used for various downstream tasks: reconstruction, classification, interpolation, retrieval, etc. We have conducted extensive experiments on vector font and icon datasets to show that our model can capture high-quality representation information and outperform the previous state-of-the-art on downstream tasks significantly.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1137.Pixels, Regions, and Objects: Multiple Enhancement for Salient Object Detection</span><br>
                <span class="as">Wang, YiandWang, RuiliandFan, XinandWang, TianzhuandHe, Xiangjian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Pixels_Regions_and_Objects_Multiple_Enhancement_for_Salient_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10031-10040.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高显著物体检测的准确率和鲁棒性，特别是在多对象和背景杂乱的复杂场景中。<br>
                    动机：目前的显著物体检测方法在处理复杂场景时存在不足，需要进一步提高准确率和鲁棒性。<br>
                    方法：提出一种名为MENet的新方法，采用人类视觉系统（HVS）的边界敏感性、内容完整性、迭代精炼和频率分解机制。设计了多级混合损失来引导网络学习像素级、区域级和对象级特征。设计了灵活的多尺度特征增强模块（ME-Module）来逐步聚合和精炼全局或详细特征。使用迭代训练策略来增强MENet双分支解码器中的边界特征和自适应特征。<br>
                    效果：在六个具有挑战性的基准数据集上进行全面评估，结果显示MENet取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Salient object detection (SOD) aims to mimic the human visual system (HVS) and cognition mechanisms to identify and segment salient objects. However, due to the complexity of these mechanisms, current methods are not perfect. Accuracy and robustness need to be further improved, particularly in complex scenes with multiple objects and background clutter. To address this issue, we propose a novel approach called Multiple Enhancement Network (MENet) that adopts the boundary sensibility, content integrity, iterative refinement, and frequency decomposition mechanisms of HVS. A multi-level hybrid loss is firstly designed to guide the network to learn pixel-level, region-level, and object-level features. A flexible multiscale feature enhancement module (ME-Module) is then designed to gradually aggregate and refine global or detailed features by changing the size order of the input feature sequence. An iterative training strategy is used to enhance boundary features and adaptive features in the dual-branch decoder of MENet. Comprehensive evaluations on six challenging benchmark datasets show that MENet achieves state-of-the-art results. Both the codes and results are publicly available at https://github.com/yiwangtz/MENet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1138.ToThePoint: Efficient Contrastive Learning of 3D Point Clouds via Recycling</span><br>
                <span class="as">Li, XinglinandChen, JiajingandOuyang, JinhuiandDeng, HanhuiandVelipasalar, SenemandWu, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ToThePoint_Efficient_Contrastive_Learning_of_3D_Point_Clouds_via_Recycling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21781-21790.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，点云处理技术发展迅速，但需要大量标注数据进行监督学习，且标注过程耗时耗力。<br>
                    动机：针对此问题，本文提出了一种新颖的对比学习方法ToThePoint，该方法利用无标签数据预训练一个主干网络，提取潜在表示用于后续任务。<br>
                    方法：与传统的对比学习方法不同，ToThePoint不仅最大化同一类点云经过不同类型增强后的特征之间的一致性，还最大化置换不变特征和最大池化后丢弃的特征之间的一致性。<br>
                    效果：在ShapeNet数据集上进行自监督学习后，ToThePoint在ModelNet40、ModelNet40C、ScanobjectNN和ShapeNet-Part等下游任务上取得了与最新基线相当甚至更好的结果，并且训练时间比基线快200倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent years have witnessed significant developments in point cloud processing, including classification and segmentation. However, supervised learning approaches need a lot of well-labeled data for training, and annotation is labor- and time-intensive. Self-supervised learning, on the other hand, uses unlabeled data, and pre-trains a backbone with a pretext task to extract latent representations to be used with the downstream tasks. Compared to 2D images, self-supervised learning of 3D point clouds is under-explored. Existing models, for self-supervised learning of 3D point clouds, rely on a large number of data samples, and require significant amount of computational resources and training time. To address this issue, we propose a novel contrastive learning approach, referred to as ToThePoint. Different from traditional contrastive learning methods, which maximize agreement between features obtained from a pair of point clouds formed only with different types of augmentation, ToThePoint also maximizes the agreement between the permutation invariant features and features discarded after max pooling. We first perform self-supervised learning on the ShapeNet dataset, and then evaluate the performance of the network on different downstream tasks. In the downstream task experiments, performed on the ModelNet40, ModelNet40C, ScanobjectNN and ShapeNet-Part datasets, our proposed ToThePoint achieves competitive, if not better results compared to the state-of-the-art baselines, and does so with significantly less training time (200 times faster than baselines)</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1139.EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding</span><br>
                <span class="as">Wu, YanminandCheng, XinhuaandZhang, RenruiandCheng, ZesenandZhang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19231-19242.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过丰富的语义线索从点云中找出自然语言描述的对象。<br>
                    动机：现有的方法要么提取耦合所有单词的句子级特征，要么更关注对象名称，这会丢失单词级信息或忽视其他属性。<br>
                    方法：提出了EDA模型，该模型明确地将句子中的文本属性解耦，并在这种精细的语言和点云对象之间进行密集对齐。具体来说，首先提出一个文本解耦模块，为每个语义组件生成文本特征。然后设计两种损失来监督两种模态之间的密集匹配：位置对齐损失和语义对齐损失。此外，还引入了一个新的视觉基础任务，即定位没有对象名称的对象，以全面评估模型的密集对齐能力。<br>
                    效果：实验结果表明，EDA在两个广泛采用的3D视觉基础数据集ScanRefer和SR3D/NR3D上取得了最先进的性能，并在我们新提出的任务上取得了绝对的领导地位。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D visual grounding aims to find the object within point clouds mentioned by free-form natural language descriptions with rich semantic cues. However, existing methods either extract the sentence-level features coupling all words or focus more on object names, which would lose the word-level information or neglect other attributes. To alleviate these issues, we present EDA that Explicitly Decouples the textual attributes in a sentence and conducts Dense Alignment between such fine-grained language and point cloud objects. Specifically, we first propose a text decoupling module to produce textual features for every semantic component. Then, we design two losses to supervise the dense matching between two modalities: position alignment loss and semantic alignment loss. On top of that, we further introduce a new visual grounding task, locating objects without object names, which can thoroughly evaluate the model's dense alignment capacity. Through experiments, we achieve state-of-the-art performance on two widely-adopted 3D visual grounding datasets, ScanRefer and SR3D/NR3D, and obtain absolute leadership on our newly-proposed task. The source code is available at https://github.com/yanmin-wu/EDA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1140.A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation From a Single RGB Image</span><br>
                <span class="as">Jiang, ChanglongandXiao, YangandWu, CunlinandZhang, MingyangandZheng, JinghongandCao, ZhiguoandZhou, JoeyTianyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8846-8855.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张RGB图像中估计3D交互手的姿势，解决手部自我遮挡和相互遮挡、两只手相似外观模式的混淆以及2D到3D关节位置映射的病态问题。<br>
                    动机：为了解决上述问题，作者提出将A2J-一种最先进的基于深度的3D单手姿势估计方法-扩展到交互手条件下的RGB领域。<br>
                    方法：通过在Transformer的非局部编码-解码框架下对A2J进行改进，构建了A2J-Transformer。主要有三个优点：1. 建立局部锚点间的自注意力，使它们具有全局空间上下文意识，以更好地捕捉抵抗遮挡的关节连接线索；2. 将每个锚点视为可学习的查询，具有自适应特征学习，以增强模式拟合能力；3. 锚点位于3D空间，而不是A2J中的2D空间，以利用3D姿势预测。<br>
                    效果：在具有挑战性的InterHand 2.6M数据集上进行的实验表明，A2J-Transformer可以实现最先进的无模型性能（在双手握持情况下，MPJPE降低了3.38mm），并且可以很好地泛化到深度领域。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1141.E2PN: Efficient SE(3)-Equivariant Point Network</span><br>
                <span class="as">Zhu, MinghanandGhaffari, MaaniandClark, WilliamA.andPeng, Huei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_E2PN_Efficient_SE3-Equivariant_Point_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1223-1232.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种从3D点云中学习SE(3)等变特征的卷积结构。<br>
                    动机：现有的网络在处理点云数据时，无法有效地进行旋转不变性的特征提取和分类。<br>
                    方法：通过结合群卷积和商表示，将SO(3)离散化为有限群，使用SO(2)作为稳定子群形成球面商特征场以节省计算，同时提出置换层从球面特征恢复SO(3)特征以保留区分旋转的能力。<br>
                    效果：实验表明，该方法在物体分类、姿态估计和关键点匹配等多种任务上取得了相当或更好的性能，同时消耗的内存更少，运行速度比现有工作更快。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a convolution structure for learning SE(3)-equivariant features from 3D point clouds. It can be viewed as an equivariant version of kernel point convolutions (KPConv), a widely used convolution form to process point cloud data. Compared with existing equivariant networks, our design is simple, lightweight, fast, and easy to be integrated with existing task-specific point cloud learning pipelines. We achieve these desirable properties by combining group convolutions and quotient representations. Specifically, we discretize SO(3) to finite groups for their simplicity while using SO(2) as the stabilizer subgroup to form spherical quotient feature fields to save computations. We also propose a permutation layer to recover SO(3) features from spherical features to preserve the capacity to distinguish rotations. Experiments show that our method achieves comparable or superior performance in various tasks, including object classification, pose estimation, and keypoint-matching, while consuming much less memory and running faster than existing work. The proposed method can foster the development of equivariant models for real-world applications based on point clouds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1142.SemiCVT: Semi-Supervised Convolutional Vision Transformer for Semantic Segmentation</span><br>
                <span class="as">Huang, HuiminandXie, ShiaoandLin, LanfenandTong, RuofengandChen, Yen-WeiandLi, YuexiangandWang, HongandHuang, YawenandZheng, Yefeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_SemiCVT_Semi-Supervised_Convolutional_Vision_Transformer_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11340-11349.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过整合卷积神经网络（CNN）和转换器的优势，解决半监督学习中全局学习能力和类别级特征的问题。<br>
                    动机：现有的半监督学习方法主要集中在像素级的一致性上，忽视了模型内部的局部-全局交互以及跨模型的类别级一致性。<br>
                    方法：提出了一种新的算法SemiCVT，该算法结合了CNN和Transformer的优点，设计了一个并行的CNN-Transformer架构，并在傅立叶域引入了局部-全局交互模式，同时提出了跨模型的类别级一致性来补充CNN和Transformer的类别级统计信息。<br>
                    效果：实验结果表明，SemiCVT在两个公共基准测试中的表现优于现有的最佳方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised learning improves data efficiency of deep models by leveraging unlabeled samples to alleviate the reliance on a large set of labeled samples. These successes concentrate on the pixel-wise consistency by using convolutional neural networks (CNNs) but fail to address both global learning capability and class-level features for unlabeled data. Recent works raise a new trend that Trans- former achieves superior performance on the entire feature map in various tasks. In this paper, we unify the current dominant Mean-Teacher approaches by reconciling intra- model and inter-model properties for semi-supervised segmentation to produce a novel algorithm, SemiCVT, that absorbs the quintessence of CNNs and Transformer in a comprehensive way. Specifically, we first design a parallel CNN-Transformer architecture (CVT) with introducing an intra-model local-global interaction schema (LGI) in Fourier domain for full integration. The inter-model class- wise consistency is further presented to complement the class-level statistics of CNNs and Transformer in a cross- teaching manner. Extensive empirical evidence shows that SemiCVT yields consistent improvements over the state-of- the-art methods in two public benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1143.DejaVu: Conditional Regenerative Learning To Enhance Dense Prediction</span><br>
                <span class="as">Borse, ShubhankarandDas, DebasmitandPark, HyojinandCai, HongandGarrepalli, RisheekandPorikli, Fatih</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Borse_DejaVu_Conditional_Regenerative_Learning_To_Enhance_Dense_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19466-19477.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用条件图像再生作为额外的监督来提高深度网络在密集预测任务（如分割、深度估计和表面法线预测）上的性能。<br>
                    动机：现有的方法在处理密集预测任务时，通常忽略了图像的结构信息，导致预测结果的边界不清晰，空间一致性差。<br>
                    方法：提出DejaVu框架，通过条件图像再生技术，将稀疏采样或选择性频率移除等技术应用于输入图像，去除其结构信息，然后使用条件生成器根据去结构化的图像和密集预测结果重建原始图像，从而鼓励基础网络在其密集预测中嵌入准确的场景结构。<br>
                    效果：实验结果表明，DejaVu在多个密集预测基准测试中的表现优于现有方法，且无需增加计算成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present DejaVu, a novel framework which leverages conditional image regeneration as additional supervision during training to improve deep networks for dense prediction tasks such as segmentation, depth estimation, and surface normal prediction. First, we apply redaction to the input image, which removes certain structural information by sparse sampling or selective frequency removal. Next, we use a conditional regenerator, which takes the redacted image and the dense predictions as inputs, and reconstructs the original image by filling in the missing structural information. In the redacted image, structural attributes like boundaries are broken while semantic context is largely preserved. In order to make the regeneration feasible, the conditional generator will then require the structure information from the other input source, i.e., the dense predictions. As such, by including this conditional regeneration objective during training, DejaVu encourages the base network to learn to embed accurate scene structure in its dense prediction. This leads to more accurate predictions with clearer boundaries and better spatial consistency. When it is feasible to leverage additional computation, DejaVu can be extended to incorporate an attention-based regeneration module within the dense prediction network, which further improves accuracy. Through extensive experiments on multiple dense prediction benchmarks such as Cityscapes, COCO, ADE20K, NYUD-v2, and KITTI, we demonstrate the efficacy of employing DejaVu during training, as it outperforms SOTA methods at no added computation cost.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1144.Gated Multi-Resolution Transfer Network for Burst Restoration and Enhancement</span><br>
                <span class="as">Mehta, NancyandDudhane, AkshayandMurala, SubrahmanyamandZamir, SyedWaqasandKhan, SalmanandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mehta_Gated_Multi-Resolution_Transfer_Network_for_Burst_Restoration_and_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22201-22210.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，爆发式图像处理越来越受欢迎，但这是一个挑战性的任务，因为单个爆发式图像会经历多次退化，并且经常有相互错位，导致幽灵和拉链伪影。<br>
                    动机：现有的爆发式恢复方法通常不考虑爆发帧之间的相互关联和非局部上下文信息，这往往限制了这些方法在具有挑战性的情况下的应用。另一个关键挑战在于爆发帧的稳健上采样。现有的上采样方法不能有效地利用单阶段和渐进上采样策略与常规和/或最近的上采样器同时使用的优点。<br>
                    方法：为了解决这些挑战，我们提出了一种新的门控多分辨率转移网络（GMTNet）来从一组低质量的原始图像重建空间精确的高质量图像。GMTNet由三个针对爆发处理任务优化的模块组成：用于特征去噪和对齐的多尺度爆发特征对齐（MBFA），用于多帧特征聚合的转置注意特征合并（TAFM），以及用于上采样合并的特征并构建高质量输出图像的分辨率转移特征上采样器（RTFU）。<br>
                    效果：我们在五个数据集上进行了详细的实验分析，验证了我们的方法，并在爆发超分辨率、爆发去噪和低光爆发增强方面设置了新的最先进的状态。我们的代码和模型可在https://github.com/nanmehta/GMTNet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Burst image processing is becoming increasingly popular in recent years. However, it is a challenging task since individual burst images undergo multiple degradations and often have mutual misalignments resulting in ghosting and zipper artifacts. Existing burst restoration methods usually do not consider the mutual correlation and non-local contextual information among burst frames, which tends to limit these approaches in challenging cases. Another key challenge lies in the robust up-sampling of burst frames. The existing up-sampling methods cannot effectively utilize the advantages of single-stage and progressive up-sampling strategies with conventional and/or recent up-samplers at the same time. To address these challenges, we propose a novel Gated Multi-Resolution Transfer Network (GMTNet) to reconstruct a spatially precise high-quality image from a burst of low-quality raw images. GMTNet consists of three modules optimized for burst processing tasks: Multi-scale Burst Feature Alignment (MBFA) for feature denoising and alignment, Transposed-Attention Feature Merging (TAFM) for multi-frame feature aggregation, and Resolution Transfer Feature Up-sampler (RTFU) to up-scale merged features and construct a high-quality output image. Detailed experimental analysis on five datasets validate our approach and sets a new state-of-the-art for burst super-resolution, burst denoising, and low-light burst enhancement. Our codes and models are available at https://github.com/nanmehta/GMTNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1145.PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D Detection</span><br>
                <span class="as">Zhang, LinfengandDong, RunpeiandTai, Hung-ShuoandMa, Kaisheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21791-21801.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行点云表示学习，并实现准确且高效的3D物体检测。<br>
                    动机：虽然点云表示学习在自动驾驶和虚拟现实等应用中取得了显著突破，但目前急需一种既准确又高效的3D物体检测方法。<br>
                    方法：本文提出了一种名为PointDistiller的结构化知识蒸馏框架，通过动态图卷积和重新加权的学习策略提取和蒸馏点云的局部几何结构，以提高知识蒸馏的效率。<br>
                    效果：实验证明，该方法在基于体素和原始点云的检测器上均优于先前的七种知识蒸馏方法，例如，压缩4倍的PointPillars学生模型在BEV和3D物体检测上的mAP提高了2.8和3.4，比其教师模型分别高出0.9和1.8 mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The remarkable breakthroughs in point cloud representation learning have boosted their usage in real-world applications such as self-driving cars and virtual reality. However, these applications usually have an urgent requirement for not only accurate but also efficient 3D object detection. Recently, knowledge distillation has been proposed as an effective model compression technique, which transfers the knowledge from an over-parameterized teacher to a lightweight student and achieves consistent effectiveness in 2D vision. However, due to point clouds' sparsity and irregularity, directly applying previous image-based knowledge distillation methods to point cloud detectors usually leads to unsatisfactory performance. To fill the gap, this paper proposes PointDistiller, a structured knowledge distillation framework for point clouds-based 3D detection. Concretely, PointDistiller includes local distillation which extracts and distills the local geometric structure of point clouds with dynamic graph convolution and reweighted learning strategy, which highlights student learning on the critical points or voxels to improve knowledge distillation efficiency. Extensive experiments on both voxels-based and raw points-based detectors have demonstrated the effectiveness of our method over seven previous knowledge distillation methods. For instance, our 4X compressed PointPillars student achieves 2.8 and 3.4 mAP improvements on BEV and 3D object detection, outperforming its teacher by 0.9 and 1.8 mAP, respectively. Codes are available in the supplementary material and will be released on Github.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1146.TopDiG: Class-Agnostic Topological Directional Graph Extraction From Remote Sensing Images</span><br>
                <span class="as">Yang, BingnanandZhang, MiandZhang, ZhanandZhang, ZhiliandHu, Xiangyun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_TopDiG_Class-Agnostic_Topological_Directional_Graph_Extraction_From_Remote_Sensing_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1265-1274.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，自动从遥感图像中提取向量的方法发展迅速，但大多数现有工作集中在特定目标上，对类别变化敏感，且难以在不同类别间实现稳定性能。<br>
                    动机：为了解决这些问题，我们提出了一种创新的类别无关模型TopDiG，直接从遥感图像中提取拓扑方向图。<br>
                    方法：首先，TopDiG采用一个关注拓扑的节点检测器（TCND）来检测节点并获取拓扑组件的紧凑感知。其次，我们提出了一种动态图监督（DGS）策略，从无序的节点中动态生成邻接图标签。最后，设计了一个方向图（DiG）生成器模块，从预测的节点中构建拓扑方向图。<br>
                    效果：在Inria、CrowdAI、GID、GF2和Massachusetts数据集上的实验证明，TopDiG是类别无关的，并在所有数据集上都取得了有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rapid development in automatic vector extraction from remote sensing images has been witnessed in recent years. However, the vast majority of existing works concentrate on a specific target, fragile to category variety, and hardly achieve stable performance crossing different categories. In this work, we propose an innovative class-agnostic model, namely TopDiG, to directly extract topological directional graphs from remote sensing images and solve these issues. Firstly, TopDiG employs a topology-concentrated node detector (TCND) to detect nodes and obtain compact perception of topological components. Secondly, we propose a dynamic graph supervision (DGS) strategy to dynamically generate adjacency graph labels from unordered nodes. Finally, the directional graph (DiG) generator module is designed to construct topological directional graphs from predicted nodes. Experiments on the Inria, CrowdAI, GID, GF2 and Massachusetts datasets empirically demonstrate that TopDiG is class-agnostic and achieves competitive performance on all datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1147.LinK: Linear Kernel for LiDAR-Based 3D Perception</span><br>
                <span class="as">Lu, TaoandDing, XiangandLiu, HaisongandWu, GangshanandWang, Limin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_LinK_Linear_Kernel_for_LiDAR-Based_3D_Perception_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1105-1115.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将二维大核的成功扩展到三维感知，解决处理三维数据时立方体增加的开销和数据稀缺稀疏导致的优化困难。<br>
                    动机：以前的工作通过引入块共享权重将内核大小从3x3x3扩展到7x7x7，但为了减少一个块内的特征变化，它只使用适度的块大小，无法实现像21x21x21这样的大内核。<br>
                    方法：我们提出了一种新的方法LinK，以卷积类似的方式实现更广泛的感知接受域，主要设计有两个。一是用线性核生成器替换静态核矩阵，为非空体素自适应提供权重；二是复用重叠块中预先计算的聚合结果，以降低计算复杂度。<br>
                    效果：该方法成功地使每个体素在21x21x21范围内感知上下文。在两个基本感知任务（3D对象检测和3D语义分割）上的大量实验表明了我们方法的有效性。特别是在nuScenes的3D检测基准测试中，我们的CenterPoint基础探测器仅通过集成LinK基础骨干就排名第一。在SemanticKITTI测试集上，我们还将强分割基线的mIoU提高了2.7%。代码可在https://github.com/MCG-NJU/LinK获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Extending the success of 2D Large Kernel to 3D perception is challenging due to: 1. the cubically-increasing overhead in processing 3D data; 2. the optimization difficulties from data scarcity and sparsity. Previous work has taken the first step to scale up the kernel size from 3x3x3 to 7x7x7 by introducing block-shared weights. However, to reduce the feature variations within a block, it only employs modest block size and fails to achieve larger kernels like the 21x21x21. To address this issue, we propose a new method, called LinK, to achieve a wider-range perception receptive field in a convolution-like manner with two core designs. The first is to replace the static kernel matrix with a linear kernel generator, which adaptively provides weights only for non-empty voxels. The second is to reuse the pre-computed aggregation results in the overlapped blocks to reduce computation complexity. The proposed method successfully enables each voxel to perceive context within a range of 21x21x21. Extensive experiments on two basic perception tasks, 3D object detection and 3D semantic segmentation, demonstrate the effectiveness of our method. Notably, we rank 1st on the public leaderboard of the 3D detection benchmark of nuScenes (LiDAR track), by simply incorporating a LinK-based backbone into the basic detector, CenterPoint. We also boost the strong segmentation baseline's mIoU with 2.7% in the SemanticKITTI test set. Code is available at https://github.com/MCG-NJU/LinK.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1148.Modeling Entities As Semantic Points for Visual Information Extraction in the Wild</span><br>
                <span class="as">Yang, ZhiboandLong, RujiaoandWang, PengfeiandSong, SiboandZhong, HumenandCheng, WenqingandBai, XiangandYao, Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Modeling_Entities_As_Semantic_Points_for_Visual_Information_Extraction_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15358-15367.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前视觉信息提取（VIE）在学术界和工业界的重要性日益增加，但评估这些方法的基准相对简单，没有充分代表真实世界的复杂情况。<br>
                    动机：我们创建了一个更具挑战性的新数据集，并探索了一种在困难条件下精确且鲁棒地从文档图像中提取关键信息的新方法。<br>
                    方法：与以往将视觉信息融入多模态架构或端到端训练文本定位和信息提取的方法不同，我们将实体明确地模型化为语义点，即实体的中心点富含描述不同实体属性和关系的语义信息。<br>
                    效果：实验表明，该方法在实体标注和链接方面相比现有最先进的模型，可以显著提高性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, Visual Information Extraction (VIE) has been becoming increasingly important in both academia and industry, due to the wide range of real-world applications. Previously, numerous works have been proposed to tackle this problem. However, the benchmarks used to assess these methods are relatively plain, i.e., scenarios with real-world complexity are not fully represented in these benchmarks. As the first contribution of this work, we curate and release a new dataset for VIE, in which the document images are much more challenging in that they are taken from real applications, and difficulties such as blur, partial occlusion, and printing shift are quite common. All these factors may lead to failures in information extraction. Therefore, as the second contribution, we explore an alternative approach to precisely and robustly extract key information from document images under such tough conditions. Specifically, in contrast to previous methods, which usually either incorporate visual information into a multi-modal architecture or train text spotting and information extraction in an end-to-end fashion, we explicitly model entities as semantic points, i.e., center points of entities are enriched with semantic information describing the attributes and relationships of different entities, which could largely benefit entity labeling and linking. Extensive experiments on standard benchmarks in this field as well as the proposed dataset demonstrate that the proposed method can achieve significantly enhanced performance on entity labeling and linking, compared with previous state-of-the-art models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1149.Learned Image Compression With Mixed Transformer-CNN Architectures</span><br>
                <span class="as">Liu, JinmingandSun, HemingandKatto, Jiro</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learned_Image_Compression_With_Mixed_Transformer-CNN_Architectures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14388-14397.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地融合卷积神经网络（CNN）和变换器在图像压缩中的优势，同时实现较高的性能和适当的复杂度？<br>
                    动机：现有的学习型图像压缩（LIC）方法主要是基于CNN或变换器的，各有优势。如何充分利用两者的优点是一个值得探索的问题。<br>
                    方法：本文提出了一种有效的并行Transformer-CNN混合物（TCM）块，结合了CNN的局部建模能力和变换器的非局部建模能力，以提高图像压缩模型的整体架构。此外，受最近熵估计模型和注意力模块进展的启发，通过使用通道挤压，提出了一种具有参数高效swin-transformer-based注意力（SWAtten）模块的通道级熵模型。<br>
                    效果：实验结果表明，与现有的LIC方法相比，所提出的方法在三个不同分辨率的数据集（即柯达、Tecnick、CLIC专业验证）上实现了最先进的率失真性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learned image compression (LIC) methods have exhibited promising progress and superior rate-distortion performance compared with classical image compression standards. Most existing LIC methods are Convolutional Neural Networks-based (CNN-based) or Transformer-based, which have different advantages. Exploiting both advantages is a point worth exploring, which has two challenges: 1) how to effectively fuse the two methods? 2) how to achieve higher performance with a suitable complexity? In this paper, we propose an efficient parallel Transformer-CNN Mixture (TCM) block with a controllable complexity to incorporate the local modeling ability of CNN and the non-local modeling ability of transformers to improve the overall architecture of image compression models. Besides, inspired by the recent progress of entropy estimation models and attention modules, we propose a channel-wise entropy model with parameter-efficient swin-transformer-based attention (SWAtten) modules by using channel squeezing. Experimental results demonstrate our proposed method achieves state-of-the-art rate-distortion performances on three different resolution datasets (i.e., Kodak, Tecnick, CLIC Professional Validation) compared to existing LIC methods. The code is at https://github.com/jmliu206/LIC_TCM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1150.PanoSwin: A Pano-Style Swin Transformer for Panorama Understanding</span><br>
                <span class="as">Ling, ZhixinandXing, ZhenandZhou, XiangdongandCao, ManliangandZhou, Guichun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17755-17764.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善全景理解中由于等矩形投影（ERP）导致的边界不连续性和空间畸变问题。<br>
                    动机：现有的CNNs和视觉Transformers在处理全景图像时，由于等矩形投影的问题，性能会严重下降。<br>
                    方法：提出一种名为PanoSwin的新架构，通过探索全景样式的移位窗口方案和新颖的音高注意力来解决边界不连续性和空间畸变问题。同时，基于球面距离和笛卡尔坐标，为全景图像调整绝对位置编码和相对位置偏差以增强全景几何信息。并设计了一种新颖的两阶段学习框架，将来自平面图像的知识转移到全景图像中。<br>
                    效果：在各种全景任务上与最先进的技术进行对比实验，包括全景对象检测、全景分类和全景布局估计，实验结果表明PanoSwin在全景理解方面非常有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional encodings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1151.Adaptive Sparse Convolutional Networks With Global Context Enhancement for Faster Object Detection on Drone Images</span><br>
                <span class="as">Du, BoweiandHuang, YechengandChen, JiaxinandHuang, Di</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Adaptive_Sparse_Convolutional_Networks_With_Global_Context_Enhancement_for_Faster_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13435-13444.png><br>
            
            <span class="tt"><span class="t0">研究问题：在资源有限的无人机平台上，如何在低延迟的情况下进行目标检测是一项重要但具有挑战性的任务。<br>
                    动机：目前的检测头基于稀疏卷积的方法虽然在平衡准确性和效率方面有效，但在整合小物体的上下文信息以及控制不同尺度前景掩码比例方面存在问题。<br>
                    方法：我们提出了一种新颖的全局上下文增强自适应稀疏卷积网络（CEASC）。它首先开发了一种上下文增强组归一化（CE-GN）层，通过将基于稀疏采样特征的统计信息替换为全局上下文信息，然后设计了一种自适应多层掩蔽策略，以生成不同尺度的最佳掩码比例，实现紧凑的前景覆盖，提高准确性和效率。<br>
                    效果：我们在VisDrone和UAVDT两个主要基准上进行了广泛的实验，结果表明，当插入到典型的最先进的检测框架（如RetinaNet和GFL V1）时，CEASC显著降低了GFLOPs并加速了推理过程，同时保持了竞争性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object detection on drone images with low-latency is an important but challenging task on the resource-constrained unmanned aerial vehicle (UAV) platform. This paper investigates optimizing the detection head based on the sparse convolution, which proves effective in balancing the accuracy and efficiency. Nevertheless, it suffers from inadequate integration of contextual information of tiny objects as well as clumsy control of the mask ratio in the presence of foreground with varying scales. To address the issues above, we propose a novel global context-enhanced adaptive sparse convolutional network (CEASC). It first develops a context-enhanced group normalization (CE-GN) layer, by replacing the statistics based on sparsely sampled features with the global contextual ones, and then designs an adaptive multi-layer masking strategy to generate optimal mask ratios at distinct scales for compact foreground coverage, promoting both the accuracy and efficiency. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the inference procedure when plugging into the typical state-of-the-art detection frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is available at https://github.com/Cuogeihong/CEASC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1152.LidarGait: Benchmarking 3D Gait Recognition With Point Clouds</span><br>
                <span class="as">Shen, ChuanfuandFan, ChaoandWu, WeiandWang, RuiandHuang, GeorgeQ.andYu, Shiqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_LidarGait_Benchmarking_3D_Gait_Recognition_With_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1054-1063.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频步态识别在受限场景中取得了令人印象深刻的结果，但在3D野外世界中，由于忽视了人类的3D结构信息，其可行性受到限制。<br>
                    动机：本研究探索了从点云中提取精确的3D步态特征，并提出了一个简单的但有效的3D步态识别框架，称为LidarGait。<br>
                    方法：我们的方法将稀疏的点云投影到深度图中以学习具有3D几何信息的表示，这显著优于现有的点对点和基于摄像头的方法。<br>
                    效果：我们构建了第一个大规模的基于激光雷达的步态识别数据集SUSTech1K，并通过实验证明：（1）3D结构信息是步态识别的重要特征；（2）LidarGait显著优于现有的基于点的和轮廓的方法；（3）在户外环境中，激光雷达传感器在步态识别上优于RGB摄像头。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video-based gait recognition has achieved impressive results in constrained scenarios. However, visual cameras neglect human 3D structure information, which limits the feasibility of gait recognition in the 3D wild world. Instead of extracting gait features from images, this work explores precise 3D gait features from point clouds and proposes a simple yet efficient 3D gait recognition framework, termed LidarGait. Our proposed approach projects sparse point clouds into depth maps to learn the representations with 3D geometry information, which outperforms existing point-wise and camera-based methods by a significant margin. Due to the lack of point cloud datasets, we build the first large-scale LiDAR-based gait recognition dataset, SUSTech1K, collected by a LiDAR sensor and an RGB camera. The dataset contains 25,239 sequences from 1,050 subjects and covers many variations, including visibility, views, occlusions, clothing, carrying, and scenes. Extensive experiments show that (1) 3D structure information serves as a significant feature for gait recognition. (2) LidarGait outperforms existing point-based and silhouette-based methods by a significant margin, while it also offers stable cross-view results. (3) The LiDAR sensor is superior to the RGB camera for gait recognition in the outdoor environment. The source code and dataset have been made available at https://lidargait.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1153.D2Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-Based Transformers</span><br>
                <span class="as">He, JianfengandGao, YuanandZhang, TianzhuandZhang, ZheandWu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2904-2914.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高计算机视觉中图像匹配的鲁棒性。<br>
                    动机：现有的CNN提取的描述符在纹理较少的区域缺乏判别能力，关键点检测器只能识别具有特定结构级别的关键点，这导致图像匹配效果不佳。<br>
                    方法：通过代理转换器的联合学习提出了一种新的图像匹配方法D2Former，包括上下文特征描述符学习（CFDL）模块和分层关键点检测器学习（HKDL）模块。<br>
                    效果：实验结果表明，D2Former在四个具有挑战性的基准测试中显著优于现有的最佳图像匹配方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Establishing pixel-level matches between image pairs is vital for a variety of computer vision applications. However, achieving robust image matching remains challenging because CNN extracted descriptors usually lack discriminative ability in texture-less regions and keypoint detectors are only good at identifying keypoints with a specific level of structure. To deal with these issues, a novel image matching method is proposed by Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-based Transformers (D2Former), including a contextual feature descriptor learning (CFDL) module and a hierarchical keypoint detector learning (HKDL) module. The proposed D2Former enjoys several merits. First, the proposed CFDL module can model long-range contexts efficiently and effectively with the aid of designed descriptor agents. Second, the HKDL module can generate keypoint detectors in a hierarchical way, which is helpful for detecting keypoints with diverse levels of structures. Extensive experimental results on four challenging benchmarks show that our proposed method significantly outperforms state-of-the-art image matching methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1154.Lite DETR: An Interleaved Multi-Scale Encoder for Efficient DETR</span><br>
                <span class="as">Li, FengandZeng, AilingandLiu, ShilongandZhang, HaoandLi, HongyangandZhang, LeiandNi, LionelM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18558-18567.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高基于DEtection TRansformer（DETR）的物体检测模型的效率。<br>
                    动机：当前DETR模型中的多尺度特征融合在编码器中引入了大量计算无效的标记，这阻碍了DETR模型的实际应用。<br>
                    方法：提出Lite DETR，一种简单而高效的端到端物体检测框架。设计了一个有效的编码器块，交替更新高级别特征和低级别特征，同时开发了一种键感知的可变形注意力来预测更可靠的注意力权重。<br>
                    效果：实验证明，Lite DETR能有效降低检测头的GFLOPs 60%，同时保持99%的原始性能。这种高效的编码器策略可以广泛应用于现有的基于DETR的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent DEtection TRansformer-based (DETR) models have obtained remarkable performance. Its success cannot be achieved without the re-introduction of multi-scale feature fusion in the encoder. However, the excessively increased tokens in multi-scale features, especially for about 75% of low-level features, are quite computationally inefficient, which hinders real applications of DETR models. In this paper, we present Lite DETR, a simple yet efficient end-to-end object detection framework that can effectively reduce the GFLOPs of the detection head by 60% while keeping 99% of the original performance. Specifically, we design an efficient encoder block to update high-level features (corresponding to small-resolution feature maps) and low-level features (corresponding to large-resolution feature maps) in an interleaved way. In addition, to better fuse cross-scale features, we develop a key-aware deformable attention to predict more reliable attention weights. Comprehensive experiments validate the effectiveness and efficiency of the proposed Lite DETR, and the efficient encoder strategy can generalize well across existing DETR-based models. The code will be released after the blind review.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1155.Attention-Based Point Cloud Edge Sampling</span><br>
                <span class="as">Wu, ChengzhiandZheng, JunweiandPfrommer, JuliusandBeyerer, J\&quot;urgen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Attention-Based_Point_Cloud_Edge_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5333-5343.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对点云进行采样以提高数据表示效果。<br>
                    动机：目前最常用的点云采样方法仍为随机采样和最远点采样，而随着神经网络的发展，出现了许多基于任务学习的方法，但这些方法大多基于生成模型，而非直接利用数学统计进行点的选择。<br>
                    方法：受图像中Canny边缘检测算法的启发，并借助注意力机制，本文提出了一种非生成的基于注意力的点云边缘采样方法（APES），该方法能捕获点云轮廓中的显著点。<br>
                    效果：定性和定量的实验结果表明，我们的采样方法在常见的基准任务上表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud sampling is a less explored research topic for this data representation. The most commonly used sampling methods are still classical random sampling and farthest point sampling. With the development of neural networks, various methods have been proposed to sample point clouds in a task-based learning manner. However, these methods are mostly generative-based, rather than selecting points directly using mathematical statistics. Inspired by the Canny edge detection algorithm for images and with the help of the attention mechanism, this paper proposes a non-generative Attention-based Point cloud Edge Sampling method (APES), which captures salient points in the point cloud outline. Both qualitative and quantitative experimental results show the superior performance of our sampling method on common benchmark tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1156.BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud Registration</span><br>
                <span class="as">Ao, ShengandHu, QingyongandWang, HanyunandXu, KaiandGuo, Yulan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1255-1264.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现点云注册框架的高精度、高效率和强泛化性之间的良好平衡。<br>
                    动机：现有的注册技术或不准确，或效率低下，或泛化能力差，因此实现这三者之间的平衡极具挑战性。<br>
                    方法：提出一种名为BUFFER的点云注册方法，通过结合点对技术和片对技术并克服其固有缺点来平衡准确性、效率和泛化性。<br>
                    效果：在真实场景的大量实验中，该方法在准确性、效率和泛化性上都取得了最佳效果，不仅在未见过的数据域上达到了最高的成功率，而且比专门针对泛化的强基线快了近30倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>An ideal point cloud registration framework should have superior accuracy, acceptable efficiency, and strong generalizability. However, this is highly challenging since existing registration techniques are either not accurate enough, far from efficient, or generalized poorly. It remains an open question that how to achieve a satisfying balance between this three key elements. In this paper, we propose BUFFER, a point cloud registration method for balancing accuracy, efficiency, and generalizability. The key to our approach is to take advantage of both point-wise and patch-wise techniques, while overcoming the inherent drawbacks simultaneously. Different from a simple combination of existing methods, each component of our network has been carefully crafted to tackle specific issues. Specifically, a Point-wise Learner is first introduced to enhance computational efficiency by predicting keypoints and improving the representation capacity of features by estimating point orientations, a Patch-wise Embedder which leverages a lightweight local feature learner is then deployed to extract efficient and general patch features. Additionally, an Inliers Generator which combines simple neural layers and general features is presented to search inlier correspondences. Extensive experiments on real-world scenarios demonstrate that our method achieves the best of both worlds in accuracy, efficiency, and generalization. In particular, our method not only reaches the highest success rate on unseen domains, but also is almost 30 times faster than the strong baselines specializing in generalization. Code is available at https://github.com/aosheng1996/BUFFER.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1157.FeatureBooster: Boosting Feature Descriptors With a Lightweight Neural Network</span><br>
                <span class="as">Wang, XinjiangandLiu, ZeyuandHu, YuandXi, WeiandYu, WenxianandZou, Danping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_FeatureBooster_Boosting_Feature_Descriptors_With_a_Lightweight_Neural_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7630-7639.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用一种轻量级网络在同一图像中改进关键点描述符。<br>
                    动机：现有的描述符在处理大光照变化或重复模式等挑战性情况时性能有限。<br>
                    方法：提出一种网络，以原始描述符和关键点的几何特性作为输入，采用基于MLP的自我增强阶段和基于Transformer的交叉增强阶段来提升描述符。<br>
                    效果：实验结果表明，该方法显著提高了各项任务的性能，尤其在处理大光照变化或重复模式等挑战性情况时表现优秀。此外，该方法计算速度快，适合实际应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a lightweight network to improve descriptors of keypoints within the same image. The network takes the original descriptors and the geometric properties of keypoints as the input, and uses an MLP-based self-boosting stage and a Transformer-based cross-boosting stage to enhance the descriptors. The boosted descriptors can be either real-valued or binary ones. We use the proposed network to boost both hand-crafted (ORB, SIFT) and the state-of-the-art learning-based descriptors (SuperPoint, ALIKE) and evaluate them on image matching, visual localization, and structure-from-motion tasks. The results show that our method significantly improves the performance of each task, particularly in challenging cases such as large illumination changes or repetitive patterns. Our method requires only 3.2ms on desktop GPU and 27ms on embedded GPU to process 2000 features, which is fast enough to be applied to a practical system. The code and trained weights are publicly available at github.com/SJTU-ViSYS/FeatureBooster.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1158.Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors</span><br>
                <span class="as">Zhang, GongjieandLuo, ZhipengandTian, ZichenandZhang, JingyiandZhang, XiaoqinandLu, Shijian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6206-6216.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在Transformer-based对象检测器中使用多尺度特征，同时减少额外的计算成本。<br>
                    动机：多尺度特征已被证明对物体检测非常有效，但往往伴随着巨大甚至难以承受的额外计算成本，尤其是在最近的基于Transformer的检测器中。<br>
                    方法：提出了迭代多尺度特征聚合（IMFA）方法，这是一种通用范式，可以在基于Transformer的对象检测器中有效利用多尺度特征。核心思想是利用稀疏的多尺度特征从关键的几个位置进行采样，并通过两个新的设计来实现。首先，IMFA重新排列了Transformer编码器-解码器的管道，使得编码的特征可以根据检测预测进行迭代更新。其次，IMFA在先前检测预测的指导下，从关键位置稀疏地采样适应尺度的特征，以进行精确的检测。结果，采样的多尺度特征虽然稀疏，但对物体检测仍然非常有用。<br>
                    效果：大量实验表明，提出的IMFA显著提高了多个基于Transformer的对象检测器的性能，而只有轻微的计算开销。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) - a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA boosts the performance of multiple Transformer-based object detectors significantly yet with only slight computational overhead.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1159.GeoMVSNet: Learning Multi-View Stereo With Geometry Perception</span><br>
                <span class="as">Zhang, ZheandPeng, RuiandHu, YuxiandWang, Ronggang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GeoMVSNet_Learning_Multi-View_Stereo_With_Geometry_Perception_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21508-21518.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的级联多视角立体（MVS）方法在估计高分辨率深度图时，忽略了粗略阶段中嵌入的重要几何信息，导致成本匹配脆弱和重建结果次优。<br>
                    动机：为了解决上述问题，本文提出了一种名为GeoMVSNet的几何感知模型，通过明确整合粗略阶段中的几何线索来进行精细的深度估计。<br>
                    方法：我们设计了一个双分支几何融合网络，从粗略估计中提取几何先验以增强更精细阶段的结构性特征提取。此外，我们还将编码有价值深度分布属性的粗略概率体积嵌入到轻量级正则化网络中，以进一步加强深度方向的几何直观性。同时，我们应用了频域滤波来减轻高频区域带来的负面影响，并采用了课程学习策略逐步提升模型的几何集成能力。为了强化模型对全场景几何感知的能力，我们提出了基于高斯混合模型假设的深度分布相似性损失。<br>
                    效果：在DTU和Tanks and Temples（T&T）数据集上的大量实验表明，我们的GeoMVSNet实现了最先进的结果，并在T&T-Advanced集上排名第一。代码可在https://github.com/doubleZ0108/GeoMVSNet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent cascade Multi-View Stereo (MVS) methods can efficiently estimate high-resolution depth maps through narrowing hypothesis ranges. However, previous methods ignored the vital geometric information embedded in coarse stages, leading to vulnerable cost matching and sub-optimal reconstruction results. In this paper, we propose a geometry awareness model, termed GeoMVSNet, to explicitly integrate geometric clues implied in coarse stages for delicate depth estimation. In particular, we design a two-branch geometry fusion network to extract geometric priors from coarse estimations to enhance structural feature extraction at finer stages. Besides, we embed the coarse probability volumes, which encode valuable depth distribution attributes, into the lightweight regularization network to further strengthen depth-wise geometry intuition. Meanwhile, we apply the frequency domain filtering to mitigate the negative impact of the high-frequency regions and adopt the curriculum learning strategy to progressively boost the geometry integration of the model. To intensify the full-scene geometry perception of our model, we present the depth distribution similarity loss based on the Gaussian-Mixture Model assumption. Extensive experiments on DTU and Tanks and Temples (T&T) datasets demonstrate that our GeoMVSNet achieves state-of-the-art results and ranks first on the T&T-Advanced set. Code is available at https://github.com/doubleZ0108/GeoMVSNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1160.DNeRV: Modeling Inherent Dynamics via Difference Neural Representation for Videos</span><br>
                <span class="as">Zhao, QiandAsif, M.SalmanandMa, Zhan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_DNeRV_Modeling_Inherent_Dynamics_via_Difference_Neural_Representation_for_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2031-2040.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视频隐含神经表示方法未能充分利用视频中的时空冗余信息。<br>
                    动机：索引基的隐含神经表示忽略了内容特定的空间特征，混合的隐含神经表示忽略了相邻帧之间的上下文依赖性，导致对大动态或大运动场景的建模能力较差。<br>
                    方法：从函数拟合的角度分析这一局限性，揭示帧差的重要性。提出差异神经表示法（DNeRV），包括内容和帧差两个流，并引入协作内容单元进行有效的特征融合。<br>
                    效果：在视频压缩、修复和插值等任务上测试DNeRV，其结果与最先进的神经压缩方法相当，并在960 x 1920的视频的后续修复和插值任务上优于现有的隐含方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing implicit neural representation (INR) methods do not fully exploit spatiotemporal redundancies in videos. Index-based INRs ignore the content-specific spatial features and hybrid INRs ignore the contextual dependency on adjacent frames, leading to poor modeling capability for scenes with large motion or dynamics. We analyze this limitation from the perspective of function fitting and reveal the importance of frame difference. To use explicit motion information, we propose Difference Neural Representation for Videos (DNeRV), which consists of two streams for content and frame difference. We also introduce a collaborative content unit for effective feature fusion. We test DNeRV for video compression, inpainting, and interpolation. DNeRV achieves competitive results against the state-of-the-art neural compression approaches and outperforms existing implicit methods on downstream inpainting and interpolation for 960 x 1920 videos.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1161.FFF: Fragment-Guided Flexible Fitting for Building Complete Protein Structures</span><br>
                <span class="as">Chen, WeijieandWang, XinyanandWang, Yuhang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_FFF_Fragment-Guided_Flexible_Fitting_for_Building_Complete_Protein_Structures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19776-19785.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过结合片段识别和结构预测方法，解决冷冻电镜技术在构建完整蛋白质结构时面临的信号噪声比低的问题。<br>
                    动机：冷冻电镜技术可以重建生物分子的三维结构，但直接从其映射中从头开始构建蛋白质结构存在困难。AlphaFold的出现为蛋白质结构预测提供了新的思路。<br>
                    方法：提出一种名为FFF的新方法，该方法通过灵活拟合将蛋白质结构预测与蛋白质结构识别相结合。首先，使用多级识别网络从输入的3D冷冻电镜图中提取各种结构特征；然后，根据提取的特征生成蛋白质结构片段；最后，通过灵活拟合使用预测的蛋白质片段构建完整的结构模型。<br>
                    效果：基准测试结果显示，FFF在构建完整的蛋白质结构方面优于基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cryo-electron microscopy (cryo-EM) is a technique for reconstructing the 3-dimensional (3D) structure of biomolecules (especially large protein complexes and molecular assemblies). As the resolution increases to the near-atomic scale, building protein structures de novo from cryo-EM maps becomes possible. Recently, recognition-based de novo building methods have shown the potential to streamline this process. However, it cannot build a complete structure due to the low signal-to-noise ratio (SNR) problem. At the same time, AlphaFold has led to a great breakthrough in predicting protein structures. This has inspired us to combine fragment recognition and structure prediction methods to build a complete structure. In this paper, we propose a new method named FFF that bridges protein structure prediction and protein structure recognition with flexible fitting. First, a multi-level recognition network is used to capture various structural features from the input 3D cryo-EM map. Next, protein structural fragments are generated using pseudo peptide vectors and a protein sequence alignment method based on these extracted features. Finally, a complete structural model is constructed using the predicted protein fragments via flexible fitting. Based on our benchmark tests, FFF outperforms the baseline meth- ods for building complete protein structures.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>