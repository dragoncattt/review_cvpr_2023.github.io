<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
  </div>
  </div>

  <div id="sortoptions">
    To search the paper by title:
    <div class="pagination_p"> 
      <a href="search.html" >search</a> 
    </div>
    </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic6</h2>
      <b>Topic words : &ensp;</b>adversarial, &ensp;image, &ensp;images, &ensp;attacks, &ensp;noise, &ensp;robustness, &ensp;attack, &ensp;robust</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1429.TrojViT: Trojan Insertion in Vision Transformers</span><br>
                <span class="as">Zheng, MengxinandLou, QianandJiang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_TrojViT_Trojan_Insertion_in_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4025-4034.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究针对视觉转换器（ViTs）的后门攻击。<br>
                    动机：虽然传统的CNNs对后门攻击的脆弱性是众所周知的，但对ViTs的后门攻击却鲜有研究。ViTs通过补丁和注意力提取全局上下文信息，与CNNs通过卷积捕获像素级局部特征的方式不同。<br>
                    方法：本文提出了一种隐蔽且实用的针对ViTs的后门攻击——TrojViT。它生成了一种补丁级的触发器，通过补丁显著性排名和注意力目标损失在ViT的DRAM内存中存储的参数上构建了一个由一些易受攻击的位组成的木马。TrojViT进一步使用参数蒸馏来减少木马的位数。一旦攻击者通过翻转易受攻击的位将木马插入ViT模型，该模型仍然对良性输入产生正常的推理精度。但当攻击者将触发器嵌入输入时，ViT模型被迫将输入分类为预定义的目标类别。<br>
                    效果：实验表明，通过对使用众所周知的RowHammer的ViT模型翻转少数由TrojViT识别的易受攻击的位，可以将该模型转换为一个后门化的模型。在多个数据集上对各种ViT模型进行大量实验，TrojViT可以在ImageNet上通过翻转345个位使99.64%的测试图像分类为目标类别。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform backdoor attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Naively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack TrojViT. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses parameter distillation to reduce the bit number of the Trojan. Once the attacker inserts the Trojan into the ViT model by flipping the vulnerable bits, the ViT model still produces normal inference accuracy with benign inputs. But when the attacker embeds a trigger into an input, the ViT model is forced to classify the input to a predefined target class. We show that flipping only few vulnerable bits identified by TrojViT on a ViT model using the well-known RowHammer can transform the model into a backdoored one. We perform extensive experiments of multiple datasets on various ViT models. TrojViT can classify 99.64% of test images to a target class by flipping 345 bits on a ViT for ImageNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1430.WeatherStream: Light Transport Automation of Single Image Deweathering</span><br>
                <span class="as">Zhang, HowardandBa, YunhaoandYang, EthanandMehra, VaranandGella, BlakeandSuzuki, AkiraandPfahnl, ArnoldandChandrappa, ChethanChinderandWong, AlexandKadambi, Achuta</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_WeatherStream_Light_Transport_Automation_of_Single_Image_Deweathering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13499-13509.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像去雾方法受限于数据集类型，且在真实世界的各种天气效果上的表现有待提高。<br>
                    动机：为了解决这一问题，我们提出了一种名为WeatherStream的自动管道，可以捕获所有真实世界的天气效果及其对应的清晰图像对。<br>
                    方法：我们利用光传输物理原理和在初始种子数据集上训练的模型来拒绝大约99.6%的不需要的场景，并能够推广到新的场景和退化情况。<br>
                    效果：通过在这个流程中收集的数据集进行训练，我们在一个精心收集的真实世界天气效果测试集上显著提高了现有天气去除方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Today single image deweathering is arguably more sensitive to the dataset type, rather than the model. We introduce WeatherStream, an automatic pipeline capturing all real-world weather effects (rain, snow, and rain fog degradations), along with their clean image pairs. Previous state-of-the-art methods that have attempted the all-weather removal task train on synthetic pairs, and are thus limited by the Sim2Real domain gap. Recent work has attempted to manually collect time multiplexed pairs, but the use of human labor limits the scale of such a dataset. We introduce a pipeline that uses the power of light-transport physics and a model trained on a small, initial seed dataset to reject approximately 99.6% of unwanted scenes. The pipeline is able to generalize to new scenes and degradations that can, in turn, be used to train existing models just like fully human-labeled data. Training on a dataset collected through this procedure leads to significant improvements on multiple existing weather removal methods on a carefully human-collected test set of real-world weather effects. The dataset and code can be found in the following website: http://visual.ee.ucla.edu/wstream.htm/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1431.Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations</span><br>
                <span class="as">Hsiung, LeiandTsai, Yun-YunandChen, Pin-YuandHo, Tsung-Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsiung_Towards_Compositional_Adversarial_Robustness_Generalizing_Adversarial_Training_to_Composite_Semantic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24658-24667.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高模型对多种语义扰动及其组合的鲁棒性，特别是在现实场景中。<br>
                    动机：虽然现有的对抗性训练方法在处理单一类型的对抗性扰动（如Lp-norm）上表现良好，但在面对更复杂的多种语义扰动及其组合时，其效果往往不佳。<br>
                    方法：本文提出了一种新的生成复合对抗性样本的方法，通过组件化的投影梯度下降和自动攻击顺序调度找到最优的攻击组合。同时，提出了广义对抗性训练（GAT）方法，将模型的鲁棒性从Lp-ball扩展到复合语义扰动，如色调、饱和度、亮度、对比度和旋转的组合。<br>
                    效果：在ImageNet和CIFAR-10数据集上的实验结果表明，GAT不仅对所有测试过的单一攻击类型具有鲁棒性，而且对任何这类攻击的组合也具有鲁棒性。此外，GAT的性能明显优于基于L-infinity-norm的基线对抗性训练方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model robustness against adversarial examples of single perturbation type such as the Lp-norm has been widely studied, yet its generalization to more realistic scenarios involving multiple semantic perturbations and their composition remains largely unexplored. In this paper, we first propose a novel method for generating composite adversarial examples. Our method can find the optimal attack composition by utilizing component-wise projected gradient descent and automatic attack-order scheduling. We then propose generalized adversarial training (GAT) to extend model robustness from Lp-ball to composite semantic perturbations, such as the combination of Hue, Saturation, Brightness, Contrast, and Rotation. Results obtained using ImageNet and CIFAR-10 datasets indicate that GAT can be robust not only to all the tested types of a single attack, but also to any combination of such attacks. GAT also outperforms baseline L-infinity-norm bounded adversarial training approaches by a significant margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1432.Structured Kernel Estimation for Photon-Limited Deconvolution</span><br>
                <span class="as">Sanghvi, YashandMao, ZhiyuanandChan, StanleyH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sanghvi_Structured_Kernel_Estimation_for_Photon-Limited_Deconvolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9863-9872.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善在低光照和相机抖动条件下拍摄的图像，特别是在强光子散粒噪声存在的情况下。<br>
                    动机：尽管现有的图像恢复网络在一些场景中表现出色，但它们主要适用于光照充足的环境，并且在光子散粒噪声强烈时性能会显著下降。<br>
                    方法：提出一种新的模糊估计技术，专门针对光子限制条件进行优化。该方法采用基于梯度的反向传播方法来估计模糊核，并通过使用运动轨迹上关键点的低维表示来对模糊核进行建模，从而大大减小了搜索空间并提高了内核估计问题的规律性。<br>
                    效果：当插入到迭代框架中时，这种新颖的低维表示提供了改进的内核估计，从而在与端到端训练的神经网络相比时，显著提高了去卷积性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Images taken in a low light condition with the presence of camera shake suffer from motion blur and photon shot noise. While state-of-the-art image restoration networks show promising results, they are largely limited to well-illuminated scenes and their performance drops significantly when photon shot noise is strong. In this paper, we propose a new blur estimation technique customized for photon-limited conditions. The proposed method employs a gradient-based backpropagation method to estimate the blur kernel. By modeling the blur kernel using a low-dimensional representation with the key points on the motion trajectory, we significantly reduce the search space and improve the regularity of the kernel estimation problem. When plugged into an iterative framework, our novel low-dimensional representation provides improved kernel estimates and hence significantly better deconvolution performance when compared to end-to-end trained neural networks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1433.Minimizing Maximum Model Discrepancy for Transferable Black-Box Targeted Attacks</span><br>
                <span class="as">Zhao, AnqiandChu, TongandLiu, YahaoandLi, WenandLi, JingjingandDuan, Lixin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Minimizing_Maximum_Model_Discrepancy_for_Transferable_Black-Box_Targeted_Attacks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8153-8162.png><br>
            
            <span class="tt"><span class="t0">研究问题：从模型差异性角度研究黑盒目标攻击问题。<br>
                    动机：现有的黑盒目标攻击方法存在成功率不高的问题，需要一种能够保证攻击成功的理论分析方法。<br>
                    方法：提出了一种新的基于模型差异性的黑盒目标攻击算法，通过最小化替代模型的最大模型差异性来生成对抗样本，从而提高攻击成功率。<br>
                    效果：在ImageNet数据集上进行了广泛的实验，并与其他现有方法进行比较，结果表明该方法具有更高的成功率和更强的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we study the black-box targeted attack problem from the model discrepancy perspective. On the theoretical side, we present a generalization error bound for black-box targeted attacks, which gives a rigorous theoretical analysis for guaranteeing the success of the attack. We reveal that the attack error on a target model mainly depends on empirical attack error on the substitute model and the maximum model discrepancy among substitute models. On the algorithmic side, we derive a new algorithm for black-box targeted attacks based on our theoretical analysis, in which we additionally minimize the maximum model discrepancy(M3D) of the substitute models when training the generator to generate adversarial examples. In this way, our model is capable of crafting highly transferable adversarial examples that are robust to the model variation, thus improving the success rate for attacking the black-box model. We conduct extensive experiments on the ImageNet dataset with different classification models, and our proposed approach outperforms existing state-of-the-art methods by a significant margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1434.Deep Random Projector: Accelerated Deep Image Prior</span><br>
                <span class="as">Li, TaihuiandWang, HengkangandZhuang, ZhongandSun, Ju</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Deep_Random_Projector_Accelerated_Deep_Image_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18176-18185.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度图像先验（DIP）在处理各种图像恢复和一般视觉逆问题的速度，以适应时间敏感的场景。<br>
                    动机：尽管深度图像先验（DIP）在无需训练数据的情况下，已在解决各种图像恢复和一般视觉逆问题上表现出巨大潜力，但其优化过程通常非常缓慢，这不可避免地阻碍了DIP在时间敏感场景中的实际应用。<br>
                    方法：本文针对图像恢复问题，对DIP进行了两项关键修改以实现显著的加速：1）在冻结随机初始化的网络权重的同时优化DIP种子；2）减少网络深度。此外，我们重新引入了明确的先验，如稀疏梯度先验——通过总变差正则化进行编码，以保持DIP的最佳性能。<br>
                    效果：我们在三个图像恢复任务上评估了所提出的方法，包括图像去噪、图像超分辨率和图像修复，与原始DIP及其变体以及使用元学习从额外数据中学习良好初始化器的竞争对手metaDIP进行了比较。我们的方法在最短的时间内获得了具有竞争力的恢复质量，明显胜出。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep image prior (DIP) has shown great promise in tackling a variety of image restoration (IR) and general visual inverse problems, needing no training data. However, the resulting optimization process is often very slow, inevitably hindering DIP's practical usage for time-sensitive scenarios. In this paper, we focus on IR, and propose two crucial modifications to DIP that help achieve substantial speedup: 1) optimizing the DIP seed while freezing randomly-initialized network weights, and 2) reducing the network depth. In addition, we reintroduce explicit priors, such as sparse gradient prior---encoded by total-variation regularization, to preserve the DIP peak performance. We evaluate the proposed method on three IR tasks, including image denoising, image super-resolution, and image inpainting, against the original DIP and variants, as well as the competing metaDIP that uses meta-learning to learn good initializers with extra data. Our method is a clear winner in obtaining competitive restoration quality in a minimal amount of time. Our code is available at https://github.com/sun-umn/Deep-Random-Projector.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1435.Revisiting Residual Networks for Adversarial Robustness</span><br>
                <span class="as">Huang, ShihuaandLu, ZhichaoandDeb, KalyanmoyandBoddeti, VishnuNaresh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Revisiting_Residual_Networks_for_Adversarial_Robustness_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8202-8211.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在填补现有研究中对卷积神经网络对抗鲁棒性提升方法的大部分关注，以及在架构设计元素（如拓扑、深度和宽度）对其影响方面的研究不足。<br>
                    动机：大多数的研究都集中在开发更有效的对抗训练方法来提高卷积神经网络的对抗鲁棒性，而对于架构设计元素如何影响对抗鲁棒性的关注却相对较少。<br>
                    方法：本研究以残差网络为研究对象，分别从块级别和网络规模级别考虑架构设计。首先通过系统实验得出一些初步的见解，然后设计了一个名为RobustResBlock的鲁棒残差块和一个名为RobustScaling的复合缩放规则，用于在期望的浮点运算数下分配深度和宽度。最后，将RobustResBlock和RobustScaling结合起来，提出了一系列具有广泛模型容量的对抗鲁棒残差网络RobustResNets。<br>
                    效果：通过对多个数据集和对抗攻击进行实验验证，RobustResNets在性能上始终优于标准的WRNs和其他现有的健壮架构，实现了最先进的AutoAttack鲁棒准确度63.7%，并且在参数数量上比其他方法紧凑2倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Efforts to improve the adversarial robustness of convolutional neural networks have primarily focused on developing more effective adversarial training methods. In contrast, little attention was devoted to analyzing the role of architectural elements (e.g., topology, depth, and width) on adversarial robustness. This paper seeks to bridge this gap and present a holistic study on the impact of architectural design on adversarial robustness. We focus on residual networks and consider architecture design at the block level as well as at the network scaling level. In both cases, we first derive insights through systematic experiments. Then we design a robust residual block, dubbed RobustResBlock, and a compound scaling rule, dubbed RobustScaling, to distribute depth and width at the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling and present a portfolio of adversarially robust residual networks, RobustResNets, spanning a broad spectrum of model capacities. Experimental validation across multiple datasets and adversarial attacks demonstrate that RobustResNets consistently outperform both the standard WRNs and other existing robust architectures, achieving state-of-the-art AutoAttack robust accuracy 63.7% with 500K external data while being 2x more compact in terms of parameters. The code is available at https://github.com/zhichao-lu/robust-residual-network.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1436.How to Backdoor Diffusion Models?</span><br>
                <span class="as">Chou, Sheng-YenandChen, Pin-YuandHo, Tsung-Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4015-4024.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究扩散模型在面对后门攻击时的鲁棒性。<br>
                    动机：扩散模型是最先进的深度学习生成模型，但对其在面对后门攻击时可能存在的局限性和风险了解不足。<br>
                    方法：提出了BadDiffusion攻击框架，通过在模型训练过程中植入后门来实施攻击。<br>
                    效果：实验结果表明，BadDiffusion能有效地创建出具有高实用性和目标特异性的受损扩散模型，且只需对预训练的干净扩散模型进行微调即可实现后门植入。同时，还探索了一些可能的风险缓解对策。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consistently lead to compromised diffusion models with high utility and target specificity. Even worse, BadDiffusion can be made cost-effective by simply finetuning a clean pre-trained diffusion model to implant backdoors. We also explore some possible countermeasures for risk mitigation. Our results call attention to potential risks and possible misuse of diffusion models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1437.Revisiting the Stack-Based Inverse Tone Mapping</span><br>
                <span class="as">Zhang, NingandYe, YuyaoandZhao, YangandWang, Ronggang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Revisiting_the_Stack-Based_Inverse_Tone_Mapping_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9162-9171.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于堆栈的逆色调映射（ITM）方法通过从单张低研究问题：现有的基于堆栈的逆色调映射（ITM）方法通过从单张低动态范围图像预测一组多曝光图像来恢复高动态范围（HDR）辐射度，但存在一些限制。<br>
                    动机：一方面，这些方法估计固定数量的图像（例如，三张曝光上和三张曝光下），可能会引入不必要的计算成本或重建不正确的结果。另一方面，它们忽视了曝光上和曝光下的模型之间的连接，因此无法充分挖掘有效特征。<br>
                    方法：我们重新审视了基于堆栈的ITM方法，并提出了一种从单张图像重建HDR辐射度的新方法，只需估计两张曝光图像。首先，我们设计了一个曝光自适应块，该块可以根据输入图像的亮度分布自适应调整曝光。其次，我们设计了一个跨模型注意块来连接曝光调整模型。第三，我们提出了一个端到端的ITM管道，通过集成多曝光融合模型。此外，我们还提出了并开放了一个多曝光数据集，该数据集指示了最优的曝光上/下水平。<br>
                    效果：实验结果表明，我们提出的方法优于一些最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current stack-based inverse tone mapping (ITM) methods can recover high dynamic range (HDR) radiance by predicting a set of multi-exposure images from a single low dynamic range image. However, there are still some limitations. On the one hand, these methods estimate a fixed number of images (e.g., three exposure-up and three exposure-down), which may introduce unnecessary computational cost or reconstruct incorrect results. On the other hand, they neglect the connections between the up-exposure and down-exposure models and thus fail to fully excavate effective features. In this paper, we revisit the stack-based ITM approaches and propose a novel method to reconstruct HDR radiance from a single image, which only needs to estimate two exposure images. At first, we design the exposure adaptive block that can adaptively adjust the exposure based on the luminance distribution of the input image. Secondly, we devise the cross-model attention block to connect the exposure adjustment models. Thirdly, we propose an end-to-end ITM pipeline by incorporating the multi-exposure fusion model. Furthermore, we propose and open a multi-exposure dataset that indicates the optimal exposure-up/down levels. Experimental results show that the proposed method outperforms some state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1438.Backdoor Defense via Deconfounded Representation Learning</span><br>
                <span class="as">Zhang, ZaixiandLiu, QiandWang, ZhicaiandLu, ZepuandHu, Qingyong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Backdoor_Defense_via_Deconfounded_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12228-12238.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络易受后门攻击，尽管已有许多方法检测和移除后门，但研究问题：深度神经网络易受后门攻击，尽管已有许多方法检测和移除后门，但如何从被污染的数据集直接获取无后门的干净模型仍不清楚。<br>
                    动机：通过构建一个因果关系图来模拟被污染数据的产生过程，发现后门攻击就像一个混淆因素，在输入图像和目标标签之间引入了虚假关联，使模型预测变得不可靠。<br>
                    方法：受因果关系启发，提出因果关系启发的后门防御（CBD），通过前门调整学习去混淆表示。具体来说，故意训练一个带有后门的模型以捕获混淆效应，而另一个干净的模型则通过最小化与带有后门模型的混淆表示的互信息并采用样本加权方案来捕获期望的因果关系。<br>
                    效果：在多个基准数据集上进行的大量实验表明，所提出的防御方法在减少后门威胁的同时保持对良性样本的高预测精度方面是有效的。进一步的分析还表明，CBD还可以抵抗潜在的适应性攻击。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by injecting a few poisoned examples into the training dataset. While extensive efforts have been made to detect and remove backdoors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input images and target labels, making the model predictions less reliable. Inspired by the causal understanding, we propose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations by employing the front-door adjustment. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employing a sample-wise re-weighting scheme. Extensive experiments on multiple benchmark datasets against 6 state-of-the-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further analysis shows that CBD can also resist potential adaptive attacks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1439.Color Backdoor: A Robust Poisoning Attack in Color Space</span><br>
                <span class="as">Jiang, WenboandLi, HongweiandXu, GuowenandZhang, Tianwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8133-8142.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决针对神经网络的后门攻击问题，特别是如何使攻击更难以察觉和防御。<br>
                    动机：现有的后门攻击方法往往牺牲了模型的鲁棒性，容易被常见的预处理防御手段击败。<br>
                    方法：本文提出了一种新的颜色后门攻击方法，通过对所有像素应用统一的色空间偏移作为触发器，实现了攻击的鲁棒性和隐蔽性。同时，利用PSNR、SSIM和LPIPS等指标定义自然度限制，并采用粒子群优化（PSO）算法寻找最优触发器。<br>
                    效果：实验结果表明，该方法在多种主流后门防御手段下仍具有优越性和鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Backdoor attacks against neural networks have been intensively investigated, where the adversary compromises the integrity of the victim model, causing it to make wrong predictions for inference samples containing a specific trigger. To make the trigger more imperceptible and human-unnoticeable, a variety of stealthy backdoor attacks have been proposed, some works employ imperceptible perturbations as the backdoor triggers, which restrict the pixel differences of the triggered image and clean image. Some works use special image styles (e.g., reflection, Instagram filter) as the backdoor triggers. However, these attacks sacrifice the robustness, and can be easily defeated by common preprocessing-based defenses. This paper presents a novel color backdoor attack, which can exhibit robustness and stealthiness at the same time. The key insight of our attack is to apply a uniform color space shift for all pixels as the trigger. This global feature is robust to image transformation operations and the triggered samples maintain natural-looking. To find the optimal trigger, we first define naturalness restrictions through the metrics of PSNR, SSIM and LPIPS. Then we employ the Particle Swarm Optimization (PSO) algorithm to search for the optimal trigger that can achieve high attack effectiveness and robustness while satisfying the restrictions. Extensive experiments demonstrate the superiority of PSO and the robustness of color backdoor against different mainstream backdoor defenses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1440.Learning Distortion Invariant Representation for Image Restoration From a Causality Perspective</span><br>
                <span class="as">Li, XinandLi, BingchenandJin, XinandLan, CuilingandChen, Zhibo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Learning_Distortion_Invariant_Representation_for_Image_Restoration_From_a_Causality_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1714-1724.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度神经网络在图像恢复中存在对不同类型或程度的实际退化泛化能力差的问题。<br>
                    动机：为了提高深度神经网络对未知退化的泛化能力，提出了一种新的训练策略。<br>
                    方法：提出一种名为“畸变不变表示学习”（DIL）的新方法，将每种畸变类型和程度视为一个特定的混杂因素，并通过消除每种退化的有害混杂效应来学习畸变不变的表示。通过从优化角度模拟不同畸变的干预来实现基于因果关系的后门准则。<br>
                    效果：大量实验表明，我们的DIL在未见过畸变类型和程度上具有良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, we have witnessed the great advancement of Deep neural networks (DNNs) in image restoration. However, a critical limitation is that they cannot generalize well to real-world degradations with different degrees or types. In this paper, we are the first to propose a novel training strategy for image restoration from the causality perspective, to improve the generalization ability of DNNs for unknown degradations. Our method, termed Distortion Invariant representation Learning (DIL), treats each distortion type and degree as one specific confounder, and learns the distortion-invariant representation by eliminating the harmful confounding effect of each degradation. We derive our DIL with the back-door criterion in causality by modeling the interventions of different distortions from the optimization perspective. Particularly, we introduce counterfactual distortion augmentation to simulate the virtual distortion types and degrees as the confounders. Then, we instantiate the intervention of each distortion with a virtual model updating based on corresponding distorted images, and eliminate them from the meta-learning perspective. Extensive experiments demonstrate the generalization capability of our DIL on unseen distortion types and degrees. Our code will be available at https://github.com/lixinustc/Causal-IR-DIL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1441.Learning a Simple Low-Light Image Enhancer From Paired Low-Light Instances</span><br>
                <span class="as">Fu, ZhenqiandYang, YanandTu, XiaotongandHuang, YueandDing, XinghaoandMa, Kai-Kuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_a_Simple_Low-Light_Image_Enhancer_From_Paired_Low-Light_Instances_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22252-22261.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在低光环境下拍摄的图像对比度差和细节丢失的问题。<br>
                    动机：现有的低光图像增强算法通常使用单一输入图像和一些手工制作的先验来调整光照，但这些方法往往无法充分揭示图像细节，因为单一图像的信息有限，手工制作的先验适应性差。<br>
                    方法：我们提出了PairLIE，一种无监督的方法，从低光图像对中学习自适应先验。首先，网络需要生成与两个输入相同的清晰图像，因为它们共享相同的图像内容。为了实现这一点，我们在网络中引入了Retinex理论，并使两个反射成分一致。其次，为了协助Retinex分解，我们提出了一个简单的自我监督机制来去除原始图像中的不适当特征。<br>
                    效果：大量的实验表明，提出的PairLIE在公共数据集上取得了与最先进的方法相当的性能，同时具有更简单的网络和更少的手工制作先验。代码可在以下链接获取：https://github.com/zhenqifu/PairLIE。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Low-light Image Enhancement (LIE) aims at improving contrast and restoring details for images captured in low-light conditions. Most of the previous LIE algorithms adjust illumination using a single input image with several handcrafted priors. Those solutions, however, often fail in revealing image details due to the limited information in a single image and the poor adaptability of handcrafted priors. To this end, we propose PairLIE, an unsupervised approach that learns adaptive priors from low-light image pairs. First, the network is expected to generate the same clean images as the two inputs share the same image content. To achieve this, we impose the network with the Retinex theory and make the two reflectance components consistent. Second, to assist the Retinex decomposition, we propose to remove inappropriate features in the raw image with a simple self-supervised mechanism. Extensive experiments on public datasets show that the proposed PairLIE achieves comparable performance against the state-of-the-art approaches with a simpler network and fewer handcrafted priors. Code is available at: https://github.com/zhenqifu/PairLIE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1442.Backdoor Cleansing With Unlabeled Data</span><br>
                <span class="as">Pang, LuandSun, TaoandLing, HaibinandChen, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Backdoor_Cleansing_With_Unlabeled_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12218-12227.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何防御外部训练的深度神经网络可能受到的后门攻击，即如何在不破坏研究问题：如何防御外部训练的深度神经网络可能受到的后门攻击，即如何在不破坏模型对干净输入的正常预测能力的情况下，消除其异常的后门行为。<br>
                    动机：由于深度神经网络的计算需求日益增加，公司和组织开始将训练过程外包。然而，外部训练的DNNs可能会受到后门攻击。因此，防御这种攻击至关重要。<br>
                    方法：本文提出了一种新的防御方法，不需要训练标签。通过精心设计的逐层权重重新初始化和知识蒸馏，该方法可以有效地清除可疑网络的后门行为，同时在其正常行为上的影响可以忽略不计。<br>
                    效果：实验表明，该方法在没有标签的情况下训练，与使用标签训练的最新防御方法相当。即使在分布外的数据上，也观察到了有希望的防御结果。这使得该方法非常实用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e, to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network  with negligible compromise in  its normal behavior. In experiments, we show that our method, trained without labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https://github.com/luluppang/BCU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1443.On the Difficulty of Unpaired Infrared-to-Visible Video Translation: Fine-Grained Content-Rich Patches Transfer</span><br>
                <span class="as">Yu, ZhenjieandLi, ShuangandShen, YiruiandLiu, ChiHaroldandWang, Shuigen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_On_the_Difficulty_of_Unpaired_Infrared-to-Visible_Video_Translation_Fine-Grained_Content-Rich_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1631-1640.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将红外视频转换为具有精细语义模式的清晰可见视频，以填补视觉差距。<br>
                    动机：现有的视觉模型主要在大量清晰的可见数据上进行训练，当部署到红外成像场景时，面临巨大的视觉差距。<br>
                    方法：提出一种新的CPTrans框架，通过平衡不同补丁的梯度来解决这个问题，实现内容丰富的补丁传输。具体来说，内容感知优化模块鼓励模型沿着目标补丁的梯度进行优化，确保视觉细节的改善。此外，内容感知的时间规范化模块强制生成器对目标补丁的运动具有鲁棒性。<br>
                    效果：实验表明，所提出的CPTrans在各种场景下实现了最先进的性能，同时比竞争方法需要更少的训练时间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Explicit visible videos can provide sufficient visual information and facilitate vision applications. Unfortunately, the image sensors of visible cameras are sensitive to light conditions like darkness or overexposure. To make up for this, recently, infrared sensors capable of stable imaging have received increasing attention in autonomous driving and monitoring. However, most prosperous vision models are still trained on massive clear visible data, facing huge visual gaps when deploying to infrared imaging scenarios. In such cases, transferring the infrared video to a distinct visible one with fine-grained semantic patterns is a worthwhile endeavor. Previous works improve the outputs by equally optimizing each patch on the translated visible results, which is unfair for enhancing the details on content-rich patches due to the long-tail effect of pixel distribution. Here we propose a novel CPTrans framework to tackle the challenge via balancing gradients of different patches, achieving the fine-grained Content-rich Patches Transferring. Specifically, the content-aware optimization module encourages model optimization along gradients of target patches, ensuring the improvement of visual details. Additionally, the content-aware temporal normalization module enforces the generator to be robust to the motions of target patches. Moreover, we extend the existing dataset InfraredCity to more challenging adverse weather conditions (rain and snow), dubbed as InfraredCity-Adverse. Extensive experiments show that the proposed CPTrans achieves state-of-the-art performance under diverse scenes while requiring less training time than competitive methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1444.Nighttime Smartphone Reflective Flare Removal Using Optical Center Symmetry Prior</span><br>
                <span class="as">Dai, YuekunandLuo, YihangandZhou, ShangchenandLi, ChongyiandLoy, ChenChange</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Nighttime_Smartphone_Reflective_Flare_Removal_Using_Optical_Center_Symmetry_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20783-20791.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地消除照片中的反射光斑。<br>
                    动机：现有的消除反射光斑的方法往往依赖于手动设计的特征，无法准确识别由各种类型的光产生的反射光斑，且在多光源场景中可能会误删除光源。<br>
                    方法：提出一种光学中心对称先验，即反射光斑和光源总是围绕镜头的光学中心对称，用于更准确地定位反射光斑的提议区域。基于此先验创建了首个反射光斑去除数据集BracketFlare，并使用连续包围来捕捉曝光不足图像中的反射光斑模式，结合正常曝光的图像合成一对有光斑和无光斑的图像。<br>
                    效果：实验表明，该方法在合成和真实世界数据集上都能有效去除反射光斑。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reflective flare is a phenomenon that occurs when light reflects inside lenses, causing bright spots or a "ghosting effect" in photos, which can impact their quality. Eliminating reflective flare is highly desirable but challenging. Many existing methods rely on manually designed features to detect these bright spots, but they often fail to identify reflective flares created by various types of light and may even mistakenly remove the light sources in scenarios with multiple light sources. To address these challenges, we propose an optical center symmetry prior, which suggests that the reflective flare and light source are always symmetrical around the lens's optical center. This prior helps to locate the reflective flare's proposal region more accurately and can be applied to most smartphone cameras. Building on this prior, we create the first reflective flare removal dataset called BracketFlare, which contains diverse and realistic reflective flare patterns. We use continuous bracketing to capture the reflective flare pattern in the underexposed image and combine it with a normally exposed image to synthesize a pair of flare-corrupted and flare-free images. With the dataset, neural networks can be trained to remove the reflective flares effectively. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1445.Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability</span><br>
                <span class="as">Ramaswamy, VikramV.andKim, SunnieS.Y.andFong, RuthandRussakovsky, Olga</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramaswamy_Overlooked_Factors_in_Concept-Based_Explanations_Dataset_Choice_Concept_Learnability_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10932-10941.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决概念基础的解释方法在深度神经网络模型解释中存在的三个被忽视的因素。<br>
                    动机：尽管概念基础的解释方法在深度学习模型解释中广泛应用，但其存在一些未被充分理解和阐述的限制。<br>
                    方法：通过对训练好的模型在新“探针”数据集上进行评估，并将模型的输出与该数据集中标记的概念相关联，来识别和分析这三个常被忽视的因素。<br>
                    效果：研究发现，选择不同的探针数据集会导致完全不同的解释，说明生成的解释不能推广到探针数据集之外；探针数据集中的概念通常比它们用于解释的目标类别更难学习，这引发了对解释正确性的质疑；人类研究表明，概念基础的解释最多只能使用32个或更少的概念，超过这个数量，解释的实用性大大降低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Concept-based interpretability methods aim to explain a deep neural network model's components and predictions using a pre-defined set of semantic concepts. These methods evaluate a trained model on a new, "probe" dataset and correlate the model's outputs with concepts labeled in that dataset. Despite their popularity, they suffer from limitations that are not well-understood and articulated in the literature. In this work, we identify and analyze three commonly overlooked factors in concept-based explanations. First, we find that the choice of the probe dataset has a profound impact on the generated explanations. Our analysis reveals that different probe datasets lead to very different explanations, suggesting that the generated explanations are not generalizable outside the probe dataset. Second, we find that concepts in the probe dataset are often harder to learn than the target classes they are used to explain, calling into question the correctness of the explanations. We argue that only easily learnable concepts should be used in concept-based explanations. Finally, while existing methods use hundreds or even thousands of concepts, our human studies reveal a much stricter upper bound of 32 concepts or less, beyond which the explanations are much less practically useful. We discuss the implications of our findings and provide suggestions for future development of concept-based interpretability methods. Code for our analysis and user interface can be found at https://github.com/princetonvisualai/OverlookedFactors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1446.Jedi: Entropy-Based Localization and Removal of Adversarial Patches</span><br>
                <span class="as">Tarchoun, BilelandBenKhalifa, AnouarandMahjoub, MohamedAliandAbu-Ghazaleh, NaelandAlouani, Ihsen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tarchoun_Jedi_Entropy-Based_Localization_and_Removal_of_Adversarial_Patches_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4087-4095.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地防御现实世界中的对抗性物理补丁攻击，提高模型的检测和恢复能力？<br>
                    动机：现有的基于输入梯度或特征分析的最有力的防御措施已被最近的基于GAN的自适应攻击所破坏，这些攻击能生成真实/自然主义的补丁。<br>
                    方法：本文提出了Jedi，一种新的对抗性补丁防御方法，对真实的补丁攻击具有抵抗力，并且与最先进的技术相比，提高了检测和恢复能力。Jedi利用了两个新的想法：（1）通过熵分析改进潜在补丁区域的识别；（2）使用一个能够完成补丁区域并过滤掉非补丁的高熵正常区域的自动编码器，来改进对抗性补丁的定位。<br>
                    效果：Jedi实现了高精度的对抗性补丁定位，这对于成功修复图像至关重要。由于Jedi依赖于输入熵分析，因此它是模型无关的，可以在不改变受保护模型的训练或推理的情况下应用于预先训练好的现成模型。Jedi在各种基准测试中平均检测到90%的对抗性补丁，并恢复了高达94%的成功补丁攻击（相比之下，LGS和Jujutsu分别为75%和65%）。即使在其他防御措施无法识别的自适应真实补丁存在的情况下，Jedi也能够继续进行检测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-world adversarial physical patches were recently shown to be successful in compromising state-of-the-art models in a variety of computer vision applications. The most promising defenses that are based on either input gradient or features analyses have been shown to be compromised by recent GAN-based adaptive attacks that generate realistic/naturalistic patches. In this paper, we propose Jedi, a new defense against adversarial patches that is resilient to realistic patch attacks, and also improves detection and recovery compared to the state of the art. Jedi leverages two new ideas: (1) it improves the identification of potential patch regions using entropy analysis: we show that the entropy of adversarial patches is high, even in naturalistic patches; and (2) it improves the localization of adversarial patches, using an autoencoder that is able to complete patch regions and filter out normal regions with high entropy that are not part of a patch. Jedi achieves high precision adversarial patch localization, which we show is critical to successfully repair the images. Since Jedi relies on an input entropy analysis, it is model-agnostic, and can be applied on pre-trained off-the-shelf models without changes to the training or inference of the protected models. Jedi detects on average 90% of adversarial patches across different benchmarks and recovers up to 94% of successful patch attacks (Compared to 75% and 65% for LGS and Jujutsu, respectively). Jedi is also able to continue detection even in the presence of adaptive realistic patches that are able to fool other defenses.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1447.Spatial-Temporal Concept Based Explanation of 3D ConvNets</span><br>
                <span class="as">Ji, YingandWang, YuandKato, Jien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Spatial-Temporal_Concept_Based_Explanation_of_3D_ConvNets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15444-15453.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决卷积神经网络（CNN）在视频识别任务中的解释性和透明度问题。<br>
                    动机：尽管CNN在各种任务上表现出色，但其决策过程缺乏透明度和可解释性，这限制了其性能的进一步提升。因此，近年来，人们对提供CNN的解释性和可解释性产生了极大的兴趣。<br>
                    方法：本文提出了一种基于空间-时间概念的解释（STCE）框架，用于解释3D ConvNets。在这个框架中，(1) 视频被表示为高级超像素，相似的超像素被聚类为一个概念，这对人来说很容易理解；(2) 解释框架为每个概念计算一个分数，该分数反映了概念在ConvNet决策过程中的重要性。<br>
                    效果：实验证明，该方法能够识别出具有不同重要性水平的概念，使我们能够深入调查这些概念对目标任务（如动作识别）的影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Convolutional neural networks (CNNs) have shown remarkable performance on various tasks. Despite its widespread adoption, the decision procedure of the network still lacks transparency and interpretability, making it difficult to enhance the performance further. Hence, there has been considerable interest in providing explanation and interpretability for CNNs over the last few years. Explainable artificial intelligence (XAI) investigates the relationship between input images or videos and output predictions. Recent studies have achieved outstanding success in explaining 2D image classification ConvNets. On the other hand, due to the high computation cost and complexity of video data, the explanation of 3D video recognition ConvNets is relatively less studied. And none of them are able to produce a high-level explanation. In this paper, we propose a STCE (Spatial-temporal Concept-based Explanation) framework for interpreting 3D ConvNets. In our approach: (1) videos are represented with high-level supervoxels, similar supervoxels are clustered as a concept, which is straightforward for human to understand; and (2) the interpreting framework calculates a score for each concept, which reflects its significance in the ConvNet decision procedure. Experiments on diverse 3D ConvNets demonstrate that our method can identify global concepts with different importance levels, allowing us to investigate the impact of the concepts on a target task, such as action recognition, in-depth. The source codes are publicly available at https://github.com/yingji425/STCE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1448.Metadata-Based RAW Reconstruction via Implicit Neural Functions</span><br>
                <span class="as">Li, LeyiandQiao, HuijieandYe, QiandYang, Qinmin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Metadata-Based_RAW_Reconstruction_via_Implicit_Neural_Functions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18196-18205.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何充分利用元数据，将RAW图像从sRGB图像中重建出来。<br>
                    动机：目前的工作中，虽然可以将RAW图像嵌入到sRGB图像中，但存在一些限制，不能完全利用元数据。<br>
                    方法：本文提出了一种新的方法，将元数据的二维坐标映射到其对应的RAW值上，并使用隐式神经网络函数重建RAW图像。<br>
                    效果：实验结果表明，该方法仅通过均匀采样即可实现显著的性能提升（平均PSNR超过10dB），并且不需要在不同相机ISP上进行预训练。此外，该方法还适用于引导超分辨率任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many low-level computer vision tasks are desirable to utilize the unprocessed RAW image as input, which remains the linear relationship between pixel values and scene radiance. Recent works advocate to embed the RAW image samples into sRGB images at capture time, and reconstruct the RAW from sRGB by these metadata when needed. However, there still exist some limitations on taking full use of the metadata. In this paper, instead of following the perspective of sRGB-to-RAW mapping, we reformulate the problem as mapping the 2D coordinates of the metadata to its RAW values conditioned on the corresponding sRGB values. With this novel formulation, we propose to reconstruct the RAW image with an implicit neural function, which achieves significant performance improvement (more than 10dB average PSNR) only with the uniform sampling. Compared with most deep learning-based approaches, our method is trained in a self-supervised way that requiring no pre-training on different camera ISPs. We perform further experiments to demonstrate the effectiveness of our method, and show that our framework is also suitable for the task of guided super-resolution.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1449.Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition</span><br>
                <span class="as">Yang, XiaoandLiu, ChangandXu, LonglongandWang, YikaiandDong, YinpengandChen, NingandSu, HangandZhu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Towards_Effective_Adversarial_Textured_3D_Meshes_on_Physical_Face_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4119-4128.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种更可靠的技术，对商业系统的对抗鲁棒性进行端到端评估。<br>
                    动机：现有的物理攻击要么容易被检测，要么对商业识别系统无效。<br>
                    方法：设计了一种具有复杂拓扑结构的人类面部的对抗性纹理3D网格（AT3D），可以打印并粘贴在攻击者的脸上以逃避防御。同时，提出了基于3D Morphable Model的低维系数空间扰动，显著提高了黑盒转移性，同时享受更快的搜索效率和更好的视觉质量。<br>
                    效果：广泛的数字和物理实验表明，该方法有效地探索了多个流行的商业服务的安全漏洞，包括三个识别API、四个反欺骗API、两个流行的手机和两个自动化门禁系统。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face recognition is a prevailing authentication solution in numerous biometric applications. Physical adversarial attacks, as an important surrogate, can identify the weaknesses of face recognition systems and evaluate their robustness before deployed. However, most existing physical attacks are either detectable readily or ineffective against commercial recognition systems. The goal of this work is to develop a more reliable technique that can carry out an end-to-end evaluation of adversarial robustness for commercial systems. It requires that this technique can simultaneously deceive black-box recognition models and evade defensive mechanisms. To fulfill this, we design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be 3D-printed and pasted on the attacker's face to evade the defenses. However, the mesh-based optimization regime calculates gradients in high-dimensional mesh space, and can be trapped into local optima with unsatisfactory transferability. To deviate from the mesh-based space, we propose to perturb the low-dimensional coefficient space based on 3D Morphable Model, which significantly improves black-box transferability meanwhile enjoying faster search efficiency and better visual quality. Extensive experiments in digital and physical scenarios show that our method effectively explores the security vulnerabilities of multiple popular commercial services, including three recognition APIs, four anti-spoofing APIs, two prevailing mobile phones and two automated access control systems.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1450.Effective Ambiguity Attack Against Passport-Based DNN Intellectual Property Protection Schemes Through Fully Connected Layer Substitution</span><br>
                <span class="as">Chen, YimingandTian, JinyuandChen, XiangyuandZhou, Jiantao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Effective_Ambiguity_Attack_Against_Passport-Based_DNN_Intellectual_Property_Protection_Schemes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8123-8132.png><br>
            
            <span class="tt"><span class="t0">研究问题：评估基于护照方法的深度模型知识产权保护的安全性。<br>
                    动机：深度神经网络的训练成本高，训练好的模型被视为有价值的知识产权。近年来，与深度模型相关的知识产权保护越来越受到关注。<br>
                    方法：提出一种新的有效模糊攻击方法，通过在护照参数前插入一个特殊设计的附件块，使用小于10%的训练数据成功伪造多个有效的护照。<br>
                    效果：使用伪造的护照，模型的表现与授权护照几乎无法区分（差异小于2%）。此外，还表明该攻击策略可以容易地推广到其他基于水印嵌入的知识产权保护方法上。同时，也给出了潜在的解决方案方向。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Since training a deep neural network (DNN) is costly, the well-trained deep models can be regarded as valuable intellectual property (IP) assets. The IP protection associated with deep models has been receiving increasing attentions in recent years. Passport-based method, which replaces normalization layers with passport layers, has been one of the few protection solutions that are claimed to be secure against advanced attacks. In this work, we tackle the issue of evaluating the security of passport-based IP protection methods. We propose a novel and effective ambiguity attack against passport-based method, capable of successfully forging multiple valid passports with a small training dataset. This is accomplished by inserting a specially designed accessory block ahead of the passport parameters. Using less than 10% of training data, with the forged passport, the model exhibits almost indistinguishable performance difference (less than 2%) compared with that of the authorized passport. In addition, it is shown that our attack strategy can be readily generalized to attack other IP protection methods based on watermark embedding. Directions for potential remedy solutions are also given.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1451.Sibling-Attack: Rethinking Transferable Adversarial Attacks Against Face Recognition</span><br>
                <span class="as">Li, ZexinandYin, BangjieandYao, TaipingandGuo, JunfengandDing, ShouhongandChen, SiminandLiu, Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Sibling-Attack_Rethinking_Transferable_Adversarial_Attacks_Against_Face_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24626-24637.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发实用的人脸识别（FR）攻击面临的一个主要挑战是目标FR模型的“黑箱”特性，即攻击者无法获取其梯度和参数信息。<br>
                    动机：尽管最近的一些研究通过利用转移性在攻击“黑箱”FR模型方面取得了重要进展，但其性能仍然有限，尤其是在对抗在线商业FR系统时，这些系统的防御性能可能较差（例如，平均ASR攻击成功率低于50%）。<br>
                    方法：受此启发，我们提出了一种新的FR攻击技术——Sibling-Attack，首次从多任务角度探索了新的攻击策略（即利用来自多关联任务的额外信息来提高攻击的转移性）。具体来说，Sibling-Attack选择一组与FR相关联的任务，并根据理论和定量分析选择了属性识别（AR）任务作为攻击任务。然后，它开发了一个优化框架，通过以下方式融合对抗性梯度信息：（1）约束跨任务特征在同一空间内；（2）一个增强任务间梯度兼容性的联合任务元优化框架；（3）一个减小攻击过程中振荡效应的跨任务梯度稳定化方法。<br>
                    效果：大量实验表明，Sibling-Attack的性能优于最先进的FR攻击技术，对两个知名且广泛使用的商业FR系统的平均ASR攻击成功率分别提高了12.61%和55.77%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A hard challenge in developing practical face recognition (FR) attacks is due to the black-box nature of the target FR model, i.e., inaccessible gradient and parameter information to attackers. While recent research took an important step towards attacking black-box FR models through leveraging transferability, their performance is still limited, especially against online commercial FR systems that can be pessimistic (e.g., a less than 50% ASR--attack success rate on average). Motivated by this, we present Sibling-Attack, a new FR attack technique for the first time explores a novel multi-task perspective (i.e., leveraging extra information from multi-correlated tasks to boost attacking transferability). Intuitively, Sibling-Attack selects a set of tasks correlated with FR and picks the Attribute Recognition (AR) task as the task used in Sibling-Attack based on theoretical and quantitative analysis. Sibling-Attack then develops an optimization framework that fuses adversarial gradient information through (1) constraining the cross-task features to be under the same space, (2) a joint-task meta optimization framework that enhances the gradient compatibility among tasks, and (3) a cross-task gradient stabilization method which mitigates the oscillation effect during attacking. Extensive experiments demonstrate that Sibling-Attack outperforms state-of-the-art FR attack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on average on state-of-the-art pre-trained FR models and two well-known, widely used commercial FR systems.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1452.Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack</span><br>
                <span class="as">Takahashi, HideakiandLiu, JingjingandLiu, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takahashi_Breaching_FedMD_Image_Recovery_via_Paired-Logits_Inversion_Attack_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12198-12207.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决联邦学习中存在的数据暴露风险，特别是在模型蒸馏的联邦学习（FedMD）中。<br>
                    动机：尽管在联邦学习中只共享公共数据集的输出逻辑斯蒂值得到的知识比直接共享易受梯度反转攻击的私有模型参数更安全，但精心设计的恶意攻击仍可能导致数据暴露。<br>
                    方法：通过训练一个利用服务器和客户端模型之间置信度差距的反转神经网络，恶意服务器可以对FedMD及其变体进行配对逻辑斯蒂转换（PLI）攻击。<br>
                    效果：实验证明，在类似FedMD的方案下，恶意服务器仅使用公共数据集的配对服务器-客户端逻辑斯蒂，就可以在所有测试基准上以高成功率重建私人图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on all tested benchmarks with a high success rate.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1453.Megahertz Light Steering Without Moving Parts</span><br>
                <span class="as">Pediredla, AdithyaandNarasimhan, SrinivasaG.andChamanzar, MaysamrezaandGkioulekas, Ioannis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pediredla_Megahertz_Light_Steering_Without_Moving_Parts_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1-12.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现高速、低成本的光转向技术，以应用于多种需要高速、可靠、低成本和波长无关的光转向的投影和成像系统。<br>
                    动机：目前的光转向技术大多需要昂贵的设备和复杂的操作，且速度较慢。我们希望通过开发新的光转向技术，解决这些问题。<br>
                    方法：我们引入了一种轻型操控技术，该技术在兆赫频率下运行，无需移动部件，成本低于一百美元。通过使用超声波波在可压缩介质（如水）中产生时空变化的折射率场，将介质转化为动态移动透镜，从而控制电输入生成波的超声波换能器来改变透镜，以声速（水中为1.5公里/秒）引导光线。<br>
                    效果：我们构建了这项技术的物理原型，并用于实现兆赫速率下的不同的扫描技术（比商用替代品如振镜扫描器快三个数量级），并展示了概念验证的投影和激光雷达应用。我们还推导出其基本限制的理论，并开发了一个物理准确的模拟器进行虚拟设计。这项技术为各种应用中的高速和低成本光转向提供了有前景的解决方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a light steering technology that operates at megahertz frequencies, has no moving parts, and costs less than a hundred dollars. Our technology can benefit many projector and imaging systems that critically rely on high-speed, reliable, low-cost, and wavelength-independent light steering, including laser scanning projectors, LiDAR sensors, and fluorescence microscopes. Our technology uses ultrasound waves to generate a spatiotemporally-varying refractive index field inside a compressible medium, such as water, turning the medium into a dynamic traveling lens. By controlling the electrical input of the ultrasound transducers that generate the waves, we can change the lens, and thus steer light, at the speed of sound (1.5 km/s in water). We build a physical prototype of this technology, use it to realize different scanning techniques at megahertz rates (three orders of magnitude faster than commercial alternatives such as galvo mirror scanners), and demonstrate proof-of-concept projector and LiDAR applications. To encourage further innovation towards this new technology, we derive the theory for its fundamental limits and develop a physically-accurate simulator for virtual design. Our technology offers a promising solution for achieving high-speed and low-cost light steering in a variety of applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1454.Learning Bottleneck Concepts in Image Classification</span><br>
                <span class="as">Wang, BowenandLi, LiangzhiandNakashima, YutaandNagahara, Hajime</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10962-10971.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何解释和理解深度神经网络的行为，提高其可解释性。<br>
                    动机：现有的AI解释方法主要提供像素级别的相关性，但可能需要专家知识来解释这些解释。<br>
                    方法：本文提出了瓶颈概念学习器（BotCL），通过在目标任务上进行训练，无需对概念进行显式监督，仅通过概念的存在/缺失来表示图像。使用自我监督和定制的正则化器，使学习到的概念易于人类理解。<br>
                    效果：通过在一些图像分类任务上进行测试，证明了BotCL重建神经网络以提高其可解释性的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI provides a way to address this challenge, mostly by providing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some recent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bottleneck Concept Learner (BotCL), which represents an image solely by the presence/absence of concepts learned through training over the target task without explicit supervision over the concepts. It uses self-supervision and tailored regularizers so that learned concepts can be human-understandable. Using some image classification tasks as our testbed, we demonstrate BotCL's potential to rebuild neural networks for better interpretability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1455.Physically Adversarial Infrared Patches With Learnable Shapes and Locations</span><br>
                <span class="as">Wei, XingxingandYu, JieandHuang, Yao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Physically_Adversarial_Infrared_Patches_With_Learnable_Shapes_and_Locations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12334-12342.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何评估红外对象探测器在现实世界中对抗对抗性示例的鲁棒性。<br>
                    动机：由于红外对象探测器在安全关键任务中的广泛应用，有必要评估它们在实际世界中对抗对抗性示例的鲁棒性。然而，目前很少有物理红外攻击在实践中实施，因为它们从数字世界到物理世界的复杂转换。<br>
                    方法：本文提出了一种名为“对抗性红外补丁”的物理上可行的红外攻击方法。考虑到红外摄像机通过捕捉物体的热辐射成像的机制，对抗性红外补丁通过在目标物体上附加一块隔热材料补丁来操纵其热分布来进行攻击。为了增强对抗性攻击，我们提出了一种新的聚合正则化方法来指导目标物体上的补丁形状和位置的同步学习。因此，一个简单的基于梯度的优化可以用来解决它们。<br>
                    效果：我们在不同对象检测任务中使用各种对象探测器验证了对抗性红外补丁。实验结果表明，我们的方法在物理环境中对行人检测器和车辆检测器实现了超过90%的攻击成功率（ASR），其中物体在不同的角度、距离、姿势和场景下被捕获。更重要的是，对抗性红外补丁易于实施，仅需要0.5小时即可在物理世界中构建，这验证了其有效性和效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Owing to the extensive application of infrared object detectors in the safety-critical tasks, it is necessary to evaluate their robustness against adversarial examples in the real world. However, current few physical infrared attacks are complicated to implement in practical application because of their complex transformation from digital world to physical world. To address this issue, in this paper, we propose a physically feasible infrared attack method called "adversarial infrared patches". Considering the imaging mechanism of infrared cameras by capturing objects' thermal radiation, adversarial infrared patches conduct attacks by attaching a patch of thermal insulation materials on the target object to manipulate its thermal distribution. To enhance adversarial attacks, we present a novel aggregation regularization to guide the simultaneous learning for the patch' shape and location on the target object. Thus, a simple gradient-based optimization can be adapted to solve for them. We verify adversarial infrared patches in different object detection tasks with various object detectors. Experimental results show that our method achieves more than 90% Attack Success Rate (ASR) versus the pedestrian detector and vehicle detector in the physical environment, where the objects are captured in different angles, distances, postures, and scenes. More importantly, adversarial infrared patch is easy to implement, and it only needs 0.5 hour to be constructed in the physical world, which verifies its effectiveness and efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1456.Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective</span><br>
                <span class="as">Zhang, WeixiaandZhai, GuangtaoandWei, YingandYang, XiaokangandMa, Kede</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14071-14081.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在推进盲图像质量评估（BIQA），预测在没有任何参考信息的情况下，人类对图像质量的感知。<br>
                    动机：目前的盲图像质量评估方法需要借助其他任务的辅助知识，但模型参数共享和损失加权通常需要手动设定。<br>
                    方法：我们开发了一种通用且自动化的多任务学习方案，通过自动确定模型参数共享和损失加权，从其他任务中挖掘辅助知识。具体来说，我们首先使用文本模板描述所有候选标签组合（来自多个任务），然后计算视觉-文本嵌入的余弦相似性的联合概率。每个任务的预测可以从联合分布中推断出来，并通过精心设计的损失函数进行优化。<br>
                    效果：我们在场景分类、失真类型识别和盲图像质量评估这三个任务上进行了全面的实验，验证了我们的盲图像质量评估方法1）从场景分类和失真类型识别任务中受益，并在多个IQA数据集上超越了最先进的方法；2）在组最大差异化竞争中更稳健；3）能更有效地重新调整不同IQA数据集的质量注释。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We aim at advancing blind image quality assessment (BIQA), which predicts the human perception of image quality without any reference information. We develop a general and automated multitask learning scheme for BIQA to exploit auxiliary knowledge from other tasks, in a way that the model parameter sharing and the loss weighting are determined automatically. Specifically, we first describe all candidate label combinations (from multiple tasks) using a textual template, and compute the joint probability from the cosine similarities of the visual-textual embeddings. Predictions of each task can be inferred from the joint distribution, and optimized by carefully designed loss functions. Through comprehensive experiments on learning three tasks - BIQA, scene classification, and distortion type identification, we verify that the proposed BIQA method 1) benefits from the scene classification and distortion type identification tasks and outperforms the state-of-the-art on multiple IQA datasets, 2) is more robust in the group maximum differentiation competition, and 3) realigns the quality annotations from different IQA datasets more effectively. The source code is available at https://github.com/zwx8981/LIQE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1457.SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries</span><br>
                <span class="as">Humayun, AhmedImtiazandBalestriero, RandallandBalakrishnan, GuhaandBaraniuk, RichardG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3789-3798.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的深度网络可视化和解释性方法主要依赖于数据空间的可视化，如评分哪些研究问题：目前的深度网络可视化和解释性方法主要依赖于数据空间的可视化，如评分哪些维度的数据负责其相关的预测或生成与给定深度网络单元或表示最匹配的新数据特征或样本。<br>
                    动机：本文的动机是开发第一种可证明精确的方法来计算深度网络在数据空间特定区域上的映射几何，包括其决策边界。<br>
                    方法：通过利用连续分段线性（CPWL）样条深度网络的理论，SplineCam精确计算了深度网络的几何形状，而无需进行诸如采样或架构简化的近似。SplineCam适用于基于CPWL激活非线性的任何深度网络架构，包括（泄漏）ReLU、绝对值、maxout和最大池化，也可以应用于回归深度网络，如隐式神经网络表示。<br>
                    效果：除了决策边界的可视化和描述外，SplineCam还能够比较架构、测量泛化能力，并在数据流形上或下从决策边界进行采样。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current Deep Network (DN) visualization and interpretability methods rely heavily on data space visualizations such as scoring which dimensions of the data are responsible for their associated prediction or generating new data features or samples that best match a given DN unit or representation. In this paper, we go one step further by developing the first provably exact method for computing the geometry of a DN's mapping -- including its decision boundary -- over a specified region of the data space. By leveraging the theory of Continuous Piecewise Linear (CPWL) spline DNs, SplineCam exactly computes a DN's geometry without resorting to approximations such as sampling or architecture simplification. SplineCam applies to any DN architecture based on CPWL activation nonlinearities, including (leaky) ReLU, absolute value, maxout, and max-pooling and can also be applied to regression DNs such as implicit neural representations. Beyond decision boundary visualization and characterization, SplineCam enables one to compare architectures, measure generalizability, and sample from the decision boundary on or off the data manifold. Project website: https://bit.ly/splinecam</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1458.Physical-World Optical Adversarial Attacks on 3D Face Recognition</span><br>
                <span class="as">Li, YanjieandLi, YiquanandDai, XuelongandGuo, SongtaoandXiao, Bin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Physical-World_Optical_Adversarial_Attacks_on_3D_Face_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24699-24708.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对抗性攻击在真实世界的3D面部识别任务上的成功率较低，因为3D打印攻击需要生成的点与表面相邻，这限制了对抗性示例的搜索空间。<br>
                    动机：为了解决真实世界的挑战，我们提出了一种新的基于结构光的对抗性攻击方法。<br>
                    方法：我们将3D重建过程和皮肤的反射率纳入优化过程，以实现端到端的攻击，并提出了3D变换不变损失和敏感性图来提高鲁棒性。<br>
                    效果：实验表明，我们的新方法可以使用较少的扰动来攻击点云和深度图像的3D面部识别系统，成功率较高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success rate of current adversarial attacks remains low on real-world 3D face recognition tasks because the 3D-printing attacks need to meet the requirement that the generated points should be adjacent to the surface, which limits the adversarial example' searching space. Additionally, they have not considered unpredictable head movements or the non-homogeneous nature of skin reflectance in the real world. To address the real-world challenges, we propose a novel structured-light attack against structured-light-based 3D face recognition. We incorporate the 3D reconstruction process and skin's reflectance in the optimization process to get the end-to-end attack and present 3D transform invariant loss and sensitivity maps to improve robustness. Our attack enables adversarial points to be placed in any position and is resilient to random head movements while maintaining the perturbation unnoticeable. Experiments show that our new method can attack point-cloud-based and depth-image-based 3D face recognition systems with a high success rate, using fewer perturbations than previous physical 3D adversarial attacks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1459.Adversarial Counterfactual Visual Explanations</span><br>
                <span class="as">Jeanneret, GuillaumeandSimon, Lo{\&quot;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeanneret_Adversarial_Counterfactual_Visual_Explanations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16425-16435.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将对抗性攻击转化为语义上有意义的扰动，以进行反事实解释？<br>
                    动机：目前的对抗性攻击在反事实解释中被视为噪声，无法直接使用。<br>
                    方法：提出一种利用去噪扩散概率模型生成对抗性攻击的方法，通过扩散模型对攻击进行优化，使其成为语义上有意义的扰动。<br>
                    效果：实验证明，该方法在多个测试集上都优于当前最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Counterfactual explanations and adversarial attacks have a related goal: flipping output labels with minimal perturbations regardless of their characteristics. Yet, adversarial attacks cannot be used directly in a counterfactual explanation perspective, as such perturbations are perceived as noise and not as actionable and understandable image modifications. Building on the robust learning literature, this paper proposes an elegant method to turn adversarial attacks into semantically meaningful perturbations, without modifying the classifiers to explain. The proposed approach hypothesizes that Denoising Diffusion Probabilistic Models are excellent regularizers for avoiding high-frequency and out-of-distribution perturbations when generating adversarial attacks. The paper's key idea is to build attacks through a diffusion model to polish them. This allows studying the target model regardless of its robustification level. Extensive experimentation shows the advantages of our counterfactual explanation approach over current State-of-the-Art in multiple testbeds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1460.Superclass Learning With Representation Enhancement</span><br>
                <span class="as">Gan, ZeyuandZhao, SuyunandKang, JinlongandShang, LiyuanandChen, HongandLi, Cuiping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Superclass_Learning_With_Representation_Enhancement_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24060-24069.png><br>
            
            <span class="tt"><span class="t0">研究问题：在许多真实场景中，数据通常根据专家知识划分为几个人工超类别，而不是图像表示。现有的分类技术由于缺乏通用语义特征，无法识别没有原始类别标签的超类别，导致性能严重受损或需要巨大的注释成本。<br>
                    动机：为了缩小这个差距，本文提出了一个名为SuperClass Learning with Representation Enhancement（SCLRE）的超类学习框架，通过利用增强的表示来识别超类别。<br>
                    方法：具体来说，SCLRE通过利用批量中的自我注意力技术，消除了原始类别的界限，增强了每个超类的表示。在增强的表示空间上，然后重建了一个超类感知的决策边界。<br>
                    效果：实验结果表明，SCLRE在CIFAR-100数据集和四个高分辨率数据集上优于基线和其他对比方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In many real scenarios, data are often divided into a handful of artificial super categories in terms of expert knowledge rather than the representations of images. Concretely, a superclass may contain massive and various raw categories, such as refuse sorting. Due to the lack of common semantic features, the existing classification techniques are intractable to recognize superclass without raw class labels, thus they suffer severe performance damage or require huge annotation costs. To narrow this gap, this paper proposes a superclass learning framework, called SuperClass Learning with Representation Enhancement(SCLRE), to recognize super categories by leveraging enhanced representation. Specifically, by exploiting the self-attention technique across the batch, SCLRE collapses the boundaries of those raw categories and enhances the representation of each superclass. On the enhanced representation space, a superclass-aware decision boundary is then reconstructed. Theoretically, we prove that by leveraging attention techniques the generalization error of SCLRE can be bounded under superclass scenarios. Experimentally, extensive results demonstrate that SCLRE outperforms the baseline and other contrastive-based methods on CIFAR-100 datasets and four high-resolution datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1461.Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations</span><br>
                <span class="as">Binder, AlexanderandWeber, LeanderandLapuschkin, SebastianandMontavon, Gr\&#x27;egoireandM\&quot;uller, Klaus-RobertandSamek, Wojciech</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Binder_Shortcomings_of_Top-Down_Randomization-Based_Sanity_Checks_for_Evaluations_of_Deep_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16143-16152.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确评估模型的解释能力？<br>
                    动机：当前的模型解释评估方法可能存在问题，需要更严谨的评估方式。<br>
                    方法：通过实验发现随机化测试在评估解释方法时存在局限性，提出了观察实验差距和识别限制的方法。<br>
                    效果：证明了基于模型随机化的健全性检查不能作为选择或丢弃解释方法的主要标准，揭示了其对解释方法排名的不适当性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While the evaluation of explanations is an important step towards trustworthy models, it needs to be done carefully, and the employed metrics need to be well-understood. Specifically model randomization testing can be overinterpreted if regarded as a primary criterion for selecting or discarding explanation methods. To address shortcomings of this test, we start by observing an experimental gap in the ranking of explanation methods between randomization-based sanity checks [1] and model output faithfulness measures (e.g. [20]). We identify limitations of model-randomization-based sanity checks for the purpose of evaluating explanations. Firstly, we show that uninformative attribution maps created with zero pixel-wise covariance easily achieve high scores in this type of checks. Secondly, we show that top-down model randomization preserves scales of forward pass activations with high probability. That is, channels with large activations have a high probility to contribute strongly to the output, even after randomization of the network on top of them. Hence, explanations after randomization can only be expected to differ to a certain extent. This explains the observed experimental gap. In summary, these results demonstrate the inadequacy of model-randomization-based sanity checks as a criterion to rank attribution methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1462.Towards Trustable Skin Cancer Diagnosis via Rewriting Model&#x27;s Decision</span><br>
                <span class="as">Yan, SiyuanandYu, ZhenandZhang, XuelinandMahapatra, DwarikanathandChandra, ShekharS.andJanda, MonikaandSoyer, PeterandGe, Zongyuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Towards_Trustable_Skin_Cancer_Diagnosis_via_Rewriting_Models_Decision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11568-11577.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在图像识别任务上表现出色，但可能过度依赖数据集内的误导因素，导致模型决策不可靠，并在真实场景中产生灾难性后果。<br>
                    动机：针对皮肤癌诊断问题，探索并解决深度神经网络的误导行为。<br>
                    方法：引入人类参与的模型训练框架，允许用户观察和纠正模型的决策逻辑。通过分析样本的共现行为自动发现误导因素，利用易获取的概念示例学习误导概念。将黑箱模型的特征表示映射到可解释的概念空间，使用户能够理解概念并通过一阶逻辑指令进行干预。<br>
                    效果：在自制的皮肤损伤数据集和几个公共皮肤损伤数据集上进行了系统评估。实验表明，该方法能有效检测和去除数据集中的误导因素，无需事先了解类别分布，也不需要完全标注的概念标签。同时，该方法使模型专注于临床相关概念，提高了模型推理的性能和可信度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks have demonstrated promising performance on image recognition tasks. However, they may heavily rely on confounding factors, using irrelevant artifacts or bias within the dataset as the cue to improve performance. When a model performs decision-making based on these spurious correlations, it can become untrustable and lead to catastrophic outcomes when deployed in the real-world scene. In this paper, we explore and try to solve this problem in the context of skin cancer diagnosis. We introduce a human-in-the-loop framework in the model training process such that users can observe and correct the model's decision logic when confounding behaviors happen. Specifically, our method can automatically discover confounding factors by analyzing the co-occurrence behavior of the samples. It is capable of learning confounding concepts using easily obtained concept exemplars. By mapping the blackbox model's feature representation onto an explainable concept space, human users can interpret the concept and intervene via first order-logic instruction. We systematically evaluate our method on our newly crafted, well-controlled skin lesion dataset and several public skin lesion datasets. Experiments show that our method can effectively detect and remove confounding factors from datasets without any prior knowledge about the category distribution and does not require fully annotated concept labels. We also show that our method enables the model to focus on clinicalrelated concepts, improving the model's performance and trustworthiness during model inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1463.Visibility Constrained Wide-Band Illumination Spectrum Design for Seeing-in-the-Dark</span><br>
                <span class="as">Niu, MuyaoandLi, ZhuoxiaoandZhong, ZhihangandZheng, Yinqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Niu_Visibility_Constrained_Wide-Band_Illumination_Spectrum_Design_for_Seeing-in-the-Dark_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13976-13985.png><br>
            
            <span class="tt"><span class="t0">研究问题：计算机视觉中的暗光视觉任务是最重要且最具挑战性的任务之一，其广泛应用和野外环境的极端复杂性使其成为一项重要任务。<br>
                    动机：现有的方法主要分为两类：1）依赖RGB的方法仅使用退化的RGB输入恢复信息（例如，低光增强）；2）不依赖RGB的方法将辅助近红外（NIR）光源下捕获的图像转换为RGB域（例如，NIR2RGB转换）。后者在完全黑暗中工作并且光源对肉眼友好，但由于其内在的模糊性而往往不稳定。<br>
                    方法：本文通过设计可见-近红外范围的辅助照明的最佳光谱来强化NIR2RGB转换，同时保持视觉友好性。我们的核心思想是量化人类视觉系统隐含的可见性约束并将其纳入设计流程。通过模拟VIS-NIR范围内的图像形成过程，在可见性约束定义的可行区域内以全可微的方式自动设计LED的最优复用。我们还使用定制的50波段滤波轮收集了大幅扩展的VIS-NIR高光谱图像数据集进行实验。<br>
                    效果：实验结果表明，与仅使用NIR相比，使用优化的宽波段照明可以显著提高任务性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Seeing-in-the-dark is one of the most important and challenging computer vision tasks due to its wide applications and extreme complexities of in-the-wild scenarios. Existing arts can be mainly divided into two threads: 1) RGB-dependent methods restore information using degraded RGB inputs only (e.g., low-light enhancement), 2) RGB-independent methods translate images captured under auxiliary near-infrared (NIR) illuminants into RGB domain (e.g., NIR2RGB translation). The latter is very attractive since it works in complete darkness and the illuminants are visually friendly to naked eyes, but tends to be unstable due to its intrinsic ambiguities. In this paper, we try to robustify NIR2RGB translation by designing the optimal spectrum of auxiliary illumination in the wide-band VIS-NIR range, while keeping visual friendliness. Our core idea is to quantify the visibility constraint implied by the human vision system and incorporate it into the design pipeline. By modeling the formation process of images in the VIS-NIR range, the optimal multiplexing of a wide range of LEDs is automatically designed in a fully differentiable manner, within the feasible region defined by the visibility constraint. We also collect a substantially expanded VIS-NIR hyperspectral image dataset for experiments by using a customized 50-band filter wheel. Experimental results show that the task can be significantly improved by using the optimized wide-band illumination than using NIR only. Codes Available: https://github.com/MyNiuuu/VCSD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1464.GamutMLP: A Lightweight MLP for Color Loss Recovery</span><br>
                <span class="as">Le, HoangM.andPrice, BrianandCohen, ScottandBrown, MichaelS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Le_GamutMLP_A_Lightweight_MLP_for_Color_Loss_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18268-18277.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在图像编码过程中恢复丢失的颜色信息？<br>
                    动机：现有的图像处理软件在将图片编码为sRGB颜色空间时，会裁剪掉大部分可见颜色，导致颜色信息丢失。<br>
                    方法：提出一种优化轻量多层感知器（MLP）模型的方法，在色域缩小步骤中预测被裁剪的值，从而恢复丢失的颜色信息。<br>
                    效果：该方法有效且高效，只需2秒即可完成优化，且仅需23KB的存储空间。通过与预训练的DNN-based gamut扩展网络和其他隐式神经表示方法进行比较，证明了其优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cameras and image-editing software often process images in the wide-gamut ProPhoto color space, encompassing 90% of all visible colors. However, when images are encoded for sharing, this color-rich representation is transformed and clipped to fit within the small-gamut standard RGB (sRGB) color space, representing only 30% of visible colors. Recovering the lost color information is challenging due to the clipping procedure. Inspired by neural implicit representations for 2D images, we propose a method that optimizes a lightweight multi-layer-perceptron (MLP) model during the gamut reduction step to predict the clipped values. GamutMLP takes approximately 2 seconds to optimize and requires only 23 KB of storage. The small memory footprint allows our GamutMLP model to be saved as metadata in the sRGB image---the model can be extracted when needed to restore wide-gamut color values. We demonstrate the effectiveness of our approach for color recovery and compare it with alternative strategies, including pre-trained DNN-based gamut expansion networks and other implicit neural representation methods. As part of this effort, we introduce a new color gamut dataset of 2200 wide-gamut/small-gamut images for training and testing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1465.RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts</span><br>
                <span class="as">Liu, HanandWu, YuhaoandZhai, ShixuanandYuan, BoandZhang, Ning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20585-20594.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索文本到图像生成模型的鲁棒性，特别是在对抗性攻击下的表现。<br>
                    动机：随着文本到图像生成技术的进步和普及，其可能带来的安全风险引起了人们的关注。然而，目前的研究主要集中在非定向设置上，对于可靠性（攻击成功率）和隐蔽性（难以察觉）的全面考虑还不够。<br>
                    方法：本文提出了RIATIG，一种通过难以察觉的例子对文本到图像模型进行可靠且难以察觉的对抗性攻击。通过将例子的制作过程形式化为优化过程，并使用基于遗传的方法来解决，提出的攻击可以以可靠的方式为文本到图像生成模型生成难以察觉的提示。<br>
                    效果：对六种流行的文本到图像生成模型的评估表明，无论是在白盒还是黑盒设置中，RIATIG攻击都具有高效和难以察觉的特性。为了方便社区在此基础上进行进一步的研究，作者已经公开了相关的实验结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The field of text-to-image generation has made remarkable strides in creating high-fidelity and photorealistic images. As this technology gains popularity, there is a growing concern about its potential security risks. However, there has been limited exploration into the robustness of these models from an adversarial perspective. Existing research has primarily focused on untargeted settings, and lacks holistic consideration for reliability (attack success rate) and stealthiness (imperceptibility). In this paper, we propose RIATIG, a reliable and imperceptible adversarial attack against text-to-image models via inconspicuous examples. By formulating the example crafting as an optimization process and solving it using a genetic-based method, our proposed attack can generate imperceptible prompts for text-to-image generation models in a reliable way. Evaluation of six popular text-to-image generation models demonstrates the efficiency and stealthiness of our attack in both white-box and black-box settings. To allow the community to build on top of our findings, we've made the artifacts available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1466.Proximal Splitting Adversarial Attack for Semantic Segmentation</span><br>
                <span class="as">Rony, J\&#x27;er\^omeandPesquet, Jean-ChristopheandBenAyed, Ismail</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rony_Proximal_Splitting_Adversarial_Attack_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20524-20533.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对抗性攻击方法主要针对分类任务，对于密集预测任务如语义分割的研究较少。<br>
                    动机：目前针对语义分割的对抗性攻击方法不能准确解决问题，对模型进行欺骗所需的扰动大小估计过大。<br>
                    方法：提出一种基于邻近分裂的白盒攻击方法，通过增广拉格朗日方法和自适应约束缩放和屏蔽策略，在非凸最小化框架内处理大量约束，生成具有更小l_infinity范数的对抗性扰动。<br>
                    效果：实验证明，该方法显著优于先前提出的攻击方法，以及适应分割的分类攻击方法，为这一密集任务提供了第一个全面的基准。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller l_infinity norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1467.Towards Transferable Targeted Adversarial Examples</span><br>
                <span class="as">Wang, ZhiboandYang, HongshanandFeng, YunheandSun, PengandGuo, HengchangandZhang, ZhifeiandRen, Kui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Transferable_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20534-20543.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成能误导模型预测特定类别的可转移的目标对抗性示例。<br>
                    动机：现有的可转移目标对抗性攻击通常无法充分描述目标类别分布，因此其转移能力有限。<br>
                    方法：提出一种可转移的目标对抗性攻击（TTAA），从标签和特征两个角度捕捉目标类别的分布信息，以生成高度可转移的目标对抗性示例。设计了一个生成对抗训练框架，包括一个生成器来产生目标对抗性示例，以及一个特征-标签双重判别器来区分生成的对抗性示例和目标类别图像。<br>
                    效果：实验表明，该方法在目标对抗性示例的转移能力上表现出色。当从VGG-19转移到DenseNet-121时，目标欺骗率达到了95.13%，显著优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transferability of adversarial examples is critical for black-box deep learning model attacks. While most existing studies focus on enhancing the transferability of untargeted adversarial attacks, few of them studied how to generate transferable targeted adversarial examples that can mislead models into predicting a specific class. Moreover, existing transferable targeted adversarial attacks usually fail to sufficiently characterize the target class distribution, thus suffering from limited transferability. In this paper, we propose the Transferable Targeted Adversarial Attack (TTAA), which can capture the distribution information of the target class from both label-wise and feature-wise perspectives, to generate highly transferable targeted adversarial examples. To this end, we design a generative adversarial training framework consisting of a generator to produce targeted adversarial examples, and feature-label dual discriminators to distinguish the generated adversarial examples from the target class images. Specifically, we design the label discriminator to guide the adversarial examples to learn label-related distribution information about the target class. Meanwhile, we design a feature discriminator, which extracts the feature-wise information with strong cross-model consistency, to enable the adversarial examples to learn the transferable distribution information. Furthermore, we introduce the random perturbation dropping to further enhance the transferability by augmenting the diversity of adversarial examples used in the training process. Experiments demonstrate that our method achieves excellent performance on the transferability of targeted adversarial examples. The targeted fooling rate reaches 95.13% when transferred from VGG-19 to DenseNet-121, which significantly outperforms the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1468.Improving Robustness of Vision Transformers by Reducing Sensitivity To Patch Corruptions</span><br>
                <span class="as">Guo, YongandStutz, DavidandSchiele, Bernt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Improving_Robustness_of_Vision_Transformers_by_Reducing_Sensitivity_To_Patch_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4108-4118.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器对图像的噪声或模糊等损坏仍然敏感，主要源于其基于补丁输入的不稳定自注意力机制。<br>
                    动机：为了提高视觉转换器的鲁棒性，减少对补丁损坏的敏感性。<br>
                    方法：提出一种新的训练方法——降低对补丁损坏的敏感性（RSPC）。首先识别并遮挡/破坏最易受损的补丁，然后通过将干净和损坏示例之间的中间特征对齐，明确减少对其的敏感性。<br>
                    效果：实验证明，RSPC大大提高了注意力层的稳定性，并在各种基准测试中持续展现出更好的鲁棒性，包括CIFAR-10/100-C、ImageNet-A、ImageNet-C和ImageNet-P。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite their success, vision transformers still remain vulnerable to image corruptions, such as noise or blur. Indeed, we find that the vulnerability mainly stems from the unstable self-attention mechanism, which is inherently built upon patch-based inputs and often becomes overly sensitive to the corruptions across patches. For example, when we only occlude a small number of patches with random noise (e.g., 10%), these patch corruptions would lead to severe accuracy drops and greatly distract intermediate attention layers. To address this, we propose a new training method that improves the robustness of transformers from a new perspective -- reducing sensitivity to patch corruptions (RSPC). Specifically, we first identify and occlude/corrupt the most vulnerable patches and then explicitly reduce sensitivity to them by aligning the intermediate features between clean and corrupted examples. We highlight that the construction of patch corruptions is learned adversarially to the following feature alignment process, which is particularly effective and essentially different from existing methods. In experiments, our RSPC greatly improves the stability of attention layers and consistently yields better robustness on various benchmarks, including CIFAR-10/100-C, ImageNet-A, ImageNet-C, and ImageNet-P.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1469.All-in-One Image Restoration for Unknown Degradations Using Adaptive Discriminative Filters for Specific Degradations</span><br>
                <span class="as">Park, DongwonandLee, ByungHyunandChun, SeYoung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_All-in-One_Image_Restoration_for_Unknown_Degradations_Using_Adaptive_Discriminative_Filters_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5815-5824.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像恢复方法在面对未知的多种退化时，无法有效应对。<br>
                    动机：为了解决这一问题，我们提出了一种自适应判别滤波器模型（ADMS）。<br>
                    方法：我们的模型通过分类退化的方式，让网络只使用约3%的网络参数来专门处理每种退化，并自适应地应用这些滤波器。<br>
                    效果：实验证明，我们的方法在雨-噪声-模糊和雨-雪-霾等多种退化的图像恢复基准数据集上，都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image restorations for single degradations have been widely studied, demonstrating excellent performance for each degradation, but can not reflect unpredictable realistic environments with unknown multiple degradations, which may change over time. To mitigate this issue, image restorations for known and unknown multiple degradations have recently been investigated, showing promising results, but require large networks or have sub-optimal architectures for potential interference among different degradations. Here, inspired by the filter attribution integrated gradients (FAIG), we propose an adaptive discriminative filter-based model for specific degradations (ADMS) to restore images with unknown degradations. Our method allows the network to contain degradation-dedicated filters only for about 3% of all network parameters per each degradation and to apply them adaptively via degradation classification (DC) to explicitly disentangle the network for multiple degradations. Our proposed method has demonstrated its effectiveness in comparison studies and achieved state-of-the-art performance in all-in-one image restoration benchmark datasets of both Rain-Noise-Blur and Rain-Snow-Haze.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1470.Turning Strengths Into Weaknesses: A Certified Robustness Inspired Attack Framework Against Graph Neural Networks</span><br>
                <span class="as">Wang, BinghuiandPang, MengandDong, Yun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Turning_Strengths_Into_Weaknesses_A_Certified_Robustness_Inspired_Attack_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16394-16403.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在设计一种攻击框架，以显著增强现有的逃避和中毒攻击。<br>
                    动机：尽管现有的攻击方法已经显示出良好的攻击性能，但图神经网络（GNNs）在测试和训练阶段对图结构的改变非常敏感。<br>
                    方法：我们的攻击框架受到认证鲁棒性的启发，首先利用随机平滑推导出节点的认证扰动大小，然后根据这个属性来重点攻击那些在图结构改变后更容易被攻击的节点。<br>
                    效果：我们将这种攻击框架应用到现有的攻击中，实验结果显示，它可以显著提高现有攻击的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph neural networks (GNNs) have achieved state-of-the-art performance in many graph-related tasks such as node classification. However, recent studies show that GNNs are vulnerable to both test-time and training-time attacks that perturb the graph structure. While the existing attack methods have shown promising attack performance, we would like to design an attack framework that can significantly enhance both the existing evasion and poisoning attacks. In particular, our attack framework is inspired by certified robustness. Certified robustness was originally used by defenders to defend against adversarial attacks. We are the first, from the attacker perspective, to leverage its properties to better attack GNNs. Specifically, we first leverage and derive nodes' certified perturbation sizes against evasion and poisoning attacks based on randomized smoothing. A larger certified perturbation size of a node indicates this node is theoretically more robust to graph perturbations. Such a property motivates us to focus more on nodes with smaller certified perturbation sizes, as they are easier to be attacked after graph perturbations. Accordingly, we design a certified robustness inspired attack loss, when incorporated into (any) existing attacks, produces our certified robustness inspired attack framework. We apply our attack framework to the existing attacks and results show it can significantly enhance the existing attacks' performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1471.Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression</span><br>
                <span class="as">Kim, JunhoandLee, Byung-KwanandRo, YongMan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Demystifying_Causal_Features_on_Adversarial_Examples_and_Causal_Inoculation_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12302-12312.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决对抗性训练网络中异常脆弱性的来源问题，并从因果关系的角度进行深入探讨。<br>
                    动机：尽管已有大量研究，但对抗性示例的起源仍然不明，引发了各种观点的争论。因此，本文提出通过因果视角来探索这一问题。<br>
                    方法：本文提出了一种名为“对抗性工具变量回归”的方法，通过在无偏环境中估计对抗性预测的因果关系，以揭示其内在的因果特征。<br>
                    效果：实验结果表明，所估计的因果特征与正确的对抗性鲁棒性预测高度相关，而最坏情况下的反事实表现出显著偏离正确预测的特征。此外，本文还展示了如何将CAusal FEatures（CAFE）有效地注入防御网络以提高对抗性鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The origin of adversarial examples is still inexplicable in research fields, and it arouses arguments from various viewpoints, albeit comprehensive investigations. In this paper, we propose a way of delving into the unexpected vulnerability in adversarially trained networks from a causal perspective, namely adversarial instrumental variable (IV) regression. By deploying it, we estimate the causal relation of adversarial prediction under an unbiased environment dissociated from unknown confounders. Our approach aims to demystify inherent causal features on adversarial examples by leveraging a zero-sum optimization game between a casual feature estimator (i.e., hypothesis model) and worst-case counterfactuals (i.e., test function) disturbing to find causal features. Through extensive analyses, we demonstrate that the estimated causal features are highly related to the correct prediction for adversarial robustness, and the counterfactuals exhibit extreme features significantly deviating from the correct prediction. In addition, we present how to effectively inoculate CAusal FEatures (CAFE) into defense networks for improving adversarial robustness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1472.MEDIC: Remove Model Backdoors via Importance Driven Cloning</span><br>
                <span class="as">Xu, QiulingandTao, GuanhongandHonorio, JeanandLiu, YingqiandAn, ShengweiandShen, GuangyuandCheng, SiyuanandZhang, Xiangyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_MEDIC_Remove_Model_Backdoors_via_Importance_Driven_Cloning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20485-20494.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何去除深度学习模型中的注入式后门？<br>
                    动机：目前，预训练的语言模型并未充分利用知识图谱中的结构化知识。<br>
                    方法：我们开发了一种新方法，通过克隆受后门影响模型的良性行为到一个新的、结构相同的模型中来去除后门。<br>
                    效果：实验结果表明，这种方法在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We develop a novel method to remove injected backdoors in deep learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result. We theoretically show our method can better recover benign functions of the backdoor model. Meanwhile, we prove our method can be more effective in removing backdoors compared with fine-tuning. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outperforming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1473.Transferable Adversarial Attacks on Vision Transformers With Token Gradient Regularization</span><br>
                <span class="as">Zhang, JianpingandHuang, YizhanandWu, WeibinandLyu, MichaelR.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Transferable_Adversarial_Attacks_on_Vision_Transformers_With_Token_Gradient_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16415-16424.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉变换器（ViTs）在计算机视觉任务中表现出色，但易受对抗性样本攻击。<br>
                    动机：转移式攻击利用局部模型生成对抗性样本并直接转移到攻击目标黑盒模型，其高效性对基于ViT的应用构成严重安全威胁。<br>
                    方法：提出Token Gradient Regularization（TGR）方法，根据ViT的结构特点，以token-wise的方式降低每个内部块中反向传播梯度的方差，并利用规范化的梯度生成对抗性样本。<br>
                    效果：在针对ViT和CNN的攻击实验中，TGR方法表现出优越性。与最先进的转移式攻击相比，TGR方法平均性能提高了8.8%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers (ViTs) have been successfully deployed in a variety of computer vision tasks, but they are still vulnerable to adversarial samples. Transfer-based attacks use a local model to generate adversarial samples and directly transfer them to attack a target black-box model. The high efficiency of transfer-based attacks makes it a severe security threat to ViT-based applications. Therefore, it is vital to design effective transfer-based attacks to identify the deficiencies of ViTs beforehand in security-sensitive scenarios. Existing efforts generally focus on regularizing the input gradients to stabilize the updated direction of adversarial samples. However, the variance of the back-propagated gradients in intermediate blocks of ViTs may still be large, which may make the generated adversarial samples focus on some model-specific features and get stuck in poor local optima. To overcome the shortcomings of existing approaches, we propose the Token Gradient Regularization (TGR) method. According to the structural characteristics of ViTs, TGR reduces the variance of the back-propagated gradient in each internal block of ViTs in a token-wise manner and utilizes the regularized gradient to generate adversarial samples. Extensive experiments on attacking both ViTs and CNNs confirm the superiority of our approach. Notably, compared to the state-of-the-art transfer-based attacks, our TGR offers a performance improvement of 8.8 % on average.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1474.Architectural Backdoors in Neural Networks</span><br>
                <span class="as">Bober-Irizar, MikelandShumailov, IliaandZhao, YirenandMullins, RobertandPapernot, Nicolas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bober-Irizar_Architectural_Backdoors_in_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24595-24604.png><br>
            
            <span class="tt"><span class="t0">研究问题：机器学习易受对抗性操纵，特别是在训练阶段，攻击者可以通过操控数据和采样过程来控制模型行为。<br>
                    动机：本文介绍了一种新的隐藏在模型架构中的后门攻击，即通过改变训练函数的归纳偏置来实现。这种攻击方式易于实施，例如发布一个包含后门的开源模型架构供他人无意识地重用。<br>
                    方法：我们提出了一种基于模型架构的后门攻击方法，该方法通过在输入和输出之间建立连接来实现。同时，我们还描述了一些可能的防护措施。<br>
                    效果：我们在不同规模的计算机视觉基准测试上评估了我们的攻击方法，并发现这种潜在的脆弱性普遍存在于各种常见的训练设置中。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Machine learning is vulnerable to adversarial manipulation. Previous literature has demonstrated that at the training stage attackers can manipulate data (Gu et al.) and data sampling procedures (Shumailov et al.) to control model behaviour. A common attack goal is to plant backdoors i.e. force the victim model to learn to recognise a trigger known only by the adversary. In this paper, we introduce a new class of backdoor attacks that hide inside model architectures i.e. in the inductive bias of the functions used to train. These backdoors are simple to implement, for instance by publishing open-source code for a backdoored model architecture that others will reuse unknowingly. We demonstrate that model architectural backdoors represent a real threat and, unlike other approaches, can survive a complete re-training from scratch. We formalise the main construction principles behind architectural backdoors, such as a connection between the input and the output, and describe some possible protections against them. We evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of common training settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1475.3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds</span><br>
                <span class="as">Xiao, AoranandHuang, JiaxingandXuan, WeihaoandRen, RuijieandLiu, KangchengandGuan, DayanandElSaddik, AbdulmotalebandLu, ShijianandXing, EricP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_3D_Semantic_Segmentation_in_the_Wild_Learning_Generalized_Models_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9382-9392.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何学习通用的3D语义分割模型，以应对各种恶劣天气条件下的自动驾驶。<br>
                    动机：现有的基准测试主要基于正常天气下的点云数据，而忽略了在各种恶劣天气下学习通用3D语义分割模型的重要性。<br>
                    方法：我们引入了SemanticSTF，这是一个不利天气下的点云数据集，提供密集的点级注释，允许我们在各种不利天气条件下研究3D语义分割。我们通过两个任务来研究通用的3D语义分割模型：1）从正常天气数据适应到不利天气数据的领域自适应3D语义分割；2）从正常天气数据中学习可泛化的模型。<br>
                    效果：我们的研究揭示了现有3D语义分割方法在遇到不利天气数据时的挑战，显示了SemanticSTF在这个有意义的研究方向上的巨大价值。此外，我们设计了一个领域随机化技术，交替地随机化点云的几何样式并聚合它们的编码嵌入，最终导致一个可以有效改善各种不利天气下的3D语义分割的可泛化模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We investigate universal 3DSS modeling with two tasks: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalized 3DSS that learns a generalizable model from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their encoded embeddings, ultimately leading to a generalizable model that effectively improves 3DSS under various adverse weather. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1476.Robust Single Image Reflection Removal Against Adversarial Attacks</span><br>
                <span class="as">Song, ZhenboandZhang, ZhenyuanandZhang, KaihaoandLuo, WenhanandFan, ZhaoxinandRen, WenqiandLu, Jianfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Robust_Single_Image_Reflection_Removal_Against_Adversarial_Attacks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24688-24698.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度单图像反射去除（SIRR）在对抗性攻击下的鲁棒性问题。<br>
                    动机：当前基于深度学习的SIRR方法由于输入图像的微小扭曲和扰动，性能会显著下降。<br>
                    方法：我们首先针对SIRR问题进行多样化的对抗性攻击，然后提出一个鲁棒的SIRR模型，该模型整合了跨尺度注意力模块、多尺度融合模块和对抗性图像判别器。通过利用多尺度机制，模型缩小了干净图像和对抗性图像特征之间的差距。图像判别器能自适应地区分清洁或噪声输入，从而获得可靠的鲁棒性。<br>
                    效果：我们在Nature、SIR^2和Real数据集上进行的大量实验表明，我们的模型显著提高了SIRR在不同场景下的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper addresses the problem of robust deep single-image reflection removal (SIRR) against adversarial attacks. Current deep learning based SIRR methods have shown significant performance degradation due to unnoticeable distortions and perturbations on input images. For a comprehensive robustness study, we first conduct diverse adversarial attacks specifically for the SIRR problem, i.e. towards different attacking targets and regions. Then we propose a robust SIRR model, which integrates the cross-scale attention module, the multi-scale fusion module, and the adversarial image discriminator. By exploiting the multi-scale mechanism, the model narrows the gap between features from clean and adversarial images. The image discriminator adaptively distinguishes clean or noisy inputs, and thus further gains reliable robustness. Extensive experiments on Nature, SIR^2, and Real datasets demonstrate that our model remarkably improves the robustness of SIRR across disparate scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1477.TrojDiff: Trojan Attacks on Diffusion Models With Diverse Targets</span><br>
                <span class="as">Chen, WeixinandSong, DawnandLi, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_TrojDiff_Trojan_Attacks_on_Diffusion_Models_With_Diverse_Targets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4035-4044.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型在各种任务中取得了巨大成功，但其训练数据的可信度难以控制或审查。本研究旨在探索扩散模型在潜在训练数据操纵下的脆弱性，并尝试回答：对训练良好的扩散模型进行Trojan攻击的难度有多大？这种Trojan攻击可以实现哪些对抗性目标？<br>
                    动机：由于扩散模型的成功取决于从不同来源收集的大规模训练数据，因此这些收集的数据的可信度很难控制或审查。<br>
                    方法：我们提出了一种针对扩散模型的有效Trojan攻击——TrojDiff，该攻击在训练过程中优化了Trojan扩散和生成过程。我们设计了新的过渡过程，使对抗性目标能够扩散到有偏的高斯分布中，并提出了一种新的Trojan生成过程参数化方法，为攻击提供了有效的训练目标。<br>
                    效果：我们在CIFAR-10和CelebA数据集上评估了TrojDiff对DDPM和DDIM扩散模型的攻击效果。结果显示，TrojDiff在不同的对抗性目标和使用不同类型的触发器的情况下，始终能实现高攻击性能，同时保持了良性环境下的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have achieved great success in a range of tasks, such as image synthesis and molecule design. As such successes hinge on large-scale training data collected from diverse sources, the trustworthiness of these collected data is hard to control or audit. In this work, we aim to explore the vulnerabilities of diffusion models under potential training data manipulations and try to answer: How hard is it to perform Trojan attacks on well-trained diffusion models? What are the adversarial targets that such Trojan attacks can achieve? To answer these questions, we propose an effective Trojan attack against diffusion models, TrojDiff, which optimizes the Trojan diffusion and generative processes during training. In particular, we design novel transitions during the Trojan diffusion process to diffuse adversarial targets into a biased Gaussian distribution and propose a new parameterization of the Trojan generative process that leads to an effective training objective for the attack. In addition, we consider three types of adversarial targets: the Trojaned diffusion models will always output instances belonging to a certain class from the in-domain distribution (In-D2D attack), out-of-domain distribution (Out-D2D-attack), and one specific instance (D2I attack). We evaluate TrojDiff on CIFAR-10 and CelebA datasets against both DDPM and DDIM diffusion models. We show that TrojDiff always achieves high attack performance under different adversarial targets using different types of triggers, while the performance in benign environments is preserved. The code is available at https://github.com/chenweixin107/TrojDiff.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1478.Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets</span><br>
                <span class="as">Wang, YimuandZhang, DinghuaiandWu, YihanandHuang, HengandZhang, Hongyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cooperation_or_Competition_Avoiding_Player_Domination_for_Multi-Target_Robustness_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20564-20574.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管深度学习取得了令人瞩目的进步，但其易受对抗性攻击的影响。大多数现有的防御方法只能抵御一种类型的攻击，而最新的工作则致力于抵御多种攻击。<br>
                    动机：为了理解多目标鲁棒性，我们将此问题视为一个讨价还价博弈，其中不同的玩家（对手）通过协商达成参数更新的联合方向。我们识别出博弈中存在的一种现象，即玩家主导，并发现这种现象导致一些现有的基于最大值的方法如MAX和MSD无法收敛。<br>
                    方法：基于我们的理论结果，我们设计了一个新颖的框架，通过调整不同对手的预算来避免玩家主导。我们在两个基准测试上进行实验，结果表明将所提出的框架应用于现有方法可以显著提高多目标鲁棒性。<br>
                    效果：我们在CIFAR-10和CelebA数据集上评估了TrojDiff对DDPM和DDIM扩散模型的攻击效果。结果显示，TrojDiff在不同的对抗性目标和使用不同类型的触发器的情况下，始终能实现高攻击性能，同时保持了良性环境下的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches were proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work steps forward at defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, and show that with this phenomenon, some of the existing max-based approaches such as MAX and MSD do not converge. Based on our theoretical results, we design a novel framework that adjusts the budgets of different adversaries to avoid player domination. Experiments on two benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target robustness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1479.Quality-Aware Pre-Trained Models for Blind Image Quality Assessment</span><br>
                <span class="as">Zhao, KaiandYuan, KunandSun, MingandLi, MadingandWen, Xing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Quality-Aware_Pre-Trained_Models_for_Blind_Image_Quality_Assessment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22302-22313.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决盲图像质量评估（BIQA）中深度学习方法因标注数据稀缺而无法充分发挥潜力的问题。<br>
                    动机：由于标注数据的稀缺，深度学习在盲图像质量评估（BIQA）中的应用受到了限制。<br>
                    方法：本文提出了一种针对BIQA的预训练任务，采用自我监督学习的方式，使模型能够从多得多的数据中学习表示。同时，提出了一种基于质量感知对比损失的方法来约束学习过程。<br>
                    效果：实验结果表明，该方法在流行的BIQA数据集上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Blind image quality assessment (BIQA) aims to automatically evaluate the perceived quality of a single image, whose performance has been improved by deep learning-based methods in recent years. However, the paucity of labeled data somewhat restrains deep learning-based BIQA methods from unleashing their full potential. In this paper, we propose to solve the problem by a pretext task customized for BIQA in a self-supervised learning manner, which enables learning representations from orders of magnitude more data. To constrain the learning process, we propose a quality-aware contrastive loss based on a simple assumption: the quality of patches from a distorted image should be similar, but vary from patches from the same image with different degradations and patches from different images. Further, we improve the existing degradation process and form a degradation space with the size of roughly 2x10^7. After pre-trained on ImageNet using our method, models are more sensitive to image quality and perform significantly better on downstream BIQA tasks. Experimental results show that our method obtains remarkable improvements on popular BIQA datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1480.Privacy-Preserving Adversarial Facial Features</span><br>
                <span class="as">Wang, ZhiboandWang, HeandJin, ShuaifanandZhang, WenwenandHu, JiahuiandWang, YanandSun, PengandYuan, WeiandLiu, KaixinandRen, Kui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Privacy-Preserving_Adversarial_Facial_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8212-8221.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过提取紧凑和判别性的人脸特征来保护人脸隐私，同时防止重建网络利用这些特征恢复原始人脸的外观。<br>
                    动机：现有的面部识别服务提供商通过从图像中提取紧凑和判别性的人脸特征并存储这些特征以进行实时识别来保护人脸隐私。然而，这些特征仍可能被用来通过构建重建网络恢复原始人脸的外观。尽管已经提出了几种保护隐私的方法，但增强人脸隐私保护的代价是准确性的降低。<br>
                    方法：本文提出了一种基于对抗性特征的人脸隐私保护（AdvFace）方法，生成保护隐私的对抗性特征，通过干扰从对抗性特征到人脸图像的映射来防御重建攻击。为此，设计了一个模拟攻击者行为的阴影模型，捕获从人脸特征到图像的映射函数，并生成对抗性潜在噪声以干扰映射。将对抗性特征而不是原始特征存储在服务器的数据库中，以防止泄露的特征暴露面部信息。此外，AdvFace不需要对人脸识别网络进行任何更改，可以作为已部署的人脸识别系统中的隐私增强插件实施。<br>
                    效果：大量实验结果表明，AdvFace在防御重建攻击的同时保持了人脸识别的准确性，优于最先进的人脸隐私保护方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face recognition service providers protect face privacy by extracting compact and discriminative facial features (representations) from images, and storing the facial features for real-time recognition. However, such features can still be exploited to recover the appearance of the original face by building a reconstruction network. Although several privacy-preserving methods have been proposed, the enhancement of face privacy protection is at the expense of accuracy degradation. In this paper, we propose an adversarial features-based face privacy protection (AdvFace) approach to generate privacy-preserving adversarial features, which can disrupt the mapping from adversarial features to facial images to defend against reconstruction attacks. To this end, we design a shadow model which simulates the attackers' behavior to capture the mapping function from facial features to images and generate adversarial latent noise to disrupt the mapping. The adversarial features rather than the original features are stored in the server's database to prevent leaked features from exposing facial information. Moreover, the AdvFace requires no changes to the face recognition network and can be implemented as a privacy-enhancing plugin in deployed face recognition systems. Extensive experimental results demonstrate that AdvFace outperforms the state-of-the-art face privacy-preserving methods in defending against reconstruction attacks while maintaining face recognition accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1481.Physics-Guided ISO-Dependent Sensor Noise Modeling for Extreme Low-Light Photography</span><br>
                <span class="as">Cao, YueandLiu, MingandLiu, ShuaiandWang, XiaotaoandLei, LeiandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Physics-Guided_ISO-Dependent_Sensor_Noise_Modeling_for_Extreme_Low-Light_Photography_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5744-5753.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管深度神经网络在许多视觉任务中取得了惊人的性能，但现有的研究问题：尽管深度神经网络在许多视觉任务中取得了惊人的性能，但现有的基于学习的方法在极端低光环境下的传感器噪声建模方面远不如物理模型。<br>
                    动机：为了挖掘基于学习的传感器噪声建模的潜力，我们研究了典型成像过程中的噪声形成，并提出了一种新的物理指导的ISO依赖性传感器噪声建模方法。<br>
                    方法：我们构建了一个基于正则化流的框架来表示CMOS相机传感器的复杂噪声特性。每个噪声模型组件都专门针对一种特定类型的噪声，并在物理模型的指导下进行。此外，我们还考虑了噪声模型中的ISO依赖性，这是现有基于学习的方法没有完全考虑到的。<br>
                    效果：与现有方法相比，所提出的噪声模型得益于其灵活的结构和准确的建模能力，可以在极端低光场景中实现更好的去噪性能。我们将公开源代码和收集的数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although deep neural networks have achieved astonishing performance in many vision tasks, existing learning-based methods are far inferior to the physical model-based solutions in extreme low-light sensor noise modeling. To tap the potential of learning-based sensor noise modeling, we investigate the noise formation in a typical imaging process and propose a novel physics-guided ISO-dependent sensor noise modeling approach. Specifically, we build a normalizing flow-based framework to represent the complex noise characteristics of CMOS camera sensors. Each component of the noise model is dedicated to a particular kind of noise under the guidance of physical models. Moreover, we take into consideration of the ISO dependence in the noise model, which is not completely considered by the existing learning-based methods. For training the proposed noise model, a new dataset is further collected with paired noisy-clean images, as well as flat-field and bias frames covering a wide range of ISO settings. Compared to existing methods, the proposed noise model benefits from the flexible structure and accurate modeling capabilities, which can help achieve better denoising performance in extreme low-light scenes. The source code and collected dataset will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1482.CAP: Robust Point Cloud Classification via Semantic and Structural Modeling</span><br>
                <span class="as">Ding, DaizongandJiang, ErlingandHuang, YuanminandZhang, MiandLi, WenxuanandYang, Min</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_CAP_Robust_Point_Cloud_Classification_via_Semantic_and_Structural_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12260-12270.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高3D点云分类模型的对抗攻击防御能力。<br>
                    动机：深度神经网络在3D点云分类任务上取得了成功，但同时也引发了对抗攻击的问题，这对真实世界的应用造成了严重损害。<br>
                    方法：设计了一种基于注意力机制和动态对比学习的防御框架，该框架能够提高现有分类模型的鲁棒性。<br>
                    效果：通过在两个数据集和三种分类模型上的大量实验，证明了该方法对各种攻击具有强大的防御能力，例如，PointNet在ModelNet40数据集上的受攻击成功率从70.2%降低到2.7%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, deep neural networks have shown great success on 3D point cloud classification tasks, which simultaneously raises the concern of adversarial attacks that cause severe damage to real-world applications. Moreover, defending against adversarial examples in point cloud data is extremely difficult due to the emergence of various attack strategies. In this work, with the insight of the fact that the adversarial examples in this task still preserve the same semantic and structural information as the original input, we design a novel defense framework for improving the robustness of existing classification models, which consists of two main modules: the attention-based pooling and the dynamic contrastive learning. In addition, we also develop an algorithm to theoretically certify the robustness of the proposed framework. Extensive empirical results on two datasets and three classification models show the robustness of our approach against various attacks, e.g., the averaged attack success rate of PointNet decreases from 70.2% to 2.7% on the ModelNet40 dataset under 9 common attacks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1483.StyLess: Boosting the Transferability of Adversarial Examples</span><br>
                <span class="as">Liang, KaishengandXiao, Bin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_StyLess_Boosting_the_Transferability_of_Adversarial_Examples_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8163-8172.png><br>
            
            <span class="tt"><span class="t0">研究问题：对抗性攻击通过向良性示例添加难以察觉的扰动来误导深度神经网络，这种研究问题：对抗性攻击通过向良性示例添加难以察觉的扰动来误导深度神经网络，这种攻击的转移性使得对抗性示例能够攻击未知架构或参数的黑箱DNN，对许多实际应用构成威胁。<br>
                    动机：现有的可转移性攻击在优化过程中没有区分风格和内容特征，限制了其攻击的转移性。<br>
                    方法：我们提出了一种新的攻击方法，称为无风格扰动（StyLess）。具体来说，我们不使用普通的网络作为替代模型，而是使用经过风格化处理的网络，通过微调自适应实例归一化来编码不同的风格特征。<br>
                    效果：我们的实验表明，这种方法可以显著提高对抗性示例的转移性。此外，我们的方法具有通用性，当与其他攻击技术结合时，可以超越最先进的可转移性攻击。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adversarial attacks can mislead deep neural networks (DNNs) by adding imperceptible perturbations to benign examples. The attack transferability enables adversarial examples to attack black-box DNNs with unknown architectures or parameters, which poses threats to many real-world applications. We find that existing transferable attacks do not distinguish between style and content features during optimization, limiting their attack transferability. To improve attack transferability, we propose a novel attack method called style-less perturbation (StyLess). Specifically, instead of using a vanilla network as the surrogate model, we advocate using stylized networks, which encode different style features by perturbing an adaptive instance normalization. Our method can prevent adversarial examples from using non-robust style features and help generate transferable perturbations. Comprehensive experiments show that our method can significantly improve the transferability of adversarial examples. Furthermore, our approach is generic and can outperform state-of-the-art transferable attacks when combined with other attack techniques.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1484.Non-Contrastive Unsupervised Learning of Physiological Signals From Video</span><br>
                <span class="as">Speth, JeremyandVance, NathanandFlynn, PatrickandCzajka, Adam</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Speth_Non-Contrastive_Unsupervised_Learning_of_Physiological_Signals_From_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14464-14474.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从RGB视频中提取微妙的周期性信号，如血容量脉搏和呼吸，实现低成本的非接触健康监测？<br>
                    动机：现有的远程脉冲估计（rPPG）方法主要依赖于深度学习解决方案，但需要依赖带有接触式PPG传感器生成的标签数据进行训练和评估。<br>
                    方法：提出了一种非对比无监督学习框架用于信号回归，以减少对标记视频数据的需求。该方法在最小化周期性和有限带宽假设的情况下，直接从未标记的视频中发现了血容量脉搏。<br>
                    效果：通过鼓励正常生理带限内的稀疏功率谱和功率谱的方差，该方法能够学习到周期性信号的视觉特征。首次使用未特别为rPPG创建的未标记视频数据进行实验，训练出稳健的脉搏率估计器。由于其有限的归纳偏置和令人印象深刻的经验结果，该方法理论上能够从视频中发现其他周期性信号，实现无需真实信号的多种生理测量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Subtle periodic signals such as blood volume pulse and respiration can be extracted from RGB video, enabling noncontact health monitoring at low cost. Advancements in remote pulse estimation -- or remote photoplethysmography (rPPG) -- are currently driven by deep learning solutions. However, modern approaches are trained and evaluated on benchmark datasets with ground truth from contact-PPG sensors. We present the first non-contrastive unsupervised learning framework for signal regression to mitigate the need for labelled video data. With minimal assumptions of periodicity and finite bandwidth, our approach discovers the blood volume pulse directly from unlabelled videos. We find that encouraging sparse power spectra within normal physiological bandlimits and variance over batches of power spectra is sufficient for learning visual features of periodic signals. We perform the first experiments utilizing unlabelled video data not specifically created for rPPG to train robust pulse rate estimators. Given the limited inductive biases and impressive empirical results, the approach is theoretically capable of discovering other periodic signals from video, enabling multiple physiological measurements without the need for ground truth signals.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1485.Adversarially Robust Neural Architecture Search for Graph Neural Networks</span><br>
                <span class="as">Xie, BeiniandChang, HengandZhang, ZiweiandWang, XinandWang, DaixinandZhang, ZhiqiangandYing, RexandZhu, Wenwu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Adversarially_Robust_Neural_Architecture_Search_for_Graph_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8143-8152.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高图神经网络（GNN）在面对对抗性攻击时的鲁棒性。<br>
                    动机：现有的防御方法无法保证在新的数据/任务或对抗性攻击下的性能，也无法从架构角度理解GNN的鲁棒性。<br>
                    方法：提出一种新的针对GNN的鲁棒神经架构搜索框架（G-RNA）。通过在搜索空间中添加图结构掩码操作来设计一个鲁棒的消息传递机制，包括各种防御操作候选，并允许我们搜索防御GNN。此外，定义了一个鲁棒性度量标准来指导搜索过程，帮助筛选出鲁棒的架构。<br>
                    效果：实验结果表明，G-RNA在基准数据集上的表现优于手动设计的鲁棒GNN和普通的图NAS基线，在对抗性攻击下的提高幅度为12.1%至23.4%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Graph Neural Networks (GNNs) obtain tremendous success in modeling relational data. Still, they are prone to adversarial attacks, which are massive threats to applying GNNs to risk-sensitive domains. Existing defensive methods neither guarantee performance facing new data/tasks or adversarial attacks nor provide insights to understand GNN robustness from an architectural perspective. Neural Architecture Search (NAS) has the potential to solve this problem by automating GNN architecture designs. Nevertheless, current graph NAS approaches lack robust design and are vulnerable to adversarial attacks. To tackle these challenges, we propose a novel Robust Neural Architecture search framework for GNNs (G-RNA). Specifically, we design a robust search space for the message-passing mechanism by adding graph structure mask operations into the search space, which comprises various defensive operation candidates and allows us to search for defensive GNNs. Furthermore, we define a robustness metric to guide the search procedure, which helps to filter robust architectures. In this way, G-RNA helps understand GNN robustness from an architectural perspective and effectively searches for optimal adversarial robust GNNs. Extensive experimental results on benchmark datasets show that G-RNA significantly outperforms manually designed robust GNNs and vanilla graph NAS baselines by 12.1% to 23.4% under adversarial attacks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1486.DR2: Diffusion-Based Robust Degradation Remover for Blind Face Restoration</span><br>
                <span class="as">Wang, ZhixinandZhang, ZiyingandZhang, XiaoyunandZheng, HuangjieandZhou, MingyuanandZhang, YaandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DR2_Diffusion-Based_Robust_Degradation_Remover_for_Blind_Face_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1704-1713.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决盲脸恢复中训练数据与真实世界情况不符的问题，即假设的退化模型和实际的退化效果之间的差距。<br>
                    动机：在盲脸恢复中，由于训练数据中的退化模型与真实世界的退化效果存在差距，导致恢复效果不佳，输出结果中常常出现伪影。然而，包含所有类型的退化以覆盖真实世界情况的训练数据既昂贵又不可行。<br>
                    方法：本文提出了基于扩散的稳健退化消除器（DR2），首先将退化图像转换为粗略但退化不变的预测，然后使用增强模块将粗糙预测恢复到高质量图像。通过利用表现良好的去噪扩散概率模型，DR2将输入图像扩散到各种类型的退化变为高斯噪声的噪声状态，并通过迭代去噪步骤捕获语义信息。<br>
                    效果：实验表明，DR2对常见的退化（如模糊、缩放、噪声和压缩）具有鲁棒性，并且可以与不同的增强模块设计兼容。在严重退化的合成和真实世界数据集上，DR2的性能超过了最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Blind face restoration usually synthesizes degraded low-quality data with a pre-defined degradation model for training, while more complex cases could happen in the real world. This gap between the assumed and actual degradation hurts the restoration performance where artifacts are often observed in the output. However, it is expensive and infeasible to include every type of degradation to cover real-world cases in the training data. To tackle this robustness issue, we propose Diffusion-based Robust Degradation Remover (DR2) to first transform the degraded image to a coarse but degradation-invariant prediction, then employ an enhancement module to restore the coarse prediction to a high-quality image. By leveraging a well-performing denoising diffusion probabilistic model, our DR2 diffuses input images to a noisy status where various types of degradation give way to Gaussian noise, and then captures semantic information through iterative denoising steps. As a result, DR2 is robust against common degradation (e.g. blur, resize, noise and compression) and compatible with different designs of enhancement modules. Experiments in various settings show that our framework outperforms state-of-the-art methods on heavily degraded synthetic and real-world datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1487.T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection</span><br>
                <span class="as">Huang, HaoandChen, ZiyanandChen, HuanranandWang, YongtaoandZhang, Kevin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20514-20523.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高黑盒攻击的转移性，同时降低时间和资源消耗。<br>
                    动机：现有的基于转移的黑盒攻击方法需要多个模型进行集成，既耗时又耗资源。<br>
                    方法：提出一种只使用单一模型进行转移式黑盒攻击的方法，通过调整训练策略和利用有限的信息防止攻击补丁过拟合。<br>
                    效果：实验证明，该方法可以显著提高攻击补丁在多个主流探测器上的黑盒转移性，同时也提高了白盒性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compared to query-based black-box attacks, transfer-based black-box attacks do not require any information of the attacked models, which ensures their secrecy. However, most existing transfer-based approaches rely on ensembling multiple models to boost the attack transferability, which is time- and resource-intensive, not to mention the difficulty of obtaining diverse models on the same task. To address this limitation, in this work, we focus on the single-model transfer-based black-box attack on object detection, utilizing only one model to achieve a high-transferability adversarial attack on multiple black-box detectors. Specifically, we first make observations on the patch optimization process of the existing method and propose an enhanced attack framework by slightly adjusting its training strategies. Then, we analogize patch optimization with regular model optimization, proposing a series of self-ensemble approaches on the input data, the attacked model, and the adversarial patch to efficiently make use of the limited information and prevent the patch from overfitting. The experimental results show that the proposed framework can be applied with multiple classical base attack methods (e.g., PGD and MIM) to greatly improve the black-box transferability of the well-optimized patch on multiple mainstream detectors, meanwhile boosting white-box performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1488.Dual-Bridging With Adversarial Noise Generation for Domain Adaptive rPPG Estimation</span><br>
                <span class="as">Du, JingdaandLiu, Si-QiandZhang, BochaoandYuen, PongC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Dual-Bridging_With_Adversarial_Noise_Generation_for_Domain_Adaptive_rPPG_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10355-10364.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高远程光电容积脉搏波(rPPG)技术在未知噪声和失真情况下的泛化能力。<br>
                    动机：尽管最新的深度rPPG方法可以处理由于头部运动、视频压缩等造成的同分布噪声，但在目标测试领域对未见过噪声和失真的泛化能力可能不足。<br>
                    方法：提出一种双桥接网络来减少领域差异，通过对准中间领域和在源领域中合成目标噪声进行更好的噪声降低。同时，提出一种新的对抗性噪声生成方法，让噪声生成器间接地与降噪器竞争，以提高降噪器的鲁棒性。<br>
                    效果：在具有不同类型干扰的三个公共数据集上评估了该方法，不同的跨领域场景下的综合结果表明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The remote photoplethysmography (rPPG) technique can estimate pulse-related metrics (e.g. heart rate and respiratory rate) from facial videos and has a high potential for health monitoring. The latest deep rPPG methods can model in-distribution noise due to head motion, video compression, etc., and estimate high-quality rPPG signals under similar scenarios. However, deep rPPG models may not generalize well to the target test domain with unseen noise and distortions. In this paper, to improve the generalization ability of rPPG models, we propose a dual-bridging network to reduce the domain discrepancy by aligning intermediate domains and synthesizing the target noise in the source domain for better noise reduction. To comprehensively explore the target domain noise, we propose a novel adversarial noise generation in which the noise generator indirectly competes with the noise reducer. To further improve the robustness of the noise reducer, we propose hard noise pattern mining to encourage the generator to learn hard noise patterns contained in the target domain features. We evaluated the proposed method on three public datasets with different types of interferences. Under different cross-domain scenarios, the comprehensive results show the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1489.Trade-Off Between Robustness and Accuracy of Vision Transformers</span><br>
                <span class="as">Li, YanxiandXu, Chang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Trade-Off_Between_Robustness_and_Accuracy_of_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7558-7568.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在计算机视觉任务上表现出色，但对输入的微小改动敏感，存在自然准确性和对此类改动的鲁棒性之间的权衡。<br>
                    动机：尽管视觉转换器（ViTs）被证明对各种类型的干扰具有固有的鲁棒性，但上述权衡仍然存在。<br>
                    方法：提出一种名为“视觉转换器的鲁棒性和准确性权衡”（TORA-ViTs）的方法，通过一对准确性和鲁棒性适配器提取预测性和鲁棒性特征，并通过一个门控融合模块调整权衡。<br>
                    效果：在ImageNet上进行的实验表明，TORA-ViTs可以在保持有竞争力的自然准确性的同时，有效地提高自然预训练的ViTs的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although deep neural networks (DNNs) have shown great successes in computer vision tasks, they are vulnerable to perturbations on inputs, and there exists a trade-off between the natural accuracy and robustness to such perturbations, which is mainly caused by the existence of robust non-predictive features and non-robust predictive features. Recent empirical analyses find Vision Transformers (ViTs) are inherently robust to various kinds of perturbations, but the aforementioned trade-off still exists for them. In this work, we propose Trade-off between Robustness and Accuracy of Vision Transformers (TORA-ViTs), which aims to efficiently transfer ViT models pretrained on natural tasks for both accuracy and robustness. TORA-ViTs consist of two major components, including a pair of accuracy and robustness adapters to extract predictive and robust features, respectively, and a gated fusion module to adjust the trade-off. The gated fusion module takes outputs of a pretrained ViT block as queries and outputs of our adapters as keys and values, and tokens from different adapters at different spatial locations are compared with each other to generate attention scores for a balanced mixing of predictive and robust features. Experiments on ImageNet with various robust benchmarks show that our TORA-ViTs can efficiently improve the robustness of naturally pretrained ViTs while maintaining competitive natural accuracy. Our most balanced setting (TORA-ViTs with lambda = 0.5) can maintain 83.7% accuracy on clean ImageNet and reach 54.7% and 38.0% accuracy under FGSM and PGD white-box attacks, respectively. In terms of various ImageNet variants, it can reach 39.2% and 56.3% accuracy on ImageNet-A and ImageNet-R and reach 34.4% mCE on ImageNet-C.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1490.Rate Gradient Approximation Attack Threats Deep Spiking Neural Networks</span><br>
                <span class="as">Bu, TongandDing, JianhaoandHao, ZechengandYu, Zhaofei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bu_Rate_Gradient_Approximation_Attack_Threats_Deep_Spiking_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7896-7906.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度尖峰神经网络（SNNs）的鲁棒性尚未完全揭示。<br>
                    动机：尖峰神经网络由于其能量效率特性和在神经形态硬件上的潜在应用而引起了广泛关注。<br>
                    方法：基于尖峰神经网络的速率编码特性，开发了一种名为“速率梯度近似攻击”（RGA）的新型特定于SNN的攻击方法。<br>
                    效果：实验结果表明，提出的RGA攻击比先前的攻击更有效，对神经元超参数不敏感。同时，实验也表明，由LIF神经元组成的速率编码SNN并不安全，需要探索由复杂神经元和其他神经元编码组成的SNN的训练方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spiking Neural Networks (SNNs) have attracted significant attention due to their energy-efficient properties and potential application on neuromorphic hardware. State-of-the-art SNNs are typically composed of simple Leaky Integrate-and-Fire (LIF) neurons and have become comparable to ANNs in image classification tasks on large-scale datasets. However, the robustness of these deep SNNs has not yet been fully uncovered. In this paper, we first experimentally observe that layers in these SNNs mostly communicate by rate coding. Based on this rate coding property, we develop a novel rate coding SNN-specified attack method, Rate Gradient Approximation Attack (RGA). We generalize the RGA attack to SNNs composed of LIF neurons with different leaky parameters and input encoding by designing surrogate gradients. In addition, we develop the time-extended enhancement to generate more effective adversarial examples. The experiment results indicate that our proposed RGA attack is more effective than the previous attack and is less sensitive to neuron hyperparameters. We also conclude from the experiment that rate-coded SNN composed of LIF neurons is not secure, which calls for exploring training methods for SNNs composed of complex neurons and other neuronal codings. Code is available at https://github.com/putshua/SNN_attack_RGA</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1491.Enhancing the Self-Universality for Transferable Targeted Attacks</span><br>
                <span class="as">Wei, ZhipengandChen, JingjingandWu, ZuxuanandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Enhancing_the_Self-Universality_for_Transferable_Targeted_Attacks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12281-12290.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度尖峰神经网络（SNNs）的鲁棒性尚未完全揭示。<br>
                    动机：尖峰神经网络由于其能量效率特性和在神经形态硬件上的潜在应用而引起了广泛关注。<br>
                    方法：基于尖峰神经网络的速率编码特性，开发了一种名为“速率梯度近似攻击”（RGA）的新型特定于SNN的攻击方法。<br>
                    效果：实验结果表明，提出的RGA攻击比先前的攻击更有效，对神经元超参数不敏感。同时，实验也表明，由LIF神经元组成的速率编码SNN并不安全，需要探索由复杂神经元和其他神经元编码组成的SNN的训练方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose a novel transfer-based targeted attack method that optimizes the adversarial perturbations without any extra training efforts for auxiliary networks on training data. Our new attack method is proposed based on the observation that highly universal adversarial perturbations tend to be more transferable for targeted attacks. Therefore, we propose to make the perturbation to be agnostic to different local regions within one image, which we called as self-universality. Instead of optimizing the perturbations on different images, optimizing on different regions to achieve self-universality can get rid of using extra data. Specifically, we introduce a feature similarity loss that encourages the learned perturbations to be universal by maximizing the feature similarity between adversarial perturbed global images and randomly cropped local regions. With the feature similarity loss, our method makes the features from adversarial perturbations to be more dominant than that of benign images, hence improving targeted transferability. We name the proposed attack method as Self-Universality (SU) attack. Extensive experiments demonstrate that SU can achieve high success rates for transfer-based targeted attacks. On ImageNet-compatible dataset, SU yields an improvement of 12% compared with existing state-of-the-art methods. Code is available at https://github.com/zhipeng-wei/Self-Universality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1492.Randomized Adversarial Training via Taylor Expansion</span><br>
                <span class="as">Jin, GaojieandYi, XinpingandWu, DengyuandMu, RonghuiandHuang, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Randomized_Adversarial_Training_via_Taylor_Expansion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16447-16457.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时提高神经网络对对抗性例子的鲁棒性和对清洁样本的准确性。<br>
                    动机：目前的对抗性训练方法可以有效提高神经网络的鲁棒性，但可能会牺牲一些准确性。<br>
                    方法：通过在训练过程中向确定性权重添加随机噪声，提出了一种新的对抗性训练方法。这种方法可以通过泰勒展开小的高斯噪声来设计，并能够使损失景观平坦，找到平坦的极小值。<br>
                    效果：实验证明，这种方法可以提高最先进的对抗性训练方法的性能，同时提高鲁棒性和清洁准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, there has been an explosion of research into developing more robust deep neural networks against adversarial examples. Adversarial training appears as one of the most successful methods. To deal with both the robustness against adversarial examples and the accuracy over clean examples, many works develop enhanced adversarial training methods to achieve various trade-offs between them. Leveraging over the studies that smoothed update on weights during training may help find flat minima and improve generalization, we suggest reconciling the robustness-accuracy trade-off from another perspective, i.e., by adding random noise into deterministic weights. The randomized weights enable our design of a novel adversarial training method via Taylor expansion of a small Gaussian noise, and we show that the new adversarial training method can flatten loss landscape and find flat minima. With PGD, CW, and Auto Attacks, an extensive set of experiments demonstrate that our method enhances the state-of-the-art adversarial training methods, boosting both robustness and clean accuracy. The code is available at https://github.com/Alexkael/Randomized-Adversarial-Training.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1493.Explaining Image Classifiers With Multiscale Directional Image Representation</span><br>
                <span class="as">Kolek, StefanandWindesheim, RobertandAndrade-Loarca, HectorandKutyniok, GittaandLevie, Ron</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kolek_Explaining_Image_Classifiers_With_Multiscale_Directional_Image_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18600-18609.png><br>
            
            <span class="tt"><span class="t0">研究问题：图像分类器的解释性差，需要解释方法来理解其决策。<br>
                    动机：现有的掩码解释方法通过平滑性约束进行正则化，防止不良的细粒度解释伪影，但这限制了掩码分离影响分类器的附近干扰模式和相关细粒度模式的能力。<br>
                    方法：提出ShearletX，一种基于剪切变换的新型图像分类器掩码解释方法，避免了平滑性约束，用剪切稀疏约束代替。<br>
                    效果：ShearletX在各种情况下都优于先前的基于掩码的解释方法，并展示了分离细粒度模式可以解释以前无法解释的现象。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image classifiers are known to be difficult to interpret and therefore require explanation methods to understand their decisions. We present ShearletX, a novel mask explanation method for image classifiers based on the shearlet transform -- a multiscale directional image representation. Current mask explanation methods are regularized by smoothness constraints that protect against undesirable fine-grained explanation artifacts. However, the smoothness of a mask limits its ability to separate fine-detail patterns, that are relevant for the classifier, from nearby nuisance patterns, that do not affect the classifier. ShearletX solves this problem by avoiding smoothness regularization all together, replacing it by shearlet sparsity constraints. The resulting explanations consist of a few edges, textures, and smooth parts of the original image, that are the most relevant for the decision of the classifier. To support our method, we propose a mathematical definition for explanation artifacts and an information theoretic score to evaluate the quality of mask explanations. We demonstrate the superiority of ShearletX over previous mask based explanation methods using these new metrics, and present exemplary situations where separating fine-detail patterns allows explaining phenomena that were not explainable before.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1494.Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction</span><br>
                <span class="as">Li, XiangandQian, XuelinandLiang, LitianandKong, LingjieandDong, QiaoleandChen, JiejunandLiu, DingxiaandYao, XiuzhongandFu, Yanwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Causally-Aware_Intraoperative_Imputation_for_Overall_Survival_Time_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15681-15690.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决早期原发性肝癌手术中生存时间预测的挑战，由于图像模式不明显，使得早期生存时间的预测变得困难。<br>
                    动机：为了解决这个问题，本文提出了一种基于因果关系推理的系统，利用手术过程中的属性及其之间的相关性作为中间监督，以弥补图像和最终生存时间之间的差距。<br>
                    方法：构建了一个因果图，并训练图像来估计用于最终生存时间预测的手术过程中的属性。提出了一种新的“Causally-aware Intraoperative Imputation Model”（CAWIM），该模型可以顺序地使用估计的因果图中的父节点来预测每个属性。为了确定因果关系的方向，提出了一个分割投票机制，该机制通过从异质性中进行因果发现，对每对相邻节点进行多次预测，从而为每对节点投票决定方向。<br>
                    效果：通过在具有长期观察的361名肝癌患者的数据集上进行的实验，证明了该方法的实用性和有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous efforts in vision community are mostly made on learning good representations from visual patterns. Beyond this, this paper emphasizes the high-level ability of causal reasoning. We thus present a case study of solving the challenging task of Overall Survival (OS) time in primary liver cancers. Critically, the prediction of OS time at the early stage remains challenging, due to the unobvious image patterns of reflecting the OS. To this end, we propose a causal inference system by leveraging the intraoperative attributes and the correlation among them, as an intermediate supervision to bridge the gap between the images and the final OS. Particularly, we build a causal graph, and train the images to estimate the intraoperative attributes for final OS prediction. We present a novel Causally-aware Intraoperative Imputation Model (CAWIM) that can sequentially predict each attribute using its parent nodes in the estimated causal graph. To determine the causal directions, we propose a splitting-voting mechanism, which votes for the direction for each pair of adjacent nodes among multiple predictions obtained via causal discovery from heterogeneity. The practicability and effectiveness of our method are demonstrated by the promising result on liver cancer dataset of 361 patients with long-term observations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1495.K3DN: Disparity-Aware Kernel Estimation for Dual-Pixel Defocus Deblurring</span><br>
                <span class="as">Yang, YanandPan, LiyuanandLiu, LiuandLiu, Miaomiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_K3DN_Disparity-Aware_Kernel_Estimation_for_Dual-Pixel_Defocus_Deblurring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13263-13272.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过双像素传感器捕获的图像对进行去模糊处理。<br>
                    动机：双像素传感器可以捕获两个视图的图像对，利用这种特性，我们提出了一种用于DP图像对去模糊的K3DN框架。<br>
                    方法：该框架包含三个模块：1）一个深度感知去模糊模块，它估计一个视差特征图，并使用这个特征图查询可训练的核集以估计描述空间变化的模糊的最佳模糊核；2）一个重模糊正则化模块，它在训练阶段重新使用模糊核，执行简单的卷积进行重模糊，并对估计的内核和视差特征进行无监督正则化；3）一个锐利区域保护模块，它识别DP图像之间零视差的聚焦区域，避免在去模糊过程中引入噪声，提高图像恢复性能。<br>
                    效果：在四个标准的DP数据集上进行的实验表明，所提出的K3DN优于最先进的方法，同时具有更少的参数和运算量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The dual-pixel (DP) sensor captures a two-view image pair in a single snapshot by splitting each pixel in half. The disparity occurs in defocus blurred regions between the two views of the DP pair, while the in-focus sharp regions have zero disparity. This motivates us to propose a K3DN framework for DP pair deblurring, and it has three modules: i) a disparity-aware deblur module. It estimates a disparity feature map, which is used to query a trainable kernel set to estimate a blur kernel that best describes the spatially-varying blur. The kernel is constrained to be symmetrical per the DP formulation. A simple Fourier transform is performed for deblurring that follows the blur model; ii) a reblurring regularization module. It reuses the blur kernel, performs a simple convolution for reblurring, and regularizes the estimated kernel and disparity feature unsupervisedly, in the training stage; iii) a sharp region preservation module. It identifies in-focus regions that correspond to areas with zero disparity between DP images, aims to avoid the introduction of noises during the deblurring process, and improves image restoration performance. Experiments on four standard DP datasets show that the proposed K3DN outperforms state-of-the-art methods, with fewer parameters and flops at the same time.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1496.DartBlur: Privacy Preservation With Detection Artifact Suppression</span><br>
                <span class="as">Jiang, BaoweiandBai, BingandLin, HaozheandWang, YuandGuo, YuchenandFang, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_DartBlur_Privacy_Preservation_With_Detection_Artifact_Suppression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16479-16488.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在保护面部隐私的同时，减少训练过程中引入的对下游任务性能有害的训练伪影。<br>
                    动机：随着AI算法的发展，如何有效保护个人隐私，特别是面部信息，成为一大关注焦点。现有的模糊和替换方法在实际应用中各有优势和不足。<br>
                    方法：提出一种新颖的去伪影模糊（DartBlur）隐私保护方法，利用深度神经网络生成模糊面部图像，同时抑制检测伪影。设计了四个特定的训练目标，以提高审查便利性和最大化检测伪影抑制。<br>
                    效果：实验证明，DartBlur在审查便利性、访问性以及抑制训练伪影方面均优于现有的替换方法和传统的模糊方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Nowadays, privacy issue has become a top priority when training AI algorithms. Machine learning algorithms are expected to benefit our daily life, while personal information must also be carefully protected from exposure. Facial information is particularly sensitive in this regard. Multiple datasets containing facial information have been taken offline, and the community is actively seeking solutions to remedy the privacy issues. Existing methods for privacy preservation can be divided into blur-based and face replacement-based methods. Owing to the advantages of review convenience and good accessibility, blur-based based methods have become a dominant choice in practice. However, blur-based methods would inevitably introduce training artifacts harmful to the performance of downstream tasks. In this paper, we propose a novel De-artifact Blurring(DartBlur) privacy-preserving method, which capitalizes on a DNN architecture to generate blurred faces. DartBlur can effectively hide facial privacy information while detection artifacts are simultaneously suppressed. We have designed four training objectives that particularly aim to improve review convenience and maximize detection artifact suppression. We associate the algorithm with an adversarial training strategy with a second-order optimization pipeline. Experimental results demonstrate that DartBlur outperforms the existing face-replacement method from both perspectives of review convenience and accessibility, and also shows an exclusive advantage in suppressing the training artifact compared to traditional blur-based methods. Our implementation is available at https://github.com/JaNg2333/DartBlur.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1497.IDGI: A Framework To Eliminate Explanation Noise From Integrated Gradients</span><br>
                <span class="as">Yang, RuoandWang, BinghuiandBilgic, Mustafa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_IDGI_A_Framework_To_Eliminate_Explanation_Noise_From_Integrated_Gradients_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23725-23734.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低深度神经网络决策解释中的噪音，提高其可解释性。<br>
                    动机：尽管基于积分梯度（IG）的方法在解释深度神经网络决策方面达到了最先进的性能，但其解释显著图常常包含噪音，降低了其可解释性。<br>
                    方法：通过分析噪音的来源，提出了一种新的减少解释噪音的方法，即重要方向梯度集成（IDGI）框架。该框架可以很容易地整合到任何使用黎曼积分进行积分梯度计算的基于IG的方法中。<br>
                    效果：通过在三种基于IG的方法上进行大量实验，发现IDGI能显著改善它们在众多可解释性度量上的表现。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Integrated Gradients (IG) as well as its variants are well-known techniques for interpreting the decisions of deep neural networks. While IG-based approaches attain state-of-the-art performance, they often integrate noise into their explanation saliency maps, which reduce their interpretability. To minimize the noise, we examine the source of the noise analytically and propose a new approach to reduce the explanation noise based on our analytical findings. We propose the Important Direction Gradient Integration (IDGI) framework, which can be easily incorporated into any IG-based method that uses the Reimann Integration for integrated gradient computation. Extensive experiments with three IG-based methods show that IDGI improves them drastically on numerous interpretability metrics.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1498.PCT-Net: Full Resolution Image Harmonization Using Pixel-Wise Color Transformations</span><br>
                <span class="as">Guerreiro, JulianJorgeAndradeandNakazawa, MitsuruandStenger, Bj\&quot;orn</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guerreiro_PCT-Net_Full_Resolution_Image_Harmonization_Using_Pixel-Wise_Color_Transformations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5917-5926.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种简单通用的高分辨率图像和谐化方法。<br>
                    动机：目前的图像和谐化方法通常在低分辨率下进行，而本文旨在直接在全分辨率图像上应用和谐化。<br>
                    方法：提出参数网络来预测全分辨率图像中每个像素的像素级颜色转换（PCTs）参数。通过实验发现仿射颜色转换既高效又有效，并探索了CNNs和Transformers作为参数网络，结果显示Transformers效果更好。<br>
                    效果：在公开的全分辨率iHarmony4数据集上评估该方法，结果显示，前景MSE和MSE值减少了20%以上，PSNR值增加了1.4dB，同时保持了轻量级的架构。在涉及20人的用户研究中，该方法实现了比最近两种其他方法更高的B-T分数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present PCT-Net, a simple and general image harmonization method that can be easily applied to images at full resolution. The key idea is to learn a parameter network that uses downsampled input images to predict the parameters for pixel-wise color transforms (PCTs) which are applied to each pixel in the full-resolution image. We show that affine color transforms are both efficient and effective, resulting in state-of-the-art harmonization results. Moreover, we explore both CNNs and Transformers as the parameter network and show that Transformers lead to better results. We evaluate the proposed method on the public full-resolution iHarmony4 dataset, which is comprised of four datasets, and show a reduction of the foreground MSE (fMSE) and MSE values by more than 20% and an increase of the PSNR value by 1.4dB while keeping the architecture light-weight. In a user study with 20 people, we show that the method achieves a higher B-T score than two other recent methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1499.Rawgment: Noise-Accounted RAW Augmentation Enables Recognition in a Wide Variety of Environments</span><br>
                <span class="as">Yoshimura, MasakazuandOtsuka, JunjiandIrie, AtsushiandOhashi, Takeshi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yoshimura_Rawgment_Noise-Accounted_RAW_Augmentation_Enables_Recognition_in_a_Wide_Variety_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14007-14017.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在具有挑战性的环境中（如极暗、模糊或高动态范围条件）训练有效的图像识别模型，而无需获取困难的数据。<br>
                    动机：创建此类环境的培训数据集既昂贵又困难，因此需要一种无需难以获得的数据集的稳健模型。<br>
                    方法：提出一种考虑噪声的RAW图像增强方法，将颜色抖动和模糊增强应用于RAW图像，然后应用非线性ISP，以产生真实的强度。此外，还引入了噪声量对齐方法，校准由增强引起的噪声属性的领域差距。<br>
                    效果：实验表明，仅使用简单的训练数据，所提出的考虑噪声的RAW图像增强方法就可以使图像识别精度在具有挑战性的环境中提高一倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image recognition models that work in challenging environments (e.g., extremely dark, blurry, or high dynamic range conditions) must be useful. However, creating training datasets for such environments is expensive and hard due to the difficulties of data collection and annotation. It is desirable if we could get a robust model without the need for hard-to-obtain datasets. One simple approach is to apply data augmentation such as color jitter and blur to standard RGB (sRGB) images in simple scenes. Unfortunately, this approach struggles to yield realistic images in terms of pixel intensity and noise distribution due to not considering the non-linearity of Image Signal Processors (ISPs) and noise characteristics of image sensors. Instead, we propose a noise-accounted RAW image augmentation method. In essence, color jitter and blur augmentation are applied to a RAW image before applying non-linear ISP, resulting in realistic intensity. Furthermore, we introduce a noise amount alignment method that calibrates the domain gap in the noise property caused by the augmentation. We show that our proposed noise-accounted RAW augmentation method doubles the image recognition accuracy in challenging environments only with simple training data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1500.The Dark Side of Dynamic Routing Neural Networks: Towards Efficiency Backdoor Injection</span><br>
                <span class="as">Chen, SiminandChen, HanlinandHaque, MirazulandLiu, CongandYang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_The_Dark_Side_of_Dynamic_Routing_Neural_Networks_Towards_Efficiency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24585-24594.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的动态神经网络在资源有限的设备上部署时，是否存在效率被操控的漏洞。<br>
                    动机：近年来，深度神经网络在资源受限的设备上的部署取得了进展，但我们发现这些网络可能存在效率被操控的漏洞。<br>
                    方法：我们提出了一种名为EfficFrog的对抗性攻击方法，通过向动态神经网络注入通用的效率后门来操纵其计算成本。<br>
                    效果：实验结果表明，EfficFrog能够有效地降低受攻击的动态神经网络在触发输入样本上的效率，同时保持清洁样本的效率基本不变。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advancements in deploying deep neural networks (DNNs) on resource-constrained devices have generated interest in input-adaptive dynamic neural networks (DyNNs). DyNNs offer more efficient inferences and enable the deployment of DNNs on devices with limited resources, such as mobile devices. However, we have discovered a new vulnerability in DyNNs that could potentially compromise their efficiency. Specifically, we investigate whether adversaries can manipulate DyNNs' computational costs to create a false sense of efficiency. To address this question, we propose EfficFrog, an adversarial attack that injects universal efficiency backdoors in DyNNs. To inject a backdoor trigger into DyNNs, EfficFrog poisons only a minimal percentage of the DyNNs' training data. During the inference phase, EfficFrog can slow down the backdoored DyNNs and abuse the computational resources of systems running DyNNs by adding the trigger to any input. To evaluate EfficFrog, we tested it on three DNN backbone architectures (based on VGG16, MobileNet, and ResNet56) using two popular datasets (CIFAR-10 and Tiny ImageNet). Our results demonstrate that EfficFrog reduces the efficiency of DyNNs on triggered input samples while keeping the efficiency of clean samples almost the same.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1501.Better &#x27;&#x27;CMOS&#x27;&#x27; Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution</span><br>
                <span class="as">Chen, XuhaiandZhang, JiangningandXu, ChaoandWang, YabiaoandWang, ChengjieandLiu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Better_CMOS_Produces_Clearer_Images_Learning_Space-Variant_Blur_Estimation_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1651-1661.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的盲图像超分辨率（SR）方法大多假设模糊核是空间不变的，但实际中的模糊由于物体运动、失焦等因素通常是空间变化的，导致高级SR方法性能下降。<br>
                    动机：为了解决这个问题，我们首先引入了两个新的具有离焦模糊的数据集，即NYUv2-BSR和Cityscapes-BSR，以支持空间变化模糊的盲SR的进一步研究。<br>
                    方法：基于这些数据集，我们设计了一个新颖的跨模态融合网络（CMOS），同时估计模糊和语义，从而提高了SR结果。它包含一个特征组交互注意力（GIA）模块，使两种模态更有效地互动并避免不一致。GIA的结构具有通用性，也可以用于其他特征的互动。<br>
                    效果：在上述数据集和真实图像上与最先进的方法进行定性和定量实验比较，例如在NYUv2-BSR上获得的PSNR/SSIM比MANet高出+1.91/+0.0048，证明了我们的方法的优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most of the existing blind image Super-Resolution (SR) methods assume that the blur kernels are space-invariant. However, the blur involved in real applications are usually space-variant due to object motion, out-of-focus, etc., resulting in severe performance drop of the advanced SR methods. To address this problem, we firstly introduce two new datasets with out-of-focus blur, i.e., NYUv2-BSR and Cityscapes-BSR, to support further researches of blind SR with space-variant blur. Based on the datasets, we design a novel Cross-MOdal fuSion network (CMOS) that estimate both blur and semantics simultaneously, which leads to improved SR results. It involves a feature Grouping Interactive Attention (GIA) module to make the two modals interact more effectively and avoid inconsistency. GIA can also be used for the interaction of other features because of the universality of its structure. Qualitative and quantitative experiments compared with state-of-the-art methods on above datasets and real-world images demonstrate the superiority of our method, e.g., obtaining PSNR/SSIM by +1.91/+0.0048 on NYUv2-BSR than MANet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1502.Backdoor Defense via Adaptively Splitting Poisoned Dataset</span><br>
                <span class="as">Gao, KuofengandBai, YangandGu, JindongandYang, YongandXia, Shu-Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Backdoor_Defense_via_Adaptively_Splitting_Poisoned_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4005-4014.png><br>
            
            <span class="tt"><span class="t0">研究问题：针对深度神经网络（DNN）易受后门攻击的问题，提出一种有效的训练阶段防御策略。<br>
                    动机：由于DNN通常使用来自不可信第三方的外部训练数据，因此在训练阶段实施强大的后门防御策略至关重要。<br>
                    方法：提出了一种基于自适应数据集分割的防御策略（ASD）。具体来说，我们采用损失引导的分割和元学习启发的分割来动态更新两个数据池。通过将干净的数据池和污染的数据池进行分割，ASD成功地在训练过程中抵御了后门攻击。<br>
                    效果：在多个基准数据集和DNN模型上进行的大量实验表明，我们的ASD在对抗六种最先进的后门攻击方面具有优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Backdoor defenses have been studied to alleviate the threat of deep neural networks (DNNs) being backdoor attacked and thus maliciously altered. Since DNNs usually adopt some external training data from an untrusted third party, a robust backdoor defense strategy during the training stage is of importance. We argue that the core of training-time defense is to select poisoned samples and to handle them properly. In this work, we summarize the training-time defenses from a unified framework as splitting the poisoned dataset into two data pools. Under our framework, we propose an adaptively splitting dataset-based defense (ASD). Concretely, we apply loss-guided split and meta-learning-inspired split to dynamically update two data pools. With the split clean data pool and polluted data pool, ASD successfully defends against backdoor attacks during training. Extensive experiments on multiple benchmark datasets and DNN models against six state-of-the-art backdoor attacks demonstrate the superiority of our ASD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1503.Wide-Angle Rectification via Content-Aware Conformal Mapping</span><br>
                <span class="as">Zhang, QiandLi, HongdongandWang, Qing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Wide-Angle_Rectification_via_Content-Aware_Conformal_Mapping_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17357-17365.png><br>
            
            <span class="tt"><span class="t0">研究问题：智能手机相机的超广角镜头经常会产生严重的图像扭曲，如曲线线性结构、不自然的歪斜面孔等。<br>
                    动机：尽管现有的大多数矫正方法都采用全局变形转换来纠正输入的广角图像，但其效果并不完全令人满意，许多不需要的残余畸变未被纠正或以牺牲预期的宽视场为代价。<br>
                    方法：本文提出了一种新的方法来解决这些挑战。具体来说，我们推导出了一种局部自适应极域保形映射来矫正广角图像。映射的参数通过深度神经网络分析图像内容自动找到。<br>
                    效果：通过对大量照片进行实验，证实了所提出的方法与所有现有方法相比具有优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the proliferation of ultra wide-angle lenses on smartphone cameras, such lenses often come with severe image distortion (e.g. curved linear structure, unnaturally skewed faces). Most existing rectification methods adopt a global warping transformation to undistort the input wide-angle image, yet their performances are not entirely satisfactory, leaving many unwanted residue distortions uncorrected or at the sacrifice of the intended wide FoV (field-of-view). This paper proposes a new method to tackle these challenges. Specifically, we derive a locally-adaptive polar-domain conformal mapping to rectify a wide-angle image. Parameters of the mapping are found automatically by analyzing image contents via deep neural networks. Experiments on large number of photos have confirmed the superior performance of the proposed method compared with all available previous methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1504.Zero-Shot Noise2Noise: Efficient Image Denoising Without Any Data</span><br>
                <span class="as">Mansour, YoussefandHeckel, Reinhard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mansour_Zero-Shot_Noise2Noise_Efficient_Image_Denoising_Without_Any_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14018-14027.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用无监督神经网络进行高质量的图像去噪，同时降低计算成本？<br>
                    动机：现有的无监督去噪方法或者需要复杂的计算，或者需要对噪声分布有了解，或者无法达到理想的图像质量。<br>
                    方法：本文提出了一种简单的两层网络ZS-N2N（零射击Noise2Noise），无需任何训练数据或噪声分布知识，即可实现高质量的图像去噪，且计算成本低。<br>
                    效果：在人工、真实世界相机和显微镜噪声的实验中，ZS-N2N方法在减少成本的同时，往往优于现有的无监督去噪方法，适用于数据稀缺和计算能力有限的应用场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, self-supervised neural networks have shown excellent image denoising performance. However, current dataset free methods are either computationally expensive, require a noise model, or have inadequate image quality. In this work we show that a simple 2-layer network, without any training data or knowledge of the noise distribution, can enable high-quality image denoising at low computational cost. Our approach is motivated by Noise2Noise and Neighbor2Neighbor and works well for denoising pixel-wise independent noise. Our experiments on artificial, real-world camera, and microscope noise show that our method termed ZS-N2N (Zero Shot Noise2Noise) often outperforms existing dataset-free methods at a reduced cost, making it suitable for use cases with scarce data availability and limited compute.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1505.Generating Anomalies for Video Anomaly Detection With Prompt-Based Feature Mapping</span><br>
                <span class="as">Liu, ZuhaoandWu, Xiao-MingandZheng, DianandLin, Kun-YuandZheng, Wei-Shi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Generating_Anomalies_for_Video_Anomaly_Detection_With_Prompt-Based_Feature_Mapping_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24500-24510.png><br>
            
            <span class="tt"><span class="t0">研究问题：在监控视频中进行异常检测是一项具有挑战性的计算机视觉任务，因为训练时只有正常视频可用。<br>
                    动机：虽然最近的工作发布了第一个虚拟异常检测数据集以帮助现实世界的检测，但存在一个异常差距，因为虚拟数据集中的限制与现实世界中的无限制形成了对比，这降低了虚拟数据集的泛化能力。同时，虚拟和现实场景之间也存在一个场景差距，包括特定于场景的异常（在一个场景中异常但在另一个场景中正常的事件）和特定于场景的属性，如监控摄像头的视角。<br>
                    方法：本文提出了一种基于提示的特征映射框架（PFMF），该框架包含一个由异常提示引导的映射网络，用于生成现实中未见过的类型无限的异常，以及一个映射适应分支，通过应用领域分类器和异常分类器来缩小场景差距。<br>
                    效果：所提出的框架在三个基准数据集上的表现优于最先进的技术。广泛的消融实验也证明了我们框架设计的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Anomaly detection in surveillance videos is a challenging computer vision task where only normal videos are available during training. Recent work released the first virtual anomaly detection dataset to assist real-world detection. However, an anomaly gap exists because the anomalies are bounded in the virtual dataset but unbounded in the real world, so it reduces the generalization ability of the virtual dataset. There also exists a scene gap between virtual and real scenarios, including scene-specific anomalies (events that are abnormal in one scene but normal in another) and scene-specific attributes, such as the viewpoint of the surveillance camera. In this paper, we aim to solve the problem of the anomaly gap and scene gap by proposing a prompt-based feature mapping framework (PFMF). The PFMF contains a mapping network guided by an anomaly prompt to generate unseen anomalies with unbounded types in the real scenario, and a mapping adaptation branch to narrow the scene gap by applying domain classifier and anomaly classifier. The proposed framework outperforms the state-of-the-art on three benchmark datasets. Extensive ablation experiments also show the effectiveness of our framework design.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1506.RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors</span><br>
                <span class="as">Wu, Rui-QiandDuan, Zheng-PengandGuo, Chun-LeandChai, ZhiandLi, Chongyi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_RIDCP_Revitalizing_Real_Image_Dehazing_via_High-Quality_Codebook_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22282-22291.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的去雾方法由于缺乏真实数据对和强大的先验知识，难以处理现实世界的模糊图像。<br>
                    动机：我们提出了一种新的真实图像去雾方法，通过合成更真实的模糊数据和在网络中引入更强大的先验知识。<br>
                    方法：(1) 我们重新思考了真实模糊图像的退化过程，并提出了考虑多种退化类型的现象学管道。(2) 我们提出了一种通过高质量码本先验（RIDCP）的真实图像去雾网络。首先，我们在大规模高质量数据集上预训练一个VQGAN以获取离散码本，封装高质量的先验知识。然后，我们使用新的归一化特征对齐模块来有效地利用高质量的特征并生成清晰的结果。<br>
                    效果：我们的实验表明，我们的退化管道大大减少了合成数据和真实数据之间的领域差距，但我们仍然难以避免这个问题，这挑战了野外HQPs匹配。因此，我们通过可控的匹配操作重新计算特征与HQPs匹配的距离，以找到更好的对应关系。我们还提供了一个基于可解释解决方案的建议，用户可以根据他们的偏好灵活调整增强程度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing dehazing approaches struggle to process real-world hazy images owing to the lack of paired real data and robust priors. In this work, we present a new paradigm for real image dehazing from the perspectives of synthesizing more realistic hazy data and introducing more robust priors into the network. Specifically, (1) instead of adopting the de facto physical scattering model, we rethink the degradation of real hazy images and propose a phenomenological pipeline considering diverse degradation types. (2) We propose a Real Image Dehazing network via high-quality Codebook Priors (RIDCP). Firstly, a VQGAN is pre-trained on a large-scale high-quality dataset to obtain the discrete codebook, encapsulating high-quality priors (HQPs). After replacing the negative effects brought by haze with HQPs, the decoder equipped with a novel normalized feature alignment module can effectively utilize high-quality features and produce clean results. However, although our degradation pipeline drastically mitigates the domain gap between synthetic and real data, it is still intractable to avoid it, which challenges HQPs matching in the wild. Thus, we re-calculate the distance when matching the features to the HQPs by a controllable matching operation, which facilitates finding better counterparts. We provide a recommendation to control the matching based on an explainable solution. Users can also flexibly adjust the enhancement degree as per their preference. Extensive experiments verify the effectiveness of our data synthesis pipeline and the superior performance of RIDCP in real image dehazing. Code and data will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1507.Backdoor Attacks Against Deep Image Compression via Adaptive Frequency Trigger</span><br>
                <span class="as">Yu, YiandWang, YufeiandYang, WenhanandLu, ShijianandTan, Yap-PengandKot, AlexC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Backdoor_Attacks_Against_Deep_Image_Compression_via_Adaptive_Frequency_Trigger_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12250-12259.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新型的针对学习型图像压缩模型的多触发器后门攻击方法。<br>
                    动机：现有的压缩系统和标准广泛使用离散余弦变换（DCT），因此，我们提出了一种在DCT域中添加触发器的基于频率的触发器注入模型。<br>
                    方法：设计了几种针对不同攻击场景的攻击目标，包括：1）通过比特率和重建质量攻击压缩质量；2）针对任务驱动的测量，如下游人脸识别和语义分割进行攻击。同时，设计了一种新颖的简单动态损失函数，以自适应地平衡不同损失项的影响，从而实现更有效的训练。<br>
                    效果：实验表明，通过我们的训练触发器注入模型和对压缩模型编码器参数的简单修改，所提出的方法可以在单个图像压缩模型中成功注入多个带有相应触发器的后门。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent deep-learning-based compression methods have achieved superior performance compared with traditional approaches. However, deep learning models have proven to be vulnerable to backdoor attacks, where some specific trigger patterns added to the input can lead to malicious behavior of the models. In this paper, we present a novel backdoor attack with multiple triggers against learned image compression models. Motivated by the widely used discrete cosine transform (DCT) in existing compression systems and standards, we propose a frequency-based trigger injection model that adds triggers in the DCT domain. In particular, we design several attack objectives for various attacking scenarios, including: 1) attacking compression quality in terms of bit-rate and reconstruction quality; 2) attacking task-driven measures, such as down-stream face recognition and semantic segmentation. Moreover, a novel simple dynamic loss is designed to balance the influence of different loss terms adaptively, which helps achieve more efficient training. Extensive experiments show that with our trained trigger injection models and simple modification of encoder parameters (of the compression model), the proposed attack can successfully inject several backdoors with corresponding triggers in a single image compression model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1508.Ensemble-Based Blackbox Attacks on Dense Prediction</span><br>
                <span class="as">Cai, ZikuiandTan, YaotengandAsif, M.Salman</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Ensemble-Based_Blackbox_Attacks_on_Dense_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4045-4055.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对稠密预测模型（如物体检测器和分割器）进行对抗性攻击？<br>
                    动机：现有的对抗性攻击方法，由单一的代理模型生成的攻击并不能转移到任意的（黑箱）受害者模型上。<br>
                    方法：我们提出了一种精心设计的集成方法，可以对多个受害者模型进行有效的攻击。特别是，我们发现个体模型权重的归一化在攻击的成功中起着关键作用。然后，我们通过根据受害者模型调整集成权重来进一步提高攻击的性能。<br>
                    效果：我们在物体检测和分割上进行了一系列的实验，以突出我们提出的方法的重要性。我们的基于集成的方法在物体检测和分割的黑盒攻击方法上表现优于现有的方法。最后，我们展示出我们的这种方法还可以生成一个单一的扰动，同时欺骗多个黑箱检测和分割模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose an approach for adversarial attacks on dense prediction models (such as object detectors and segmentation). It is well known that the attacks generated by a single surrogate model do not transfer to arbitrary (blackbox) victim models. Furthermore, targeted attacks are often more challenging than the untargeted attacks. In this paper, we show that a carefully designed ensemble can create effective attacks for a number of victim models. In particular, we show that normalization of the weights for individual models plays a critical role in the success of the attacks. We then demonstrate that by adjusting the weights of the ensemble according to the victim model can further improve the performance of the attacks. We performed a number of experiments for object detectors and segmentation to highlight the significance of the our proposed methods. Our proposed ensemble-based method outperforms existing blackbox attack methods for object detection and segmentation. Finally we show that our proposed method can also generate a single perturbation that can fool multiple blackbox detection and segmentation models simultaneously.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1509.sRGB Real Noise Synthesizing With Neighboring Correlation-Aware Noise Model</span><br>
                <span class="as">Fu, ZixuanandGuo, LanqingandWen, Bihan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_sRGB_Real_Noise_Synthesizing_With_Neighboring_Correlation-Aware_Noise_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1683-1691.png><br>
            
            <span class="tt"><span class="t0">研究问题：在标准RGB（sRGB）域中，由于复杂的噪声分布，对真实噪声进行建模和合成具有挑战性。<br>
                    动机：虽然大多数深度噪声生成器使用端到端训练的模型来合成sRGB的真实噪声，但由于缺乏明确的噪声建模，其合成噪声的质量会降低。<br>
                    方法：我们提出将真实噪声不仅建模为依赖于底层清洁图像像素强度，而且还与其在局部区域内的相邻噪声实现高度相关。相应地，我们在信号依赖性的基础上，提出了一种学习其相邻相关性的新型噪声合成框架。<br>
                    效果：通过提出的噪声模型，我们的框架大大缩小了合成噪声和真实噪声之间的分布差距。我们的实验表明，我们生成的“真实”sRGB有噪图像可以用于训练有监督的深度去噪器，从而大大提高其真实的去噪结果，与流行的经典去噪器或在其他sRGB噪声生成器上训练的深度去噪器相比，提高了很大程度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modeling and synthesizing real noise in the standard RGB (sRGB) domain is challenging due to the complicated noise distribution. While most of the deep noise generators proposed to synthesize sRGB real noise using an end-to-end trained model, the lack of explicit noise modeling degrades the quality of their synthesized noise. In this work, we propose to model the real noise as not only dependent on the underlying clean image pixel intensity, but also highly correlated to its neighboring noise realization within the local region. Correspondingly, we propose a novel noise synthesizing framework by explicitly learning its neighboring correlation on top of the signal dependency. With the proposed noise model, our framework greatly bridges the distribution gap between synthetic noise and real noise. We show that our generated "real" sRGB noisy images can be used for training supervised deep denoisers, thus to improve their real denoising results with a large margin, comparing to the popular classic denoisers or the deep denoisers that are trained on other sRGB noise generators. The code will be available at https://github.com/xuan611/sRGB-Real-Noise-Synthesizing.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1510.BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning</span><br>
                <span class="as">Oh, ChangdaeandHwang, HyejiandLee, Hee-youngandLim, YongTaekandJung, GeunyoungandJung, JiyoungandChoi, HosikandSong, Kyungwoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Oh_BlackVIP_Black-Box_Visual_Prompting_for_Robust_Transfer_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24224-24235.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对大规模预训练模型进行参数高效的迁移学习，特别是在无法直接访问模型参数和内存容量有限的情况下。<br>
                    动机：现有的迁移学习方法需要完全的模型参数和大量的内存，但在实际应用中，预训练模型往往是黑箱API或专有软件，无法直接访问参数，且内存需求大。<br>
                    方法：提出一种名为BlackVIP的方法，通过设计输入相关的图像形状视觉提示来改进少样本适应和分布/位置偏移的鲁棒性，并使用SPSA-GC有效估计目标模型的梯度以更新Coordinator。<br>
                    效果：在16个数据集上的大量实验表明，BlackVIP能够在不访问模型参数的情况下实现对不同领域的鲁棒适应，且内存需求最小。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the surge of large-scale pre-trained models (PTMs), fine-tuning these models to numerous downstream tasks becomes a crucial problem. Consequently, parameter efficient transfer learning (PETL) of large models has grasped huge attention. While recent PETL methods showcase impressive performance, they rely on optimistic assumptions: 1) the entire parameter set of a PTM is available, and 2) a sufficiently large memory capacity for the fine-tuning is equipped. However, in most real-world applications, PTMs are served as a black-box API or proprietary software without explicit parameter accessibility. Besides, it is hard to meet a large memory requirement for modern PTMs. In this work, we propose black-box visual prompting (BlackVIP), which efficiently adapts the PTMs without knowledge about model architectures and parameters. BlackVIP has two components; 1) Coordinator and 2) simultaneous perturbation stochastic approximation with gradient correction (SPSA-GC). The Coordinator designs input-dependent image-shaped visual prompts, which improves few-shot adaptation and robustness on distribution/location shift. SPSA-GC efficiently estimates the gradient of a target model to update Coordinator. Extensive experiments on 16 datasets demonstrate that BlackVIP enables robust adaptation to diverse domains without accessing PTMs' parameters, with minimal memory requirements. Code: https://github.com/changdaeoh/BlackVIP</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1511.Joint HDR Denoising and Fusion: A Real-World Mobile HDR Image Dataset</span><br>
                <span class="as">Liu, ShuaizhengandZhang, XindongandSun, LingchenandLiang, ZhetongandZeng, HuiandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Joint_HDR_Denoising_and_Fusion_A_Real-World_Mobile_HDR_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13966-13975.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高手机拍摄的HDR图像质量。<br>
                    动机：现有的HDR图像数据集大多由DSLR相机在白天拍摄，限制了其在手机HDR成像研究中的应用。<br>
                    方法：首次使用手机摄像头构建HDR图像数据集Mobile-HDR，并设计了一种基于transformer和金字塔交叉注意力对齐模块的模型，用于聚合不同曝光帧的高度相关特征，进行联合HDR去噪和融合。<br>
                    效果：实验证明，该方法在手机HDR成像上具有优势，且数据集有效可用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mobile phones have become a ubiquitous and indispensable photographing device in our daily life, while the small aperture and sensor size make mobile phones more susceptible to noise and over-saturation, resulting in low dynamic range (LDR) and low image quality. It is thus crucial to develop high dynamic range (HDR) imaging techniques for mobile phones. Unfortunately, the existing HDR image datasets are mostly constructed by DSLR cameras in daytime, limiting their applicability to the study of HDR imaging for mobile phones. In this work, we develop, for the first time to our best knowledge, an HDR image dataset by using mobile phone cameras, namely Mobile-HDR dataset. Specifically, we utilize three mobile phone cameras to collect paired LDR-HDR images in the raw image domain, covering both daytime and nighttime scenes with different noise levels. We then propose a transformer based model with a pyramid cross-attention alignment module to aggregate highly correlated features from different exposure frames to perform joint HDR denoising and fusion. Experiments validate the advantages of our dataset and our method on mobile HDR imaging. Dataset and codes are available at https://github.com/shuaizhengliu/Joint-HDRDN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1512.Detecting Backdoors During the Inference Stage Based on Corruption Robustness Consistency</span><br>
                <span class="as">Liu, XiaogengandLi, MinghuiandWang, HaoyuandHu, ShengshanandYe, DengpanandJin, HaiandWu, LibingandXiao, Chaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Detecting_Backdoors_During_the_Inference_Stage_Based_on_Corruption_Robustness_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16363-16372.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络容易受到后门攻击，如何有效地在推理阶段检测触发样本是一个重要的问题。<br>
                    动机：现有的检测方法需要对受害者模型有高度的可访问性、额外的干净数据或对后门触发器外观的知识，这限制了它们的实用性。<br>
                    方法：本文提出了一种新的测试时触发样本检测方法——TeCo，它只需要受害者模型的硬标签输出，而不需要任何额外的信息。该方法通过计算不同损坏下预测结果转变的严重程度偏差来评估测试时的鲁棒性一致性。<br>
                    效果：实验表明，与最先进的防御方法相比，TeCo在不同的后门攻击、数据集和模型架构上表现更好，AUROC提高了10%，稳定性提高了5倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks are proven to be vulnerable to backdoor attacks. Detecting the trigger samples during the inference stage, i.e., the test-time trigger sample detection, can prevent the backdoor from being triggered. However, existing detection methods often require the defenders to have high accessibility to victim models, extra clean data, or knowledge about the appearance of backdoor triggers, limiting their practicality. In this paper, we propose the test-time corruption robustness consistency evaluation (TeCo), a novel test-time trigger sample detection method that only needs the hard-label outputs of the victim models without any extra information. Our journey begins with the intriguing observation that the backdoor-infected models have similar performance across different image corruptions for the clean images, but perform discrepantly for the trigger samples. Based on this phenomenon, we design TeCo to evaluate test-time robustness consistency by calculating the deviation of severity that leads to predictions' transition across different corruptions. Extensive experiments demonstrate that compared with state-of-the-art defenses, which even require either certain information about the trigger types or accessibility of clean data, TeCo outperforms them on different backdoor attacks, datasets, and model architectures, enjoying a higher AUROC by 10% and 5 times of stability. The code is available at https://github.com/CGCL-codes/TeCo</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1513.Black-Box Sparse Adversarial Attack via Multi-Objective Optimisation</span><br>
                <span class="as">Williams, PhoenixNealeandLi, Ke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Williams_Black-Box_Sparse_Adversarial_Attack_via_Multi-Objective_Optimisation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12291-12301.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络易受对抗性图像影响，在安全关键任务中的可靠性引发关注。<br>
                    动机：现有的稀疏对抗攻击方法往往难以同时最小化修改像素的数量和修改的大小，且需要大量查询并假设可以无限制地访问目标神经网络。<br>
                    方法：提出一种新的多目标稀疏攻击算法，该算法在攻击过程中有效降低修改像素的数量和大小。算法借鉴了进化计算的思想，并引入了一种机制来优先处理与攻击者目标一致的目标。<br>
                    效果：该方法在CIFAR-10和ImageNet训练的DNN分类器上优于现有的稀疏攻击方法，仅需要少量查询预算，在达到有竞争力的攻击成功率的同时，对像素的干扰更少。总的来说，这种新的攻击算法通过同时最小化修改像素的数量和大小，解决了当前稀疏攻击方法的限制，其结果展示了该方法在受限场景中的有效性，强调了其在增强DNN安全性方面的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) are susceptible to adversarial images, raising concerns about their reliability in safety-critical tasks. Sparse adversarial attacks, which limit the number of modified pixels, have shown to be highly effective in causing DNNs to misclassify. However, existing methods often struggle to simultaneously minimize the number of modified pixels and the size of the modifications, often requiring a large number of queries and assuming unrestricted access to the targeted DNN. In contrast, other methods that limit the number of modified pixels often permit unbounded modifications, making them easily detectable. To address these limitations, we propose a novel multi-objective sparse attack algorithm that efficiently minimizes the number of modified pixels and their size during the attack process. Our algorithm draws inspiration from evolutionary computation and incorporates a mechanism for prioritizing objectives that aligns with an attacker's goals. Our approach outperforms existing sparse attacks on CIFAR-10 and ImageNet trained DNN classifiers while requiring only a small query budget, attaining competitive attack success rates while perturbing fewer pixels. Overall, our proposed attack algorithm provides a solution to the limitations of current sparse attack methods by jointly minimizing the number of modified pixels and their size. Our results demonstrate the effectiveness of our approach in restricted scenarios, highlighting its potential to enhance DNN security.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1514.HDR Imaging With Spatially Varying Signal-to-Noise Ratios</span><br>
                <span class="as">Chi, YihengandZhang, XingguangandChan, StanleyH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chi_HDR_Imaging_With_Spatially_Varying_Signal-to-Noise_Ratios_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5724-5734.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的HDR图像融合算法和去噪算法无法处理在低光环境下动态范围巨大且噪声空间变化的情况。<br>
                    动机：由于在低光环境下，一个曝光的动态范围可能非常大，并且噪声是空间变化的，这导致现有的图像去噪算法和HDR融合算法都无法很好地处理这种情况。<br>
                    方法：我们提出了一种新的方法，称为空间变化高动态范围（SV-HDR）融合网络，用于同时去噪和融合图像。我们在自定义设计的多尺度转换器框架中引入了一个新的曝光共享块。<br>
                    效果：在各种测试条件下，所提出的SV-HDR的性能优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While today's high dynamic range (HDR) image fusion algorithms are capable of blending multiple exposures, the acquisition is often controlled so that the dynamic range within one exposure is narrow. For HDR imaging in photon-limited situations, the dynamic range can be enormous and the noise within one exposure is spatially varying. Existing image denoising algorithms and HDR fusion algorithms both fail to handle this situation, leading to severe limitations in low-light HDR imaging. This paper presents two contributions. Firstly, we identify the source of the problem. We find that the issue is associated with the co-existence of (1) spatially varying signal-to-noise ratio, especially the excessive noise due to very dark regions, and (2) a wide luminance range within each exposure. We show that while the issue can be handled by a bank of denoisers, the complexity is high. Secondly, we propose a new method called the spatially varying high dynamic range (SV-HDR) fusion network to simultaneously denoise and fuse images. We introduce a new exposure-shared block within our custom-designed multi-scale transformer framework. In a variety of testing conditions, the performance of the proposed SV-HDR is better than the existing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1515.Progressive Backdoor Erasing via Connecting Backdoor and Adversarial Attacks</span><br>
                <span class="as">Mu, BingxuandNiu, ZhenxingandWang, LeandWang, XueandMiao, QiguangandJin, RongandHua, Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mu_Progressive_Backdoor_Erasing_via_Connecting_Backdoor_and_Adversarial_Attacks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20495-20503.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络（DNN）容易受到后门攻击和对抗性攻击，这两种攻击通常被视为不同的问题并分别解决。<br>
                    动机：本文发现后门攻击和对抗性攻击之间存在有趣的联系，即植入后门的模型其对抗性示例与触发样本的行为相似，都会激活DNN的同一子集神经元。<br>
                    方法：基于此观察，提出了一种新的渐进式后门擦除（PBE）算法，通过利用无目标对抗性攻击逐步净化受感染的模型。<br>
                    效果：在5种最先进的后门攻击下，实验表明，该方法可以在不显著降低干净样本性能的情况下有效擦除后门触发器，并显著优于现有的防御方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks (DNNs) are known to be vulnerable to both backdoor attacks as well as adversarial attacks. In the literature, these two types of attacks are commonly treated as distinct problems and solved separately, since they belong to training-time and inference-time attacks respectively. However, in this paper we find an intriguing connection between them: for a model planted with backdoors, we observe that its adversarial examples have similar behaviors as its triggered samples, i.e., both activate the same subset of DNN neurons. It indicates that planting a backdoor into a model will significantly affect the model's adversarial examples. Based on this observations, a novel Progressive Backdoor Erasing (PBE) algorithm is proposed to progressively purify the infected model by leveraging untargeted adversarial attacks. Different from previous backdoor defense methods, one significant advantage of our approach is that it can erase backdoor even when the additional clean dataset is unavailable. We empirically show that, against 5 state-of-the-art backdoor attacks, our AFT can effectively erase the backdoor triggers without obvious performance degradation on clean samples and significantly outperforms existing defense methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1516.DAA: A Delta Age AdaIN Operation for Age Estimation via Binary Code Transformer</span><br>
                <span class="as">Chen, PingandZhang, XingpengandLi, YeandTao, JuandXiao, BinandWang, BingandJiang, Zongjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DAA_A_Delta_Age_AdaIN_Operation_for_Age_Estimation_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15836-15845.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过计算机任务实现裸眼识别年龄。<br>
                    动机：由于难以获取每个年龄的代表性对比图像，计算机任务通常忽略了与其他人的年龄进行比较的想法。<br>
                    方法：设计Delta Age AdaIN（DAA）操作以获取每个年龄的特征差异，并通过学习代表均值和标准差的值来获取每个年龄的风格图。将迁移学习输入作为年龄自然数的二进制代码以获取连续的年龄特征信息。<br>
                    效果：在多个面部年龄数据集上，与最先进的方法相比，该方法具有更好的性能和更少的参数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Naked eye recognition of age is usually based on comparison with the age of others. However, this idea is ignored by computer tasks because it is difficult to obtain representative contrast images of each age. Inspired by the transfer learning, we designed the Delta Age AdaIN (DAA) operation to obtain the feature difference with each age, which obtains the style map of each age through the learned values representing the mean and standard deviation. We let the input of transfer learning as the binary code of age natural number to obtain continuous age feature information. The learned two groups of values in Binary code mapping are corresponding to the mean and standard deviation of the comparison ages. In summary, our method consists of four parts: FaceEncoder, DAA operation, Binary code mapping, and AgeDecoder modules. After getting the delta age via AgeDecoder, we take the average value of all comparison ages and delta ages as the predicted age. Compared with state-of-the-art methods, our method achieves better performance with fewer parameters on multiple facial age datasets. Code is available at https://github.com/redcping/Delta_Age_AdaIN</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1517.Can&#x27;t Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders</span><br>
                <span class="as">Sha, ZeyangandHe, XinleiandYu, NingandBackes, MichaelandZhang, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sha_Cant_Steal_Cont-Steal_Contrastive_Stealing_Attacks_Against_Image_Encoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16373-16383.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的预训练语言模型缺乏对丰富的结构化知识的利用，如何通过结合大规模文本语料库和知识图谱来训练一种增强的语言表示模型。<br>
                    动机：知识图谱中的有信息量的实体可以通过外部知识来增强语言表示，提升模型在各种NLP任务上的性能。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出ERNIE模型，该模型能同时充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised representation learning techniques have been developing rapidly to make full use of unlabeled images. They encode images into rich features that are oblivious to downstream tasks. Behind their revolutionary representation power, the requirements for dedicated model designs and a massive amount of computation resources expose image encoders to the risks of potential model stealing attacks - a cheap way to mimic the well-trained encoder performance while circumventing the demanding requirements. Yet conventional attacks only target supervised classifiers given their predicted labels and/or posteriors, which leaves the vulnerability of unsupervised encoders unexplored. In this paper, we first instantiate the conventional stealing attacks against encoders and demonstrate their severer vulnerability compared with downstream classifiers. To better leverage the rich representation of encoders, we further propose Cont-Steal, a contrastive-learning-based attack, and validate its improved stealing effectiveness in various experiment settings. As a takeaway, we appeal to our community's attention to the intellectual property protection of representation learning techniques, especially to the defenses against encoder stealing attacks like ours.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1518.Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision</span><br>
                <span class="as">Tripathi, AditayandSingh, RishubhandChakraborty, AnirbanandShenoy, Pradeep</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_Edges_to_Shapes_to_Concepts_Adversarial_Augmentation_for_Robust_Vision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24470-24479.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度视觉模型过于依赖低层次的纹理特征，导致泛化能力差。<br>
                    动机：提出一种简单、轻量级的对抗性增强技术，以解决深度神经网络中的纹理偏差问题。<br>
                    方法：通过将一张图像的边缘图叠加到另一张打乱补丁的图像上，并使用随机确定的比例混合，生成增强的图像。然后让模型对增强的图像进行分类，从而学习整体形状以进行准确的预测。<br>
                    效果：实验结果表明，这种增强技术在各种数据集和神经网络结构上都显著提高了分类准确性和鲁棒性。例如，对于ViT-S模型，分类准确性提高了6%。在自然对抗性和分布外数据集（如ImageNet-A和ImageNet-R）上，分别获得了28%和8.5%的增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent work has shown that deep vision models tend to be overly dependent on low-level or "texture" features, leading to poor generalization. Various data augmentation strategies have been proposed to overcome this so-called texture bias in DNNs. We propose a simple, lightweight adversarial augmentation technique that explicitly incentivizes the network to learn holistic shapes for accurate prediction in an object classification setting. Our augmentations superpose edgemaps from one image onto another image with shuffled patches, using a randomly determined mixing proportion, with the image label of the edgemap image. To classify these augmented images, the model needs to not only detect and focus on edges but distinguish between relevant and spurious edges. We show that our augmentations significantly improve classification accuracy and robustness measures on a range of datasets and neural architectures. As an example, for ViT-S, We obtain absolute gains on classification accuracy gains up to 6%. We also obtain gains of up to 28% and 8.5% on natural adversarial and out-of-distribution datasets like ImageNet-A (for ViTB) and ImageNet-R (for ViT-S), respectively. Analysis using a range of probe datasets shows substantially increased shape sensitivity in our trained models, explaining the observed improvement in robustness and classification accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1519.Feature Separation and Recalibration for Adversarial Robustness</span><br>
                <span class="as">Kim, WooJaeandCho, YoonkiandJung, JunsikandYoon, Sung-Eui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Feature_Separation_and_Recalibration_for_Adversarial_Robustness_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8183-8192.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络易受对抗性攻击，因为特征层面的扰动累积。目前的方法通过停用导致模型误预测的不稳健特征激活来提高模型的鲁棒性，但作者认为这些恶意激活仍然包含有区分性的线索，通过重新校准可以捕获额外的有用信息以进行正确的模型预测。<br>
                    动机：现有的方法虽然可以提高模型的鲁棒性，但仍有改进空间。作者提出一种新的、易于插入的方法——特征分离和重新校准（FSR），通过对恶意的、不稳健的激活进行分离和重新校准，生成更鲁棒的特征映射。<br>
                    方法：FSR方法首先将输入特征图分离为帮助模型做出正确预测的稳健特征和导致模型在对抗性攻击下误预测的非稳健特征。然后，对非稳健激活进行调整，以恢复可能对模型预测有用的线索。<br>
                    效果：大量实验证明，FSR方法优于传统的去活技术，并通过微小的计算开销将现有对抗训练方法的鲁棒性提高了8.57%。代码可在https://github.com/wkim97/FSR获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks are susceptible to adversarial attacks due to the accumulation of perturbations in the feature level, and numerous works have boosted model robustness by deactivating the non-robust feature activations that cause model mispredictions. However, we claim that these malicious activations still contain discriminative cues and that with recalibration, they can capture additional useful information for correct model predictions. To this end, we propose a novel, easy-to-plugin approach named Feature Separation and Recalibration (FSR) that recalibrates the malicious, non-robust activations for more robust feature maps through Separation and Recalibration. The Separation part disentangles the input feature map into the robust feature with activations that help the model make correct predictions and the non-robust feature with activations that are responsible for model mispredictions upon adversarial attack. The Recalibration part then adjusts the non-robust activations to restore the potentially useful cues for model predictions. Extensive experiments verify the superiority of FSR compared to traditional deactivation techniques and demonstrate that it improves the robustness of existing adversarial training methods by up to 8.57% with small computational overhead. Codes are available at https://github.com/wkim97/FSR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1520.The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</span><br>
                <span class="as">Kang, Gi-CheonandKim, SungdongandKim, Jin-HwaandKwak, DonghyunandZhang, Byoung-Tak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_The_Dialog_Must_Go_On_Improving_Visual_Dialog_via_Generative_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6746-6756.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉对话（VisDial）任务中的问题，即如何通过图像和对话历史来回答问题。<br>
                    动机：目前的视觉对话模型主要通过监督学习或在相关视觉语言数据集上的预训练进行训练，但这种方法需要大量的标注数据。因此，本文提出了一种半监督学习方法，利用网络上的无标签图像来训练视觉对话模型。<br>
                    方法：本文提出的方法是生成式自我训练（GST）。首先，通过检测分布外的数据来检索领域内的图片，然后通过多模态条件文本生成生成关于这些图片的合成对话。最后，使用原始的VisDial数据和合成的对话数据训练对话代理。<br>
                    效果：实验结果表明，GST在VisDial v1.0和v0.9数据集上取得了新的最先进的结果。此外，GST对视觉和文本对抗攻击具有鲁棒性，并且在低数据量的情况下也表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual dialog (VisDial) is a task of answering a sequence of questions grounded in an image, using the dialog history as context. Prior work has trained the dialog agents solely on VisDial data via supervised learning or leveraged pre-training on related vision-and-language datasets. This paper presents a semi-supervised learning approach for visually-grounded dialog, called Generative Self-Training (GST), to leverage unlabeled images on the Web. Specifically, GST first retrieves in-domain images through out-of-distribution detection and generates synthetic dialogs regarding the images via multimodal conditional text generation. GST then trains a dialog agent on the synthetic and the original VisDial data. As a result, GST scales the amount of training data up to an order of magnitude that of VisDial (1.2M to 12.9M QA data). For robust training of the synthetic dialogs, we also propose perplexity-based data selection and multimodal consistency regularization. Evaluation on VisDial v1.0 and v0.9 datasets shows that GST achieves new state-of-the-art results on both datasets. We further observe the robustness of GST against both visual and textual adversarial attacks. Finally, GST yields strong performance gains in the low-data regime. Code is available at https://github.com/gicheonkang/gst-visdial.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1521.Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks</span><br>
                <span class="as">Li, SiminandZhang, ShuningandChen, GujunandWang, DongandFeng, PuandWang, JiakaiandLiu, AishanandYi, XinandLiu, Xianglong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Towards_Benchmarking_and_Assessing_Visual_Naturalness_of_Physical_World_Adversarial_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12324-12333.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的物理世界对抗攻击评估方法存在误差、偏差和不一致的问题。<br>
                    动机：为了解决这些问题，本文提出了一种评估物理世界对抗攻击自然度的新方法。<br>
                    方法：首先，创建了第一个包含人类评级和注视的物理攻击自然度（PAN）数据集。其次，引入了双重先验对齐（DPA）网络，该网络旨在将人类知识嵌入模型推理过程中，以模仿人类的自然度评估方式和注视行为。<br>
                    效果：实验结果表明，该方法能够更准确地评估物理世界对抗攻击的自然度，并有助于改进和自动评估自然度的研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Physical world adversarial attack is a highly practical and threatening attack, which fools real world deep learning systems by generating conspicuous and maliciously crafted real world artifacts. In physical world attacks, evaluating naturalness is highly emphasized since human can easily detect and remove unnatural attacks. However, current studies evaluate naturalness in a case-by-case fashion, which suffers from errors, bias and inconsistencies. In this paper, we take the first step to benchmark and assess visual naturalness of physical world attacks, taking autonomous driving scenario as the first attempt. First, to benchmark attack naturalness, we contribute the first Physical Attack Naturalness (PAN) dataset with human rating and gaze. PAN verifies several insights for the first time: naturalness is (disparately) affected by contextual features (i.e., environmental and semantic variations) and correlates with behavioral feature (i.e., gaze signal). Second, to automatically assess attack naturalness that aligns with human ratings, we further introduce Dual Prior Alignment (DPA) network, which aims to embed human knowledge into model reasoning process. Specifically, DPA imitates human reasoning in naturalness assessment by rating prior alignment and mimics human gaze behavior by attentive prior alignment. We hope our work fosters researches to improve and automatically assess naturalness of physical world attacks. Our code and exemplar data can be found at https://github.com/zhangsn-19/PAN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1522.ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing</span><br>
                <span class="as">Li, XiaodanandChen, YuefengandZhu, YaoandWang, ShuhuiandZhang, RongandXue, Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ImageNet-E_Benchmarking_Neural_Network_Robustness_via_Attribute_Editing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20371-20381.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习模型对图像属性的改变非常敏感，如何提高模型的鲁棒性？<br>
                    动机：目前的研究中，大部分关注于模型在分布外数据上的鲁棒性，而忽视了模型在分布内数据上可能存在的问题。本研究通过在分布内数据上进行模型调试，探索模型可能对哪些对象属性敏感。<br>
                    方法：创建了一个包含背景、大小、位置和方向控制的对象编辑工具包，并创建了一个名为ImageNet-E的严格基准，用于评估模型在对象属性方面的鲁棒性。使用这个基准，评估了当前包括卷积神经网络和视觉转换器在内的深度学习模型的性能。<br>
                    效果：研究发现，大多数模型对属性变化非常敏感。背景的微小变化会导致top-1准确率平均下降9.23%。同时，一些被认为具有鲁棒性的模型，如对抗训练的模型，实际上在面对属性变化时的表现比基础模型更差。基于这些发现，提出了通过预处理、架构设计和训练策略来提高属性鲁棒性的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies have shown that higher accuracy on ImageNet usually leads to better robustness against different corruptions. In this paper, instead of following the traditional research paradigm that investigates new out-of-distribution corruptions or perturbations deep models may encounter, we conduct model debugging in in-distribution data to explore which object attributes a model may be sensitive to. To achieve this goal, we create a toolkit for object editing with controls of backgrounds, sizes, positions, and directions, and create a rigorous benchmark named ImageNet-E(diting) for evaluating the image classifier robustness in terms of object attributes. With our ImageNet-E, we evaluate the performance of current deep learning models, including both convolutional neural networks and vision transformers. We find that most models are quite sensitive to attribute changes. An imperceptible change in the background can lead to an average of 9.23% drop on top-1 accuracy. We also evaluate some robust models including both adversarially trained models and other robust trained models and find that some models show worse robustness against attribute changes than vanilla models. Based on these findings, we discover ways to enhance attribute robustness with preprocessing, architecture designs, and training strategies. We hope this work can provide some insights to the community and open up a new avenue for research in robust computer vision. The code and dataset will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1523.Real-Time Controllable Denoising for Image and Video</span><br>
                <span class="as">Zhang, ZhaoyangandJiang, YitongandShao, WenqiandWang, XiaogangandLuo, PingandLin, KaimoandGu, Jinwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Real-Time_Controllable_Denoising_for_Image_and_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14028-14038.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决传统基于滤波的去噪方法在调整去噪强度时需要反复进行网络推理的问题，以及现有神经网络模型在实时用户交互中无法实现任意去噪级别调整的问题。<br>
                    动机：传统的基于滤波的去噪方法可以通过调整滤波强度来实现去噪级别的调整，但对于神经网络模型来说，每次调整去噪强度都需要进行网络推理，这在实时用户交互中几乎不可能实现。<br>
                    方法：本文提出了一种名为Real-time Controllable Denoising (RCD)的深度图像和视频去噪管道，该管道通过替换现有的CNN模型的最后一层输出（通常输出单个噪声图）为一个轻量级模块来输出多个噪声图，实现了对任意去噪级别的实时编辑。<br>
                    效果：实验表明，RCD可以在不牺牲原有性能的情况下，为各种现有的重量级模型实现实时可编辑的图像和视频去噪，且无需进行网络推理。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Controllable image denoising aims to generate clean samples with human perceptual priors and balance sharpness and smoothness. In traditional filter-based denoising methods, this can be easily achieved by adjusting the filtering strength. However, for NN (Neural Network)-based models, adjusting the final denoising strength requires performing network inference each time, making it almost impossible for real-time user interaction. In this paper, we introduce Real-time Controllable Denoising (RCD), the first deep image and video denoising pipeline that provides a fully controllable user interface to edit arbitrary denoising levels in real-time with only one-time network inference. Unlike existing controllable denoising methods that require multiple denoisers and training stages, RCD replaces the last output layer (which usually outputs a single noise map) of an existing CNN-based model with a lightweight module that outputs multiple noise maps. We propose a novel Noise Decorrelation process to enforce the orthogonality of the noise feature maps, allowing arbitrary noise level control through noise map interpolation. This process is network-free and does not require network inference. Our experiments show that RCD can enable real-time editable image and video denoising for various existing heavy-weight models without sacrificing their original performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1524.SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection</span><br>
                <span class="as">Xiang, TiangeandZhang, YixiaoandLu, YongyiandYuille, AlanL.andZhang, ChaoyiandCai, WeidongandZhou, Zongwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiang_SQUID_Deep_Feature_In-Painting_for_Unsupervised_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23890-23901.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用放射成像协议中的结构化信息，进行图像修复和异常检测？<br>
                    动机：放射成像协议产生的图像具有高度相似性，且患者之间的解剖结构重复出现。为了利用这种结构化信息，我们提出了空间感知的记忆队列用于放射成像图像的修复和异常检测（简称SQUID）。<br>
                    方法：我们使用空间感知的记忆队列对固有的解剖结构进行分类，形成重复的模式。在推理过程中，它可以识别图像中的异常（未见过/修改过的模式）。<br>
                    效果：实验结果表明，SQUID在两个胸部X射线基准数据集上的无监督异常检测中，至少比13种最先进的方法高出5个百分点，这是通过计算曲线下面积（AUC）得出的。此外，我们还创建了一个新的数据集（DigitAnatomy），该数据集合成了胸部解剖的空间相关性和一致的形状。我们希望DigitAnatomy能够促进异常检测方法的开发、评估和可解释性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. To exploit this structured information, we propose the use of Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (abbreviated as SQUID). We show that SQUID can taxonomize the ingrained anatomical structures into recurrent patterns; and in the inference, it can identify anomalies (unseen/modified patterns) in the image. SQUID surpasses 13 state-of-the-art methods in unsupervised anomaly detection by at least 5 points on two chest X-ray benchmark datasets measured by the Area Under the Curve (AUC). Additionally, we have created a new dataset (DigitAnatomy), which synthesizes the spatial correlation and consistent shape in chest anatomy. We hope DigitAnatomy can prompt the development, evaluation, and interpretability of anomaly detection methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1525.Visual Recognition-Driven Image Restoration for Multiple Degradation With Intrinsic Semantics Recovery</span><br>
                <span class="as">Yang, ZizhengandHuang, JieandChang, JiahaoandZhou, ManandYu, HuandZhang, JinghaoandZhao, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Visual_Recognition-Driven_Image_Restoration_for_Multiple_Degradation_With_Intrinsic_Semantics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14059-14070.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度图像识别模型在应用于低质量图像时性能显著下降。<br>
                    动机：目前的图像恢复或领域适应方法要么关注视觉质量而非识别质量，要么需要任务特定的语义标注进行训练。<br>
                    方法：提出一种名为VRD-IR的视觉识别驱动的图像恢复网络，用于处理多种类型的未知图像损坏，通过在一个模型中从视觉识别的角度恢复高质量图像。<br>
                    效果：实验表明，VRD-IR优于现有的图像恢复方法，并在分类、检测和人员再识别等高级任务上表现出优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep image recognition models suffer a significant performance drop when applied to low-quality images since they are trained on high-quality images. Although many studies have investigated to solve the issue through image restoration or domain adaptation, the former focuses on visual quality rather than recognition quality, while the latter requires semantic annotations for task-specific training. In this paper, to address more practical scenarios, we propose a Visual Recognition-Driven Image Restoration network for multiple degradation, dubbed VRD-IR, to recover high-quality images from various unknown corruption types from the perspective of visual recognition within one model. Concretely, we harmonize the semantic representations of diverse degraded images into a unified space in a dynamic manner, and then optimize them towards intrinsic semantics recovery. Moreover, a prior-ascribing optimization strategy is introduced to encourage VRD-IR to couple with various downstream recognition tasks better. Our VRD-IR is corruption- and recognition-agnostic, and can be inserted into various recognition tasks directly as an image enhancement module. Extensive experiments on multiple image distortions demonstrate that our VRD-IR surpasses existing image restoration methods and show superior performance on diverse high-level tasks, including classification, detection, and person re-identification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1526.You Are Catching My Attention: Are Vision Transformers Bad Learners Under Backdoor Attacks?</span><br>
                <span class="as">Yuan, ZenghuiandZhou, PanandZou, KaiandCheng, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_You_Are_Catching_My_Attention_Are_Vision_Transformers_Bad_Learners_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24605-24615.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器（ViTs）在产业化过程中面临的安全挑战，即后门攻击。<br>
                    动机：虽然视觉转换器（ViTs）在计算机视觉领域取得了显著的成功，但其对补丁级触发器的敏感性使其容易受到后门攻击的威胁。<br>
                    方法：作者设计了一种名为BadViT的新型后门攻击框架，利用通用的补丁级触发器来捕获模型的注意力，从而操纵ViTs的生存机制以迷惑自身。此外，作者还提出了BadViT的隐形变体，通过限制触发器扰动的强度来增加攻击的隐蔽性。<br>
                    效果：实验证明，BadViT是一种有效的针对ViTs的后门攻击方法，其依赖于毒药的数量较少，收敛性好，且可转移至下游任务。同时，作者还从现有高级防御方案的角度探讨了ViTs内部后门攻击的风险。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs), which made a splash in the field of computer vision (CV), have shaken the dominance of convolutional neural networks (CNNs). However, in the process of industrializing ViTs, backdoor attacks have brought severe challenges to security. The success of ViTs benefits from the self-attention mechanism. However, compared with CNNs, we find that this mechanism of capturing global information within patches makes ViTs more sensitive to patch-wise triggers. Under such observations, we delicately design a novel backdoor attack framework for ViTs, dubbed BadViT, which utilizes a universal patch-wise trigger to catch the model's attention from patches beneficial for classification to those with triggers, thereby manipulating the mechanism on which ViTs survive to confuse itself. Furthermore, we propose invisible variants of BadViT to increase the stealth of the attack by limiting the strength of the trigger perturbation. Through a large number of experiments, it is proved that BadViT is an efficient backdoor attack method against ViTs, which is less dependent on the number of poisons, with satisfactory convergence, and is transferable for downstream tasks. Furthermore, the risks inside of ViTs to backdoor attacks are also explored from the perspective of existing advanced defense schemes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1527.STDLens: Model Hijacking-Resilient Federated Learning for Object Detection</span><br>
                <span class="as">Chow, Ka-HoandLiu, LingandWei, WenqiandIlhan, FatihandWu, Yanzhao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chow_STDLens_Model_Hijacking-Resilient_Federated_Learning_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16343-16351.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在联邦学习中防止模型劫持攻击。<br>
                    动机：联邦学习虽然有许多优点，但容易受到模型劫持攻击，攻击者可以通过在协作学习过程中植入特洛伊木马梯度来控制物体检测系统的行为。<br>
                    方法：本文提出了STDLens，一种保护联邦学习免受此类攻击的原则性方法。我们首先调查了现有的缓解机制，并分析了它们由于在梯度上的空间聚类分析中的固有错误而失败的原因。基于这些见解，我们引入了一个三层取证框架，以识别和驱逐特洛伊木马梯度，并在联邦学习的进程中恢复性能。<br>
                    效果：实验表明，STDLens可以保护联邦学习免受不同类型的模型劫持攻击，并在识别和删除特洛伊木马梯度方面优于现有方法，具有显著更高的精度和更低的误报率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STDLens can protect FL against different model hijacking attacks and outperform existing methods in identifying and removing Trojaned gradients with significantly higher precision and much lower false-positive rates. The source code is available at https://github.com/git-disl/STDLens.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1528.Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline</span><br>
                <span class="as">Ji, WeiandLi, JingjingandBian, ChengandZhou, ZongweiandZhao, JiayingandYuille, AlanL.andCheng, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Multispectral_Video_Semantic_Segmentation_A_Benchmark_Dataset_and_Baseline_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1094-1104.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现在复杂场景和恶劣条件下的鲁棒和可靠的语义分割。<br>
                    动机：现有的方法主要依赖RGB图像输入，但在恶劣天气或光线条件下效果不佳。因此，研究者开始探索使用RGB和红外热成像（RGBT）图像作为输入的多光谱语义分割方法。<br>
                    方法：本文提出了一种新的任务——多光谱视频语义分割（MVSS），并创建了一个包含738个校准后的RGB和热视频以及3545个细粒度像素级26类别语义标注的MVSeg数据集。同时，研究者还提出了一种有效的MVSS基线模型MVNet，这是首个从多光谱和时间上下文中联合学习语义表示的模型。<br>
                    效果：实验证明，多光谱视频输入的使用显著提高了语义分割的效果，而MVNet基线模型的有效性也得到了验证。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust and reliable semantic segmentation in complex scenes is crucial for many real-life applications such as autonomous safe driving and nighttime rescue. In most approaches, it is typical to make use of RGB images as input. They however work well only in preferred weather conditions; when facing adverse conditions such as rainy, overexposure, or low-light, they often fail to deliver satisfactory results. This has led to the recent investigation into multispectral semantic segmentation, where RGB and thermal infrared (RGBT) images are both utilized as input. This gives rise to significantly more robust segmentation of image objects in complex scenes and under adverse conditions. Nevertheless, the present focus in single RGBT image input restricts existing methods from well addressing dynamic real-world scenes. Motivated by the above observations, in this paper, we set out to address a relatively new task of semantic segmentation of multispectral video input, which we refer to as Multispectral Video Semantic Segmentation, or MVSS in short. An in-house MVSeg dataset is thus curated, consisting of 738 calibrated RGB and thermal videos, accompanied by 3,545 fine-grained pixel-level semantic annotations of 26 categories. Our dataset contains a wide range of challenging urban scenes in both daytime and nighttime. Moreover, we propose an effective MVSS baseline, dubbed MVNet, which is to our knowledge the first model to jointly learn semantic representations from multispectral and temporal contexts. Comprehensive experiments are conducted using various semantic segmentation models on the MVSeg dataset. Empirically, the engagement of multispectral video input is shown to lead to significant improvement in semantic segmentation; the effectiveness of our MVNet baseline has also been verified.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1529.An In-Depth Exploration of Person Re-Identification and Gait Recognition in Cloth-Changing Conditions</span><br>
                <span class="as">Li, WeijiaandHou, SaihuiandZhang, ChunjieandCao, ChunshuiandLiu, XuandHuang, YongzhenandZhao, Yao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_An_In-Depth_Exploration_of_Person_Re-Identification_and_Gait_Recognition_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13824-13833.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决监控摄像头下目标行人的衣物变化问题，即人员重识别和步态识别的目标一致。<br>
                    动机：由于缺乏合适的衣物变化基准，基于视频的人员重识别在衣物变化问题上鲜有研究，而步态识别则常在受控条件下进行。<br>
                    方法：本文提出了一个衣物变化基准（CCPG），这是一个衣物变化数据集，具有几个亮点：（1）提供了200个身份标识，户外和室内共捕获了超过16K个序列；（2）每个身份标识有七种不同的衣物变化状态，这在以前的数据集上很少见；（3）为了便于研究，提供了RGB和剪影版数据。此外，为了系统地研究衣物变化问题，对基于视频的人员重识别和步态识别方法进行了全面实验。<br>
                    效果：实验结果表明，在不同的衣物变化条件下，人员重识别和步态识别各自具有优越性，并表明步态识别是解决衣物变化问题的一个潜在方案。该数据集将在https://github.com/BNU-IVC/CCPG上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The target of person re-identification (ReID) and gait recognition is consistent, that is to match the target pedestrian under surveillance cameras. For the cloth-changing problem, video-based ReID is rarely studied due to the lack of a suitable cloth-changing benchmark, and gait recognition is often researched under controlled conditions. To tackle this problem, we propose a Cloth-Changing benchmark for Person re-identification and Gait recognition (CCPG). It is a cloth-changing dataset, and there are several highlights in CCPG, (1) it provides 200 identities and over 16K sequences are captured indoors and outdoors, (2) each identity has seven different cloth-changing statuses, which is hardly seen in previous datasets, (3) RGB and silhouettes version data are both available for research purposes. Moreover, aiming to investigate the cloth-changing problem systematically, comprehensive experiments are conducted on video-based ReID and gait recognition methods. The experimental results demonstrate the superiority of ReID and gait recognition separately in different cloth-changing conditions and suggest that gait recognition is a potential solution for addressing the cloth-changing problem. Our dataset will be available at https://github.com/BNU-IVC/CCPG.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1530.Learning Weather-General and Weather-Specific Features for Image Restoration Under Multiple Adverse Weather Conditions</span><br>
                <span class="as">Zhu, YuruiandWang, TianyuandFu, XueyangandYang, XuanyuandGuo, XinandDai, JifengandQiao, YuandHu, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Learning_Weather-General_and_Weather-Specific_Features_for_Image_Restoration_Under_Multiple_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21747-21758.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过使用单一的网络参数集，去除与多种恶劣天气条件相关的图像失真。<br>
                    动机：不同天气条件下的失真图像既包含一般特性，也包含其特定特性。<br>
                    方法：设计了一个高效的统一框架，采用两阶段训练策略，分别学习通用和特定于天气的特征。第一阶段的目标是学习通用特征，第二阶段的目标是自适应地扩展每种天气类型的特定参数。<br>
                    效果：实验结果表明，该方法在所有的合成和真实世界基准数据集上都取得了优越的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image restoration under multiple adverse weather conditions aims to remove weather-related artifacts by using the single set of network parameters. In this paper, we find that distorted images under different weather conditions contain general characteristics as well as their specific characteristics. Inspired by this observation, we design an efficient unified framework with a two-stage training strategy to explore the weather-general and weather-specific features. The first training stage aims to learn the weather-general features by taking the images under various weather conditions as the inputs and outputting the coarsely restored results. The second training stage aims to learn to adaptively expand the specific parameters for each weather type in the deep model, where requisite positions for expansion of weather-specific parameters are learned automatically. Hence, we can obtain an efficient and unified model for image restoration under multiple adverse weather conditions. Moreover, we build the first real-world benchmark dataset with multiple weather conditions to better deal with real-world weather scenarios. Experimental results show that our method achieves superior performance on all the synthetic and real-world benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1531.Private Image Generation With Dual-Purpose Auxiliary Classifier</span><br>
                <span class="as">Chen, ChenandLiu, DaochangandMa, SiqiandNepal, SuryaandXu, Chang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Private_Image_Generation_With_Dual-Purpose_Auxiliary_Classifier_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20361-20370.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在保证图像生成的隐私性的同时，提高生成图像的质量和实用性？<br>
                    动机：在医疗等敏感数据有限的领域，保护隐私的图像生成非常重要。然而，由于隐私预算的限制，这可能会影响生成图像的质量和实用性。<br>
                    方法：提出了一种新的私有图像生成方法，该方法将一个双用途辅助分类器（交替从真实数据和伪造数据中学习）纳入了差分隐私GAN的训练中。此外，通过如顺序训练等特定的训练策略，可以加速生成器的收敛并进一步提高性能。<br>
                    效果：在MNIST、Fashion-MNIST和CelebA三个基准测试中，该方法在所有指标上都取得了新的最先进的成果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Privacy-preserving image generation has been important for segments such as medical domains that have sensitive and limited data. The benefits of guaranteed privacy come at the costs of generated images' quality and utility due to the privacy budget constraints. The utility is currently measured by the gen2real accuracy (g2r%), i.e., the accuracy on real data of a downstream classifier trained using generated data. However, apart from this standard utility, we identify the "reversed utility" as another crucial aspect, which computes the accuracy on generated data of a classifier trained using real data, dubbed as real2gen accuracy (r2g%). Jointly considering these two views of utility, the standard and the reversed, could help the generation model better improve transferability between fake and real data. Therefore, we propose a novel private image generation method that incorporates a dual-purpose auxiliary classifier, which alternates between learning from real data and fake data, into the training of differentially private GANs. Additionally, our deliberate training strategies such as sequential training contributes to accelerating the generator's convergence and further boosting the performance upon exhausting the privacy budget. Our results achieve new state-of-the-arts over all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1532.Generating Aligned Pseudo-Supervision From Non-Aligned Data for Image Restoration in Under-Display Camera</span><br>
                <span class="as">Feng, RuichengandLi, ChongyiandChen, HuaijinandLi, ShuaiandGu, JinweiandLoy, ChenChange</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Generating_Aligned_Pseudo-Supervision_From_Non-Aligned_Data_for_Image_Restoration_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5013-5022.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于难以收集大规模且完全对齐的配对训练数据用于Under-Display Camera (UDC)图像恢复，因此以前的研究方法依赖于基于监视器的图像系统或基于模拟的方法，牺牲了数据的逼真性并引入了领域差距。<br>
                    动机：本文重新审视了经典的立体设置用于训练数据收集——使用一个UDC和一个标准相机捕捉同一场景的两个图像。主要思想是从高质量的参考图像“复制”细节并将其“粘贴”到UDC图像上。虽然能够生成真实的训练对，但这种设置容易受到透视和景深变化引起的空间错位的影响。这个问题由于UDC和普通图像之间的大领域差异而进一步复杂化，这是UDC恢复所特有的。<br>
                    方法：本文通过一种新的基于变压器的框架来减轻非平凡的领域差异和空间错位，该框架为相应的UDC输入生成对齐良好且高质量的目标数据。这是通过两个精心设计的组件——域对齐模块（DAM）和几何对齐模块（GAM）实现的，这两个组件鼓励在UDC和普通视图之间发现稳健和准确的对应关系。<br>
                    效果：大量的实验表明，高质量且对齐良好的伪UDC训练对对于训练一个鲁棒的恢复网络是有益的。代码和数据集可在https://github.com/jnjaby/AlignFormer获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the difficulty in collecting large-scale and perfectly aligned paired training data for Under-Display Camera (UDC) image restoration, previous methods resort to monitor-based image systems or simulation-based methods, sacrificing the realness of the data and introducing domain gaps. In this work, we revisit the classic stereo setup for training data collection -- capturing two images of the same scene with one UDC and one standard camera. The key idea is to "copy" details from a high-quality reference image and "paste" them on the UDC image. While being able to generate real training pairs, this setting is susceptible to spatial misalignment due to perspective and depth of field changes. The problem is further compounded by the large domain discrepancy between the UDC and normal images, which is unique to UDC restoration. In this paper, we mitigate the non-trivial domain discrepancy and spatial misalignment through a novel Transformer-based framework that generates well-aligned yet high-quality target data for the corresponding UDC input. This is made possible through two carefully designed components, namely, the Domain Alignment Module (DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate discovery of correspondence between the UDC and normal views. Extensive experiments show that high-quality and well-aligned pseudo UDC training pairs are beneficial for training a robust restoration network. Code and the dataset are available at https://github.com/jnjaby/AlignFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1533.CRAFT: Concept Recursive Activation FacTorization for Explainability</span><br>
                <span class="as">Fel, ThomasandPicard, AgustinandB\&#x27;ethune, LouisandBoissin, ThibautandVigouroux, DavidandColin, JulienandCad\`ene, R\&#x27;emiandSerre, Thomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2711-2721.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的属性方法主要突出模型决策最重要的图像区域，但无法传达模型在这些位置看到了什么信息。<br>
                    动机：为了解决这个问题，本文提出了Craft方法，通过生成基于概念的解释来同时确定“什么”和“哪里”。<br>
                    方法：Craft引入了三个新的元素：（i）一种递归策略，用于检测和分解跨层的概念；（ii）一种新的方法，使用Sobol指数更准确地估计概念的重要性；（iii）使用隐式微分来解锁概念归属图。<br>
                    效果：实验表明，递归分解产生有意义且准确的概念，提出的概念重要性估计技术比之前的方法更忠实于模型。在评估该方法对人实验者的有用性时，发现Craft在两个测试场景中显著改善（而在第三个场景中，包括Craft在内的当前方法都无法提供帮助）。总的来说，虽然在开发适用于实际场景的通用可解释性方法方面仍有很多工作要做，但在适当的粒度级别上识别有意义的概念可以提供超越属性方法的有用和补充信息。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Attribution methods are a popular class of explainability methods that use heatmaps to depict the most important areas of an image that drive a model decision. Nevertheless, recent work has shown that these methods have limited utility in practice, presumably because they only highlight the most salient parts of an image (i.e., "where" the model looked) and do not communicate any information about "what" the model saw at those locations. In this work, we try to fill in this gap with Craft -- a novel approach to identify both "what" and "where" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that our recursive decomposition generates meaningful and accurate concepts and that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on the utility benchmark, we find that our approach significantly improves on two of the three test scenarios (while none of the current methods including ours help on the third). Overall, our study suggests that, while much work remains toward the development of general explainability methods that are useful in practical scenarios, the identification of meaningful concepts at the proper level of granularity yields useful and complementary information beyond that afforded by attribution methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1534.All-in-Focus Imaging From Event Focal Stack</span><br>
                <span class="as">Lou, HanyueandTeng, MingguiandYang, YixinandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lou_All-in-Focus_Imaging_From_Event_Focal_Stack_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17366-17375.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单次拍摄的图像中生成高质量的全焦点图像。<br>
                    动机：传统的焦点堆栈方法需要多次拍摄同一场景的不同距离，无法很好地应用于动态场景。由于单图像去焦和去模糊问题的病态性质，从单次拍摄生成高质量的全焦点图像具有挑战性。<br>
                    方法：本文提出了事件焦点堆栈，定义为在连续对焦扫描期间捕获的事件流。给定一张任意距离聚焦的RGB图像，我们探索了事件流的高时间分辨率，从中自动选择重新对焦的时间戳并重建相应的重新对焦的图像以形成焦点堆栈。通过选定时间戳周围的邻近事件引导，我们可以合并带有适当权重的焦点堆栈并恢复清晰的全焦点图像。<br>
                    效果：在合成和真实数据集上的实验结果均优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Traditional focal stack methods require multiple shots to capture images focused at different distances of the same scene, which cannot be applied to dynamic scenes well. Generating a high-quality all-in-focus image from a single shot is challenging, due to the highly ill-posed nature of the single-image defocus and deblurring problem. In this paper, to restore an all-in-focus image, we propose the event focal stack which is defined as event streams captured during a continuous focal sweep. Given an RGB image focused at an arbitrary distance, we explore the high temporal resolution of event streams, from which we automatically select refocusing timestamps and reconstruct corresponding refocused images with events to form a focal stack. Guided by the neighbouring events around the selected timestamps, we can merge the focal stack with proper weights and restore a sharp all-in-focus image. Experimental results on both synthetic and real datasets show superior performance over state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1535.Label-Free Liver Tumor Segmentation</span><br>
                <span class="as">Hu, QixinandChen, YixiongandXiao, JunfeiandSun, ShuwenandChen, JienengandYuille, AlanL.andZhou, Zongwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Label-Free_Liver_Tumor_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7422-7432.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在证明AI模型能够准确分割肝肿瘤，无需手动标注。<br>
                    动机：现有的肝肿瘤分割方法需要大量的手动标注，耗时耗力。<br>
                    方法：利用合成的CT扫描肿瘤进行训练，这些合成肿瘤形状和纹理逼真，能有效训练AI模型进行肝肿瘤分割。<br>
                    效果：实验结果表明，使用合成肿瘤训练的AI模型在肝肿瘤分割上的表现与真实肿瘤训练的模型相近，且能自动生成大量小肿瘤样本，有助于提高早期癌症的检测成功率。同时，这种方法也大大减少了手动标注的需求。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We demonstrate that AI models can accurately segment liver tumors without the need for manual annotation by using synthetic tumors in CT scans. Our synthetic tumors have two intriguing advantages: (I) realistic in shape and texture, which even medical professionals can confuse with real tumors; (II) effective for training AI models, which can perform liver tumor segmentation similarly to the model trained on real tumors--this result is exciting because no existing work, using synthetic tumors only, has thus far reached a similar or even close performance to real tumors. This result also implies that manual efforts for annotating tumors voxel by voxel (which took years to create) can be significantly reduced in the future. Moreover, our synthetic tumors can automatically generate many examples of small (or even tiny) synthetic tumors and have the potential to improve the success rate of detecting small liver tumors, which is critical for detecting the early stages of cancer. In addition to enriching the training data, our synthesizing strategy also enables us to rigorously assess the AI robustness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1536.Defining and Quantifying the Emergence of Sparse Concepts in DNNs</span><br>
                <span class="as">Ren, JieandLi, MingjieandChen, QiruiandDeng, HuiqiandZhang, Quanshi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Defining_and_Quantifying_the_Emergence_of_Sparse_Concepts_in_DNNs_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20280-20289.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过训练深度神经网络（DNN）来揭示概念涌现的现象。<br>
                    动机：我们发现DNN的推理分数可以被分解为几个交互概念的效果，这些概念可以理解为稀疏符号图模型中的推理模式，从而解释了DNN。<br>
                    方法：我们使用这种图模型来解释DNN，并证明该图模型可以很好地模拟DNN在大量不同掩码样本上的输出。此外，这种图模型可以进一步简化并重写为And-Or图（AOG），而不会损失太多的解释准确性。<br>
                    效果：实验结果表明，这种方法可以有效地解释DNN的推理过程，并且生成的AOG具有良好的解释准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as inference patterns in a sparse, symbolic graphical model, which explains the DNN. The faithfulness of using such a graphical model to explain the DNN is theoretically guaranteed, because we prove that the graphical model can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a graphical model can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy. The code is released at https://github.com/sjtu-xai-lab/aog.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1537.Adversarial Robustness via Random Projection Filters</span><br>
                <span class="as">Dong, MinjingandXu, Chang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Adversarial_Robustness_via_Random_Projection_Filters_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4077-4086.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在各种任务中表现出色，但容易受到对抗性攻击。大多数防御策略都集中在对抗性训练上，但在白盒设置下，通过梯度上升可以找到导致损失增加的攻击性扰动，这使得仅使用传统的对抗性训练难以达到满意的鲁棒性能。<br>
                    动机：为了解决这一问题，我们利用随机投影的特性，提出用随机投影滤波器替换部分卷积滤波器，并从理论上探索了所提出的合成滤波器的几何表示保留性。<br>
                    方法：我们采用随机投影滤波器替换部分卷积滤波器的方法，并通过Johnson-Lindenstrauss引理理论探索了所提出的合成滤波器的几何表示保留性。<br>
                    效果：我们在多个网络和数据集上进行了充分的评估。实验结果表明，所提出的随机投影滤波器优于最先进的基线方法。代码可在https://github.com/UniSerj/Random-Projection-Filters获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep Neural Networks show superior performance in various tasks but are vulnerable to adversarial attacks. Most defense techniques are devoted to the adversarial training strategies, however, it is difficult to achieve satisfactory robust performance only with traditional adversarial training. We mainly attribute it to that aggressive perturbations which lead to the loss increment can always be found via gradient ascent in white-box setting. Although some noises can be involved to prevent attacks from deriving precise gradients on inputs, there exist trade-offs between the defense capability and natural generalization. Taking advantage of the properties of random projection, we propose to replace part of convolutional filters with random projection filters, and theoretically explore the geometric representation preservation of proposed synthesized filters via Johnson-Lindenstrauss lemma. We conduct sufficient evaluation on multiple networks and datasets. The experimental results showcase the superiority of proposed random projection filters to state-of-the-art baselines. The code is available on https://github.com/UniSerj/Random-Projection-Filters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1538.Model-Agnostic Gender Debiased Image Captioning</span><br>
                <span class="as">Hirota, YusukeandNakashima, YutaandGarcia, Noa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hirota_Model-Agnostic_Gender_Debiased_Image_Captioning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15191-15200.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像描述模型在训练集中存在的性别偏见问题。<br>
                    动机：虽然已有的研究通过强制模型关注人物来减少性别误分类，但这反过来又会产生性别刻板印象的词语，以牺牲预测正确性别为代价。<br>
                    方法：作者提出了一个名为LIBRA的框架，该框架通过学习合成的有偏样本来减少两种类型的性别偏见，纠正性别误分类并将性别刻板印象的词语改为更中性的词语。<br>
                    效果：实验结果表明，LIBRA框架能有效降低图像描述模型中的性别偏见，提高其对性别的预测准确性，并改变生成的性别刻板印象词语。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image captioning models are known to perpetuate and amplify harmful societal bias in the training set. In this work, we aim to mitigate such gender bias in image captioning models. While prior work has addressed this problem by forcing models to focus on people to reduce gender misclassification, it conversely generates gender-stereotypical words at the expense of predicting the correct gender. From this observation, we hypothesize that there are two types of gender bias affecting image captioning models: 1) bias that exploits context to predict gender, and 2) bias in the probability of generating certain (often stereotypical) words because of gender. To mitigate both types of gender biases, we propose a framework, called LIBRA, that learns from synthetically biased samples to decrease both types of biases, correcting gender misclassification and changing gender-stereotypical words to more neutral ones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1539.OpenGait: Revisiting Gait Recognition Towards Better Practicality</span><br>
                <span class="as">Fan, ChaoandLiang, JunhaoandShen, ChuanfuandHou, SaihuiandHuang, YongzhenandYu, Shiqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9707-9716.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管步态识别技术在室内数据集上取得了显著进展，但在自然环境下的表现却较差，且一些从室内数据集得出的结论无法推广到实际应用。<br>
                    动机：为了提高步态识别技术的实用性，本文旨在通过全面的基准测试，而不仅仅是优化某一特定模型的性能。<br>
                    方法：首先开发了一个灵活高效的步态识别代码库OpenGait，然后基于OpenGait重新审视了步态识别的最新发展，并进行了消融实验。根据这些发现，开发了一个结构简单、实验有效、实用稳健的基线模型GaitBase。<br>
                    效果：在多个公共数据集上，GaitBase与当前许多步态识别方法进行了全面比较，结果表明，无论室内还是室外情况，GaitBase在大多数情况下都表现出显著的强性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gait recognition is one of the most critical long-distance identification technologies and increasingly gains popularity in both research and industry communities. Despite the significant progress made in indoor datasets, much evidence shows that gait recognition techniques perform poorly in the wild. More importantly, we also find that some conclusions drawn from indoor datasets cannot be generalized to real applications. Therefore, the primary goal of this paper is to present a comprehensive benchmark study for better practicality rather than only a particular model for better performance. To this end, we first develop a flexible and efficient gait recognition codebase named OpenGait. Based on OpenGait, we deeply revisit the recent development of gait recognition by re-conducting the ablative experiments. Encouragingly,we detect some unperfect parts of certain prior woks, as well as new insights. Inspired by these discoveries, we develop a structurally simple, empirically powerful, and practically robust baseline model, GaitBase. Experimentally, we comprehensively compare GaitBase with many current gait recognition methods on multiple public datasets, and the results reflect that GaitBase achieves significantly strong performance in most cases regardless of indoor or outdoor situations. Code is available at https://github.com/ShiqiYu/OpenGait.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1540.The Best Defense Is a Good Offense: Adversarial Augmentation Against Adversarial Attacks</span><br>
                <span class="as">Frosio, IuriandKautz, Jan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Frosio_The_Best_Defense_Is_a_Good_Offense_Adversarial_Augmentation_Against_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4067-4076.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提前防止对抗性攻击？<br>
                    动机：大部分防御对抗性攻击的方法都是在攻击发生后进行防御，本文提出一种全新的视角和框架A^5（对抗性增强的对抗性攻击防御）。<br>
                    方法：通过自动神经网络扰动分析工具，制造一个防御性的扰动，保证任何对输入的攻击（在给定的强度内）都会失败。<br>
                    效果：实验证明，A^5在MNIST、CIFAR10、FashionMNIST和Tinyimagenet等数据集上的表现优于最先进的认证防御方法。同时，A^5还可以用于创建具有认证鲁棒性的物理对象。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many defenses against adversarial attacks (e.g. robust classifiers, randomization, or image purification) use countermeasures put to work only after the attack has been crafted. We adopt a different perspective to introduce A^5 (Adversarial Augmentation Against Adversarial Attacks), a novel framework including the first certified preemptive defense against adversarial attacks. The main idea is to craft a defensive perturbation to guarantee that any attack (up to a given magnitude) towards the input in hand will fail. To this aim, we leverage existing automatic perturbation analysis tools for neural networks. We study the conditions to apply A^5 effectively, analyze the importance of the robustness of the to-be-defended classifier, and inspect the appearance of the robustified images. We show effective on-the-fly defensive augmentation with a robustifier network that ignores the ground truth label, and demonstrate the benefits of robustifier and classifier co-training. In our tests, A^5 consistently beats state of the art certified defenses on MNIST, CIFAR10, FashionMNIST and Tinyimagenet. We also show how to apply A^5 to create certifiably robust physical objects. The released code at https://github.com/NVlabs/A5 allows experimenting on a wide range of scenarios beyond the man-in-the-middle attack tested here, including the case of physical attacks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1541.GaitGCI: Generative Counterfactual Intervention for Gait Recognition</span><br>
                <span class="as">Dou, HuanzhangandZhang, PengyiandSu, WeiandYu, YunlongandLin, YiningandLi, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dou_GaitGCI_Generative_Counterfactual_Intervention_for_Gait_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5578-5588.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的步态识别方法易受干扰因素影响，难以关注到反映有效行走模式的区域。<br>
                    动机：为了解决步态识别中的根本问题，提出了一种生成式反事实干预框架——GaitGCI。<br>
                    方法：GaitGCI由反事实干预学习（CIL）和多样性约束动态卷积（DCDC）组成。CIL利用因果关系推断来减轻干扰因素的影响；DCDC自适应地生成样本事实/反事实注意力以感知样本属性。<br>
                    效果：实验表明，提出的GaitGCI能够有效地关注反映步态模式的区分性和可解释性区域；模型无关，可以插入现有模型以提高性能，几乎无需额外成本；在任意场景下（实验室内和野外）都能高效地实现最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gait is one of the most promising biometrics that aims to identify pedestrians from their walking patterns. However, prevailing methods are susceptible to confounders, resulting in the networks hardly focusing on the regions that reflect effective walking patterns. To address this fundamental problem in gait recognition, we propose a Generative Counterfactual Intervention framework, dubbed GaitGCI, consisting of Counterfactual Intervention Learning (CIL) and Diversity-Constrained Dynamic Convolution (DCDC). CIL leverages causal inference to alleviate the impact of confounders by maximizing the likelihood difference between factual/counterfactual attention. DCDC adaptively generates sample-wise factual/counterfactual attention to perceive the sample properties. With matrix decomposition and diversity constraint, DCDC guarantees the model's efficiency and effectiveness. Extensive experiments indicate that proposed GaitGCI: 1) could effectively focus on the discriminative and interpretable regions that reflect gait patterns; 2) is model-agnostic and could be plugged into existing models to improve performance with nearly no extra cost; 3) efficiently achieves state-of-the-art performance on arbitrary scenarios (in-the-lab and in-the-wild).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1542.Adversarially Masking Synthetic To Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation</span><br>
                <span class="as">Li, GuangruiandKang, GuoliangandWang, XiaohanandWei, YunchaoandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Adversarially_Masking_Synthetic_To_Mimic_Real_Adaptive_Noise_Injection_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20464-20474.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决合成数据标签和真实世界点云之间的领域差异，特别是在存在噪声的情况下进行语义分割的问题。<br>
                    动机：由于现实世界的传感器可能会受到各种环境条件的影响，收集到的点云数据通常包含意外和不规则的噪声，这导致在理想合成数据上训练的模型无法在真实数据上取得满意的分割结果。<br>
                    方法：本文设计了一个新颖的可学习掩蔽模块，通过学习在适应过程中遮蔽源点来缩小由目标噪声引起的领域差距。我们还将Gumbel-Softmax操作纳入掩蔽模块，使其能够生成二进制掩码并通过梯度反向传播进行端到端训练。<br>
                    效果：实验结果表明，该方法能有效缩小合成数据和真实世界点云之间的领域差距，提高语义分割的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper considers the synthetic-to-real adaptation of point cloud semantic segmentation, which aims to segment the real-world point clouds with only synthetic labels available. Contrary to synthetic data which is integral and clean, point clouds collected by real-world sensors typically contain unexpected and irregular noise because the sensors may be impacted by various environmental conditions. Consequently, the model trained on ideal synthetic data may fail to achieve satisfactory segmentation results on real data. Influenced by such noise, previous adversarial training methods, which are conventional for 2D adaptation tasks, become less effective. In this paper, we aim to mitigate the domain gap caused by target noise via learning to mask the source points during the adaptation procedure. To this end, we design a novel learnable masking module, which takes source features and 3D coordinates as inputs. We incorporate Gumbel-Softmax operation into the masking module so that it can generate binary masks and be trained end-to-end via gradient back-propagation. With the help of adversarial training, the masking module can learn to generate source masks to mimic the pattern of irregular target noise, thereby narrowing the domain gap. We name our method "Adversarial Masking" as adversarial training and learnable masking module depend on each other and cooperate with each other to mitigate the domain gap. Experiments on two synthetic-to-real adaptation benchmarks verify the effectiveness of the proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1543.Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts</span><br>
                <span class="as">Croce, FrancescoandRebuffi, Sylvestre-AlviseandShelhamer, EvanandGowal, Sven</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Croce_Seasoning_Model_Soups_for_Robustness_to_Adversarial_and_Natural_Distribution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12313-12323.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个对多种威胁具有鲁棒性的分类器，同时避免在训练过程中需要对所有攻击有知识并且对未见过的数据分布偏移仍然脆弱的问题。<br>
                    动机：现有的方法需要对所有的攻击有知识才能进行训练，并且在面对未见过的数据分布偏移时仍然很脆弱。<br>
                    方法：通过获取对抗性稳健的模型汤（即参数的线性组合），在不同的l_p-范数有界对手中平滑地权衡鲁棒性，从而获得对所有威胁都具有鲁棒性的模型。<br>
                    效果：实验证明，这种方法可以获得对所有威胁都具有鲁棒性的模型，并且在一些情况下，其对给定的l_p-范数对手的鲁棒性甚至超过了专门针对该对手的模型。最后，展示了对抗性稳健的模型汤可以作为一种有效的工具，通过少数例子适应数据分布的偏移。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adversarial training is widely used to make classifiers robust to a specific threat or adversary, such as l_p-norm bounded perturbations of a given p-norm. However, existing methods for training classifiers robust to multiple threats require knowledge of all attacks during training and remain vulnerable to unseen distribution shifts. In this work, we describe how to obtain adversarially-robust model soups (i.e., linear combinations of parameters) that smoothly trade-off robustness to different l_p-norm bounded adversaries. We demonstrate that such soups allow us to control the type and level of robustness, and can achieve robustness to all threats without jointly training on all of them. In some cases, the resulting model soups are more robust to a given l_p-norm adversary than the constituent model specialized against that same adversary. Finally, we show that adversarially-robust model soups can be a viable tool to adapt to distribution shifts from a few examples.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1544.Introducing Competition To Boost the Transferability of Targeted Adversarial Examples Through Clean Feature Mixup</span><br>
                <span class="as">Byun, JunyoungandKwon, Myung-JoonandCho, SeungjuandKim, YoonjiandKim, Changick</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Byun_Introducing_Competition_To_Boost_the_Transferability_of_Targeted_Adversarial_Examples_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24648-24657.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度神经网络易受对抗性示例影响，通过微小的输入修改可能导致错误预测。<br>
                    动机：对抗性示例在模型间具有可转移性，但定向攻击由于决策边界的差异成功率较低。为提高定向对抗性示例的可转移性，提出在优化过程中引入竞争机制。<br>
                    方法：在两种新的竞争者噪声存在的情况下进行对抗性扰动：针对不同目标类别的对抗性扰动和针对正确类别的友好扰动。通过这些竞争对手，即使对抗性示例欺骗网络提取特定特征导致目标类别，这种干扰也可以被其他竞争对手抑制。因此，在这种竞争中，对抗性示例应利用更多样化的特征来压倒其干扰，从而提高其在不同模型之间的可转移性。<br>
                    效果：在ImageNet-Compatible和CIFAR-10数据集上的大量实验结果表明，该方法优于现有基线方法，且计算复杂度低。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks are widely known to be susceptible to adversarial examples, which can cause incorrect predictions through subtle input modifications. These adversarial examples tend to be transferable between models, but targeted attacks still have lower attack success rates due to significant variations in decision boundaries. To enhance the transferability of targeted adversarial examples, we propose introducing competition into the optimization process. Our idea is to craft adversarial perturbations in the presence of two new types of competitor noises: adversarial perturbations towards different target classes and friendly perturbations towards the correct class. With these competitors, even if an adversarial example deceives a network to extract specific features leading to the target class, this disturbance can be suppressed by other competitors. Therefore, within this competition, adversarial examples should take different attack strategies by leveraging more diverse features to overwhelm their interference, leading to improving their transferability to different models. Considering the computational complexity, we efficiently simulate various interference from these two types of competitors in feature space by randomly mixing up stored clean features in the model inference and named this method Clean Feature Mixup (CFM). Our extensive experimental results on the ImageNet-Compatible and CIFAR-10 datasets show that the proposed method outperforms the existing baselines with a clear margin. Our code is available at https://github.com/dreamflake/CFM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1545.A Whac-a-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others</span><br>
                <span class="as">Li, ZhihengandEvtimov, IvanandGordo, AlbertandHazirbas, CanerandHassner, TalandFerrer, CristianCantonandXu, ChenliangandIbrahim, Mark</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20071-20082.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的机器学习模型存在学习到不能泛化的捷径，这降低了模型的可靠性。<br>
                    动机：现实世界的图像中充满了从背景到纹理的多种视觉线索。提高视觉系统的可靠性的关键是了解现有方法能否克服多个捷径，或者在“打地鼠”游戏中挣扎，即消除一个捷径会放大对其他捷径的依赖。<br>
                    方法：我们提出了两个基准测试：1）UrbanCars，一个具有精确控制误导性线索的数据集；2）ImageNet-W，一个基于ImageNet的水印记（一种影响几乎所有现代视觉模型的捷径）评估集。除了纹理和背景，ImageNet-W还允许我们研究从自然图像训练中出现的多个捷径。<br>
                    效果：我们发现，包括大型基础模型在内的计算机视觉模型，无论训练集、架构还是监督方式如何，当存在多个捷径时都会挣扎。即使是专门设计用来对抗捷径的方法，也会在“打地鼠”困境中挣扎。为了应对这一挑战，我们提出了最后一层集成（Last Layer Ensemble），这是一种简单而有效的方法，可以在不打地鼠的情况下减轻多个捷径的影响。我们的结果表明，多捷径缓解是一个被忽视的挑战，对于提高视觉系统的可靠性至关重要。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Machine learning models have been found to learn shortcuts---unintended decision rules that are unable to generalize---undermining models' reliability. Previous works address this problem under the tenuous assumption that only a single shortcut exists in the training data. Real-world images are rife with multiple visual cues from background to texture. Key to advancing the reliability of vision systems is understanding whether existing methods can overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where mitigating one shortcut amplifies reliance on others. To address this shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely controlled spurious cues, and 2) ImageNet-W, an evaluation set based on ImageNet for watermark, a shortcut we discovered affects nearly every modern vision model. Along with texture and background, ImageNet-W allows us to study multiple shortcuts emerging from training on natural images. We find computer vision models, including large foundation models---regardless of training set, architecture, and supervision---struggle when multiple shortcuts are present. Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole dilemma. To tackle this challenge, we propose Last Layer Ensemble, a simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole behavior. Our results surface multi-shortcut mitigation as an overlooked challenge critical to advancing the reliability of vision systems. The datasets and code are released: https://github.com/facebookresearch/Whac-A-Mole.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1546.Neumann Network With Recursive Kernels for Single Image Defocus Deblurring</span><br>
                <span class="as">Quan, YuhuiandWu, ZicongandJi, Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Quan_Neumann_Network_With_Recursive_Kernels_for_Single_Image_Defocus_Deblurring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5754-5763.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从模糊的、散焦的图像中恢复出清晰、聚焦的图像。<br>
                    动机：由于散焦模糊效应的空间变化和显著的大小变化，这是一个具有挑战性的恢复任务。<br>
                    方法：提出了一种可学习的递归核表示（RKR）来表达散焦核，通过将递归、可分离和正原子核的线性组合来表达散焦核，从而实现了空间变化的散焦模糊过程的紧凑而有效的物理编码参数化。然后，提出了一种物理驱动的高效深度模型，并设计了一种跨尺度融合结构用于SIDD，同时引入了重模糊损失来规范RKR的学习。<br>
                    效果：实验表明，该方法在性能上显著优于现有方法，且模型大小与顶级方法相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single image defocus deblurring (SIDD) refers to recovering an all-in-focus image from a defocused blurry one. It is a challenging recovery task due to the spatially-varying defocus blurring effects with significant size variation. Motivated by the strong correlation among defocus kernels of different sizes and the blob-type structure of defocus kernels, we propose a learnable recursive kernel representation (RKR) for defocus kernels that expresses a defocus kernel by a linear combination of recursive, separable and positive atom kernels, leading to a compact yet effective and physics-encoded parametrization of the spatially-varying defocus blurring process. Afterwards, a physics-driven and efficient deep model with a cross-scale fusion structure is presented for SIDD, with inspirations from the truncated Neumann series for approximating the matrix inversion of the RKR-based blurring operator. In addition, a reblurring loss is proposed to regularize the RKR learning. Extensive experiments show that, our proposed approach significantly outperforms existing ones, with a model size comparable to that of the top methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1547.RWSC-Fusion: Region-Wise Style-Controlled Fusion Network for the Prohibited X-Ray Security Image Synthesis</span><br>
                <span class="as">Duan, LuwenandWu, MinandMao, LijianandYin, JunandXiong, JianpingandLi, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_RWSC-Fusion_Region-Wise_Style-Controlled_Fusion_Network_for_the_Prohibited_X-Ray_Security_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22398-22407.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效检测安全检查X射线图像中的违禁物品？<br>
                    动机：由于X射线安检图像的丰富性和多样性，以及其对于训练检测模型的重要性，解决数据不足的问题是必要的。<br>
                    方法：提出一种区域感知风格控制的融合（RWSC-Fusion）网络，通过将违禁物品叠加到正常的X射线安检图像上，合成违禁X射线安检图像。该网络在结构和损失函数上都有所创新，以生成更真实的X射线安检图像。<br>
                    效果：通过对私有和公共SIXray数据集进行评估，证明合成的X射线安检图像具有足够的可靠性，可以有效地扩充违禁X射线安检图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic prohibited item detection in security inspection X-ray images is necessary for transportation.The abundance and diversity of the X-ray security images with prohibited item, termed as prohibited X-ray security images, are essential for training the detection model. In order to solve the data insufficiency, we propose a RegionWise Style-Controlled Fusion (RWSC-Fusion) network, which superimposes the prohibited items onto the normal X-ray security images, to synthesize the prohibited X-ray security images. The proposed RWSC-Fusion innovates both network structure and loss functions to generate more realistic X-ray security images. Specifically, a RWSCFusion module is designed to enable the region-wise fusion by controlling the appearance of the overlapping region with novel modulation parameters. In addition, an EdgeAttention (EA) module is proposed to effectively improve the sharpness of the synthetic images. As for the unsupervised loss function, we propose the Luminance loss in Logarithmic form (LL) and Correlation loss of Saturation Difference (CSD), to optimize the fused X-ray security images in terms of luminance and saturation. We evaluate the authenticity and the training effect of the synthetic X-ray security images on private and public SIXray dataset. The results confirm that our synthetic images are reliable enough to augment the prohibited Xray security images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1548.Self-Supervised Blind Motion Deblurring With Deep Expectation Maximization</span><br>
                <span class="as">Li, JiandWang, WeixiandNan, YuesongandJi, Hui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Self-Supervised_Blind_Motion_Deblurring_With_Deep_Expectation_Maximization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13986-13996.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从模糊的图像中恢复清晰的图像，特别是在相机抖动引起的模糊情况下。<br>
                    动机：现有的深度学习方法需要大量的模糊/潜在图像对进行训练，而本文提出了一种无需数据集的深度学习方法来去除静态场景图像中的均匀和非均匀模糊效果。<br>
                    方法：该方法通过深度神经网络（DNN）对潜在图像进行重参数化，并提出了蒙特卡洛期望最大化（MCEM）方法来训练DNN，而无需任何潜在图像。蒙特卡洛模拟通过朗之万动力学实现。<br>
                    效果：实验表明，该方法在去除静态场景图像的运动模糊方面显著优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>When taking a picture, any camera shake during the shutter time can result in a blurred image. Recovering a sharp image from the one blurred by camera shake is a challenging yet important problem. Most existing deep learning methods use supervised learning to train a deep neural network (DNN) on a dataset of many pairs of blurred/latent images. In contrast, this paper presents a dataset-free deep learning method for removing uniform and non-uniform blur effects from images of static scenes. Our method involves a DNN-based re-parametrization of the latent image, and we propose a Monte Carlo Expectation Maximization (MCEM) approach to train the DNN without requiring any latent images. The Monte Carlo simulation is implemented via Langevin dynamics. Experiments showed that the proposed method outperforms existing methods significantly in removing motion blur from images of static scenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1549.Dynamic Generative Targeted Attacks With Pattern Injection</span><br>
                <span class="as">Feng, WeiweiandXu, NanqingandZhang, TianzhuandZhang, Yongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Dynamic_Generative_Targeted_Attacks_With_Pattern_Injection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16404-16414.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高目标攻击的有效性和广泛性。<br>
                    动机：现有的目标攻击方法主要依赖特定实例或全局数据集，忽视了目标类别的真实分布，导致攻击效果有限。<br>
                    方法：提出一种基于因果图的生成式攻击模型，该模型由一个交叉注意力引导的卷积模块和一个模式注入模块组成。前者采用动态和静态卷积核分别处理特定实例和全局数据集，后者利用模式原型编码目标模式，以指导目标对抗样本的生成。<br>
                    效果：实验表明，该方法在13个模型上的表现优于10种现有攻击方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adversarial attacks can evaluate model robustness and have been of great concerns in recent years. Among various attacks, targeted attacks aim at misleading victim models to output adversary-desired predictions, which are more challenging and threatening than untargeted ones. Existing targeted attacks can be roughly divided into instancespecific and instance-agnostic attacks. Instance-specific attacks craft adversarial examples via iterative gradient updating on the specific instance. In contrast, instanceagnostic attacks learn a universal perturbation or a generative model on the global dataset to perform attacks. However they rely too much on the classification boundary of substitute models, ignoring the realistic distribution of target class, which may result in limited targeted attack performance. And there is no attempt to simultaneously combine the information of the specific instance and the global dataset. To deal with these limitations, we first conduct an analysis via a causal graph and propose to craft transferable targeted adversarial examples by injecting target patterns. Based on this analysis, we introduce a generative attack model composed of a cross-attention guided convolution module and a pattern injection module. Concretely, the former adopts a dynamic convolution kernel and a static convolution kernel for the specific instance and the global dataset, respectively, which can inherit the advantages of both instance-specific and instance-agnostic attacks. And the pattern injection module utilizes a pattern prototype to encode target patterns, which can guide the generation of targeted adversarial examples. Besides, we also provide rigorous theoretical analysis to guarantee the effectiveness of our method. Extensive experiments demonstrate that our method show superior performance than 10 existing adversarial attacks against 13 models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1550.PointCert: Point Cloud Classification With Deterministic Certified Robustness Guarantees</span><br>
                <span class="as">Zhang, JinghuaiandJia, JinyuanandLiu, HongbinandGong, NeilZhenqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PointCert_Point_Cloud_Classification_With_Deterministic_Certified_Robustness_Guarantees_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9496-9505.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决点云分类器在对抗性扰动下易受攻击的问题，以及现有防御方法的健壮性保证存在概率性的问题。<br>
                    动机：点云分类在自动驾驶和增强现实等安全关键应用中起着重要作用，但现有的防御方法存在健壮性保证的概率性问题。<br>
                    方法：提出了一种名为PointCert的通用框架，可以将任意点云分类器转化为具有确定性健壮性保证的抗对抗性点云的分类器。<br>
                    效果：通过在ModelNet和ScanObjectNN基准数据集上的系统评估，结果显示PointCert即使面对健壮性保证存在概率性的先进防御方法，也表现出了显著的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point cloud classification is an essential component in many security-critical applications such as autonomous driving and augmented reality. However, point cloud classifiers are vulnerable to adversarially perturbed point clouds. Existing certified defenses against adversarial point clouds suffer from a key limitation: their certified robustness guarantees are probabilistic, i.e., they produce an incorrect certified robustness guarantee with some probability. In this work, we propose a general framework, namely PointCert, that can transform an arbitrary point cloud classifier to be certifiably robust against adversarial point clouds with deterministic guarantees. PointCert certifiably predicts the same label for a point cloud when the number of arbitrarily added, deleted, and/or modified points is less than a threshold. Moreover, we propose multiple methods to optimize the certified robustness guarantees of PointCert in three application scenarios. We systematically evaluate PointCert on ModelNet and ScanObjectNN benchmark datasets. Our results show that PointCert substantially outperforms state-of-the-art certified defenses even though their robustness guarantees are probabilistic.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1551.Don&#x27;t Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis</span><br>
                <span class="as">Fel, ThomasandDucoffe, MelanieandVigouroux, DavidandCad\`ene, R\&#x27;emiandCapelle, Mika\&quot;elandNicod\`eme, ClaireandSerre, Thomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16153-16163.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地解释深度神经网络的决策过程。<br>
                    动机：现有的采样方法在估计单个像素的重要性时存在偏差和其它人为因素，导致当前的解释性方法不可靠。<br>
                    方法：提出EVA（使用验证扰动分析进行解释）——首个保证对扰动空间进行全面探索的解释性方法。具体来说，我们利用验证扰动分析的时间效率、可追踪性和保证流形的全面覆盖等有益特性，来有效地描述最有可能驱动模型决策的输入变量。<br>
                    效果：我们在多个基准上进行了系统评估，并展示了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A variety of methods have been proposed to try to explain how deep neural networks make their decisions. Key to those approaches is the need to sample the pixel space efficiently in order to derive importance maps. However, it has been shown that the sampling methods used to date introduce biases and other artifacts, leading to inaccurate estimates of the importance of individual pixels and severely limit the reliability of current explainability methods. Unfortunately, the alternative -- to exhaustively sample the image space is computationally prohibitive. In this paper, we introduce EVA (Explaining using Verified perturbation Analysis) -- the first explainability method guarantee to have an exhaustive exploration of a perturbation space. Specifically, we leverage the beneficial properties of verified perturbation analysis -- time efficiency, tractability and guaranteed complete coverage of a manifold -- to efficiently characterize the input variables that are most likely to drive the model decision. We evaluate the approach systematically and demonstrate state-of-the-art results on multiple benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1552.Defending Against Patch-Based Backdoor Attacks on Self-Supervised Learning</span><br>
                <span class="as">Tejankar, AjinkyaandSanjabi, MaziarandWang, QifanandWang, SinongandFirooz, HamedandPirsiavash, HamedandTan, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tejankar_Defending_Against_Patch-Based_Backdoor_Attacks_on_Self-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12239-12249.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何防御基于补丁的数据投毒后门攻击对自监督学习的影响。<br>
                    动机：现有的自监督学习方法存在被基于补丁的数据投毒后门攻击的风险，即攻击者可以通过污染一小部分未标记的数据使得受害者在训练模型时植入后门。<br>
                    方法：本文提出了一种三步防御流程，首先在被污染的数据上训练模型，然后使用训练好的模型在训练数据中搜索并移除被污染的样本，最后在清理过的数据集上训练最终模型。<br>
                    效果：实验结果表明，该方法能有效防御此类攻击，提高了模型在含有触发器的图像上的准确率，且优于其他基线和最先进的防御方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, self-supervised learning (SSL) was shown to be vulnerable to patch-based data poisoning backdoor attacks. It was shown that an adversary can poison a small part of the unlabeled data so that when a victim trains an SSL model on it, the final model will have a backdoor that the adversary can exploit. This work aims to defend self-supervised learning against such attacks. We use a three-step defense pipeline, where we first train a model on the poisoned data. In the second step, our proposed defense algorithm (PatchSearch) uses the trained model to search the training data for poisoned samples and removes them from the training set. In the third step, a final model is trained on the cleaned-up training set. Our results show that PatchSearch is an effective defense. As an example, it improves a model's accuracy on images containing the trigger from 38.2% to 63.7% which is very close to the clean model's accuracy, 64.6%. Moreover, we show that PatchSearch outperforms baselines and state-of-the-art defense approaches including those using additional clean, trusted data. Our code is available at https://github.com/UCDvision/PatchSearch</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1553.AGAIN: Adversarial Training With Attribution Span Enlargement and Hybrid Feature Fusion</span><br>
                <span class="as">Yin, ShenglinandYao, KeluandShi, ShengandDu, YangzhouandXiao, Zhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_AGAIN_Adversarial_Training_With_Attribution_Span_Enlargement_and_Hybrid_Feature_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20544-20553.png><br>
            
            <span class="tt"><span class="t0">研究问题：对抗训练的深度神经网络在训练和测试阶段表现出显著的鲁棒性差距。<br>
                    动机：对抗训练的深度神经网络在训练时具有高鲁棒性，但在测试时表现较差，这是由于其输入图像的关注范围较小。<br>
                    方法：提出一种通用的方法来提高对抗训练方法的鲁棒性泛化，通过扩大学习到的注意力范围以及使用混合特征统计数据进行特征融合。<br>
                    效果：实验证明该方法能有效提高对抗训练的深度神经网络的鲁棒性，超越先前的最佳方法，同时提供了理论分析以证明其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The deep neural networks (DNNs) trained by adversarial training (AT) usually suffered from significant robust generalization gap, i.e., DNNs achieve high training robustness but low test robustness. In this paper, we propose a generic method to boost the robust generalization of AT methods from the novel perspective of attribution span. To this end, compared with standard DNNs, we discover that the generalization gap of adversarially trained DNNs is caused by the smaller attribution span on the input image. In other words, adversarially trained DNNs tend to focus on specific visual concepts on training images, causing its limitation on test robustness. In this way, to enhance the robustness, we propose an effective method to enlarge the learned attribution span. Besides, we use hybrid feature statistics for feature fusion to enrich the diversity of features. Extensive experiments show that our method can effectively improves robustness of adversarially trained DNNs, outperforming previous SOTA methods. Furthermore, we provide a theoretical analysis of our method to prove its effectiveness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1554.Adversarial Normalization: I Can Visualize Everything (ICE)</span><br>
                <span class="as">Choi, HoyoungandJin, SeungwanandHan, Kyungsik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Adversarial_Normalization_I_Can_Visualize_Everything_ICE_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12115-12124.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉转换器的可解释性可视化效果。<br>
                    动机：现有的视觉转换器模型的可解释性可视化方法存在挑战，如结构依赖、学习过程中的非线性不稳定性以及自注意力得分对相关性的有限反映等。<br>
                    方法：提出一种名为ICE的新方法，该方法使模型能够直接预测图像中每个补丁的类别，从而推进视觉转换器的有效可视化。通过预测不影响图像类别的补丁的背景类别，将背景与前景区域区分开来。<br>
                    效果：在ImageNet-Segmentation数据集上，ICE在所有四种情况下都优于其他可解释性可视化方法。在CUB-200-2011和PASCALVOC07/12数据集上，ICE实现了与最先进的方法相当的性能。在ImageNet数据集上，将ICE融入DeiT-S的编码器后，效率提高了44.01%。其性能和效率与最先进的剪枝模型EViT相当，证明了ICE的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers use [CLS] tokens to predict image classes. Their explainability visualization has been studied using relevant information from [CLS] tokens or focusing on attention scores during self-attention. Such visualization, however, is challenging because of the dependence of the structure of a vision transformer on skip connections and attention operators, the instability of non-linearities in the learning process, and the limited reflection of self-attention scores on relevance. We argue that the output vectors for each input patch token in a vision transformer retain the image information of each patch location, which can facilitate the prediction of an image class. In this paper, we propose ICE (Adversarial Normalization: I Can visualize Everything), a novel method that enables a model to directly predict a class for each patch in an image; thus, advancing the effective visualization of the explainability of a vision transformer. Our method distinguishes background from foreground regions by predicting background classes for patches that do not determine image classes. We used the DeiT-S model, the most representative model employed in studies, on the explainability visualization of vision transformers. On the ImageNet-Segmentation dataset, ICE outperformed all explainability visualization methods for four cases depending on the model size. We also conducted quantitative and qualitative analyses on the tasks of weakly-supervised object localization and unsupervised object discovery. On the CUB-200-2011 and PASCALVOC07/12 datasets, ICE achieved comparable performance to the state-of-the-art methods. We incorporated ICE into the encoder of DeiT-S and improved efficiency by 44.01% on the ImageNet dataset over that achieved by the original DeiT-S model. We showed performance on the accuracy and efficiency comparable to EViT, the state-of-the-art pruning model, demonstrating the effectiveness of ICE. The code is available at https://github.com/Hanyang-HCC-Lab/ICE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1555.Reinforcement Learning-Based Black-Box Model Inversion Attacks</span><br>
                <span class="as">Han, GyojinandChoi, JaehyunandLee, HaeilandKim, Junmo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Reinforcement_Learning-Based_Black-Box_Model_Inversion_Attacks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20504-20513.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的黑盒模型倒置攻击在预设的查询次数内无法保证完成攻击过程，或无法达到与白盒攻击相同的性能水平。<br>
                    动机：为了克服这些限制，我们提出了一种基于强化学习的黑盒模型倒置攻击方法。<br>
                    方法：我们将潜在空间搜索定义为马尔可夫决策过程（MDP）问题，并使用强化学习来解决。我们的方法利用生成的图像的置信度分数为代理提供奖励。<br>
                    效果：实验结果在不同的数据集和模型上表明，我们的攻击成功地恢复了目标模型的私有信息，达到了最先进的攻击性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model inversion attacks are a type of privacy attack that reconstructs private data used to train a machine learning model, solely by accessing the model. Recently, white-box model inversion attacks leveraging Generative Adversarial Networks (GANs) to distill knowledge from public datasets have been receiving great attention because of their excellent attack performance. On the other hand, current black-box model inversion attacks that utilize GANs suffer from issues such as being unable to guarantee the completion of the attack process within a predetermined number of query accesses or achieve the same level of performance as white-box attacks. To overcome these limitations, we propose a reinforcement learning-based black-box model inversion attack. We formulate the latent space search as a Markov Decision Process (MDP) problem and solve it with reinforcement learning. Our method utilizes the confidence scores of the generated images to provide rewards to an agent. Finally, the private data can be reconstructed using the latent vectors found by the agent trained in the MDP. The experiment results on various datasets and models demonstrate that our attack successfully recovers the private information of the target model by achieving state-of-the-art attack performance. We emphasize the importance of studies on privacy-preserving machine learning by proposing a more advanced black-box model inversion attack.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1556.Learning a Practical SDR-to-HDRTV Up-Conversion Using New Dataset and Degradation Models</span><br>
                <span class="as">Guo, ChengandFan, LeidongandXue, ZiyuandJiang, Xiuhua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Learning_a_Practical_SDR-to-HDRTV_Up-Conversion_Using_New_Dataset_and_Degradation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22231-22241.png><br>
            
            <span class="tt"><span class="t0">研究问题：在媒体行业中，当用户拥有HDR-WCG电视时，如何将SDR视频转换为HDRTV。<br>
                    动机：由于大部分现有素材仍然是SDR，而用户拥有的是HDR-WCG电视，因此需要一种有效的方法进行SDR到HDRTV的上转换。<br>
                    方法：我们提出了新的HDRTV数据集（称为HDRTV4K）和新的HDR-to-SDR降级模型。然后，我们使用这些数据训练了一个亮度分割网络（LSN），该网络由一个全局映射主干和两个Transformer分支组成，分别处理亮部和暗部的亮度范围。我们还更新了评估标准，通过定制的度量标准和主观实验进行评估。<br>
                    效果：我们的实验结果表明，这种方法可以显著提高观看体验，并且其有效性已经通过消融研究得到证明。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In media industry, the demand of SDR-to-HDRTV up-conversion arises when users possess HDR-WCG (high dynamic range-wide color gamut) TVs while most off-the-shelf footage is still in SDR (standard dynamic range). The research community has started tackling this low-level vision task by learning-based approaches. When applied to real SDR, yet, current methods tend to produce dim and desaturated result, making nearly no improvement on viewing experience. Different from other network-oriented methods, we attribute such deficiency to training set (HDR-SDR pair). Consequently, we propose new HDRTV dataset (dubbed HDRTV4K) and new HDR-to-SDR degradation models. Then, it's used to train a luminance-segmented network (LSN) consisting of a global mapping trunk, and two Transformer branches on bright and dark luminance range. We also update assessment criteria by tailored metrics and subjective experiment. Finally, ablation studies are conducted to prove the effectiveness. Our work is available at: https://github.com/AndreGuo/HDRTVDM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1557.Patch-Craft Self-Supervised Training for Correlated Image Denoising</span><br>
                <span class="as">Vaksman, GregoryandElad, Michael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vaksman_Patch-Craft_Self-Supervised_Training_for_Correlated_Image_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5795-5804.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一种无需知道噪声模型或访问真实目标的图像去噪模型，以处理未知相关噪声。<br>
                    动机：现有的图像去噪方法需要成对的损坏图像和对应的真实目标，但在许多应用中，这样的数据是不存在的。<br>
                    方法：提出一种新的自我监督训练技术，通过捕捉容易获取的噪声突发，构造人工补丁工艺图像作为训练目标。<br>
                    效果：通过大量的合成和真实图像噪声实验评估，证明了该方法在处理未知相关噪声上的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Supervised neural networks are known to achieve excellent results in various image restoration tasks. However, such training requires datasets composed of pairs of corrupted images and their corresponding ground truth targets. Unfortunately, such data is not available in many applications. For the task of image denoising in which the noise statistics is unknown, several self-supervised training methods have been proposed for overcoming this difficulty. Some of these require knowledge of the noise model, while others assume that the contaminating noise is uncorrelated, both assumptions are too limiting for many practical needs. This work proposes a novel self-supervised training technique suitable for the removal of unknown correlated noise. The proposed approach neither requires knowledge of the noise model nor access to ground truth targets. The input to our algorithm consists of easily captured bursts of noisy shots. Our algorithm constructs artificial patch-craft images from these bursts by patch matching and stitching, and the obtained crafted images are used as targets for the training. Our method does not require registration of the different images within the burst. We evaluate the proposed framework through extensive experiments with synthetic and real image noise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1558.Single Image Backdoor Inversion via Robust Smoothed Classifiers</span><br>
                <span class="as">Sun, MingjieandKolter, Zico</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Single_Image_Backdoor_Inversion_via_Robust_Smoothed_Classifiers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8113-8122.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决如何通过优化过程将干净图像集合转换为目标类别，以恢复插入到机器学习模型中的后门触发器的问题。<br>
                    动机：尽管后门反转方法已成为许多后门检测和防御方法的支柱，但关于需要多少个干净图像才能成功恢复后门的研究却很少。<br>
                    方法：本文提出了一种名为SmoothInv的方法，该方法首先构建了一个鲁棒的平滑版后门分类器，然后进行针对目标类别的引导式图像合成，以揭示后门模式。<br>
                    效果：实验结果表明，与现有方法相比，SmoothInv能够从单个图像中可靠地恢复成功的后门，同时保持对原始后门的高保真度。此外，我们还展示了如何从后门分类器中识别出被后门化的目标类别。最后，我们提出了两种针对我们的方法的反制措施，并证明在面对适应性攻击者时，SmoothInv仍然具有鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Backdoor inversion, the process of finding a backdoor trigger inserted into a machine learning model, has become the pillar of many backdoor detection and defense methods. Previous works on backdoor inversion often recover the backdoor through an optimization process to flip a support set of clean images into the target class. However, it is rarely studied and understood how large this support set should be to recover a successful backdoor. In this work, we show that one can reliably recover the backdoor trigger with as few as a single image. Specifically, we propose the SmoothInv method, which first constructs a robust smoothed version of the backdoored classifier and then performs guided image synthesis towards the target class to reveal the backdoor pattern. SmoothInv requires neither an explicit modeling of the backdoor via a mask variable, nor any complex regularization schemes, which has become the standard practice in backdoor inversion methods. We perform both quantitaive and qualitative study on backdoored classifiers from previous published backdoor attacks. We demonstrate that compared to existing methods, SmoothInv is able to recover successful backdoors from single images, while maintaining high fidelity to the original backdoor. We also show how we identify the target backdoored class from the backdoored classifier. Last, we propose and analyze two countermeasures to our approach and show that SmoothInv remains robust in the face of an adaptive attacker. Our code is available at https://github.com/locuslab/smoothinv.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1559.Evading DeepFake Detectors via Adversarial Statistical Consistency</span><br>
                <span class="as">Hou, YangandGuo, QingandHuang, YihaoandXie, XiaofeiandMa, LeiandZhao, Jianjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12271-12280.png><br>
            
            <span class="tt"><span class="t0">研究问题：随着深度伪造技术（DeepFake）的飞速发展，如何有效地检测出这些伪造的图片成为了一个重要的问题。<br>
                    动机：目前的深度伪造检测方法主要依赖于检测真实图片和生成的图片在空间和频率域上的统计差异，但这些方法往往容易被最新的深度伪造技术所欺骗。<br>
                    方法：本文提出了一种针对深度伪造检测器的统计一致性攻击（StatAttack），该方法通过在生成的假图片上添加一些统计敏感的自然退化效果（如曝光、模糊和噪声），并使用分布感知损失来优化这些退化效果，使得生成的对抗样本的特征分布接近真实的自然图片。此外，作者还进一步将这种方法扩展到了更强大的多层级退化版本（MStatAttack）。<br>
                    效果：在四个基于空间的检测器和两个基于频率的检测器上进行的大量实验表明，无论是在白盒还是黑盒设置下，本文提出的方法都能有效地欺骗现有的深度伪造检测器。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, as various realistic face forgery techniques known as DeepFake improves by leaps and bounds, more and more DeepFake detection techniques have been proposed. These methods typically rely on detecting statistical differences between natural (i.e., real) and DeepFake-generated images in both spatial and frequency domains. In this work, we propose to explicitly minimize the statistical differences to evade state-of-the-art DeepFake detectors. To this end, we propose a statistical consistency attack (StatAttack) against DeepFake detectors, which contains two main parts. First, we select several statistical-sensitive natural degradations (i.e., exposure, blur, and noise) and add them to the fake images in an adversarial way. Second, we find that the statistical differences between natural and DeepFake images are positively associated with the distribution shifting between the two kinds of images, and we propose to use a distribution-aware loss to guide the optimization of different degradations. As a result, the feature distributions of generated adversarial examples is close to the natural images. Furthermore, we extend the StatAttack to a more powerful version, MStatAttack, where we extend the single-layer degradation to multi-layer degradations sequentially and use the loss to tune the combination weights jointly. Comprehensive experimental results on four spatial-based detectors and two frequency-based detectors with four datasets demonstrate the effectiveness of our proposed attack method in both white-box and black-box settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1560.OCTET: Object-Aware Counterfactual Explanations</span><br>
                <span class="as">Zemni, MehdiandChen, Micka\&quot;elandZablocki, \&#x27;EloiandBen-Younes, H\&#x27;ediandP\&#x27;erez, PatrickandCord, Matthieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zemni_OCTET_Object-Aware_Counterfactual_Explanations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15062-15071.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为深度视觉模型提供可解释性，特别是在复杂的城市场景中。<br>
                    动机：在安全关键应用（如自动驾驶）中广泛使用深度视觉模型，其可解释性成为关注焦点。<br>
                    方法：提出一种以物体为中心的反事实解释生成框架，通过将查询图像编码到易于进行物体级操作的潜在空间中，使用户能够控制反事实生成过程中要探索的搜索方向。<br>
                    效果：在驾驶场景的反事实解释基准测试上进行一系列实验，结果显示该方法不仅适用于分类，还可以用于语义分割模型的解释。并通过用户研究测量反事实解释在理解决策模型中的有用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. Code is available at https://github.com/valeoai/OCTET.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1561.Polarized Color Image Denoising</span><br>
                <span class="as">Li, ZhuoxiaoandJiang, HaiyangandCao, MingdengandZheng, Yinqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Polarized_Color_Image_Denoising_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9873-9882.png><br>
            
            <span class="tt"><span class="t0">研究问题：单芯片偏振彩色摄影在提供视觉纹理和物体表面信息的同时，使用额外的研究问题：单芯片偏振彩色摄影在提供视觉纹理和物体表面信息的同时，使用额外的定向偏振滤波器阵列会降低光子计数和信噪比，导致图像噪声大，影响偏振分析性能。<br>
                    动机：传统的图像处理流程由于通道中隐含的物理约束过于复杂，难以应对这种挑战。<br>
                    方法：本文提出了一种噪声建模方法进行真实数据合成，并设计了一种受视觉Transformer启发的强大网络结构。<br>
                    效果：通过实验评估，我们采集了一个真实的偏振彩色图像数据集，包括原始短曝光有噪声图像和长曝光参考图像对，证明了我们的方法在数据合成和偏振彩色图像去噪方面的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Single-chip polarized color photography provides both visual textures and object surface information in one snapshot. However, the use of an additional directional polarizing filter array tends to lower photon count and SNR, when compared to conventional color imaging. As a result, such a bilayer structure usually leads to unpleasant noisy images and undermines performance of polarization analysis, especially in low-light conditions. It is a challenge for traditional image processing pipelines owing to the fact that the physical constraints exerted implicitly in the channels are excessively complicated. In this paper, we propose to tackle this issue through a noise modeling method for realistic data synthesis and a powerful network structure inspired by vision Transformer. A real-world polarized color image dataset of paired raw short-exposed noisy images and long-exposed reference images is captured for experimental evaluation, which has demonstrated the effectiveness of our approaches for data synthesis and polarized color image denoising.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1562.A Unified HDR Imaging Method With Pixel and Patch Level</span><br>
                <span class="as">Yan, QingsenandChen, WeiyeandZhang, SongandZhu, YuandSun, JinqiuandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_A_Unified_HDR_Imaging_Method_With_Pixel_and_Patch_Level_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22211-22220.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地映射低动态范围（LDR）图像到高动态范围（HDR），特别是在动态场景中，避免由于物体运动或相机跳跃引起的重影问题。<br>
                    动机：尽管已有一些基于深度神经网络（DNNs）的方法被提出以缓解重影问题，但在存在运动和饱和度的情况下，这些方法无法生成令人满意的结果。<br>
                    方法：我们提出了一种混合的高动态范围去重影网络，称为HyHDRNet，通过学习参考和非参考图像之间的复杂关系来生成视觉上令人愉悦的HDR图像。该网络由内容对齐子网络和基于Transformer的融合子网络组成。<br>
                    效果：我们在四个广泛使用的公共HDR图像去重影数据集上测试了该方法。实验证明，HyHDRNet在数量和质量上都优于最先进的方法，能够生成具有统一纹理和颜色的吸引人的HDR可视化效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mapping Low Dynamic Range (LDR) images with different exposures to High Dynamic Range (HDR) remains nontrivial and challenging on dynamic scenes due to ghosting caused by object motion or camera jitting. With the success of Deep Neural Networks (DNNs), several DNNs-based methods have been proposed to alleviate ghosting, they cannot generate approving results when motion and saturation occur. To generate visually pleasing HDR images in various cases, we propose a hybrid HDR deghosting network, called HyHDRNet, to learn the complicated relationship between reference and non-reference images. The proposed HyHDRNet consists of a content alignment subnetwork and a Transformer-based fusion subnetwork. Specifically, to effectively avoid ghosting from the source, the content alignment subnetwork uses patch aggregation and ghost attention to integrate similar content from other non-reference images with patch level and suppress undesired components with pixel level. To achieve mutual guidance between patch-level and pixel-level, we leverage a gating module to sufficiently swap useful information both in ghosted and saturated regions. Furthermore, to obtain a high-quality HDR image, the Transformer-based fusion subnetwork uses a Residual Deformable Transformer Block (RDTB) to adaptively merge information for different exposed regions. We examined the proposed method on four widely used public HDR image deghosting datasets. Experiments demonstrate that HyHDRNet outperforms state-of-the-art methods both quantitatively and qualitatively, achieving appealing HDR visualization with unified textures and colors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1563.Zero-Shot Dual-Lens Super-Resolution</span><br>
                <span class="as">Xu, RuikangandYao, MingdeandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Zero-Shot_Dual-Lens_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9130-9139.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用现有的手机设备上的不对称双镜头配置，实现对同一场景的超分辨率重建。<br>
                    动机：由于相机的未知获取过程（如微小的相机运动），即使在同一设备上，建模真实的超分辨率也存在图像特定的退化问题。<br>
                    方法：本文提出了一种零样本双镜头超分辨率解决方案（ZeDuSR），仅在测试时使用双镜头对来学习特定于图像的超分辨率模型。<br>
                    效果：通过提出退化不变的对齐方法和退化感知的训练策略，ZeDuSR能够充分利用单个双镜头对内的信息，实验证明其在合成和现实世界的双镜头数据集上都优于现有解决方案。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The asymmetric dual-lens configuration is commonly available on mobile devices nowadays, which naturally stores a pair of wide-angle and telephoto images of the same scene to support realistic super-resolution (SR). Even on the same device, however, the degradation for modeling realistic SR is image-specific due to the unknown acquisition process (e.g., tiny camera motion). In this paper, we propose a zero-shot solution for dual-lens SR (ZeDuSR), where only the dual-lens pair at test time is used to learn an image-specific SR model. As such, ZeDuSR adapts itself to the current scene without using external training data, and thus gets rid of generalization difficulty. However, there are two major challenges to achieving this goal: 1) dual-lens alignment while keeping the realistic degradation, and 2) effective usage of highly limited training data. To overcome these two challenges, we propose a degradation-invariant alignment method and a degradation-aware training strategy to fully exploit the information within a single dual-lens pair. Extensive experiments validate the superiority of ZeDuSR over existing solutions on both synthesized and real-world dual-lens datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1564.Fantastic Breaks: A Dataset of Paired 3D Scans of Real-World Broken Objects and Their Complete Counterparts</span><br>
                <span class="as">Lamb, NikolasandPalmer, CameronandMolloy, BenjaminandBanerjee, SeanandBanerjee, NatashaKholgade</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lamb_Fantastic_Breaks_A_Dataset_of_Paired_3D_Scans_of_Real-World_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4681-4691.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动形状修复方法缺乏真实破损几何形状的数据集。<br>
                    动机：我们提出了Fantastic Breaks（及其获取方式），这是一个包含150个破损物体扫描、防水和清理后的3D网格的数据集，与完整的对应物进行了配对和几何对齐。<br>
                    方法：通过详细分析断裂几何，我们揭示了Fantastic Breaks与使用几何和物理基础方法生成的合成断裂数据集之间的差异。我们展示了使用多种基于学习的预先在合成数据集上训练并在Fantastic Breaks子集上重新训练的方法进行形状修复评估。<br>
                    效果：实验结果表明，Fantastic Breaks可以显著提高形状修复的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automated shape repair approaches currently lack access to datasets that describe real-world damaged geometry. We present Fantastic Breaks (and Where to Find Them: https://terascale-all-sensing-research-studio.github.io/FantasticBreaks), a dataset containing scanned, waterproofed, and cleaned 3D meshes for 150 broken objects, paired and geometrically aligned with complete counterparts. Fantastic Breaks contains class and material labels, proxy repair parts that join to broken meshes to generate complete meshes, and manually annotated fracture boundaries. Through a detailed analysis of fracture geometry, we reveal differences between Fantastic Breaks and synthetic fracture datasets generated using geometric and physics-based methods. We show experimental shape repair evaluation with Fantastic Breaks using multiple learning-based approaches pre-trained with synthetic datasets and re-trained with subset of Fantastic Breaks.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>