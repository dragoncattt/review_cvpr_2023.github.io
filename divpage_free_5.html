<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic5</h2>
      <b>Topic words : &ensp;</b>image, &ensp;text, &ensp;language, &ensp;tasks, &ensp;visual, &ensp;pre, &ensp;training, &ensp;vision</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1162.Picture That Sketch: Photorealistic Image Generation From Abstract Sketches</span><br>
                <span class="as">Koley, SubhadeepandBhunia, AyanKumarandSain, AneeshanandChowdhury, PinakiNathandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Koley_Picture_That_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6850-6861.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将非专业的手绘草图转化为照片般真实的图像。<br>
                    动机：现有的技术需要以边缘地图式的草图作为起点，而本文的目标是处理抽象的徒手人类草图。<br>
                    方法：提出一个解耦的编码器-解码器训练范式，其中解码器是一个仅在照片上训练的StyleGAN。此外，还提出了一种自回归草图映射器，用于将草图映射到StyleGAN的潜在空间。<br>
                    效果：实验结果表明，该方法可以生成始终逼真的照片般真实的结果，并展示了一些下游任务，如精细的草图基于图像检索等。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given an abstract, deformed, ordinary sketch from untrained amateurs like you and me, this paper turns it into a photorealistic image - just like those shown in Fig. 1(a), all non-cherry-picked. We differ significantly from prior art in that we do not dictate an edgemap-like sketch to start with, but aim to work with abstract free-hand human sketches. In doing so, we essentially democratise the sketch-to-photo pipeline, "picturing" a sketch regardless of how good you sketch. Our contribution at the outset is a decoupled encoder-decoder training paradigm, where the decoder is a StyleGAN trained on photos only. This importantly ensures that generated results are always photorealistic. The rest is then all centred around how best to deal with the abstraction gap between sketch and photo. For that, we propose an autoregressive sketch mapper trained on sketch-photo pairs that maps a sketch to the StyleGAN latent space. We further introduce specific designs to tackle the abstract nature of human sketches, including a fine-grained discriminative loss on the back of a trained sketch-photo retrieval model, and a partial-aware sketch augmentation strategy. Finally, we showcase a few downstream tasks our generation model enables, amongst them is showing how fine-grained sketch-based image retrieval, a well-studied problem in the sketch community, can be reduced to an image (generated) to image retrieval task, surpassing state-of-the-arts. We put forward generated results in the supplementary for everyone to scrutinise. Project page: https://subhadeepkoley.github.io/PictureThatSketch</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1163.Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information</span><br>
                <span class="as">Su, WeijieandZhu, XizhouandTao, ChenxinandLu, LeweiandLi, BinandHuang, GaoandQiao, YuandWang, XiaogangandZhou, JieandDai, Jifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15888-15899.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用大规模模型的潜力，提出各种预训练策略以支持来自不同来源的大量数据。<br>
                    动机：目前的预训练方法采用多阶段预训练系统，复杂的流程可能增加预训练的不确定性和不稳定性，因此希望这些策略能够以单阶段的方式进行整合。<br>
                    方法：提出了一种通用的多模态互信息公式作为统一的优化目标，并证明所有主流方法都是该框架的特例。在这个统一的视角下，提出了一种名为“最大化多模态互信息预训练”（M3I预训练）的一体化单阶段预训练方法。<br>
                    效果：该方法在各种视觉基准测试中的表现优于以往的预训练方法，包括ImageNet分类、COCO对象检测、LVIS长尾对象检测和ADE20k语义分割等。特别是在公共数据集设置下，成功预训练了一个十亿级别的参数图像主干网络，并在各种基准测试中取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To effectively exploit the potential of large-scale models, various pre-training strategies supported by massive data from different sources are proposed, including supervised pre-training, weakly-supervised pre-training, and self-supervised pre-training. It has been proved that combining multiple pre-training strategies and data from various modalities/sources can greatly boost the training of large-scale models. However, current works adopt a multi-stage pre-training system, where the complex pipeline may increase the uncertainty and instability of the pre-training. It is thus desirable that these strategies can be integrated in a single-stage manner. In this paper, we first propose a general multi-modal mutual information formula as a unified optimization target and demonstrate that all mainstream approaches are special cases of our framework. Under this unified perspective, we propose an all-in-one single-stage pre-training approach, named Maximizing Multi-modal Mutual Information Pre-training (M3I Pre-training). Our approach achieves better performance than previous pre-training methods on various vision benchmarks, including ImageNet classification, COCO object detection, LVIS long-tailed object detection, and ADE20k semantic segmentation. Notably, we successfully pre-train a billion-level parameter image backbone and achieve state-of-the-art performance on various benchmarks under public data setting. Code shall be released at https://github.com/OpenGVLab/M3I-Pretraining.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1164.Aligning Bag of Regions for Open-Vocabulary Object Detection</span><br>
                <span class="as">Wu, SizeandZhang, WenweiandJin, ShengandLiu, WentaoandLoy, ChenChange</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15254-15264.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉语言模型和物体检测器无法充分利用图像中语义概念的组合结构。<br>
                    动机：为了解决这一问题，我们提出了一种新的方法，将相关联的区域组合成“bag”，并将这些区域的嵌入作为句子中的词来处理。<br>
                    方法：我们将一组上下文相关的区域组合成一个"bag"，将这些区域的嵌入作为句子中的词来处理，然后将它们发送到视觉语言模型的文本编码器中，以获取"bag-of-regions"嵌入，该嵌入被学习以与冻结的视觉语言模型提取的特征对齐。<br>
                    效果：在常用的Faster R-CNN上应用我们的方法，在开放词汇COCO和LVIS基准测试的新类别上，我们的方法是之前最佳结果的4.6倍盒AP 50和2.8倍mask AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs. Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond individual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP 50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github.com/wusize/ovdet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1165.CLIP2Scene: Towards Label-Efficient 3D Scene Understanding by CLIP</span><br>
                <span class="as">Chen, RunnanandLiu, YouquanandKong, LingdongandZhu, XingeandMa, YuexinandLi, YikangandHou, YuenanandQiao, YuandWang, Wenping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7020-7030.png><br>
            
            <span class="tt"><span class="t0">研究问题：探索如何将CLIP知识应用于3D场景理解。<br>
                    动机：尽管CLIP在2D图像-文本预训练模型中取得了显著的成果，但其在3D场景理解中的应用尚未被探索。<br>
                    方法：提出CLIP2Scene框架，将CLIP的知识从2D图像-文本预训练模型转移到3D点云网络中。设计了一个基于语义的跨模态对比学习框架来预训练3D网络。<br>
                    效果：在SemanticKITTI、nuScenes和ScanNet上进行实验，首次实现了无标注的3D语义分割，并在有标注数据微调后，显著优于其他自监督方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive Language-Image Pre-training (CLIP) achieves promising results in 2D zero-shot and few-shot learning. Despite the impressive performance in 2D, applying CLIP to help the learning in 3D scene understanding has yet to be explored. In this paper, we make the first attempt to investigate how CLIP knowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yet effective framework that transfers CLIP knowledge from 2D image-text pre-trained models to a 3D point cloud network. We show that the pre-trained 3D network yields impressive performance on various downstream tasks, i.e., annotation-free and fine-tuning with labelled data for semantic segmentation. Specifically, built upon CLIP, we design a Semantic-driven Cross-modal Contrastive Learning framework that pre-trains a 3D network via semantic and spatial-temporal consistency regularization. For the former, we first leverage CLIP's text semantics to select the positive and negative point samples and then employ the contrastive loss to train the 3D network. In terms of the latter, we force the consistency between the temporally coherent point cloud features and their corresponding image features. We conduct experiments on SemanticKITTI, nuScenes, and ScanNet. For the first time, our pre-trained network achieves annotation-free 3D semantic segmentation with 20.8% and 25.08% mIoU on nuScenes and ScanNet, respectively. When fine-tuned with 1% or 100% labelled data, our method significantly outperforms other self-supervised methods, with improvements of 8% and 1% mIoU, respectively. Furthermore, we demonstrate the generalizability for handling cross-domain datasets. Code is publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1166.Prefix Conditioning Unifies Language and Label Supervision</span><br>
                <span class="as">Saito, KuniakiandSohn, KihyukandZhang, XiangandLi, Chun-LiangandLee, Chen-YuandSaenko, KateandPfister, Tomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Prefix_Conditioning_Unifies_Language_and_Label_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2861-2870.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何结合大规模分类数据集和图像-标题数据集进行预训练，以充分利用两者的互补优势。<br>
                    动机：图像-标题数据集具有更开放的领域，包含更广泛的场景类型和词汇，而大规模分类数据集可以提供平衡标签分布的精细类别。<br>
                    方法：提出了一种新的预训练策略，使用分类和标题数据集进行联合训练，通过引入前缀令牌来解决数据集偏差问题，使语言编码器能够学习两个数据集的特征。<br>
                    效果：实验表明，该方法提高了零样本图像识别的性能，并增强了对图像级别的分布偏移的鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pretraining visual models on web-scale image-caption datasets has recently emerged as a powerful alternative to traditional pretraining on image classification data. Image-caption datasets are more "open-domain", containing broader scene types and vocabulary words, and result in models that have strong performance in few- and zero-shot recognition tasks. However large-scale classification datasets can provide fine-grained categories with a balanced label distribution. In this work, we study a pretraining strategy that uses both classification and caption datasets to unite their complementary benefits. First, we show that naively unifying the datasets results in sub-optimal performance in downstream zero-shot recognition tasks, as the model is affected by dataset bias: the coverage of image domains and vocabulary words is different in each dataset. We address this problem with novel Prefix Conditioning, a simple yet effective method that helps disentangle dataset biases from visual concepts. This is done by introducing prefix tokens that inform the language encoder of the input data type (e.g., classification vs caption) at training time. Our approach allows the language encoder to learn from both datasets while also tailoring feature extraction to each dataset. Prefix conditioning is generic and can be easily integrated into existing VL pretraining objectives, such as CLIP or UniCL. In experiments, we show that it improves zero-shot image recognition and robustness to image-level distribution shift.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1167.GD-MAE: Generative Decoder for MAE Pre-Training on LiDAR Point Clouds</span><br>
                <span class="as">Yang, HonghuiandHe, TongandLiu, JiahengandChen, HuaandWu, BoxiandLin, BinbinandHe, XiaofeiandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_GD-MAE_Generative_Decoder_for_MAE_Pre-Training_on_LiDAR_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9403-9414.png><br>
            
            <span class="tt"><span class="t0">研究问题：探索大规模3D点云中的Masked Autoencoders (MAE)。<br>
                    动机：现有的3D MAE框架设计复杂，或者采用复杂的遮蔽策略，我们提出一个更简单的方法。<br>
                    方法：应用生成解码器进行MAE（GD-MAE），自动合并周围上下文，以层次融合的方式恢复被遮蔽的几何知识。<br>
                    效果：在Waymo、KITTI和ONCE等大型基准测试中表现出色，不仅达到最先进的结果，而且在Waymo数据集上即使只有20%的标记数据也能达到相当的精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the tremendous progress of Masked Autoencoders (MAE) in developing vision tasks such as image and video, exploring MAE in large-scale 3D point clouds remains challenging due to the inherent irregularity. In contrast to previous 3D MAE frameworks, which either design a complex decoder to infer masked information from maintained regions or adopt sophisticated masking strategies, we instead propose a much simpler paradigm. The core idea is to apply a Generative Decoder for MAE (GD-MAE) to automatically merges the surrounding context to restore the masked geometric knowledge in a hierarchical fusion manner. In doing so, our approach is free from introducing the heuristic design of decoders and enjoys the flexibility of exploring various masking strategies. The corresponding part costs less than 12% latency compared with conventional methods, while achieving better performance. We demonstrate the efficacy of the proposed method on several large-scale benchmarks: Waymo, KITTI, and ONCE. Consistent improvement on downstream detection tasks illustrates strong robustness and generalization capability. Not only our method reveals state-of-the-art results, but remarkably, we achieve comparable accuracy even with 20% of the labeled data on the Waymo dataset. Code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1168.Towards Robust Tampered Text Detection in Document Image: New Dataset and New Solution</span><br>
                <span class="as">Qu, ChenfanandLiu, ChongyuandLiu, YuliangandChen, XinhongandPeng, DezhiandGuo, FengjunandJin, Lianwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Towards_Robust_Tampered_Text_Detection_in_Document_Image_New_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5937-5946.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地检测图像中被篡改的文本。<br>
                    动机：由于其在信息安全中的重要作用，对文档图像中篡改文本的检测越来越受到关注。<br>
                    方法：提出了一种新的框架，称为文档篡改检测器（DTD），包括频率感知头（FPH）和多视图迭代解码器（MID），以捕捉复杂场景中的更精细线索。同时设计了一种新的训练方式，称为篡改检测课程学习（CLTD）。<br>
                    效果：实验表明，提出的DTD在各种类型的文档图像上的表现优于先前最先进的技术，提高了9.2%，26.3%和12.3%的F-measure。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, tampered text detection in document image has attracted increasingly attention due to its essential role on information security. However, detecting visually consistent tampered text in photographed document images is still a main challenge. In this paper, we propose a novel framework to capture more fine-grained clues in complex scenarios for tampered text detection, termed as Document Tampering Detector (DTD), which consists of a Frequency Perception Head (FPH) to compensate the deficiencies caused by the inconspicuous visual features, and a Multi-view Iterative Decoder (MID) for fully utilizing the information of features in different scales. In addition, we design a new training paradigm, termed as Curriculum Learning for Tampering Detection (CLTD), which can address the confusion during the training procedure and thus to improve the robustness for image compression and the ability to generalize. To further facilitate the tampered text detection in document images, we construct a large-scale document image dataset, termed as DocTamper, which contains 170,000 document images of various types. Experiments demonstrate that our proposed DTD outperforms previous state-of-the-art by 9.2%, 26.3% and 12.3% in terms of F-measure on the DocTamper testing set, and the cross-domain testing sets of DocTamper-FCD and DocTamper-SCD, respectively. Codes and dataset will be available at https://github.com/qcf-568/DocTamper.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1169.VoP: Text-Video Co-Operative Prompt Tuning for Cross-Modal Retrieval</span><br>
                <span class="as">Huang, SitengandGong, BiaoandPan, YulinandJiang, JianwenandLv, YiliangandLi, YuyuanandWang, Donglin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_VoP_Text-Video_Co-Operative_Prompt_Tuning_for_Cross-Modal_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6565-6574.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行文本-视频跨模态检索，同时避免增加大量参数和知识遗忘的问题。<br>
                    动机：目前的研究大多通过添加重型模块来调整预训练的CLIP模型，这既增加了巨大的计算负担，也导致从上游模型中的知识遗忘。<br>
                    方法：提出VoP（Text-Video Co-operative Prompt Tuning）框架，引入视频和文本提示，可以视为仅具有0.1%可训练参数的强大基线。此外，基于视频的时空特性，开发了三种新的视频提示机制，以改善不同规模的可训练参数的性能。<br>
                    效果：实验表明，与完全微调相比，增强的VoP在五个文本-视频检索基准上实现了1.4%的平均R@1增益，参数开销减少了6倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video & text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achieves a 1.4% average R@1 gain across five text-video retrieval benchmarks with 6x less parameter overhead. The code will be available at https://github.com/bighuang624/VoP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1170.Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR</span><br>
                <span class="as">Sain, AneeshanandBhunia, AyanKumarandKoley, SubhadeepandChowdhury, PinakiNathandChattopadhyay, SoumitriandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sain_Exploiting_Unlabelled_Photos_for_Stronger_Fine-Grained_SBIR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6873-6883.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决细粒度草图基图像检索（FG-SBIR）中的两个关键问题，即黄金标准三元组损失无法强制整体潜在空间几何，以及训练高精度模型的草图数量永远不足。<br>
                    动机：为了解决上述问题，作者提出了一种改进的三元组损失和一种新的知识蒸馏模块，并设计了一种新颖的即插即用训练范式。<br>
                    方法：首先，对标准的三元组损失进行了修改，以明确在照片/草图实例之间进行分离。其次，提出了一种新的知识蒸馏模块，可以利用照片数据进行模型训练。最后，将这两个模块插入到新的即插即用训练范式中，以实现更稳定的训练。<br>
                    效果：实验结果表明，该方法不仅显著超越了现有技术，而且在新类别上的泛化结果也令人满意。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper advances the fine-grained sketch-based image retrieval (FG-SBIR) literature by putting forward a strong baseline that overshoots prior state-of-the art by  11%. This is not via complicated design though, but by addressing two critical issues facing the community (i) the gold standard triplet loss does not enforce holistic latent space geometry, and (ii) there are never enough sketches to train a high accuracy model. For the former, we propose a simple modification to the standard triplet loss, that explicitly enforces separation amongst photos/sketch instances. For the latter, we put forward a novel knowledge distillation module can leverage photo data for model training. Both modules are then plugged into a novel plug-n-playable training paradigm that allows for more stable training. More specifically, for (i) we employ an intra-modal triplet loss amongst sketches to bring sketches of the same instance closer from others, and one more amongst photos to push away different photo instances while bringing closer a structurally augmented version of the same photo (offering a gain of 4-6%). To tackle (ii), we first pre-train a teacher on the large set of unlabelled photos over the aforementioned intra-modal photo triplet loss. Then we distill the contextual similarity present amongst the instances in the teacher's embedding space to that in the student's embedding space, by matching the distribution over inter-feature distances of respective samples in both embedding spaces (delivering a further gain of 4-5%). Apart from outperforming prior arts significantly, our model also yields satisfactory results on generalising to new classes. Project page: https://aneeshan95.github.io/Sketch_PVT/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1171.Seeing What You Miss: Vision-Language Pre-Training With Semantic Completion Learning</span><br>
                <span class="as">Ji, YataiandTu, RongchengandJiang, JieandKong, WeijieandCai, ChengfeiandZhao, WenzheandWang, HongfaandYang, YujiuandLiu, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Seeing_What_You_Miss_Vision-Language_Pre-Training_With_Semantic_Completion_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6789-6798.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉-语言预训练模型中跨模态对齐的问题，以学习不同模态之间的正确对应信息。<br>
                    动机：现有的视觉-语言预训练模型在跨模态对齐方面存在局限，主要关注基于可见上下文的局部对齐，忽视了全局语义特征的学习。<br>
                    方法：受自然语言处理预训练领域成功应用的掩码语言建模任务启发，提出一种新颖的语义完成学习（SCL）任务，以促进全局-局部对齐。具体来说，SCL任务通过捕获另一模态的对应信息来补充被遮蔽数据缺失的语义，从而学习更具代表性的全局特征。<br>
                    效果：实验结果表明，该方法在各种视觉-语言基准测试中取得了最先进的性能，如视觉问答、图像-文本检索和视频-文本检索。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct corresponding information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corresponding information from the other modality, promoting learning more representative global features which have a great impact on the performance of downstream tasks. Moreover, we present a flexible vision encoder, which enables our model to perform image-text and video-text multimodal tasks simultaneously. Experimental results show that our proposed method obtains state-of-the-art performance on various vision-language benchmarks, such as visual question answering, image-text retrieval, and video-text retrieval.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1172.Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning</span><br>
                <span class="as">Jiang, QianandChen, ChangyouandZhao, HanandChen, LiqunandPing, QingandTran, SonDinhandXu, YiandZeng, BelindaandChilimbi, Trishul</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Understanding_and_Constructing_Latent_Modality_Structures_in_Multi-Modal_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7661-7671.png><br>
            
            <span class="tt"><span class="t0">研究问题：对比损失在多模态学习表示中被广泛应用，但其对下游任务性能的影响尚未明确。<br>
                    动机：尽管对比损失鼓励模态在潜在空间中完全匹配，但完全的模态对齐并不一定能带来最佳的下游预测任务性能。<br>
                    方法：本文提出了三种构建潜在模态结构的方法，包括深度特征分离损失用于模内正则化、布朗桥损失用于模间正则化和几何一致性损失用于模内和模间正则化。<br>
                    效果：通过在两个流行的多模态表示学习框架上进行实验，证明了该方法在零/少次图像分类、图像-文本检索、视觉问答、视觉推理和视觉蕴含等任务上的有效性和通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular multi-modal representation learning frameworks: the CLIP-based two-tower model and the ALBEF-based fusion model. We test our model on a variety of tasks including zero/few-shot image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our method achieves consistent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1173.Variational Distribution Learning for Unsupervised Text-to-Image Generation</span><br>
                <span class="as">Kang, MinsooandLee, DoyupandKim, JiseobandKim, SaehoonandHan, Bohyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Variational_Distribution_Learning_for_Unsupervised_Text-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23380-23389.png><br>
            
            <span class="tt"><span class="t0">研究问题：在训练过程中，当图像的文本描述不可用时，如何基于深度神经网络提出一种文本到图像的生成算法。<br>
                    动机：现有的图像描述方法只是简单地生成训练图像的伪真实句子，我们采用预训练的CLIP模型，该模型能够在联合空间中正确对齐图像和相应文本的嵌入，从而在零射击识别任务上表现良好。<br>
                    方法：通过最大化条件成对图像-文本CLIP嵌入的数据对数似然性来优化文本到图像生成模型。为了更好地对齐两个领域的数据，我们采用基于变分推理的原理，有效地估计给定图像及其CLIP特征的隐藏文本嵌入的近似后验。<br>
                    效果：实验结果表明，所提出的框架在无监督和半监督的文本到图像生成设置下大大优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training images using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly aligning embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recognition tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a variational inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the proposed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1174.Cross-Domain Image Captioning With Discriminative Finetuning</span><br>
                <span class="as">Dess{\`\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dessi_Cross-Domain_Image_Captioning_With_Discriminative_Finetuning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6935-6944.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练神经网络描述器模仿人类生成的参考，但不优化任何特定的通信目标，导致产生模糊的描述。<br>
                    动机：通过自我监督的判别性通信目标对现成的神经网络描述器进行微调，有助于恢复直观、视觉描述性的语言，使其更能反映图像内容。<br>
                    方法：给定一个目标图像，系统必须学会生成一个描述，使现成的文本条件图像检索器能够在一组候选图像中识别出该图像。我们使用流行的ClipCap描述器进行实验，并使用BLIP复制了主要结果。<br>
                    效果：在与地面真实人类描述的相似性方面，当非微调模型在相同的字幕数据集上进行训练和测试时，来自判别性微调的描述落后于那些由未微调模型生成的描述。然而，当我们使用模型为非领域数据集生成描述时，我们的判别性微调描述器生成的描述比未经微调的描述更接近人类参考。我们还进一步表明，在概念性字幕数据集上，判别性微调的描述对于人类注释者执行图像判别任务比香草ClipCap描述或地面真实描述更有用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural captioners are typically trained to mimic human-generated references without optimizing for any specific communication goal, leading to problems such as the generation of vague captions. In this paper, we show that fine-tuning an out-of-the-box neural captioner with a self-supervised discriminative communication objective helps to recover a plain, visually descriptive language that is more informative about image contents. Given a target image, the system must learn to produce a description that enables an out-of-the-box text-conditioned image retriever to identify such image among a set of candidates. We experiment with the popular ClipCap captioner, also replicating the main results with BLIP. In terms of similarity to ground-truth human descriptions, the captions emerging from discriminative finetuning lag slightly behind those generated by the non-finetuned model, when the latter is trained and tested on the same caption dataset. However, when the model is used without further tuning to generate captions for out-of-domain datasets, our discriminatively-finetuned captioner generates descriptions that resemble human references more than those produced by the same captioner without finetuning. We further show that, on the Conceptual Captions dataset, discriminatively finetuned captions are more helpful than either vanilla ClipCap captions or ground-truth captions for human annotators tasked with an image discrimination task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1175.Accelerating Vision-Language Pretraining With Free Language Modeling</span><br>
                <span class="as">Wang, TengandGe, YixiaoandZheng, FengandCheng, RanandShan, YingandQie, XiaohuandLuo, Ping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23161-23170.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉-语言预训练模型（VLP）虽然表现优秀，但训练成本高，尤其是在大型网络数据集上，因为其收敛速度慢且训练时间长。<br>
                    动机：提高VLP的训练效率的主要障碍在于被遮蔽的语言模型（MLM）中的预测率和损坏率相互关联，即为了达到适当的损坏率，需要排除大部分输出标记不参与预测损失。<br>
                    方法：为加速VLP的收敛，提出一种新的预训练任务——自由语言建模（FLM），它可以实现100%的预测率和任意的损坏率。通过允许每个待预测的标记自定义损坏范围，FLM成功地将预测率与损坏率解耦。<br>
                    效果：实验表明，与基于MLM的方法相比，FLM可以在相同的GPU时间内训练出性能相当的模型，同时预训练时间减少了2.5倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The state of the arts in vision-language pretraining (VLP) achieves exemplary performance but suffers from high training costs resulting from slow convergence and long training time, especially on large-scale web datasets. An essential obstacle to training efficiency lies in the entangled prediction rate (percentage of tokens for reconstruction) and corruption rate (percentage of corrupted tokens) in masked language modeling (MLM), that is, a proper corruption rate is achieved at the cost of a large portion of output tokens being excluded from prediction loss. To accelerate the convergence of VLP, we propose a new pretraining task, namely, free language modeling (FLM), that enables a 100% prediction rate with arbitrary corruption rates. FLM successfully frees the prediction rate from the tie-up with the corruption rate while allowing the corruption spans to be customized for each token to be predicted. FLM-trained models are encouraged to learn better and faster given the same GPU time by exploiting bidirectional contexts more flexibly. Extensive experiments show FLM could achieve an impressive 2.5x pretraining time reduction in comparison to the MLM-based methods, while keeping competitive performance on both vision-language understanding and generation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1176.Masked Autoencoders Enable Efficient Knowledge Distillers</span><br>
                <span class="as">Bai, YutongandWang, ZeyuandXiao, JunfeiandWei, ChenandWang, HuiyuandYuille, AlanL.andZhou, YuyinandXie, Cihang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Masked_Autoencoders_Enable_Efficient_Knowledge_Distillers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24256-24265.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从预训练模型中提取知识，特别是遮蔽自动编码器。<br>
                    动机：优化像素重建损失和减小教师模型与学生模型的中间特征图之间的距离，设计出一种计算效率高的知识蒸馏框架。<br>
                    方法：通过仅使用一小部分可见的补丁和使用部分执行（即前几层的输入的前向传播）的教师模型来获取中间特征图，进行知识蒸馏。<br>
                    效果：与直接蒸馏微调模型相比，蒸馏预训练模型显著提高了下游性能。例如，通过将一个预训练的ViT-L模型的知识蒸馏到一个ViT-B模型中，该方法实现了84.0%的ImageNet top-1准确率，比直接蒸馏微调的ViT-L模型的基线高出1.2%。更有趣的是，即使在极高的遮蔽比率下，该方法也能从教师模型中稳健地提取知识。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies the potential of distilling knowledge from pre-trained models, especially Masked Autoencoders. Our approach is simple: in addition to optimizing the pixel reconstruction loss on masked inputs, we minimize the distance between the intermediate feature map of the teacher model and that of the student model. This design leads to a computationally efficient knowledge distillation framework, given 1) only a small visible subset of patches is used, and 2) the (cumbersome) teacher model only needs to be partially executed, i.e., forward propagate inputs through the first few layers, for obtaining intermediate feature maps. Compared to directly distilling fine-tuned models, distilling pre-trained models substantially improves downstream performance. For example, by distilling the knowledge from an MAE pre-trained ViT-L into a ViT-B, our method achieves 84.0% ImageNet top-1 accuracy, outperforming the baseline of directly distilling a fine-tuned ViT-L by 1.2%. More intriguingly, our method can robustly distill knowledge from teacher models even with extremely high masking ratios: e.g., with 95% masking ratio where merely TEN patches are visible during distillation, our ViT-B competitively attains a top-1 ImageNet accuracy of 83.6%; surprisingly, it can still secure 82.4% top-1 ImageNet accuracy by aggressively training with just FOUR visible patches (98% masking ratio). The code will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1177.TinyMIM: An Empirical Study of Distilling MIM Pre-Trained Models</span><br>
                <span class="as">Ren, SuchengandWei, FangyunandZhang, ZhengandHu, Han</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_TinyMIM_An_Empirical_Study_of_Distilling_MIM_Pre-Trained_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3687-3697.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将大型视觉Transformer的预训练成功转移到小型模型上。<br>
                    动机：虽然大型视觉Transformer的预训练模型在大规模图像语料库上表现良好，但小型模型无法或只能从这种预训练方法中获得少量收益，这对实际应用至关重要。<br>
                    方法：通过知识蒸馏技术将大型基于遮蔽图像建模（MIM）的预训练模型的成功转移到小型模型上。系统地研究了知识蒸馏框架中的不同选项，包括蒸馏目标、损失、输入、网络正则化、顺序蒸馏等。<br>
                    效果：实验结果表明，使用所有ViT-Tiny、ViT-Small和ViT-base模型，我们在ImageNet-1K分类任务上的微调精度提高了+4.2%/+2.4%/+1.4%。我们的TinyMIM模型在AE20K语义分割任务上达到了52.2 mIoU，比MAE基线高出+4.1。我们的TinyMIM模型在ImageNet-1K图像分类任务上达到了79.6%的top-1精度，为相同大小和计算预算的小型视觉模型设定了新的记录。这强大的性能表明，开发小型视觉Transformer模型的另一种方法是探索更好的训练方法，而不是像大多数先前的工作那样在架构中引入归纳偏差。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked image modeling (MIM) performs strongly in pre-training large vision Transformers (ViTs). However, small models that are critical for real-world applications cannot or only marginally benefit from this pre-training approach. In this paper, we explore distillation techniques to transfer the success of large MIM-based pre-trained models to smaller ones. We systematically study different options in the distillation framework, including distilling targets, losses, input, network regularization, sequential distillation, etc, revealing that: 1) Distilling token relations is more effective than CLS token- and feature-based distillation; 2) An intermediate layer of the teacher network as target perform better than that using the last layer when the depth of the student mismatches that of the teacher; 3) Weak regularization is preferred; etc. With these findings, we achieve significant fine-tuning accuracy improvements over the scratch MIM pre-training on ImageNet-1K classification, using all the ViT-Tiny, ViT-Small, and ViT-base models, with +4.2%/+2.4%/+1.4% gains, respectively. Our TinyMIM model of base size achieves 52.2 mIoU in AE20K semantic segmentation, which is +4.1 higher than the MAE baseline. Our TinyMIM model of tiny size achieves 79.6% top-1 accuracy on ImageNet-1K image classification, which sets a new record for small vision models of the same size and computation budget. This strong performance suggests an alternative way for developing small vision Transformer models, that is, by exploring better training methods rather than introducing inductive biases into architectures as in most previous works. Code is available at https://github.com/OliverRensu/TinyMIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1178.OneFormer: One Transformer To Rule Universal Image Segmentation</span><br>
                <span class="as">Jain, JiteshandLi, JiachenandChiu, MangTikandHassani, AliandOrlov, NikitaandShi, Humphrey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2989-2998.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现图像分割的统一框架，以同时提高语义、实例和全景分割的性能。<br>
                    动机：现有的图像分割方法需要分别对语义、实例或全景分割进行训练，缺乏统一性。理想的情况是，一个真正的通用框架只需训练一次，就能在所有三个图像分割任务上达到最先进的性能。<br>
                    方法：提出了一种名为OneFormer的通用图像分割框架，通过多任务一次训练的设计将分割与任务统一起来。首先，提出了一种任务条件联合训练策略，使得在一个单一的多任务训练过程中可以基于每个领域的真值（语义、实例和全景分割）进行训练。其次，引入了任务令牌，使模型根据手头的任务进行动态调整，支持多任务训练和推理。第三，提出了在训练过程中使用查询-文本对比损失来建立更好的任务间和类别间的区分。<br>
                    效果：实验结果表明，尽管后者是在每个任务上单独进行训练的，但OneFormer模型在所有三个分割任务上都优于专门的Mask2Former模型，显示出了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Universal Image Segmentation is not a new concept.Past attempts to unify image segmentation include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, Cityscapes, and COCO, despite the latter being trained on each task individually. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1179.GeoLayoutLM: Geometric Pre-Training for Visual Information Extraction</span><br>
                <span class="as">Luo, ChuweiandCheng, ChangxuandZheng, QiandYao, Cong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_GeoLayoutLM_Geometric_Pre-Training_for_Visual_Information_Extraction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7092-7101.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练模型在知识图谱中利用外部知识增强语言表示的问题，以及视觉信息提取（VIE）任务中的关系抽取（RE）问题。<br>
                    动机：目前的预训练语言模型和视觉信息提取模型在处理关系抽取任务时，由于缺乏对几何信息的充分利用和预训练与微调阶段目标差距大，导致性能受限。<br>
                    方法：本文提出了一种名为GeoLayoutLM的多模态框架，通过设计三个特殊的几何相关预训练任务进行几何预训练，并设计了新的、经过几何预训练任务预训练和关系抽取微调的关系头来丰富和增强特征表示。<br>
                    效果：实验结果表明，GeoLayoutLM在语义实体识别任务上取得了高度竞争的成绩，并在关系抽取任务上显著优于先前最先进的模型（例如，FUNSD数据集上的关系抽取F1分数从80.35%提升到89.45%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual information extraction (VIE) plays an important role in Document Intelligence. Generally, it is divided into two tasks: semantic entity recognition (SER) and relation extraction (RE). Recently, pre-trained models for documents have achieved substantial progress in VIE, particularly in SER. However, most of the existing models learn the geometric representation in an implicit way, which has been found insufficient for the RE task since geometric information is especially crucial for RE. Moreover, we reveal another factor that limits the performance of RE lies in the objective gap between the pre-training phase and the fine-tuning phase for RE. To tackle these issues, we propose in this paper a multi-modal framework, named GeoLayoutLM, for VIE. GeoLayoutLM explicitly models the geometric relations in pre-training, which we call geometric pre-training. Geometric pre-training is achieved by three specially designed geometry-related pre-training tasks. Additionally, novel relation heads, which are pre-trained by the geometric pre-training tasks and fine-tuned for RE, are elaborately designed to enrich and enhance the feature representation. According to extensive experiments on standard VIE benchmarks, GeoLayoutLM achieves highly competitive scores in the SER task and significantly outperforms the previous state-of-the-arts for RE (e.g.,the F1 score of RE on FUNSD is boosted from 80.35% to 89.45%).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1180.Open-Category Human-Object Interaction Pre-Training via Language Modeling Framework</span><br>
                <span class="as">Zheng, SipengandXu, BoshenandJin, Qin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19392-19402.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有针对人与物体交互（HOI）的预测方法受限于有限的监督数据和现实生活中可能的交互组合数量，且无法很好地扩展到开放类别。<br>
                    动机：为了解决这一问题，我们提出了OpenCat，这是一个将人与物体交互预测转化为序列生成的语言模型框架。<br>
                    方法：我们将HOI三元组通过一种序列化方案转换为标记序列，使模型能够利用语言模型框架的开放词汇来预测新的交互类别。同时，我们还从图像-标题对中收集了大量弱监督数据，并设计了几个辅助代理任务进行预训练。<br>
                    效果：实验表明，OpenCat显著提高了HOI的性能，尤其是在广泛的罕见和未见过的类别上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-object interaction (HOI) has long been plagued by the conflict between limited supervised data and a vast number of possible interaction combinations in real life. Current methods trained from closed-set data predict HOIs as fixed-dimension logits, which restricts their scalability to open-set categories. To address this issue, we introduce OpenCat, a language modeling framework that reformulates HOI prediction as sequence generation. By converting HOI triplets into a token sequence through a serialization scheme, our model is able to exploit the open-set vocabulary of the language modeling framework to predict novel interaction classes with a high degree of freedom. In addition, inspired by the great success of vision-language pre-training, we collect a large amount of weakly-supervised data related to HOI from image-caption pairs, and devise several auxiliary proxy tasks, including soft relational matching and human-object relation prediction, to pre-train our model. Extensive experiments show that our OpenCat significantly boosts HOI performance, particularly on a broad range of rare and unseen categories.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1181.CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not</span><br>
                <span class="as">Sain, AneeshanandBhunia, AyanKumarandChowdhury, PinakiNathandKoley, SubhadeepandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2765-2775.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在利用CLIP进行基于草图的零样本图像检索（ZS-SBIR）。<br>
                    动机：受基础模型最新进展和其提供的无与伦比的泛化能力的启发，首次将其定制以使草图社区受益。<br>
                    方法：提出了如何最好地实现这种协同作用的新设计，包括类别设置和细粒度设置("全部")。解决方案的核心是一个提示学习设置。<br>
                    效果：实验结果表明，通过考虑草图特定的提示，我们已经拥有一个超过所有先前艺术的类别级ZS-SBIR系统，大幅度提高了24.8%。在细粒度设置方面则更为棘手，需要深入研究这种协同作用。为此，我们提出了两种特定的设计来解决问题的细粒度匹配特性：（i）一种额外的正则化损失，以确保草图和照片之间的相对分离在整个类别中是均匀的，这与黄金标准的独立三元组损失不同；（ii）一种巧妙的补丁混洗技术，有助于建立草图-照片对之间的实例级结构对应关系。采用这些设计，我们再次观察到比先前最先进的技术提高了约26.9%的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we leverage CLIP for zero-shot sketch based image retrieval (ZS-SBIR). We are largely inspired by recent advances on foundation models and the unparalleled generalisation ability they seem to offer, but for the first time tailor it to benefit the sketch community. We put forward novel designs on how best to achieve this synergy, for both the category setting and the fine-grained setting ("all"). At the very core of our solution is a prompt learning setup. First we show just via factoring in sketch-specific prompts, we already have a category-level ZS-SBIR system that overshoots all prior arts, by a large margin (24.8%) - a great testimony on studying the CLIP and ZS-SBIR synergy. Moving onto the fine-grained setup is however trickier, and requires a deeper dive into this synergy. For that, we come up with two specific designs to tackle the fine-grained matching nature of the problem: (i) an additional regularisation loss to ensure the relative separation between sketches and photos is uniform across categories, which is not the case for the gold standard standalone triplet loss, and (ii) a clever patch shuffling technique to help establishing instance-level structural correspondences between sketch-photo pairs. With these designs, we again observe significant performance gains in the region of 26.9% over previous state-of-the-art. The take-home message, if any, is the proposed CLIP and prompt learning paradigm carries great promise in tackling other sketch-related tasks (not limited to ZS-SBIR) where data scarcity remains a great challenge. Project page: https://aneeshan95.github.io/Sketch_LVM/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1182.Learning To Generate Language-Supervised and Open-Vocabulary Scene Graph Using Pre-Trained Visual-Semantic Space</span><br>
                <span class="as">Zhang, YongandPan, YingweiandYao, TingandHuang, RuiandMei, TaoandChen, Chang-Wen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2915-2924.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决场景图生成（SGG）在实际应用中遇到的两个难题：1）训练SGG模型需要耗时的人工标注；2）封闭的对象类别限制了SGG模型识别训练集以外的新对象的能力。<br>
                    动机：为了解决这些问题，作者提出了一种新颖的方法，利用预训练的视觉语义空间（VSS）来触发语言监督和开放词汇的场景图生成。<br>
                    方法：首先，通过解析图像的语言描述，将图像转化为语义图，从而获得廉价的场景图监督数据。然后，直接通过预训练的VSS中的区域-词对齐，将这些语义图中的名词短语与图像区域进行关联。最后，基于视觉上关联的对象，自然地构建关系表示，以实现开放词汇的场景图生成。<br>
                    效果：通过在Visual Genome数据集上的大量实验，证明了该方法在各种场景图生成任务（如监督/语言监督、封闭/开放词汇）上均取得了优于现有方法的性能，展示了预训练的VSS在更实际的场景中进行场景图生成的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scene graph generation (SGG) aims to abstract an image into a graph structure, by representing objects as graph nodes and their relations as labeled edges. However, two knotty obstacles limit the practicability of current SGG methods in real-world scenarios: 1) training SGG models requires time-consuming ground-truth annotations, and 2) the closed-set object categories make the SGG models limited in their ability to recognize novel objects outside of training corpora. To address these issues, we novelly exploit a powerful pre-trained visual-semantic space (VSS) to trigger language-supervised and open-vocabulary SGG in a simple yet effective manner. Specifically, cheap scene graph supervision data can be easily obtained by parsing image language descriptions into semantic graphs. Next, the noun phrases on such semantic graphs are directly grounded over image regions through region-word alignment in the pre-trained VSS. In this way, we enable open-vocabulary object detection by performing object category name grounding with a text prompt in this VSS. On the basis of visually-grounded objects, the relation representations are naturally built for relation recognition, pursuing open-vocabulary SGG. We validate our proposed approach with extensive experiments on the Visual Genome benchmark across various SGG scenarios (i.e., supervised / language-supervised, closed-set / open-vocabulary). Consistent superior performances are achieved compared with existing methods, demonstrating the potential of exploiting pre-trained VSS for SGG in more practical scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1183.DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking</span><br>
                <span class="as">Yang, LijinandKong, QuanandYang, Hsuan-KungandKehl, WadimandSato, YoichiandKobori, Norimasa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23130-23140.png><br>
            
            <span class="tt"><span class="t0">研究问题：理解视频中的密集动作是视觉模型泛化的基本挑战。<br>
                    动机：通过组合已知的原始元素，特别是处理新的复合结构，可以实现泛化。<br>
                    方法：提出一种从粗到细的组合性表示学习方法，将原始查询句子分解为不同的粒度级别，然后通过对比排名约束学习视频和重新组合的查询之间的正确对应关系。<br>
                    效果：在Charades-CG和ActivityNet-CG两个数据集上进行的实验表明，该方法具有优越的组合泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding dense action in videos is a fundamental challenge towards the generalization of vision models. Several works show that compositionality is key to achieving generalization by combining known primitive elements, especially for handling novel composited structures. Compositional temporal grounding is the task of localizing dense action by using known words combined in novel ways in the form of novel query sentences for the actual grounding. In recent works, composition is assumed to be learned from pairs of whole videos and language embeddings through large scale self-supervised pre-training. Alternatively, one can process the video and language into word-level primitive elements, and then only learn fine-grained semantic correspondences. Both approaches do not consider the granularity of the compositions, where different query granularity corresponds to different video segments. Therefore, a good compositional representation should be sensitive to different video and query granularity. We propose a method to learn a coarse-to-fine compositional representation by decomposing the original query sentence into different granular levels, and then learning the correct correspondences between the video and recombined queries through a contrastive ranking constraint. Additionally, we run temporal boundary prediction in a coarse-to-fine manner for precise grounding boundary detection. Experiments are performed on two datasets Charades-CG and ActivityNet-CG showing the superior compositional generalizability of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1184.Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners</span><br>
                <span class="as">Zhang, RenruiandHu, XiangfeiandLi, BohaoandHuang, SiyuanandDeng, HanqiuandQiao, YuandGao, PengandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15211-15222.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探讨如何利用各种预训练模式的多样化先验知识，来更好地进行少次学习。<br>
                    动机：在低数据量的情况下，视觉识别需要深度神经网络从有限的训练样本中学习泛化表示。最近的CLIP基方法通过对比语言-图像预训练显示出了有前景的少次性能提升。<br>
                    方法：我们提出了CaFo，一种级联基础模型，它结合了各种预训练范式的多样化先验知识，以更好地进行少次学习。CaFo整合了CLIP的语言对比知识、DINO的视觉对比知识、DALL-E的视觉生成知识和GPT-3的语言生成知识。具体来说，CaFo的工作方式是“提示、生成、然后缓存”。<br>
                    效果：实验结果表明，通过这种协作，CaFo可以充分发挥不同预训练方法的潜力，并将它们统一起来，实现少次分类的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual recognition in low-data regimes requires deep neural networks to learn generalized representations from limited training samples. Recently, CLIP-based methods have shown promising few-shot performance benefited from the contrastive language-image pre-training. We then question, if the more diverse pre-training knowledge can be cascaded to further assist few-shot representation learning. In this paper, we propose CaFo, a Cascade of Foundation models that incorporates diverse prior knowledge of various pre training paradigms for better few-shot learning. Our CaFo incorporates CLIP's language-contrastive knowledge, DINO's vision-contrastive knowledge, DALL-E's vision generative knowledge, and GPT-3's language-generative knowledge. Specifically, CaFo works by 'Prompt, Generate, then Cache'. Firstly, we leverage GPT-3 to produce textual inputs for prompting CLIP with rich downstream linguistic semantics. Then, we generate synthetic images via DALL-E to expand the few-shot training data without any manpower. At last, we introduce a learnable cache model to adaptively blend the predictions from CLIP and DINO. By such col laboration, CaFo can fully unleash the potential of different pre-training methods and unify them to perform state-of the-art for few-shot classification. Code is available at https://github.com/ZrrSkywalker/CaFo.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1185.NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations</span><br>
                <span class="as">Hsu, JoyandMao, JiayuanandWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_NS3D_Neuro-Symbolic_Grounding_of_3D_Objects_and_Relations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2614-2623.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将3D场景中的对象属性和关系与语言联系起来，以支持各种人工智能任务。<br>
                    动机：3D领域的可变性带来了标签成本高和3D语言复杂性两大挑战。因此，模型需要满足数据高效、能适应不同数据分布和未见过的任务语义形式，并能处理复杂的语言语义（如视角锚定和多对象引用）。<br>
                    方法：提出NS3D，一个用于3D场景的神经符号框架。NS3D通过利用大型语言到代码模型将语言转化为具有层次结构的程序，程序中的不同功能模块由神经网络实现。特别是，NS3D通过引入能有效解决高阶关系（即涉及两个以上对象的关系）的功能模块，扩展了先前的神经符号视觉推理方法。模块化和组合架构使NS3D在ReferIt3D视依赖任务上取得了最先进的结果。<br>
                    效果：NS3D在数据效率和泛化设置上表现出显著改善的性能，并在一项未见过的3D问答任务上实现了零样本转移。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Grounding object properties and relations in 3D scenes is a prerequisite for a wide range of artificial intelligence tasks, such as visually grounded dialogues and embodied manipulation. However, the variability of the 3D domain induces two fundamental challenges: 1) the expense of labeling and 2) the complexity of 3D grounded language. Hence, essential desiderata for models are to be data-efficient, generalize to different data distributions and tasks with unseen semantic forms, as well as ground complex language semantics (e.g., view-point anchoring and multi-object reference). To address these challenges, we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates language into programs with hierarchical structures by leveraging large language-to-code models. Different functional modules in the programs are implemented as neural networks. Notably, NS3D extends prior neuro-symbolic visual reasoning methods by introducing functional modules that effectively reason about high-arity relations (i.e., relations among more than two objects), key in disambiguating objects in complex 3D scenes. Modular and compositional architecture enables NS3D to achieve state-of-the-art results on the ReferIt3D view-dependence task, a 3D referring expression comprehension benchmark. Importantly, NS3D shows significantly improved performance on settings of data-efficiency and generalization, and demonstrate zero-shot transfer to an unseen 3D question-answering task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1186.VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking</span><br>
                <span class="as">Wang, LiminandHuang, BingkunandZhao, ZhiyuandTong, ZhanandHe, YinanandWang, YiandWang, YaliandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14549-14560.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个具有强大基础模型，能够泛化到各种下游任务。<br>
                    动机：虽然规模是建立强大基础模型的主要因素，但训练数十亿参数的视频基础模型仍然具有挑战性。<br>
                    方法：本文提出了视频掩码自动编码器（VideoMAE），这是一种可扩展的通用自我监督预训练器，用于构建视频基础模型。通过核心设计对模型和数据进行缩放，并提出了双掩蔽策略进行有效预训练。<br>
                    效果：实验结果表明，VideoMAE可以有效地预训练十亿级别的模型，并在Kinetics和Something-Something等数据集上取得了新的最先进的性能。此外，预训练的视频ViT模型在各种下游任务上也表现出了有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also introduce a progressive training paradigm that involves initial pre-training on the diverse multi-sourced unlabeled dataset, followed by fine-tuning on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1187.RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training</span><br>
                <span class="as">Xie, Chen-WeiandSun, SiyangandXiong, XiongandZheng, YunandZhao, DeliandZhou, Jingren</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19265-19274.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决训练CLIP模型需要大量图像-文本对，数据需求大的问题。<br>
                    动机：现有的CLIP模型在各种下游任务上表现出色，但训练过程中需要大量的图像-文本对来记忆各种语义概念，数据需求大。<br>
                    方法：提出一种新颖且高效的框架RA-CLIP，通过在线检索来增强嵌入。具体来说，我们将部分图像-文本数据作为参考集进行抽样。对于输入的图像，从参考集中检索相关的图像-文本对以丰富输入图像的表示。这个过程可以看作是开卷考试：有了参考集作为小抄，该方法不需要记住训练数据中的所有视觉概念，而是通过利用参考集中图像和文本之间的对应关系来探索如何识别视觉概念。<br>
                    效果：在10个图像分类数据集和2个目标检测数据集上进行的全面实验表明，RA-CLIP在零样本图像分类任务上比基础的CLIP模型有大幅度的提升（+12.7%），线性探测图像分类任务（+6.9%）和零样本ROI分类任务（+2.8%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive Language-Image Pre-training (CLIP) is attracting increasing attention for its impressive zero-shot recognition performance on different down-stream tasks. However, training CLIP is data-hungry and requires lots of image-text pairs to memorize various semantic concepts. In this paper, we propose a novel and efficient framework: Retrieval Augmented Contrastive Language-Image Pre-training (RA-CLIP) to augment embeddings by online retrieval. Specifically, we sample part of image-text data as a hold-out reference set. Given an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of input image. This process can be considered as an open-book exam: with the reference set as a cheat sheet, the proposed method doesn't need to memorize all visual concepts in the training data. It explores how to recognize visual concepts by exploiting correspondence between images and texts in the cheat sheet. The proposed RA-CLIP implements this idea and comprehensive experiments are conducted to show how RA-CLIP works. Performances on 10 image classification datasets and 2 object detection datasets show that RA-CLIP outperforms vanilla CLIP baseline by a large margin on zero-shot image classification task (+12.7%), linear probe image classification task (+6.9%) and zero-shot ROI classification task (+2.8%).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1188.Teacher-Generated Spatial-Attention Labels Boost Robustness and Accuracy of Contrastive Models</span><br>
                <span class="as">Yao, YushiandYe, ChangandHe, JunfengandElsayed, GamaleldinF.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Teacher-Generated_Spatial-Attention_Labels_Boost_Robustness_and_Accuracy_of_Contrastive_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23282-23291.png><br>
            
            <span class="tt"><span class="t0">研究问题：人类空间注意力可以提供关于视觉场景中重要区域的信息，这是否可以用于自监督表示学习？<br>
                    动机：收集大规模的人类注意力标签数据集非常昂贵，因此需要构建一个辅助的教师模型来预测人类的注意力。<br>
                    方法：通过在相对较小的标记数据集上训练教师模型，生成ImageNet的图像（伪）注意力标签。然后，在标准的对比模型配置中添加一个简单的输出头，该头被训练以根据教师模型的伪标签预测每个图像的注意力图。<br>
                    效果：我们发现，与普通的对比模型相比，使用教师指导训练的对比模型预测的空间注意力图更符合人类的关注点。此外，我们的方法提高了对比模型在ImageNet和ImageNet-C上的分类准确性和鲁棒性。最后，我们发现，通过在ImageNet、ImageNet-C、CIFAR10和CIFAR10-C数据集上测量精度-召回性能，模型表示对图像检索任务变得更有用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human spatial attention conveys information about theregions of visual scenes that are important for perform-ing visual tasks. Prior work has shown that the informa-tion about human attention can be leveraged to benefit var-ious supervised vision tasks. Might providing this weakform of supervision be useful for self-supervised represen-tation learning? Addressing this question requires collect-ing large datasets with human attention labels. Yet, col-lecting such large scale data is very expensive. To addressthis challenge, we construct an auxiliary teacher model topredict human attention, trained on a relatively small la-beled dataset. This teacher model allows us to generate im-age (pseudo) attention labels for ImageNet. We then traina model with a primary contrastive objective; to this stan-dard configuration, we add a simple output head trained topredict the attentional map for each image, guided by thepseudo labels from teacher model. We measure the qual-ity of learned representations by evaluating classificationperformance from the frozen learned embeddings as wellas performance on image retrieval tasks. We find that thespatial-attention maps predicted from the contrastive modeltrained with teacher guidance aligns better with human at-tention compared to vanilla contrastive models. Moreover,we find that our approach improves classification accuracyand robustness of the contrastive models on ImageNet andImageNet-C. Further, we find that model representationsbecome more useful for image retrieval task as measuredby precision-recall performance on ImageNet, ImageNet-C,CIFAR10, and CIFAR10-C datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1189.Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning</span><br>
                <span class="as">Yang, AntoineandNagrani, ArshaandSeo, PaulHongsuckandMiech, AntoineandPont-Tuset, JordiandLaptev, IvanandSivic, JosefandSchmid, Cordelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10714-10726.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模可获取的叙述视频进行预训练，以实现多模态单阶段密集事件描述模型。<br>
                    动机：现有的标注数据集无法满足这种统一模型的训练需求，而未标注的视频可以作为有效的训练数据。<br>
                    方法：通过将转录语音的句子边界重新定义为伪事件边界，并将转录语音的句子作为伪事件描述，来利用未标注的叙述视频进行密集视频描述。<br>
                    效果：在YT-Temporal-1B数据集上预训练的Vid2Seq模型在各种密集视频描述基准测试中（包括YouCook2、ViTT和ActivityNet Captions）都取得了显著改进，并在视频段落描述、视频片段描述以及少样本设置等任务中表现出良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captioning, and to few-shot settings. Our code is publicly available at https://antoyang.github.io/vid2seq.html.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1190.Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering</span><br>
                <span class="as">Zang, ChuanqiandWang, HanqingandPei, MingtaoandLiang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19027-19036.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频问答（VideoQA）由于需要从冗余信息中捕捉准确的模态间关联，因此具有挑战性。<br>
                    动机：现有方法主要关注任务的显性挑战，如多模态特征提取、视频-文本对齐和融合等，但在推理答案时依赖于统计证据，忽视了多模态数据中的潜在偏见。<br>
                    方法：我们从因果关系表示的角度研究多模态数据的关系结构，并提出一种新的推理框架。对于视觉数据，与答案无关的问题对象可能建立简单的匹配关联；对于文本数据，模型倾向于局部短语语义，这可能偏离长句子中的全局语义。因此，为了提高模型的泛化能力，我们通过显式捕获与问题语义有因果关系的视觉特征，并减弱局部语言语义对问题回答的影响，来发现真实的关联。<br>
                    效果：在两个大型因果视频问答数据集上的实验结果表明，我们提出的框架1)提高了现有视频问答主干的准确性，2)在复杂场景和问题上表现出鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video Question Answering (VideoQA) is challenging as it requires capturing accurate correlations between modalities from redundant information. Recent methods focus on the explicit challenges of the task, e.g. multimodal feature extraction, video-text alignment and fusion. Their frameworks reason the answer relying on statistical evidence causes, which ignores potential bias in the multimodal data. In our work, we investigate relational structure from a causal representation perspective on multimodal data and propose a novel inference framework. For visual data, question-irrelevant objects may establish simple matching associations with the answer. For textual data, the model prefers the local phrase semantics which may deviate from the global semantics in long sentences. Therefore, to enhance the generalization of the model, we discover the real association by explicitly capturing visual features that are causally related to the question semantics and weakening the impact of local language semantics on question answering. The experimental results on two large causal VideoQA datasets verify that our proposed framework 1) improves the accuracy of the existing VideoQA backbone, 2) demonstrates robustness on complex scenes and questions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1191.A Simple Framework for Text-Supervised Semantic Segmentation</span><br>
                <span class="as">Yi, MuyangandCui, QuanandWu, HaoandYang, ChengandYoshie, OsamuandLu, Hongtao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_A_Simple_Framework_for_Text-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7071-7080.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像-文本对比的语义分割问题，并探索无特殊网络架构的预训练模型在语义分割任务上的效果。<br>
                    动机：虽然图像-文本对比的语义分割是一个新颖的研究主题，但早期的开创性方法受限于特定设计的网络架构。本文发现，一个普通的对比语言-图像预训练（CLIP）模型本身就是一个很好的文本监督语义分割器。<br>
                    方法：首先，我们揭示了由于其优化由密集对齐视觉和语言表示驱动，普通CLIP在本地化和分割方面表现不佳。然后，我们提出了局部驱动对齐（LoDA）来解决这个问题，其中CLIP的优化是由稀疏地对齐局部表示驱动的。最后，我们提出了一个简单的分割（SimSeg）框架。LoDA和SimSeg共同改善了普通的CLIP，产生了令人印象深刻的语义分割结果。<br>
                    效果：我们的模型在PASCAL VOC 2012、PASCAL Context和COCO数据集上的表现优于先前最先进的方法，且差距较大。代码和模型可以在github.com/muyangyi/SimSeg获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-supervised semantic segmentation is a novel research topic that allows semantic segments to emerge with image-text contrasting. However, pioneering methods could be subject to specifically designed network architectures. This paper shows that a vanilla contrastive language-image pre-training (CLIP) model is an effective text-supervised semantic segmentor by itself. First, we reveal that a vanilla CLIP is inferior to localization and segmentation due to its optimization being driven by densely aligning visual and language representations. Second, we propose the locality-driven alignment (LoDA) to address the problem, where CLIP optimization is driven by sparsely aligning local representations. Third, we propose a simple segmentation (SimSeg) framework. LoDA and SimSeg jointly ameliorate a vanilla CLIP to produce impressive semantic segmentation results. Our method outperforms previous state-of-the-art methods on PASCAL VOC 2012, PASCAL Context and COCO datasets by large margins. Code and models are available at github.com/muyangyi/SimSeg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1192.Dynamic Inference With Grounding Based Vision and Language Models</span><br>
                <span class="as">Uzkent, BurakandGarg, AmanmeetandZhu, WentaoandDoshi, KevalandYi, JingruandWang, XiaolongandOmar, Mohamed</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2624-2633.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高大规模预训练语言模型的效率？<br>
                    动机：现有的大型预训练语言模型存在大量的计算冗余，影响了运行效率。<br>
                    方法：提出动态推理机制，通过动态跳过多头自注意力和前馈网络层，以及动态令牌剪枝和融合，来优化模型的运行效率。<br>
                    效果：实验结果显示，该方法可以显著提高最先进模型MDETR和GLIP在指代表达式理解、分割和VQA任务上的运行效率，同时仅使准确率下降0.3%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformers have been recently utilized for vision and language tasks successfully. For example, recent image and language models with more than 200M parameters have been proposed to learn visual grounding in the pre-training step and show impressive results on downstream vision and language tasks. On the other hand, there exists a large amount of computational redundancy in these large models which skips their run-time efficiency. To address this problem, we propose dynamic inference for grounding based vision and language models conditioned on the input image-text pair. We first design an approach to dynamically skip multihead self-attention and feed forward network layers across two backbones and multimodal network. Additionally, we propose dynamic token pruning and fusion for two backbones. In particular, we remove redundant tokens at different levels of the backbones and fuse the image tokens with the language tokens in an adaptive manner. To learn policies for dynamic inference, we train agents using reinforcement learning. In this direction, we replace the CNN backbone in a recent grounding-based vision and language model, MDETR, with a vision transformer and call it ViTMDETR. Then, we apply our dynamic inference method to ViTMDETR, called D-ViTDMETR, and perform experiments on image-language tasks. Our results show that we can improve the run-time efficiency of the state-of-the-art models MDETR and GLIP by up to  50% on Referring Expression Comprehension and Segmentation, and VQA with only maximum  0.3% accuracy drop.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1193.Visual-Language Prompt Tuning With Knowledge-Guided Context Optimization</span><br>
                <span class="as">Yao, HantaoandZhang, RuiandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6757-6767.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高预训练视觉-语言模型（VLM）在未见过类别任务上的泛化能力？<br>
                    动机：现有的代表性CoOp方法通过结合可学习的文本标记和类别标记来获取特定的文本知识，但这种方法对于未见过类别的任务的泛化能力较差。<br>
                    方法：提出了一种新的知识引导上下文优化（KgCoOp）方法，通过构建一个正则化项来保证基本通用文本知识可以被嵌入到由可学习提示生成的特殊文本知识中，以提高其对未见过类别任务的泛化能力。<br>
                    效果：广泛的实验表明，提出的知识引导上下文优化方法在提示调优任务上表现优秀，即在较少的训练时间内取得了更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prompt tuning is an effective way to adapt the pretrained visual-language model (VLM) to the downstream task using task-related textual tokens. Representative CoOp-based works combine the learnable textual tokens with the class tokens to obtain specific textual knowledge. However, the specific textual knowledge has worse generalizable to the unseen classes because it forgets the essential general textual knowledge having a strong generalization ability. To tackle this issue, we introduce a novel Knowledge-guided Context Optimization (KgCoOp) to enhance the generalization ability of the learnable prompt for unseen classes. To remember the essential general knowledge, KgCoOp constructs a regularization term to ensure that the essential general textual knowledge can be embedded into the special textual knowledge generated by the learnable prompt. Especially, KgCoOp minimizes the discrepancy between the textual embeddings generated by learned prompts and the hand-crafted prompts. Finally, adding the KgCoOp upon the contrastive loss can make a discriminative prompt for both seen and unseen tasks. Extensive evaluation of several benchmarks demonstrates that the proposed Knowledge-guided Context Optimization is an efficient method for prompt tuning, i.e., achieves better performance with less training time.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1194.Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models</span><br>
                <span class="as">Xu, JiaruiandLiu, SifeiandVahdat, ArashandByeon, WonminandWang, XiaolongandDeMello, Shalini</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2955-2966.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种名为ODISE的开放词汇扩散式全景分割模型，将预训练的文本-图像扩散模型和判别模型统一起来进行开放词汇全景分割。<br>
                    动机：文本-图像扩散模型具有以多样化的开放词汇语言描述生成高质量图像的强大能力，而文本-图像判别模型如CLIP则擅长将图像分类为开放词汇标签。作者希望利用这两种模型的冻结内部表示来进行野外任意类别的全景分割。<br>
                    方法：通过联合训练文本-图像扩散模型和判别模型，利用两者的冻结内部表示进行全景分割。<br>
                    效果：在开放词汇全景分割和语义分割任务上，该方法都大幅超越了先前的技术。特别是在只用COCO数据集训练的情况下，该方法在ADE20K数据集上实现了23.4 PQ和30.0 mIoU，比先前的技术分别提高了8.3 PQ和7.9 mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffusion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP, on the other hand, are good at classifying images into open-vocabulary labels. We leverage the frozen internal representations of both these models to perform panoptic segmentation of any category in the wild. Our approach outperforms the previous state of the art by significant margins on both open-vocabulary panoptic and semantic segmentation tasks. In particular, with COCO training only, our method achieves 23.4 PQ and 30.0 mIoU on the ADE20K dataset, with 8.3 PQ and 7.9 mIoU absolute improvement over the previous state of the art. We open-source our code and models at https://github.com/NVlabs/ODISE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1195.Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision</span><br>
                <span class="as">Xu, JilanandHou, JunlinandZhang, YuejieandFeng, RuiandWang, YiandQiao, YuandXie, Weidi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2935-2944.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放词汇语义分割（OVS）问题，即对任意类别的对象进行分割，而不是预定义的封闭集类别。<br>
                    动机：目前的模型需要使用掩码注释进行预训练，而本文提出了一种无需使用掩码注释的基于变压器的OVS模型。<br>
                    方法：本文提出的OVSegmentor模型通过基于插槽注意力的绑定模块将图像像素组装成一组可学习的组令牌，并将组令牌与相应的标题嵌入对齐。同时，提出了两种代理任务进行训练，即掩码实体完成和跨图像掩码一致性。<br>
                    效果：在PASCAL VOC 2012、PASCAL Context和COCO Object三个基准数据集上进行零样本转移，仅使用3%的数据（4M vs 134M）进行预训练，就取得了优于最先进方法的分割结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we consider the problem of open-vocabulary semantic segmentation (OVS), which aims to segment objects of arbitrary classes instead of pre-defined, closed-set categories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annotations. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention based binding module, and aligns the group tokens to the corresponding caption embedding. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given the group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consistent mask predictions between images that contain shared entities, which encourages the model to learn visual invariance. Third, we construct CC4M dataset for pre-training by filtering CC12M with frequently appeared entities, which significantly improves training efficiency. Fourth, we perform zero-shot transfer on three benchmark datasets, PASCAL VOC 2012, PASCAL Context, and COCO Object. Our model achieves superior segmentation results over the state-of-the-art method by using only 3% data (4M vs 134M) for pre-training. Code and pre-trained models will be released for future research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1196.Learning Conditional Attributes for Compositional Zero-Shot Learning</span><br>
                <span class="as">Wang, QingshengandLiu, LingqiaoandJing, ChenchenandChen, HaoandLiang, GuoqiangandWang, PengandShen, Chunhua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_Conditional_Attributes_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11197-11206.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决组合零样本学习（CZSL）中属性与不同对象交互的挑战，即如何让模型识别基于已学概念（如属性-对象组合）的新组合概念。<br>
                    动机：在CZSL中，一个挑战是如何对与不同对象交互的属性进行建模，例如"wet apple"和"wet cat"中的"wet"属性是不同的。<br>
                    方法：我们提出一个属性学习框架，包含一个属性超学习器和一个属性基学习器，通过学习条件属性嵌入来解决这一问题。<br>
                    效果：实验结果表明，我们的模型在CZSL基准测试上表现优于其他最先进的方法，验证了学习条件属性的重要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compositional Zero-Shot Learning (CZSL) aims to train models to recognize novel compositional concepts based on learned concepts such as attribute-object combinations. One of the challenges is to model attributes interacted with different objects, e.g., the attribute "wet" in "wet apple" and "wet cat" is different. As a solution, we provide analysis and argue that attributes are conditioned on the recognized object and input image and explore learning conditional attribute embeddings by a proposed attribute learning framework containing an attribute hyper learner and an attribute base learner. By encoding conditional attributes, our model enables to generate flexible attribute embeddings for generalization from seen to unseen compositions. Experiments on CZSL benchmarks, including the more challenging C-GQA dataset, demonstrate better performances compared with other state-of-the-art approaches and validate the importance of learning conditional attributes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1197.Prompting Large Language Models With Answer Heuristics for Knowledge-Based Visual Question Answering</span><br>
                <span class="as">Shao, ZhenweiandYu, ZhouandWang, MengandYu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14974-14983.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模语言模型获取知识驱动的视觉问答所需的外部知识。<br>
                    动机：现有的方法从显式的知识库中检索所需知识，这往往会引入与问题无关的信息，限制了模型的性能。<br>
                    方法：提出一种名为Prophet的概念简单框架，通过向GPT-3提供答案启发式来获取知识驱动的视觉问答所需的外部知识。<br>
                    效果：Prophet在两个具有挑战性的知识驱动视觉问答数据集上显著优于所有现有最先进的方法，在OK-VQA和A-OKVQA的测试集上分别达到了61.1%和55.7%的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet---a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are encoded into the prompts to enable GPT-3 to better comprehend the task thus enhancing its capacity. Prophet significantly outperforms all existing state-of-the-art methods on two challenging knowledge-based VQA datasets, OK-VQA and A-OKVQA, delivering 61.1% and 55.7% accuracies on their testing sets, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1198.IFSeg: Image-Free Semantic Segmentation via Vision-Language Model</span><br>
                <span class="as">Yun, SukminandPark, SeongHyeonandSeo, PaulHongsuckandShin, Jinwoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yun_IFSeg_Image-Free_Semantic_Segmentation_via_Vision-Language_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2967-2977.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉语言预训练在无特定图像和注释的语义分割任务中的应用问题。<br>
                    动机：现有的视觉语言预训练模型需要额外的图像或分割标注来适应下游分割任务，但这种方法需要大量的额外数据。<br>
                    方法：本文提出了一种名为IFSeg的新方法，通过生成视觉语言驱动的人工图像分割对，将预训练的视觉语言模型更新为分割任务。<br>
                    效果：实验结果表明，该方法不仅为这个新任务建立了有效的基线，而且与依赖更强监督（如特定图像和分割掩码）的现有方法相比，表现出强大的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-language (VL) pre-training has recently gained much attention for its transferability and flexibility in novel concepts (e.g., cross-modality transfer) across various visual tasks. However, VL-driven segmentation has been under-explored, and the existing approaches still have the burden of acquiring additional training images or even segmentation annotations to adapt a VL model to downstream segmentation tasks. In this paper, we introduce a novel image-free segmentation task where the goal is to perform semantic segmentation given only a set of the target semantic categories, but without any task-specific images and annotations. To tackle this challenging task, our proposed method, coined IFSeg, generates VL-driven artificial image-segmentation pairs and updates a pre-trained VL model to a segmentation task. We construct this artificial training data by creating a 2D map of random semantic categories and another map of their corresponding word tokens. Given that a pre-trained VL model projects visual and text tokens into a common space where tokens that share the semantics are located closely, this artificially generated word map can replace the real image inputs for such a VL model. Through an extensive set of experiments, our model not only establishes an effective baseline for this novel task but also demonstrates strong performances compared to existing methods that rely on stronger supervision, such as task-specific images and segmentation masks. Code is available at https://github.com/alinlab/ifseg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1199.Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding</span><br>
                <span class="as">Alper, MorrisandFiman, MichaelandAverbuch-Elor, Hadar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Alper_Is_BERT_Blind_Exploring_the_Effect_of_Vision-and-Language_Pretraining_on_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6778-6788.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探讨视觉-语言预训练是否能提高涉及隐性视觉推理的纯文本任务的性能，主要关注零样本探测方法。<br>
                    动机：大多数人类使用视觉想象力来理解和推理语言，但像BERT这样的模型通过在仅基于文本的预训练期间获得的知识来推理语言。本研究调查视觉-语言预训练是否可以改善涉及隐性视觉推理的纯文本任务的性能。<br>
                    方法：提出了一套视觉语言理解（VLU）任务用于探测文本编码器模型的视觉推理能力，以及各种非视觉自然语言理解（NLU）任务进行比较。还提出了一种新的零样本知识探测方法，Stroop探测，用于将诸如CLIP的模型应用于无需预测头（如BERT的掩码语言建模头）的纯文本任务。<br>
                    效果：结果显示，最先进的多模态训练的文本编码器在VLU任务上优于单模态训练的文本编码器，但在NLU任务上表现不佳，为之前关于多模态模型NLU能力的混合结果提供了新的视角。结论是，预训练期间接触图像提供了固有的视觉推理知识，这反映在需要隐性视觉推理的语言任务中。这些发现对于更广泛的多模态学习背景具有重要意义，为在这种背景下选择文本编码器提供了原则性指导。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lending new context to previously mixed results regarding the NLU capabilities of multimodal models. We conclude that exposure to images during pretraining affords inherent visual reasoning knowledge that is reflected in language-only tasks that require implicit visual reasoning. Our findings bear importance in the broader context of multimodal learning, providing principled guidelines for the choice of text encoders used in such contexts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1200.Generative Bias for Robust Visual Question Answering</span><br>
                <span class="as">Cho, JaeWonandKim, Dong-JinandRyu, HyeonggonandKweon, InSo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_Generative_Bias_for_Robust_Visual_Question_Answering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11681-11690.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉问答（VQA）任务中，模型往往会利用数据集中的偏见进行预测。<br>
                    动机：为了解决这一问题，本文提出了一种新的生成方法，通过直接从目标模型中训练一个偏置模型来消除这种偏见。<br>
                    方法：该方法使用生成网络，通过结合对抗性目标和知识蒸馏，学习目标模型的偏见。然后，我们使用这个偏置模型来消除目标模型的偏见。<br>
                    效果：在多个VQA偏见数据集上进行了广泛的实验，包括VQA-CP2、VQA-CP1、GQA-OOD和VQA-CE，结果显示，该方法在使用LXMERT架构时在VQA-CP2上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of Visual Question Answering (VQA) is known to be plagued by the issue of VQA models exploiting biases within the dataset to make its final prediction. Various previous ensemble based debiasing methods have been proposed where an additional model is purposefully trained to be biased in order to train a robust target model. However, these methods compute the bias for a model simply from the label statistics of the training data or from single modal branches. In this work, in order to better learn the bias a target VQA model suffers from, we propose a generative method to train the bias model directly from the target model, called GenB. In particular, GenB employs a generative network to learn the bias in the target model through a combination of the adversarial objective and knowledge distillation. We then debias our target model with GenB as a bias model, and show through extensive experiments the effects of our method on various VQA bias datasets including VQA-CP2, VQA-CP1, GQA-OOD, and VQA-CE, and show state-of-the-art results with the LXMERT architecture on VQA-CP2.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1201.Data-Free Sketch-Based Image Retrieval</span><br>
                <span class="as">Chaudhuri, AbhraandBhunia, AyanKumarandSong, Yi-ZheandDutta, Anjan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chaudhuri_Data-Free_Sketch-Based_Image_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12084-12093.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在没有训练数据的情况下，利用预训练的分类模型进行跨模态检索。<br>
                    动机：深度学习模型的隐私和匿名性问题促使了无数据学习的研究，同时获取成对的照片-草图数据集困难，使得该设置具有实用性。<br>
                    方法：提出一种名为Data-Free Sketch-Based Image Retrieval (DF-SBIR)的跨模态无数据学习设置，其中在单一模态中进行分类训练的教师模型必须被学生用来学习用于检索的跨模态度量空间。<br>
                    效果：该方法在Sketchy, TU-Berlin, QuickDraw基准测试上进行了评估，设计了基于现有无数据学习文献的各种基线，并观察到该方法以显著的优势超越了所有基线。该方法还实现了与依赖数据的方案相竞争的mAPs，而无需任何训练数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rising concerns about privacy and anonymity preservation of deep learning models have facilitated research in data-free learning. Primarily based on data-free knowledge distillation, models developed in this area so far have only been able to operate in a single modality, performing the same kind of task as that of the teacher. For the first time, we propose Data-Free Sketch-Based Image Retrieval (DF-SBIR), a cross-modal data-free learning setting, where teachers trained for classification in a single modality have to be leveraged by students to learn a cross-modal metric-space for retrieval. The widespread availability of pre-trained classification models, along with the difficulty in acquiring paired photo-sketch datasets for SBIR justify the practicality of this setting. We present a methodology for DF-SBIR, which can leverage knowledge from models independently trained to perform classification on photos and sketches. We evaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks, designing a variety of baselines based on existing data-free learning literature, and observe that our method surpasses all of them by significant margins. Our method also achieves mAPs competitive with data-dependent approaches, all the while requiring no training data. Implementation is available at https://github.com/abhrac/data-free-sbir.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1202.Intrinsic Physical Concepts Discovery With Object-Centric Predictive Models</span><br>
                <span class="as">Tang, QuandZhu, XiangyuandLei, ZhenandZhang, Zhaoxiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Intrinsic_Physical_Concepts_Discovery_With_Object-Centric_Predictive_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23252-23261.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过观察和理解世界，发现并理解物理概念。<br>
                    动机：人类智能的核心在于通过观察环境，以对象和关系的方式，无监督地感知环境，从而发现抽象的物理概念。<br>
                    方法：本文提出了一种名为PHYCINE的系统，该系统可以在不同抽象级别中推断物理概念，而无需监督。<br>
                    效果：实证评估表明，由我们的系统推断出的变量与相应的物理概念的属性相符。我们还发现，包含发现的物理概念变量的对象表示可以帮助在因果关系推理任务（即COMPHY）中实现更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability to discover abstract physical concepts and understand how they work in the world through observing lies at the core of human intelligence. The acquisition of this ability is based on compositionally perceiving the environment in terms of objects and relations in an unsupervised manner. Recent approaches learn object-centric representations and capture visually observable concepts of objects, e.g., shape, size, and location. In this paper, we take a step forward and try to discover and represent intrinsic physical concepts such as mass and charge. We introduce the PHYsical Concepts Inference NEtwork (PHYCINE), a system that infers physical concepts in different abstract levels without supervision. The key insights underlining PHYCINE are two-fold, commonsense knowledge emerges with prediction, and physical concepts of different abstract levels should be reasoned in a bottom-to-up fashion. Empirical evaluation demonstrates that variables inferred by our system work in accordance with the properties of the corresponding physical concepts. We also show that object representations containing the discovered physical concepts variables could help achieve better performance in causal reasoning tasks, i.e., COMPHY.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1203.Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training</span><br>
                <span class="as">Luo, DezhaoandHuang, JiaboandGong, ShaogangandJin, HailinandLiu, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Towards_Generalisable_Video_Moment_Retrieval_Visual-Dynamic_Injection_to_Image-Text_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23045-23055.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频时刻检索（VMR）中视觉和文本的关联性至关重要，但现有方法严重依赖于独立的预训练特征提取器进行视觉和文本理解。<br>
                    动机：现有的图像-文本预训练模型在捕捉视频变化方面存在限制，因此需要一种通用的方法来增强模型对视频时刻的理解。<br>
                    方法：我们提出了一种名为视觉动态注入（VDI）的通用方法，从大规模的图像-文本数据中探索多模态关联性以促进可泛化的VMR。我们通过从视频帧中提取视觉上下文和空间动态信息，并显式地强制它们与描述视频变化的短语（如动词）对齐，使模型能够更准确地进行视频-文本对齐。<br>
                    效果：我们在两个VMR基准数据集（Charades-STA和ActivityNet-Captions）上进行了广泛的实验，并在所有测试样本中实现了最先进的性能。特别是在涉及新场景和词汇的分布外分割上，VDI表现出显著的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The correlation between the vision and text is essential for video moment retrieval (VMR), however, existing methods heavily rely on separate pre-training feature extractors for visual and textual understanding. Without sufficient temporal boundary annotations, it is non-trivial to learn universal video-text alignments. In this work, we explore multi-modal correlations derived from large-scale image-text data to facilitate generalisable VMR. To address the limitations of image-text pre-training models on capturing the video changes, we propose a generic method, referred to as Visual-Dynamic Injection (VDI), to empower the model's understanding of video moments. Whilst existing VMR methods are focusing on building temporal-aware video features, being aware of the text descriptions about the temporal changes is also critical but originally overlooked in pre-training by matching static images with sentences. Therefore, we extract visual context and spatial dynamic information from video frames and explicitly enforce their alignments with the phrases describing video changes (e.g. verb). By doing so, the potentially relevant visual and motion patterns in videos are encoded in the corresponding text embeddings (injected) so to enable more accurate video-text alignments. We conduct extensive experiments on two VMR benchmark datasets (Charades-STA and ActivityNet-Captions) and achieve state-of-the-art performances. Especially, VDI yields notable advantages when being tested on the out-of-distribution splits where the testing samples involve novel scenes and vocabulary.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1204.Reproducible Scaling Laws for Contrastive Language-Image Learning</span><br>
                <span class="as">Cherti, MehdiandBeaumont, RomainandWightman, RossandWortsman, MitchellandIlharco, GabrielandGordon, CadeandSchuhmann, ChristophandSchmidt, LudwigandJitsev, Jenia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2818-2829.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究大规模对比语言-图像预训练（CLIP）的缩放定律，并使用公共LAION数据集和开源OpenCLIP仓库进行实验。<br>
                    动机：尽管神经网络的扩大规模已经带来了显著的性能提升，但之前的研究主要依赖于私有数据和模型，或者只关注单一的语言或视觉学习。因此，本研究试图填补这一空白。<br>
                    方法：通过在LAION数据集上训练多达20亿的图像-文本对，我们探索了多种下游任务的缩放定律，包括零样本分类、检索、线性探测和端到端微调。<br>
                    效果：我们发现训练分布对缩放定律起着关键作用。尽管OpenAI和OpenCLIP模型具有相同的模型架构和类似的训练方法，但其缩放行为却有所不同。我们将所有的评估工作流程和模型开源，以确保研究的可重复性，并使更多的人能够接触到缩放定律的研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data & models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the OpenAI and OpenCLIP models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public CLIP models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study is available at https://github.com/LAION-AI/scaling-laws-openclip.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1205.Learning Customized Visual Models With Retrieval-Augmented Knowledge</span><br>
                <span class="as">Liu, HaotianandSon, KilhoandYang, JianweiandLiu, CeandGao, JianfengandLee, YongJaeandLi, Chunyuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Customized_Visual_Models_With_Retrieval-Augmented_Knowledge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15148-15158.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用网络规模的数据收集和昂贵的预训练，以及从网络上检索到的相关图像-文本对来构建目标领域的定制视觉模型。<br>
                    动机：现有的图像-文本对比学习模型如CLIP，虽然具有强大的任务转移能力，但其高通用性和可用性是通过大规模的数据收集和昂贵的预训练实现的。<br>
                    方法：提出REACT框架，通过从网络规模的数据库中检索最相关的图像-文本对（占CLIP预训练数据的3%）作为外部知识，然后仅训练新的模块化块，同时冻结所有原始权重，以定制模型。<br>
                    效果：实验结果表明，REACT在分类、检索、检测和分割任务上的效果显著，特别是在零样本分类任务上，与CLIP相比，ImageNet上的性能提高了5.4%，ELEVATER基准测试（20个数据集）上的性能提高了3.7%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-text contrastive learning models such as CLIP have demonstrated strong task transfer ability. The high generality and usability of these visual models is achieved via a web-scale data collection process to ensure broad concept coverage, followed by expensive pre-training to feed all the knowledge into model weights. Alternatively, we propose REACT, REtrieval-Augmented CusTomization, a framework to acquire the relevant web knowledge to build customized visual models for target domains. We retrieve the most relevant image-text pairs ( 3% of CLIP pre-training data) from the web-scale database as external knowledge and propose to customize the model by only training new modularized blocks while freezing all the original weights. The effectiveness of REACT is demonstrated via extensive experiments on classification, retrieval, detection and segmentation tasks, including zero, few, and full-shot settings. Particularly, on the zero-shot classification task, compared with CLIP, it achieves up to 5.4% improvement on ImageNet and 3.7% on the ELEVATER benchmark (20 datasets).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1206.Open Vocabulary Semantic Segmentation With Patch Aligned Contrastive Learning</span><br>
                <span class="as">Mukhoti, JishnuandLin, Tsung-YuandPoursaeed, OmidandWang, RuiandShah, AshishandTorr, PhilipH.S.andLim, Ser-Nam</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19413-19423.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个模型，使其能够将图像的特定区域与给定的文本输入相对应，从而在无需任何分割标注的情况下无缝转移到开放词汇语义分割任务。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：提出了一种修改后的CLIP对比损失兼容性函数——Patch Aligned Contrastive Learning (PACL)，用于训练视觉编码器和文本编码器的CLS令牌之间的对齐。通过这种对齐，模型可以识别图像中对应于给定文本输入的区域，从而无需任何分割标注即可无缝转移到开放词汇语义分割任务。<br>
                    效果：使用预训练的CLIP编码器和PACL，我们在4个不同的分割基准测试上实现了开放词汇零样本分割任务的最新状态：Pascal VOC、Pascal Context、COCO Stuff和ADE20K。此外，我们还证明PACL也适用于图像级别的预测，并且当与CLIP主干一起使用时，在12个图像分类数据集上提供了比CLIP更好的零样本分类准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder. With such an alignment, a model can identify regions of an image corresponding to a given text input, and therefore transfer seamlessly to the task of open vocabulary semantic segmentation without requiring any segmentation annotations during training. Using pre-trained CLIP encoders with PACL, we are able to set the state-of-the-art on the task of open vocabulary zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC, Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also applicable to image-level predictions and when used with a CLIP backbone, provides a general improvement in zero-shot classification accuracy compared to CLIP, across a suite of 12 image classification datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1207.Co-Training 2L Submodels for Visual Recognition</span><br>
                <span class="as">Touvron, HugoandCord, MatthieuandOquab, MaximeandBojanowski, PiotrandVerbeek, JakobandJ\&#x27;egou, Herv\&#x27;e</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Touvron_Co-Training_2L_Submodels_for_Visual_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11701-11710.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用子模型共训练方法提高神经网络的训练效果。<br>
                    动机：现有的预训练语言模型和图像识别模型在训练过程中，往往忽视了网络中不同层次的信息，导致模型性能受限。<br>
                    方法：提出一种子模型共训练方法，通过随机激活网络中的部分层并跳过其他层，生成两个“子模型”，然后让这两个子模型相互作为“软教师”进行训练，提供互补的交叉熵损失。<br>
                    效果：实验证明，该方法能有效提高神经网络的训练效果，适用于多种最新的网络架构，并在图像分类和语义分割等任务上取得了新的最优结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces submodel co-training, a regularization method related to co-training, self-distillation and stochastic depth. Given a neural network to be trained, for each sample we implicitly instantiate two altered networks, "submodels", with stochastic depth: i.e. activating only a subset of the layers and skipping others. Each network serves as a soft teacher to the other, by providing a cross-entropy loss that complements the regular softmax cross-entropy loss provided by the one-hot label. Our approach, dubbed "cosub", uses a single set of weights, and does not involve a pre-trained external model or temporal averaging. Experimentally, we show that submodel co-training is effective to train backbones for recognition tasks such as image classification and semantic segmentation, and that our approach is compatible with multiple recent architectures, including RegNet, PiT, and Swin. We report new state-of-the-art results for vision transformers trained on ImageNet only. For instance, a ViT-B pre-trained with cosub on Imagenet-21k achieves 87.4% top-1 acc. on Imagenet-val.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1208.Understanding Masked Autoencoders via Hierarchical Latent Variable Models</span><br>
                <span class="as">Kong, LingjingandMa, MartinQ.andChen, GuangyiandXing, EricP.andChi, YuejieandMorency, Louis-PhilippeandZhang, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Autoencoders_via_Hierarchical_Latent_Variable_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7918-7928.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在对基于掩蔽图像区域重建的自监督学习框架——掩蔽自动编码器（MAE）进行理论性理解，并为其提供理论保证。<br>
                    动机：尽管掩蔽自动编码器在各种视觉任务中取得了显著的成功，但其理论基础仍然缺乏。<br>
                    方法：我们将底层数据生成过程形式化为一个分层潜在变量模型，并证明在合理的假设下，MAE可以识别出该模型中的一组潜在变量，解释了为什么MAE可以从像素中提取高层次信息。<br>
                    效果：我们的理论为现有的实证观察提供了一致的解释，并为掩蔽重建范式的潜在实证改进和基本限制提供了见解。我们的实验结果验证了我们的理论洞见。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical insights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1209.Photo Pre-Training, but for Sketch</span><br>
                <span class="as">Li, KeandPang, KaiyueandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2754-2764.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用基于照片的预训练来提高草图理解能力？<br>
                    动机：由于草图数据的稀缺，社区在设计上做出了一些“特殊”的选择，如强制使用基于照片的预训练（即无草图）。我们想知道这种预训练是否能真正有益于草图理解。<br>
                    方法：我们培养了预训练阶段学习的照片数据的拓扑结构，并将其作为下游草图任务的“免费”监督源。具体来说，我们使用细粒度的草图基图像检索（FG-SBIR）来展示我们对预训练的新视角。在这种背景下，从照片中学习到的拓扑信息监督在每次微调步骤中都起作用——预训练模型中的相邻照片在每次FG-SBIR更新中保持相邻。我们将这种邻域一致性约束描述为照片排名问题，并将其形成简洁的跨模态三元组损失。我们还展示了如何更好地将此目标作为元目标而不是与主要FG-SBIR目标并行优化。<br>
                    效果：仅通过改变预训练，我们在所有五个产品级FG-SBIR基准测试中都取得了显著的优势（有时>10%）。最美妙的是，我们发现这样的巨大飞跃只需要几行额外的代码就能实现！我们的实现可以在https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The sketch community has faced up to its unique challenges over the years, that of data scarcity however still remains the most significant to date. This lack of sketch data has imposed on the community a few "peculiar" design choices -- the most representative of them all is perhaps the coerced utilisation of photo-based pre-training (i.e., no sketch), for many core tasks that otherwise dictates specific sketch understanding. In this paper, we ask just the one question -- can we make such photo-based pre-training, to actually benefit sketch? Our answer lies in cultivating the topology of photo data learned at pre-training, and use that as a "free" source of supervision for downstream sketch tasks. In particular, we use fine-grained sketch-based image retrieval (FG-SBIR), one of the most studied and data-hungry sketch tasks, to showcase our new perspective on pre-training. In this context, the topology-informed supervision learned from photos act as a constraint that take effect at every fine-tuning step -- neighbouring photos in the pre-trained model remain neighbours under each FG-SBIR updates. We further portray this neighbourhood consistency constraint as a photo ranking problem and formulate it into a neat cross-modal triplet loss. We also show how this target is better leveraged as a meta objective rather than optimised in parallel with the main FG-SBIR objective. With just this change on pre-training, we beat all previously published results on all five product-level FG-SBIR benchmarks with significant margins (sometimes >10%). And the most beautiful thing, as we note, is such gigantic leap is made possible with just a few extra lines of code! Our implementation is available at https://github.com/KeLi-SketchX/Photo-Pre-Training-But-for-Sketch</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1210.Bidirectional Cross-Modal Knowledge Exploration for Video Recognition With Pre-Trained Vision-Language Models</span><br>
                <span class="as">Wu, WenhaoandWang, XiaohanandLuo, HaipengandWang, JingdongandYang, YiandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Bidirectional_Cross-Modal_Knowledge_Exploration_for_Video_Recognition_With_Pre-Trained_Vision-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6620-6630.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练的视觉语言模型（VLMs）在视频识别任务中实现有效的知识转移。<br>
                    动机：预训练的VLMs在图像-文本对的大型数据集上表现出强大的迁移能力，构建视觉和文本领域的桥梁是其最大的价值所在。<br>
                    方法：提出一种名为BIKE的新框架，通过跨模态桥接探索双向知识转移。引入视频属性关联机制，利用视频到文本的知识生成辅助的视频识别文本属性；同时提出时间概念检测机制，使用文本到视频的专业知识以无参数的方式捕获时间显著性，增强视频表示。<br>
                    效果：在六个流行的视频数据集上进行广泛研究，包括Kinetics-400 & 600、UCF-101、HMDB-51、ActivityNet和Charades，结果显示该方法在各种识别场景中实现了最先进的性能，如通用、零样本和少样本视频识别。在具有挑战性的Kinetics-400上，最好的模型达到了88.6%的准确率，这是目前最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-language models (VLMs) pre-trained on large-scale image-text pairs have demonstrated impressive transferability on various visual tasks. Transferring knowledge from such powerful VLMs is a promising direction for building effective video recognition models. However, current exploration in this field is still limited. We believe that the greatest value of pre-trained VLMs lies in building a bridge between visual and textual domains. In this paper, we propose a novel framework called BIKE, which utilizes the cross-modal bridge to explore bidirectional knowledge: i) We introduce the Video Attribute Association mechanism, which leverages the Video-to-Text knowledge to generate textual auxiliary attributes for complementing video recognition. ii) We also present a Temporal Concept Spotting mechanism that uses the Text-to-Video expertise to capture temporal saliency in a parameter-free manner, leading to enhanced video representation. Extensive studies on six popular video datasets, including Kinetics-400 & 600, UCF-101, HMDB-51, ActivityNet and Charades, show that our method achieves state-of-the-art performance in various recognition scenarios, such as general, zero-shot, and few-shot video recognition. Our best model achieves a state-of-the-art accuracy of 88.6% on the challenging Kinetics-400 using the released CLIP model. The code is available at https://github.com/whwu95/BIKE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1211.Pic2Word: Mapping Pictures to Words for Zero-Shot Composed Image Retrieval</span><br>
                <span class="as">Saito, KuniakiandSohn, KihyukandZhang, XiangandLi, Chun-LiangandLee, Chen-YuandSaenko, KateandPfister, Tomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19305-19314.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决组合图像检索（CIR）中需要大量标注三元组的问题，提出了一种零样本组合图像检索（ZS-CIR）方法。<br>
                    动机：现有的组合图像检索方法依赖于使用标注的三元组进行监督学习，但标注这些三元组的成本高且限制了CIR的广泛应用。<br>
                    方法：我们提出了一种新的方法Pic2Word，它只需要弱标注的图像-标题对和未标注的图像数据集进行训练。<br>
                    效果：与现有的有监督CIR模型相比，我们的模型在各种ZS-CIR任务上表现出强大的泛化能力，并在常见的CIR基准测试CIRR和Fashion-IQ上优于几种有监督的CIR方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In Composed Image Retrieval (CIR), a user combines a query image with text to describe their intended target. Existing methods rely on supervised learning of CIR models using labeled triplets consisting of the query image, text specification, and the target image. Labeling such triplets is expensive and hinders broad applicability of CIR. In this work, we propose to study an important task, Zero-Shot Composed Image Retrieval (ZS-CIR), whose goal is to build a CIR model without requiring labeled triplets for training. To this end, we propose a novel method, called Pic2Word, that requires only weakly labeled image-caption pairs and unlabeled image datasets to train. Unlike existing supervised CIR models, our model trained on weakly labeled or unlabeled datasets shows strong generalization across diverse ZS-CIR tasks, e.g., attribute editing, object composition, and domain conversion. Our approach outperforms several supervised CIR methods on the common CIR benchmark, CIRR and Fashion-IQ.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1212.Improving Image Recognition by Retrieving From Web-Scale Image-Text Data</span><br>
                <span class="as">Iscen, AhmetandFathi, AlirezaandSchmid, Cordelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Iscen_Improving_Image_Recognition_by_Retrieving_From_Web-Scale_Image-Text_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19295-19304.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过检索增强模型提高计算机视觉任务的识别能力。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：引入基于注意力的记忆模块，学习每个检索到的示例的重要性，并构建大规模的记忆数据集。<br>
                    效果：实验结果表明，该方法在ImageNet-LT、Places-LT和Webvision等数据集上取得了最先进的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Retrieval augmented models are becoming increasingly popular for computer vision tasks after their recent success in NLP problems. The goal is to enhance the recognition capabilities of the model by retrieving similar examples for the visual input from an external memory set. In this work, we introduce an attention-based memory module, which learns the importance of each retrieved example from the memory. Compared to existing approaches, our method removes the influence of the irrelevant retrieved examples, and retains those that are beneficial to the input query. We also thoroughly study various ways of constructing the memory dataset. Our experiments show the benefit of using a massive-scale memory dataset of 1B image-text pairs, and demonstrate the performance of different memory representations. We evaluate our method in three different classification tasks, namely long-tailed recognition, learning with noisy labels, and fine-grained classification, and show that it achieves state-of-the-art accuracies in ImageNet-LT, Places-LT and Webvision datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1213.Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding</span><br>
                <span class="as">Tan, ChaoleiandLin, ZihangandHu, Jian-FangandZheng, Wei-ShiandLai, Jianhuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18973-18982.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频段落定位（VPG）是视觉语言理解中的关键但具有挑战性的任务，旨在从未修剪的视频中联合定位多个事件，并使用段落查询描述。<br>
                    动机：解决此问题的一个关键挑战是理解视觉和文本模态之间的复杂语义关系。以前的模型只关注从单一层面（即句子级别）对视频和文本之间的上下文信息进行建模，忽略了不同语义级别的丰富视觉-文本对应关系，如视频-单词和视频-段落对应。<br>
                    方法：我们提出了一种新的分层语义对应网络（HSCNet），通过学习分层语义对齐来探索多级视觉-文本对应关系，并通过对各种级别的查询进行密集监督来实现。具体来说，我们开发了一个分层编码器，将多模态输入编码为在不同级别上的语义对齐表示。为了利用编码器中学习的分层语义对应来进行多级监督，我们还设计了一个分层解码器，根据更高级别的语义逐步进行更精细的定位。<br>
                    效果：大量实验表明，HSCNet及其方法在ActivityNet-Captions和TACoS两个具有挑战性的基准测试中显著优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video Paragraph Grounding (VPG) is an essential yet challenging task in vision-language understanding, which aims to jointly localize multiple events from an untrimmed video with a paragraph query description. One of the critical challenges in addressing this problem is to comprehend the complex semantic relations between visual and textual modalities. Previous methods focus on modeling the contextual information between the video and text from a single-level perspective (i.e., the sentence level), ignoring rich visual-textual correspondence relations at different semantic levels, e.g., the video-word and video-paragraph correspondence. To this end, we propose a novel Hierarchical Semantic Correspondence Network (HSCNet), which explores multi-level visual-textual correspondence by learning hierarchical semantic alignment and utilizes dense supervision by grounding diverse levels of queries. Specifically, we develop a hierarchical encoder that encodes the multi-modal inputs into semantics-aligned representations at different levels. To exploit the hierarchical semantic correspondence learned in the encoder for multi-level supervision, we further design a hierarchical decoder that progressively performs finer grounding for lower-level queries conditioned on higher-level semantics. Extensive experiments demonstrate the effectiveness of HSCNet and our method significantly outstrips the state-of-the-arts on two challenging benchmarks, i.e., ActivityNet-Captions and TACoS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1214.Dynamic Graph Enhanced Contrastive Learning for Chest X-Ray Report Generation</span><br>
                <span class="as">Li, MingjieandLin, BingqianandChen, ZicongandLin, HaokunandLiang, XiaodanandChang, Xiaojun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3334-3343.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用动态知识图谱和对比学习进行胸部X射线报告生成。<br>
                    动机：现有的基于数据驱动的神经网络在自动放射学报告任务中存在严重的视觉和文本偏见，且固定的医学知识图谱无法保证最合适的知识范围，限制了效果。<br>
                    方法：提出一种具有动态结构和节点的知识图谱DCL，通过自下而上的方式从检索到的报告中提取特定知识来添加额外的节点或重新定义关系，将每个图像特征与其自己的更新后的图谱集成后输入解码器模块进行报告生成。<br>
                    效果：在IU-Xray和MIMIC-CXR数据集上评估，DCL在这些两个基准上优于先前最先进的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic radiology reporting has great clinical potential to relieve radiologists from heavy workloads and improve diagnosis interpretation. Recently, researchers have enhanced data-driven neural networks with medical knowledge graphs to eliminate the severe visual and textual bias in this task. The structures of such graphs are exploited by using the clinical dependencies formed by the disease topic tags via general knowledge and usually do not update during the training process. Consequently, the fixed graphs can not guarantee the most appropriate scope of knowledge and limit the effectiveness. To address the limitation, we propose a knowledge graph with Dynamic structure and nodes to facilitate chest X-ray report generation with Contrastive Learning, named DCL. In detail, the fundamental structure of our graph is pre-constructed from general knowledge. Then we explore specific knowledge extracted from the retrieved reports to add additional nodes or redefine their relations in a bottom-up manner. Each image feature is integrated with its very own updated graph before being fed into the decoder module for report generation. Finally, this paper introduces Image-Report Contrastive and Image-Report Matching losses to better represent visual features and textual information. Evaluated on IU-Xray and MIMIC-CXR datasets, our DCL outperforms previous state-of-the-art models on these two benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1215.BiCro: Noisy Correspondence Rectification for Multi-Modality Data via Bi-Directional Cross-Modal Similarity Consistency</span><br>
                <span class="as">Yang, ShuoandXu, ZhaopanandWang, KaiandYou, YangandYao, HongxunandLiu, TongliangandXu, Min</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BiCro_Noisy_Correspondence_Rectification_for_Multi-Modality_Data_via_Bi-Directional_Cross-Modal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19883-19892.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多模态学习中的一种基本技术，跨模态匹配的问题，需要将各种感官模式投影到一个共享的特征空间中。<br>
                    动机：由于大规模且正确对齐的数据对对于模型训练至关重要，因此收集和精确标注多模态数据集非常困难。然而，互联网上收集的共现数据对（如图像-文本对）已被广泛用于此领域，但这些廉价收集的数据集不可避免地包含许多不匹配的数据对，这已被证明会对模型性能产生负面影响。<br>
                    方法：为了解决这个问题，我们提出了一个名为BiCro（双向跨模态相似性一致性）的通用框架，可以很容易地集成到现有的跨模态匹配模型中，并提高它们对噪声数据的鲁棒性。具体来说，BiCro的目标是为噪声数据对估计软标签，以反映其真实的对应程度。<br>
                    效果：我们在三个流行的跨模态匹配数据集上的实验表明，我们的方法显著提高了各种匹配模型的噪声鲁棒性，并以明显的优势超越了最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model's performance. To address this, we propose a general framework called BiCro (Bidirectional Cross-modal similarity consistency), which can be easily integrated into existing cross-modal matching models and improve their robustness against noisy data. Specifically, BiCro aims to estimate soft labels for noisy data pairs to reflect their true correspondence degree. The basic idea of BiCro is motivated by that -- taking image-text matching as an example -- similar images should have similar textual descriptions and vice versa. Then the consistency of these two similarities can be recast as the estimated soft labels to train the matching model. The experiments on three popular cross-modal matching datasets demonstrate that our method significantly improves the noise-robustness of various matching models, and surpass the state-of-the-art by a clear margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1216.Beyond Appearance: A Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks</span><br>
                <span class="as">Chen, WeihuaandXu, XianzheandJia, JianandLuo, HaoandWang, YaohuaandWang, FanandJin, RongandSun, Xiuyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Beyond_Appearance_A_Semantic_Controllable_Self-Supervised_Learning_Framework_for_Human-Centric_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15050-15061.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从大量的未标记人类图像中学习通用的人类表示，以最大程度地提高下游以人为中心的任务的性能。<br>
                    动机：由于其广泛的应用，以人为中心的视觉任务越来越受到研究关注。现有的自我监督学习方法未能充分利用人类图像的先验知识来构建伪语义标签和导入更多的语义信息到学习到的表示中。<br>
                    方法：提出一种名为SOLIDER的自我监督学习框架，利用人类图像的先验知识建立伪语义标签，并将更多的语义信息导入到学习到的表示中。同时，考虑到不同的下游任务需要不同比例的语义信息和外观信息，SOLIDER引入了一个带有语义控制器的条件网络，用户可以根据需要调整控制器生成具有不同比例语义信息的表示。<br>
                    效果：在六个下游以人为中心的视觉任务上验证了SOLIDER，其在各项任务上都优于现有技术，并为这些任务建立了新的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-centric visual tasks have attracted increasing research attention due to their widespread applications. In this paper, we aim to learn a general human representation from massive unlabeled human images which can benefit downstream human-centric tasks to the maximum extent. We call this method SOLIDER, a Semantic cOntrollable seLf-supervIseD lEaRning framework. Unlike the existing self-supervised learning methods, prior knowledge from human images is utilized in SOLIDER to build pseudo semantic labels and import more semantic information into the learned representation. Meanwhile, we note that different downstream tasks always require different ratios of semantic information and appearance information. For example, human parsing requires more semantic information, while person re-identification needs more appearance information for identification purpose. So a single learned representation cannot fit for all requirements. To solve this problem, SOLIDER introduces a conditional network with a semantic controller. After the model is trained, users can send values to the controller to produce representations with different ratios of semantic information, which can fit different needs of downstream tasks. Finally, SOLIDER is verified on six downstream human-centric visual tasks. It outperforms state of the arts and builds new baselines for these tasks. The code is released in https://github.com/tinyvision/SOLIDER.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1217.Position-Guided Text Prompt for Vision-Language Pre-Training</span><br>
                <span class="as">Wang, JinpengandZhou, PanandShou, MikeZhengandYan, Shuicheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Position-Guided_Text_Prompt_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23242-23251.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉语言预训练（VLP）模型在许多下游任务中缺乏视觉基础/定位能力的问题。<br>
                    动机：VLP模型在对齐图像和文本对方面表现出了强大的能力，但在许多下游任务中，如视觉推理，它们往往缺乏关键的视觉基础/定位能力。<br>
                    方法：本文提出了一种新的位置引导的文本提示（PTP）范式，以提高使用VLP训练的跨模态模型的视觉基础能力。具体来说，PTP将图像划分为NxN的块，并通过在VLP中使用的广泛对象检测器识别每个块中的对象。然后，它将视觉基础任务重新表述为一个填空问题，给定一个PTP，鼓励模型预测给定块中的对象或回归给定对象的块，例如填充"P"或"O"在PTP "The block P has a O"中。<br>
                    效果：通过将PTP引入几种最先进的VLP框架，观察到代表性的跨模态学习模型架构和几个基准测试中显著的改进，例如ViLT基线在零射击Flickr30K检索（平均召回率@1 +4.8）和COCO字幕（CIDEr +5.3）上的表现优于BLIP基线。此外，由于PTP在推理时丢弃其对象检测器，而后者不能，因此PTP实现了与基于对象检测的方法相当的结果，并且推理速度更快。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-Language Pre-Training (VLP) has shown promising capabilities to align image and text pairs, facilitating a broad variety of cross-modal learning tasks. However, we observe that VLP models often lack the visual grounding/localization capability which is critical for many downstream tasks such as visual reasoning. In this work, we propose a novel Position-guided Text Prompt (PTP) paradigm to enhance the visual grounding ability of cross-modal models trained with VLP. Specifically, in the VLP phase, PTP divides the image into NxN blocks, and identifies the objects in each block through the widely used object detector in VLP. It then reformulates the visual grounding task into a fill-in-the-blank problem given a PTP by encouraging the model to predict the objects in the given blocks or regress the blocks of a given object, e.g. filling "P" or "O" in a PTP "The block P has a O". This mechanism improves the visual grounding capability of VLP models and thus helps them better handle various downstream tasks. By introducing PTP into several state-of-the-art VLP frameworks, we observe consistently significant improvements across representative cross-modal learning model architectures and several benchmarks, e.g. zero-shot Flickr30K Retrieval (+4.8 in average recall@1) for ViLT baseline, and COCO Captioning (+5.3 in CIDEr) for SOTA BLIP baseline. Moreover, PTP achieves comparable results with object-detector based methods, and much faster inference speed since PTP discards its object detector for inference while the later cannot. Our code and pre-trained weight will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1218.An Empirical Study of End-to-End Video-Language Transformers With Masked Visual Modeling</span><br>
                <span class="as">Fu, Tsu-JuiandLi, LinjieandGan, ZheandLin, KevinandWang, WilliamYangandWang, LijuanandLiu, Zicheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22898-22909.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索在视频-语言预训练中，通过遮蔽视觉模型（MVM）进行预训练的有效性。<br>
                    动机：尽管在视频输入上进行遮蔽帧模型等重建目标的研究已经在视频-语言预训练中得到探索，但之前的研究并未找到一种真正有效的MVM策略，能够大幅度提升下游性能。<br>
                    方法：本文基于完全端到端的VIdeO-LanguagE Transformer（VIOLET），系统地考察了MVM在VidL学习中的潜力。我们探索了8种不同的MVM重建目标，从低级别的像素值和定向梯度，到高级别的深度图、光流、离散视觉标记和潜在视觉特征。<br>
                    效果：实验结果表明，使用MVM目标预训练的VIOLETv2模型在13个VidL基准测试中取得了显著改进，包括视频问答、视频字幕生成和文本到视频检索等任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked visual modeling (MVM) has been recently proven effective for visual pre-training. While similar reconstructive objectives on video inputs (e.g., masked frame modeling) have been explored in video-language (VidL) pre-training, previous studies fail to find a truly effective MVM strategy that can largely benefit the downstream performance. In this work, we systematically examine the potential of MVM in the context of VidL learning. Specifically, we base our study on a fully end-to-end VIdeO-LanguagE Transformer (VIOLET), where the supervision from MVM training can be backpropagated to the video pixel space. In total, eight different reconstructive targets of MVM are explored, from low-level pixel values and oriented gradients to high-level depth maps, optical flow, discrete visual tokens, and latent visual features. We conduct comprehensive experiments and provide insights into the factors leading to effective MVM training, resulting in an enhanced model VIOLETv2. Empirically, we show VIOLETv2 pre-trained with MVM objective achieves notable improvements on 13 VidL benchmarks, ranging from video question answering, video captioning, to text-to-video retrieval.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1219.Revisiting Temporal Modeling for CLIP-Based Image-to-Video Knowledge Transferring</span><br>
                <span class="as">Liu, RuyangandHuang, JingjiaandLi, GeandFeng, JiashiandWu, XinglongandLi, ThomasH.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6555-6564.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将图像-文本预训练模型（如CLIP）扩展到视频领域，特别是在进行图像到视频的知识转移时如何进行有效的时间建模。<br>
                    动机：当前的图像-文本预训练模型在大规模图像-文本数据对中学习到了令人印象深刻的多模态知识，这为改善视频领域的视觉表示学习提供了潜力。然而，现有的时间建模机制往往针对高级别的语义主导任务或低级别的视觉模式主导任务，无法同时处理这两种情况。<br>
                    方法：本文提出了一种名为Spatial-Temporal Auxiliary Network (STAN)的简单而有效的时间建模机制，该机制通过分解的空间-时间模块实现了多级CLIP特征的空间-时间上下文化，从而实现了低级别和高级别的知识转移。<br>
                    效果：在两个代表性的视频任务：视频-文本检索和视频识别上进行的大量实验表明，STAN模型在各种数据集上优于最先进的方法，包括MSR-VTT、DiDeMo、LSMDC、MSVD、Kinetics-400和Something-Something-V2。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-text pretrained models, e.g., CLIP, have shown impressive general multi-modal knowledge learned from large-scale image-text data pairs, thus attracting increasing attention for their potential to improve visual representation learning in the video domain. In this paper, based on the CLIP model, we revisit temporal modeling in the context of image-to-video knowledge transferring, which is the key point for extending image-text pretrained models to the video domain. We find that current temporal modeling mechanisms are tailored to either high-level semantic-dominant tasks (e.g., retrieval) or low-level visual pattern-dominant tasks (e.g., recognition), and fail to work on the two cases simultaneously. The key difficulty lies in modeling temporal dependency while taking advantage of both high-level and low-level knowledge in CLIP model. To tackle this problem, we present Spatial-Temporal Auxiliary Network (STAN) -- a simple and effective temporal modeling mechanism extending CLIP model to diverse video tasks. Specifically, to realize both low-level and high-level knowledge transferring, STAN adopts a branch structure with decomposed spatial-temporal modules that enable multi-level CLIP features to be spatial-temporally contextualized. We evaluate our method on two representative video tasks: Video-Text Retrieval and Video Recognition. Extensive experiments demonstrate the superiority of our model over the state-of-the-art methods on various datasets, including MSR-VTT, DiDeMo, LSMDC, MSVD, Kinetics-400, and Something-Something-V2. Codes will be available at https://github.com/farewellthree/STAN</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1220.Learning To Name Classes for Vision and Language Models</span><br>
                <span class="as">Parisot, SarahandYang, YongxinandMcDonagh, Steven</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Parisot_Learning_To_Name_Classes_for_Vision_and_Language_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23477-23486.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过大规模视觉和语言模型实现零样本识别，并解决对手工构造的类别名称选择敏感以及难以适应新小型数据集的问题。<br>
                    动机：目前的模型在零样本识别上表现优秀，但对类别名称的选择过于敏感，且难以适应新的小数据集。<br>
                    方法：提出利用现有数据为每个类别学习最优的词嵌入作为视觉内容的函数，通过在冻结的模型上学习新的词嵌入，以保留新类别的零样本能力，并易于适应新的数据集，同时调整可能错误、描述不清或模糊的类别名称。<br>
                    效果：该方法可以容易地集成到图像分类和目标检测流程中，在多个场景下显著提高性能，并提供对模型偏见和标签错误的洞察。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large scale vision and language models can achieve impressive zero-shot recognition performance by mapping class specific text queries to image content. Two distinct challenges that remain however, are high sensitivity to the choice of handcrafted class names that define queries, and the difficulty of adaptation to new, smaller datasets. Towards addressing these problems, we propose to leverage available data to learn, for each class, an optimal word embedding as a function of the visual content. By learning new word embeddings on an otherwise frozen model, we are able to retain zero-shot capabilities for new classes, easily adapt models to new datasets, and adjust potentially erroneous, non-descriptive or ambiguous class names. We show that our solution can easily be integrated in image classification and object detection pipelines, yields significant performance gains in multiple scenarios and provides insights into model biases and labelling errors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1221.Coreset Sampling From Open-Set for Fine-Grained Self-Supervised Learning</span><br>
                <span class="as">Kim, SungnyunandBae, SangminandYun, Se-Young</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Coreset_Sampling_From_Open-Set_for_Fine-Grained_Self-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7537-7547.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模无标注开放数据集进行细粒度任务的预训练，并解决开放数据集与目标数据集之间的分布不匹配问题。<br>
                    动机：现有的细粒度任务依赖于专家知识进行标注，并且需要一种通用的模型来处理特定领域的各种下游任务。最近的自我监督学习（SSL）是一种无需标注就能预训练模型的有效方法，可以作为任何下游任务的有效初始化。<br>
                    方法：在预训练阶段，我们引入了一种新的开放集自我监督学习问题，假设存在大规模的未标注开放集和细粒度的目标数据集。我们提出了SimCore算法，通过在潜在空间中选择距离目标数据集最近的核心集（即开放集的子集），来解决开放集与目标数据集之间的分布不匹配问题。<br>
                    效果：通过包括十一个细粒度数据集和七个开放集在内的大量实验设置，我们证明了SimCore算法能显著提高表示学习性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning in general domains has constantly been extended to domain-specific tasks requiring the recognition of fine-grained characteristics. However, real-world applications for fine-grained tasks suffer from two challenges: a high reliance on expert knowledge for annotation and necessity of a versatile model for various downstream tasks in a specific domain (e.g., prediction of categories, bounding boxes, or pixel-wise annotations). Fortunately, the recent self-supervised learning (SSL) is a promising approach to pretrain a model without annotations, serving as an effective initialization for any downstream tasks. Since SSL does not rely on the presence of annotation, in general, it utilizes the large-scale unlabeled dataset, referred to as an open-set. In this sense, we introduce a novel Open-Set Self-Supervised Learning problem under the assumption that a large-scale unlabeled open-set is available, as well as the fine-grained target dataset, during a pretraining phase. In our problem setup, it is crucial to consider the distribution mismatch between the open-set and target dataset. Hence, we propose SimCore algorithm to sample a coreset, the subset of an open-set that has a minimum distance to the target dataset in the latent space. We demonstrate that SimCore significantly improves representation learning performance through extensive experimental settings, including eleven fine-grained datasets and seven open-sets in various downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1222.Divide and Conquer: Answering Questions With Object Factorization and Compositional Reasoning</span><br>
                <span class="as">Chen, ShiandZhao, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Divide_and_Conquer_Answering_Questions_With_Object_Factorization_and_Compositional_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6736-6745.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉推理方法无法处理新的对象或现实世界中的误导性偏见，并且无法解释其决策背后的原理。<br>
                    动机：受人类对视觉世界的推理启发，我们试图从组合的角度解决上述挑战。<br>
                    方法：我们提出了一个由原则性物体分解方法和新颖的神经模块网络组成的整体框架。我们的分解方法根据物体的关键特征进行分解，并自动导出代表各种物体的原型。<br>
                    效果：这个框架能够回答具有不同对象的问题，无论这些对象在训练期间是否可用，并能克服有偏的问答分布问题。此外，除了增强的泛化能力外，我们的框架还提供了一个可解释的界面，以理解模型的决策过程。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans have the innate capability to answer diverse questions, which is rooted in the natural ability to correlate different concepts based on their semantic relationships and decompose difficult problems into sub-tasks. On the contrary, existing visual reasoning methods assume training samples that capture every possible object and reasoning problem, and rely on black-boxed models that commonly exploit statistical priors. They have yet to develop the capability to address novel objects or spurious biases in real-world scenarios, and also fall short of interpreting the rationales behind their decisions. Inspired by humans' reasoning of the visual world, we tackle the aforementioned challenges from a compositional perspective, and propose an integral framework consisting of a principled object factorization method and a novel neural module network. Our factorization method decomposes objects based on their key characteristics, and automatically derives prototypes that represent a wide range of objects. With these prototypes encoding important semantics, the proposed network then correlates objects by measuring their similarity on a common semantic space and makes decisions with a compositional reasoning process. It is capable of answering questions with diverse objects regardless of their availability during training, and overcoming the issues of biased question-answer distributions. In addition to the enhanced generalizability, our framework also provides an interpretable interface for understanding the decision-making process of models. Our code is available at https://github.com/szzexpoi/POEM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1223.SceneTrilogy: On Human Scene-Sketch and Its Complementarity With Photo and Text</span><br>
                <span class="as">Chowdhury, PinakiNathandBhunia, AyanKumarandSain, AneeshanandKoley, SubhadeepandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10972-10983.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将场景理解扩展到人类草图，并从草图、照片和文本三种不同且互补的模态中获取完整的场景表示。<br>
                    动机：现有的方法学习的是刚性的三向嵌入，而我们的目标是学习一种灵活的联合嵌入，以充分支持这种互补性带来的"可选性"。<br>
                    方法：首先，通过结合信息瓶颈和条件可逆神经网络，从草图、照片和文本中提取模态特定的组件；然后，使用修改后的跨注意力机制整合来自草图、照片和文本的模态无关实例。<br>
                    效果：实验结果表明，我们的嵌入可以适应多种与场景相关的任务，包括首次因包含草图而实现的任务，而无需进行任何特定任务的修改。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we extend scene understanding to include that of human sketch. The result is a complete trilogy of scene representation from three diverse and complementary modalities -- sketch, photo, and text. Instead of learning a rigid three-way embedding and be done with it, we focus on learning a flexible joint embedding that fully supports the "optionality" that this complementarity brings. Our embedding supports optionality on two axis: (i) optionality across modalities -- use any combination of modalities as query for downstream tasks like retrieval, (ii) optionality across tasks -- simultaneously utilising the embedding for either discriminative (e.g., retrieval) or generative tasks (e.g., captioning). This provides flexibility to end-users by exploiting the best of each modality, therefore serving the very purpose behind our proposal of a trilogy at the first place. First, a combination of information-bottleneck and conditional invertible neural networks disentangle the modality-specific component from modality-agnostic in sketch, photo, and text. Second, the modality-agnostic instances from sketch, photo, and text are synergised using a modified cross-attention. Once learned, we show our embedding can accommodate a multi-facet of scene-related tasks, including those enabled for the first time by the inclusion of sketch, all without any task-specific modifications. Project Page: http://www.pinakinathc.me/scenetrilogy</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1224.Mobile User Interface Element Detection via Adaptively Prompt Tuning</span><br>
                <span class="as">Gu, ZhangxuanandXu, ZhuoerandChen, HaoxingandLan, JunandMeng, ChanghuaandWang, Weiqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11155-11164.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对象检测方法在处理包含额外光学字符识别（OCR）信息的移动用户界面（MUI）元素时存在困难。<br>
                    动机：由于MUI元素包含描述其内容和功能的额外OCR信息，但往往被忽视，因此需要开发一种能够有效利用这些信息的新方法。<br>
                    方法：本文提出了一种新的MUI元素检测数据集MUI-zh，并设计了一个自适应提示调优（APT）模块来利用这些有区别性的OCR信息。APT是一种轻量且有效的模块，用于在不同的模态中联合优化类别提示。<br>
                    效果：通过在几个现有的基于CLIP的检测器上进行实验，发现该方法在两个数据集上都取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent object detection approaches rely on pretrained vision-language models for image-text alignment. However, they fail to detect the Mobile User Interface (MUI) element since it contains additional OCR information, which describes its content and function but is often ignored. In this paper, we develop a new MUI element detection dataset named MUI-zh and propose an Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR information. APT is a lightweight and effective module to jointly optimize category prompts across different modalities. For every element, APT uniformly encodes its visual features and OCR descriptions to dynamically adjust the representation of frozen category prompts. We evaluate the effectiveness of our plug-and-play APT upon several existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments show that our method achieves considerable improvements on two datasets. The datasets is available at github.com/antmachineintelligence/MUI-zh.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1225.Generating Human Motion From Textual Descriptions With Discrete Representations</span><br>
                <span class="as">Zhang, JianrongandZhang, YangsongandCun, XiaodongandZhang, YongandZhao, HongweiandLu, HongtaoandShen, XiandShan, Ying</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14730-14740.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过基于向量量化变分自编码器（VQ-VAE）和生成预训练转换器（GPT）的条件生成框架，从纹理描述中生成人类运动。<br>
                    动机：尽管现有的方法在人类运动生成方面取得了一定的成果，但仍存在训练-测试不一致性和数据集限制等问题。<br>
                    方法：本文提出了一种简单的CNN-based VQ-VAE和GPT结合的方法，通过使用常见的EMA和代码重置训练策略以及引入简单的损坏策略来减轻训练-测试不一致性。<br>
                    效果：实验结果表明，该方法在人类运动生成任务上表现优于竞争方法，例如在最大的数据集HumanML3D上，该方法在文本与生成运动一致性（R-Precision）方面表现相当，但在FID指标上大幅领先于MotionDiffuse方法。然而，分析表明数据集大小是该方法的一个限制。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation. Our implementation is available on the project page: https://mael-zys.github.io/T2M-GPT/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1226.Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks</span><br>
                <span class="as">Li, HaoandZhu, JinguoandJiang, XiaohuandZhu, XizhouandLi, HongshengandYuan, ChunandWang, XiaohuaandQiao, YuandWang, XiaogangandWang, WenhaiandDai, Jifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2691-2700.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何消除预训练模型在特定任务微调上的不一致性，实现通用感知模型的目标。<br>
                    动机：现有的通用模型在多功能性和性能上都不够理想。<br>
                    方法：提出Uni-Perceiver v2，这是首个能处理大规模视觉和视觉语言任务的通用模型。图像通过通用区域建议进行编码，文本则通过基于Transformer的语言模型进行编码。然后通过一个与任务无关的解码器对编码表示进行转换。不同的任务被统一为最大似然估计问题。并提出有效的优化技术——任务平衡梯度归一化，以确保稳定的多任务学习。<br>
                    效果：实验表明，Uni-Perceiver v2在多功能性和性能上都超过了所有现有的通用模型。同时，与需要特定任务微调的公认强基线相比，Uni-Perceiver v2在广泛的视觉和视觉语言任务上都能取得有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the remarkable success of foundation models, their task-specific fine-tuning paradigm makes them inconsistent with the goal of general perception modeling. The key to eliminating this inconsistency is to use generalist models for general task modeling. However, existing attempts at generalist models are inadequate in both versatility and performance. In this paper, we propose Uni-Perceiver v2, which is the first generalist model capable of handling major large-scale vision and vision-language tasks with competitive performance. Specifically, images are encoded as general region proposals, while texts are encoded via a Transformer-based language model. The encoded representations are transformed by a task-agnostic decoder. Different tasks are formulated as a unified maximum likelihood estimation problem. We further propose an effective optimization technique named Task-Balanced Gradient Normalization to ensure stable multi-task learning with an unmixed sampling strategy, which is helpful for tasks requiring large batch-size training. After being jointly trained on various tasks, Uni-Perceiver v2 is capable of directly handling downstream tasks without any task-specific adaptation. Results show that Uni-Perceiver v2 outperforms all existing generalist models in both versatility and performance. Meanwhile, compared with the commonly-recognized strong baselines that require tasks-specific fine-tuning, Uni-Perceiver v2 achieves competitive performance on a broad range of vision and vision-language tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1227.Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning With Multimodal Models</span><br>
                <span class="as">Lin, ZhiqiuandYu, SamuelandKuang, ZhiyiandPathak, DeepakandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19325-19337.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用跨模态信息进行少次学习，以更有效地理解新的概念。<br>
                    动机：传统的少次学习基准测试只使用来自单一模态的少次样本，但这样的样本可能不足以描述整个概念类别。相比之下，人类利用跨模态信息来高效地学习新的概念。<br>
                    方法：通过阅读关于狗的信息并听它们叫，构建了一个更好的视觉狗分类器。具体来说，我们利用了最新的多模态基础模型（如CLIP）本质上是跨模态的，将不同的模态映射到相同的表示空间这一事实。<br>
                    效果：通过重新使用类名作为额外的一次训练样本，我们在视觉语言适应方面取得了SOTA结果。此外，我们的这种方法可以改善现有的方法，如前缀调整和分类器集成。最后，为了探索视觉和语言之外的其他模态，我们构建了第一个（据我们所知）音频视觉少次基准测试，并使用跨模态训练提高了图像和音频分类的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better visual dog classifier by reading about dogs and listening to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for vision-language adaptation. Furthermore, we show that our approach can benefit existing methods such as prefix tuning and classifier ensembling. Finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification. We hope our success can inspire future works to embrace cross-modality for even broader domains and tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1228.3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions</span><br>
                <span class="as">Decatur, DaleandLang, ItaiandHanocka, Rana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Decatur_3D_Highlighter_Localizing_Regions_on_3D_Shapes_via_Text_Descriptions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20930-20939.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用文本输入在3D网格上定位语义区域。<br>
                    动机：目前的系统缺乏对"领域外"定位的解释能力，我们的目标是让系统能够理解如何在3D形状上放置非直观相关的概念。<br>
                    方法：我们提出了一种名为3D Highlighter的技术，通过使用神经场将文本描述进行上下文化处理，并用概率加权混合来给形状的相应区域上色。我们的神经网络优化由预训练的CLIP编码器引导，无需任何3D数据集或3D注释。<br>
                    效果：实验结果表明，3D Highlighter能够在各种输入形状上进行定位，具有高度的灵活性、通用性和生成定位的能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present 3D Highlighter, a technique for localizing semantic regions on a mesh using text as input. A key feature of our system is the ability to interpret "out-of-domain" localizations. Our system demonstrates the ability to reason about where to place non-obviously related concepts on an input 3D shape, such as adding clothing to a bare 3D animal model. Our method contextualizes the text description using a neural field and colors the corresponding region of the shape using a probability-weighted blend. Our neural optimization is guided by a pre-trained CLIP encoder, which bypasses the need for any 3D datasets or 3D annotations. Thus, 3D Highlighter is highly flexible, general, and capable of producing localizations on a myriad of input shapes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1229.PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</span><br>
                <span class="as">Ding, RunyuandYang, JihanandXue, ChuhuiandZhang, WenqingandBai, SongandQi, Xiaojuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7010-7019.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-vocabulary scene understanding aims to localize and recognize unseen categories beyond the annotated label space. The recent breakthrough of 2D open-vocabulary perception is largely driven by Internet-scale paired image-text data with rich vocabulary concepts. However, this success cannot be directly transferred to 3D scenarios due to the inaccessibility of large-scale 3D-text pairs. To this end, we propose to distill knowledge encoded in pre-trained vision-language (VL) foundation models through captioning multi-view images from 3D, which allows explicitly associating 3D and semantic-rich captions. Further, to foster coarse-to-fine visual-semantic representation learning from captions, we design hierarchical 3D-caption pairs, leveraging geometric constraints between 3D scenes and multi-view images. Finally, by employing contrastive learning, the model learns language-aware embeddings that connect 3D and text for open-vocabulary tasks. Our method not only remarkably outperforms baseline methods by 25.8%   44.7% hIoU and 14.5%   50.4% hAP_ 50  in open-vocabulary semantic and instance segmentation, but also shows robust transferability on challenging zero-shot domain transfer tasks. See the project website at https://dingry.github.io/projects/PLA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1230.Visual Programming: Compositional Visual Reasoning Without Training</span><br>
                <span class="as">Gupta, TanmayandKembhavi, Aniruddha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14953-14962.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种神经符号方法VISPROG，用于解决复杂和组合性视觉任务。<br>
                    动机：VISPROG避免了任何特定任务的训练需求，利用大型语言模型的上下文学习能力生成类似Python的模块化程序，然后执行这些程序以获取解决方案和全面且可解释的理由。<br>
                    方法：VISPROG使用大规模语言模型的上下文学习能力生成类似Python的模块化程序，并调用各种现成的计算机视觉模型、图像处理程序或Python函数来产生可以被后续程序部分使用的中间输出。<br>
                    效果：VISPROG在4个不同的任务上展示了其灵活性，包括组合性视觉问答、基于图像对的零样本推理、基于事实知识的对象标记以及语言引导的图像编辑。作者认为，像VISPROG这样的神经符号方法是一条令人兴奋的途径，可以方便有效地扩展AI系统的范围，以满足人们可能希望执行的复杂任务的需求。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present VISPROG, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. VISPROG avoids the need for any task-specific training. Instead, it uses the in-context learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing routines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VISPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like VISPROG are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1231.Freestyle Layout-to-Image Synthesis</span><br>
                <span class="as">Xue, HanandHuang, ZhiwuandSun, QianruandSong, LiandZhang, Wenjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Freestyle_Layout-to-Image_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14256-14266.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索布局到图像合成（LIS）模型的自由风格能力，即模型能在给定布局上生成未见过语义（如类别、属性和风格）的能力。<br>
                    动机：由于大规模预训练的语言-图像模型的发展，一些基于有限基础类别的判别模型（如图像分类和目标检测）被赋予了预测未见过类别的能力。受此启发，我们选择利用大规模预训练的文本到图像扩散模型来实现未见过语义的生成。<br>
                    方法：我们引入了一个名为修正交叉注意力（RCA）的新模块，可以方便地插入扩散模型以整合语义掩码。在模型的每个交叉注意力层中应用这种"插件"，以修正图像和文本标记之间的注意力映射。<br>
                    效果：广泛的实验表明，所提出的扩散网络能产生真实且自由风格的布局到图像生成结果，具有多样化的文本输入，这有很高的潜力引发一系列应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Typical layout-to-image synthesis (LIS) models generate images for a closed set of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work, we explore the freestyle capability of the model, i.e., how far can it generate unseen semantics (e.g., classes, attributes, and styles) onto a given layout, and call the task Freestyle LIS (FLIS). Thanks to the development of large-scale pre-trained language-image models, a number of discriminative models (e.g., image classification and object detection) trained on limited base classes are empowered with the ability of unseen class prediction. Inspired by this, we opt to leverage large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics. The key challenge of FLIS is how to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-learned knowledge, e.g., the model never sees "a unicorn sitting on a bench" during its pre-training. To this end, we introduce a new module called Rectified Cross-Attention (RCA) that can be conveniently plugged in the diffusion model to integrate semantic masks. This "plug-in" is applied in each cross-attention layer of the model to rectify the attention maps between image and text tokens. The key idea of RCA is to enforce each text token to act on the pixels in a specified region, allowing us to freely put a wide variety of semantics from pre-trained knowledge (which is general) onto the given layout (which is specific). Extensive experiments show that the proposed diffusion network produces realistic and freestyle layout-to-image generation results with diverse text inputs, which has a high potential to spawn a bunch of interesting applications. Code is available at https://github.com/essunny310/FreestyleNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1232.Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator</span><br>
                <span class="as">Wang, ShijieandChang, JianlongandLi, HaojieandWang, ZhihuiandOuyang, WanliandTian, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Open-Set_Fine-Grained_Retrieval_via_Prompting_Vision-Language_Evaluator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19381-19391.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在开放世界中处理未知子类别的细粒度检索问题。<br>
                    动机：现有的方法主要针对封闭世界，所有子类别都预先定义，难以从未知子类别中获取区分性知识，因此无法处理开放世界中不可避免的未知子类别。<br>
                    方法：提出一种新的视觉语言评估器（PLEor）框架，基于最近引入的对比语言-图像预训练（CLIP）模型进行开放集细粒度检索。PLEor利用预训练的CLIP模型推断包括预定义和未知子类别在内的类别特定差异，并将其转移到在封闭场景中训练的主干网络。设计一个双提示方案使预训练的CLIP模型对类别特定差异敏感，并通过知识蒸馏机制将类别特定差异转移到主干网络。<br>
                    效果：实验表明，PLEor在开放集细粒度检索数据集上取得了良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-set fine-grained retrieval is an emerging challenge that requires an extra capability to retrieve unknown subcategories during evaluation. However, current works are rooted in the close-set scenarios, where all the subcategories are pre-defined, and make it hard to capture discriminative knowledge from unknown subcategories, consequently failing to handle the inevitable unknown subcategories in open-world scenarios. In this work, we propose a novel Prompting vision-Language Evaluator (PLEor) framework based on the recently introduced contrastive language-image pretraining (CLIP) model, for open-set fine-grained retrieval. PLEor could leverage pre-trained CLIP model to infer the discrepancies encompassing both pre-defined and unknown subcategories, called category-specific discrepancies, and transfer them to the backbone network trained in the close-set scenarios. To make pre-trained CLIP model sensitive to category-specific discrepancies, we design a dual prompt scheme to learn a vision prompt specifying the category-specific discrepancies, and turn random vectors with category names in a text prompt into category-specific discrepancy descriptions. Moreover, a vision-language evaluator is proposed to semantically align the vision and text prompts based on CLIP model, and reinforce each other. In addition, we propose an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone network using knowledge distillation mechanism. A variety of quantitative and qualitative experiments show that our PLEor achieves promising performance on open-set fine-grained retrieval datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1233.Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation</span><br>
                <span class="as">Sarto, SaraandBarraco, ManueleandCornia, MarcellaandBaraldi, LorenzoandCucchiara, Rita</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sarto_Positive-Augmented_Contrastive_Learning_for_Image_and_Video_Captioning_Evaluation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6914-6924.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地评估视觉-语言架构生成的字幕。<br>
                    动机：现有的评估指标无法充分反映人类对图像和视频字幕的判断，需要新的评估方法。<br>
                    方法：提出一种基于对比学习的评估指标PAC-S，通过在训练数据中添加生成的图像和文本来统一对比视觉语义空间的学习。<br>
                    效果：实验证明，PAC-S在多个数据集上与人类判断高度相关，优于现有参考指标如CIDEr和SPICE以及参考自由指标如CLIP-Score。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1234.Improving Visual Representation Learning Through Perceptual Understanding</span><br>
                <span class="as">Tukra, SamyakhandHoffman, FrederickandChatfield, Ken</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tukra_Improving_Visual_Representation_Learning_Through_Perceptual_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14486-14495.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过改进预训练模型来提高图像表示的学习效果。<br>
                    动机：现有的掩蔽自动编码器（MAE）在图像表示学习上存在不足，需要更好的捕捉高级别的图像细节。<br>
                    方法：提出了一种基于感知相似性的改进型MAE，结合了生成图像和真实图像的感知相似性、多尺度训练和自适应判别器增强等技术。<br>
                    效果：实验结果显示，该方法不仅提高了像素重建效果，而且在下游任务中表现更好，无需额外的预训练模型或数据即可实现ImageNet-1K的78.1% top-1准确率，微调后可达88.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present an extension to masked autoencoders (MAE) which improves on the representations learnt by the model by explicitly encouraging the learning of higher scene-level features. We do this by: (i) the introduction of a perceptual similarity term between generated and real images (ii) incorporating several techniques from the adversarial training literature including multi-scale training and adaptive discriminator augmentation. The combination of these results in not only better pixel reconstruction but also representations which appear to capture better higher-level details within images. More consequentially, we show how our method, Perceptual MAE, leads to better performance when used for downstream tasks outperforming previous methods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and up to 88.1% when fine-tuning, with similar results for other downstream tasks, all without use of additional pre-trained models or data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1235.AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning</span><br>
                <span class="as">Wang, RunqiandDuan, XiaoyueandKang, GuoliangandLiu, JianzhuangandLin, ShaohuiandXu, SongcenandL\&quot;u, JinhuandZhang, Baochang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AttriCLIP_A_Non-Incremental_Learner_for_Incremental_Knowledge_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3654-3663.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决传统连续学习模型在处理新类别或任务时，需要逐渐扩大分类器的特定权重并存储历史数据以减轻偏见和灾难性遗忘的问题。<br>
                    动机：现有的连续学习模型需要逐渐扩大分类器的特定权重并存储历史数据以减轻偏见和灾难性遗忘。<br>
                    方法：本文提出了一种非递增学习器，名为AttriCLIP，用于逐步提取新类别或任务的知识。具体来说，AttriCLIP建立在预训练的视觉语言模型CLIP之上，其图像编码器和文本编码器固定用于从图像和文本提示中提取特征。每个文本提示由类别名称和从我们设计的属性库中选择的固定数量的可学习参数组成，这些参数作为属性。由于我们计算视觉和文本相似性进行分类，因此AttriCLIP是一种非递增学习器。属性提示有效地缓解了灾难性遗忘并避免了构建重播内存。<br>
                    效果：实验结果表明，该方法在现实场景中的领域转移和长序列学习方面优于基于CLIP和先前最先进的连续学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning aims to enable a model to incrementally learn knowledge from sequentially arrived data. Previous works adopt the conventional classification architecture, which consists of a feature extractor and a classifier. The feature extractor is shared across sequentially arrived tasks or classes, but one specific group of weights of the classifier corresponding to one new class should be incrementally expanded. Consequently, the parameters of a continual learner gradually increase. Moreover, as the classifier contains all historical arrived classes, a certain size of the memory is usually required to store rehearsal data to mitigate classifier bias and catastrophic forgetting. In this paper, we propose a non-incremental learner, named AttriCLIP, to incrementally extract knowledge of new classes or tasks. Specifically, AttriCLIP is built upon the pre-trained visual-language model CLIP. Its image encoder and text encoder are fixed to extract features from both images and text prompts. Each text prompt consists of a category name and a fixed number of learnable parameters which are selected from our designed attribute bank and serve as attributes. As we compute the visual and textual similarity for classification, AttriCLIP is a non-incremental learner. The attribute prompts, which encode the common knowledge useful for classification, can effectively mitigate the catastrophic forgetting and avoid constructing a replay memory. We empirically evaluate our AttriCLIP and compare it with CLIP-based and previous state-of-the-art continual learning methods in realistic settings with domain-shift and long-sequence learning. The results show that our method performs favorably against previous state-of-the-arts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1236.Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-Commerce</span><br>
                <span class="as">Jin, YangandLi, YongzhiandYuan, ZehuanandMu, Yadong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Learning_Instance-Level_Representation_for_Large-Scale_Multi-Modal_Pretraining_in_E-Commerce_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11060-11069.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在建立一个通用的多模态基础模型，具有扩展到电子商务中大规模下游应用的能力。<br>
                    动机：由于自然图像和产品图像之间存在显著差异，直接将这些框架应用于电子商务的产品级表示将不可避免地产生次优结果。<br>
                    方法：我们提出了一种名为ECLIP的实例中心多模态预训练范例，通过设计一个引入一组可学习实例查询的解码器架构来明确聚合实例级语义。<br>
                    效果：在1亿个与电子商务相关的数据上进行预训练后，ECLIP成功提取了更通用、语义丰富且稳健的表示。大量实验结果表明，无需进一步微调，ECLIP在广泛的下游任务上大幅超越了现有方法，显示出对现实世界电子商务应用的强大迁移能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper aims to establish a generic multi-modal foundation model that has the scalable capability to massive downstream applications in E-commerce. Recently, large-scale vision-language pretraining approaches have achieved remarkable advances in the general domain. However, due to the significant differences between natural and product images, directly applying these frameworks for modeling image-level representations to E-commerce will be inevitably sub-optimal. To this end, we propose an instance-centric multi-modal pretraining paradigm called ECLIP in this work. In detail, we craft a decoder architecture that introduces a set of learnable instance queries to explicitly aggregate instance-level semantics. Moreover, to enable the model to focus on the desired product instance without reliance on expensive manual annotations, two specially configured pretext tasks are further proposed. Pretrained on the 100 million E-commerce-related data, ECLIP successfully extracts more generic, semantic-rich, and robust representations. Extensive experimental results show that, without further fine-tuning, ECLIP surpasses existing methods by a large margin on a broad range of downstream tasks, demonstrating the strong transferability to real-world E-commerce applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1237.Mask3D: Pre-Training 2D Vision Transformers by Learning Masked 3D Priors</span><br>
                <span class="as">Hou, JiandDai, XiaoliangandHe, ZijianandDai, AngelaandNie{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hou_Mask3D_Pre-Training_2D_Vision_Transformers_by_Learning_Masked_3D_Priors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13510-13519.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更有效地理解二维骨干网络中的三维结构先验？<br>
                    动机：目前的二维骨干网络（如ViT和ResNets）在处理三维结构信息时存在不足。<br>
                    方法：提出Mask3D，利用现有的大规模RGB-D数据进行自监督预训练，将三维先验嵌入到二维学习的特征表示中。通过在单个RGB-D帧中对RGB和深度补丁进行掩蔽，形成预文本重建任务。<br>
                    效果：实验表明，Mask3D能有效地将三维先验嵌入到强大的二维ViT骨干网络中，提高了各种场景理解任务（如语义分割、实例分割和目标检测）的表示学习能力。在ScanNet、NYUv2和Cityscapes图像理解任务上，Mask3D显著优于现有的自监督三维预训练方法，在ScanNet图像语义分割任务上比最先进的Pri3D提高了+6.5% mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current popular backbones in computer vision, such as Vision Transformers (ViT) and ResNets are trained to perceive the world from 2D images. However, to more effectively understand 3D structural priors in 2D backbones, we propose Mask3D to leverage existing large-scale RGB-D data in a self-supervised pre-training to embed these 3D priors into 2D learned feature representations. In contrast to traditional 3D contrastive learning paradigms requiring 3D reconstructions or multi-view correspondences, our approach is simple: we formulate a pre-text reconstruction task by masking RGB and depth patches in individual RGB-D frames. We demonstrate the Mask3D is particularly effective in embedding 3D priors into the powerful 2D ViT backbone, enabling improved representation learn- ing for various scene understanding tasks, such as semantic segmentation, instance segmentation and object detection. Experiments show that Mask3D notably outperforms exist- ing self-supervised 3D pre-training approaches on ScanNet, NYUv2, and Cityscapes image understanding tasks, with an improvement of +6.5% mIoU against the state-of-the-art Pri3D on ScanNet image semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1238.Multimodal Prompting With Missing Modalities for Visual Recognition</span><br>
                <span class="as">Lee, Yi-LunandTsai, Yi-HsuanandChiu, Wei-ChenandLee, Chen-Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14943-14952.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多模态学习在视觉识别中的两个挑战：1）训练或测试中出现缺失模态的情况；2）没有足够的计算资源对重型转换模型进行微调。<br>
                    动机：现有的预训练语言模型很少考虑结合知识图谱，而知识图谱可以提供丰富的结构化知识事实以更好地理解语言。<br>
                    方法：利用大规模文本语料库和知识图谱训练增强的语言表示模型（ERNIE），将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we tackle two challenges in multimodal learning for visual recognition: 1) when missing-modality occurs either during training or testing in real-world situations; and 2) when the computation resources are not available to finetune on heavy transformer models. To this end, we propose to utilize prompt learning and mitigate the above two challenges together. Specifically, our modality-missing-aware prompts can be plugged into multimodal transformers to handle general missing-modality cases, while only requiring less than 1% learnable parameters compared to training the entire model. We further explore the effect of different prompt configurations and analyze the robustness to missing modality. Extensive experiments are conducted to show the effectiveness of our prompt learning framework that improves the performance under various missing-modality cases, while alleviating the requirement of heavy model re-training. Code is available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1239.A-Cap: Anticipation Captioning With Commonsense Knowledge</span><br>
                <span class="as">Vo, DucMinhandLuong, Quoc-AnandSugimoto, AkihiroandNakayama, Hideki</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vo_A-Cap_Anticipation_Captioning_With_Commonsense_Knowledge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10824-10833.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用稀疏的时间序列图像集合，生成未见过的目标图像的标题。<br>
                    动机：人类能够根据过去收集到的少量视觉线索预测未来，为了模拟这种能力，研究人员提出了一种新的任务——预期描述（Anticipation Captioning）。<br>
                    方法：研究人员提出了一种名为A-CAP的模型，该模型将常识知识融入到预先训练好的视觉语言模型中，使其能够预测标题。<br>
                    效果：通过在自定义视觉故事数据集上进行定性和定量评估，A-CAP表现优于其他图像描述方法，为预期描述建立了坚实的基线。同时，研究人员也解决了这个任务中固有的挑战。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans possess the capacity to reason about the future based on a sparse collection of visual cues acquired over time. In order to emulate this ability, we introduce a novel task called Anticipation Captioning, which generates a caption for an unseen oracle image using a sparsely temporally-ordered set of images. To tackle this new task, we propose a model called A-CAP, which incorporates commonsense knowledge into a pre-trained vision-language model, allowing it to anticipate the caption. Through both qualitative and quantitative evaluations on a customized visual storytelling dataset, A-CAP outperforms other image captioning methods and establishes a strong baseline for anticipation captioning. We also address the challenges inherent in this task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1240.Learning 3D Representations From 2D Pre-Trained Models via Image-to-Point Masked Autoencoders</span><br>
                <span class="as">Zhang, RenruiandWang, LiuhuiandQiao, YuandGao, PengandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21769-21780.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于3D数据集的稀缺性，如何从2D预训练模型中获取高质量的3D表示。<br>
                    动机：现有的2D预训练模型已经能够很好地学习图像数据的特征，我们希望通过这些模型来引导3D自编码器的学习，从而获得更好的3D特征。<br>
                    方法：提出一种名为I2P-MAE的方法，通过图像到点的掩蔽自动编码器进行自我监督预训练。首先利用现成的2D模型提取输入点云的多视图视觉特征，然后进行两种类型的图像到点学习方案。一种是引入2D引导的掩蔽策略，保留语义上重要的点标记为可见；另一种是强制这些可见的标记在解码器后重建多视图2D特征，使网络能够有效地继承高级2D语义进行判别式3D建模。<br>
                    效果：实验结果表明，未经任何微调的冻结I2P-MAE在ModelNet40上达到了93.4%的准确率，与现有的全训练方法具有竞争力。通过对ScanObjectNN的最困难分割进行进一步微调，I2P-MAE实现了90.11%的最先进的准确率，比第二名高出3.68%，显示出优越的可转移能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-training by numerous image data has become de-facto for robust 2D representations. In contrast, due to the expensive data processing, a paucity of 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which reconstructs the masked point tokens with an encoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D models to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learning schemes. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible. Compared to random masking, the network can better concentrate on significant 3D structures with key spatial cues. For another, we enforce these visible tokens to reconstruct multi-view 2D features after the decoder. This enables the network to effectively inherit high-level 2D semantics for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to existing fully trained methods. By further fine-tuning on on ScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11% accuracy, +3.68% to the second-best, demonstrating superior transferable capacity. Code is available at https://github.com/ZrrSkywalker/I2P-MAE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1241.OVTrack: Open-Vocabulary Multiple Object Tracking</span><br>
                <span class="as">Li, SiyuanandFischer, TobiasandKe, LeiandDing, HenghuiandDanelljan, MartinandYu, Fisher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OVTrack_Open-Vocabulary_Multiple_Object_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5567-5577.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多目标跟踪（MOT）方法仅依赖于少数预定义的对象类别，无法应对真实世界中可能遇到的各种对象。<br>
                    动机：为了解决这一问题，研究人员提出了开放词汇多目标跟踪（OVTrack）任务，并开发了一种能够追踪任意对象类别的开放词汇跟踪器。<br>
                    方法：OVTrack的设计基于两个关键要素：一是利用视觉语言模型进行分类和关联；二是通过知识蒸馏进行数据增强策略，以从去噪扩散概率模型中学习稳健的外观特征。<br>
                    效果：实验结果表明，OVTrack在大规模、大词汇量的TAO基准测试中取得了新的最先进水平，且仅通过静态图像进行训练。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The ability to recognize, localize and track dynamic objects in a scene is fundamental to many real-world applications, such as self-driving and robotic systems. Yet, traditional multiple object tracking (MOT) benchmarks rely only on a few object categories that hardly represent the multitude of possible objects that are encountered in the real world. This leaves contemporary MOT methods limited to a small set of pre-defined object categories. In this paper, we address this limitation by tackling a novel task, open-vocabulary MOT, that aims to evaluate tracking beyond pre-defined training categories. We further develop OVTrack, an open-vocabulary tracker that is capable of tracking arbitrary object classes. Its design is based on two key ingredients: First, leveraging vision-language models for both classification and association via knowledge distillation; second, a data hallucination strategy for robust appearance feature learning from denoising diffusion probabilistic models. The result is an extremely data-efficient open-vocabulary tracker that sets a new state-of-the-art on the large-scale, large-vocabulary TAO benchmark, while being trained solely on static images. The project page is at https://www.vis.xyz/pub/ovtrack/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1242.ConvNeXt V2: Co-Designing and Scaling ConvNets With Masked Autoencoders</span><br>
                <span class="as">Woo, SanghyunandDebnath, ShoubhikandHu, RonghangandChen, XinleiandLiu, ZhuangandKweon, InSoandXie, Saining</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16133-16142.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提升视觉识别领域的表现，特别是在无监督学习设置下。<br>
                    动机：尽管现代的ConvNets（如ConvNeXt模型）在各种应用场景中表现出色，但直接结合监督学习和自监督学习框架并未带来预期的效果。<br>
                    方法：开发了一个完全卷积的掩蔽自动编码器框架，并对其进行了优化。同时，对ConvNeXt架构进行了升级，引入了新的全局响应归一化（GRN）层，以增强通道间特征的竞争性。<br>
                    效果：新模型系列ConvNeXt V2显著提升了纯ConvNets在ImageNet分类、ADE20K分割和COCO检测等不同识别基准上的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt models, have demonstrated strong performance across different application scenarios. Like many other architectures, ConvNeXt models were designed under the supervised learning setting with ImageNet labels. It is natural to expect ConvNeXt can also benefit from state-of-the-art self-supervised learning frameworks such as masked autoencoders (MAE), which was originally designed with Transformers. However, we show that simply combining the two designs yields subpar performance. In this paper, we develop an efficient and fully-convolutional masked autoencoder framework. We then upgrade the ConvNeXt architecture with a new Global Response Normalization (GRN) layer. GRN enhances inter-channel feature competition and is crucial for pre-training with masked input. The new model family, dubbed ConvNeXt V2, is a complete training recipe that synergizes both the architectural improvement and the advancement in self-supervised learning. With ConvNeXt V2, we are able to significantly advance pure ConvNets' performance across different recognition benchmarks including ImageNet classification, ADE20K segmentation and COCO detection. To accommodate different use cases, we provide pre-trained ConvNeXt V2 models of a wide range of complexity: from an efficient 3.7M-parameter Atto model that achieves 76.8% top-1 accuracy on ImageNet, to a 650M Huge model that can reach a state-of-the-art 88.9% accuracy using public training data only.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1243.Evolved Part Masking for Self-Supervised Learning</span><br>
                <span class="as">Feng, ZhanzhouandZhang, Shiliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Evolved_Part_Masking_for_Self-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10386-10395.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的掩蔽图像建模方法采用固定的掩蔽模式来引导自监督训练，这限制了视觉线索的建模能力。<br>
                    动机：本文提出了一种改进的部分基于掩蔽的方法，以在自监督学习中追求更通用的视觉线索建模。<br>
                    方法：该方法基于一个自适应部分分割模块，利用正在训练的视觉模型构建一个部分图，并通过图切割对部分进行划分。划分后的部分的准确性与预训练模型的能力相当，从而在不同的训练阶段产生进化的掩蔽模式。<br>
                    效果：实验结果表明，该方法在各种任务上都有显著的性能提升，包括图像分类、目标检测和语义分割。例如，在相同的训练周期下，它在ImageNet-1K分类和ADE20K分割上分别比最近的MAE高出0.69%和1.61%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing Masked Image Modeling methods apply fixed mask patterns to guide the self-supervised training. As those patterns resort to different criteria to mask local regions, sticking to a fixed pattern leads to limited vision cues modeling capability. This paper proposes an evolved part-based masking to pursue more general visual cues modeling in self-supervised learning. Our method is based on an adaptive part partition module, which leverages the vision model being trained to construct a part graph, and partitions parts with graph cut. The accuracy of partitioned parts is on par with the capability of the pre-trained model, leading to evolved mask patterns at different training stages. It generates simple patterns at the initial training stage to learn low-level visual cues, which hence evolves to eliminate accurate object parts to reinforce the learning of object semantics and contexts. Our method does not require extra pre-trained models or annotations, and effectively ensures the training efficiency by evolving the training difficulty. Experiment results show that it substantially boosts the performance on various tasks including image classification, object detection, and semantic segmentation. For example, it outperforms the recent MAE by 0.69% on imageNet-1K classification and 1.61% on ADE20K segmentation with the same training epochs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1244.Learning Attention As Disentangler for Compositional Zero-Shot Learning</span><br>
                <span class="as">Hao, ShaozheandHan, KaiandWong, Kwan-YeeK.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hao_Learning_Attention_As_Disentangler_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15315-15324.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过学习已见组合中的属性和对象，并将概念知识结合到未见组合中，实现零样本学习（CZSL）。<br>
                    动机：现有的CZSL方法需要学习属性-对象的解耦，但效果并不理想。<br>
                    方法：提出利用交叉注意力作为组合解耦器来学习解耦的概念嵌入。并通过在注意力级别应用正则化，进一步约束解耦器学习感兴趣的概念。<br>
                    效果：在三个CZSL基准数据集上的实验表明，该方法在封闭世界和开放世界中的表现均显著优于先前的工作，建立了新的最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compositional zero-shot learning (CZSL) aims at learning visual concepts (i.e., attributes and objects) from seen compositions and combining concept knowledge into unseen compositions. The key to CZSL is learning the disentanglement of the attribute-object composition. To this end, we propose to exploit cross-attentions as compositional disentanglers to learn disentangled concept embeddings. For example, if we want to recognize an unseen composition "yellow flower", we can learn the attribute concept "yellow" and object concept "flower" from different yellow objects and different flowers respectively. To further constrain the disentanglers to learn the concept of interest, we employ a regularization at the attention level. Specifically, we adapt the earth mover's distance (EMD) as a feature similarity metric in the cross-attention module. Moreover, benefiting from concept disentanglement, we improve the inference process and tune the prediction score by combining multiple concept probabilities. Comprehensive experiments on three CZSL benchmark datasets demonstrate that our method significantly outperforms previous works in both closed- and open-world settings, establishing a new state-of-the-art. Project page: https://haoosz.github.io/ade-czsl/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1245.GeneCIS: A Benchmark for General Conditional Image Similarity</span><br>
                <span class="as">Vaze, SagarandCarion, NicolasandMisra, Ishan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vaze_GeneCIS_A_Benchmark_for_General_Conditional_Image_Similarity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6862-6872.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的语言表示模型如何更好地利用结构化知识，提升语言理解能力。<br>
                    动机：预训练的语言模型如BERT在大规模语料库上表现优秀，但缺乏对知识图谱中结构化知识的利用。<br>
                    方法：提出ERNIE模型，结合大规模文本语料库和知识图谱进行联合训练，以充分利用词汇、句法和知识信息。<br>
                    效果：实验结果显示，ERNIE在各种知识驱动任务上取得了显著改进，并在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We argue that there are many notions of 'similarity' and that models, like humans, should be able to adapt to these dynamically. This contrasts with most representation learning methods, supervised or self-supervised, which learn a fixed embedding function and hence implicitly assume a single notion of similarity. For instance, models trained on ImageNet are biased towards object categories, while a user might prefer the model to focus on colors, textures or specific elements in the scene. In this paper, we propose the GeneCIS ('genesis') benchmark, which measures models' ability to adapt to a range of similarity conditions. Extending prior work, our benchmark is designed for zero-shot evaluation only, and hence considers an open-set of similarity conditions. We find that baselines from powerful CLIP models struggle on GeneCIS and that performance on the benchmark is only weakly correlated with ImageNet accuracy, suggesting that simply scaling existing methods is not fruitful. We further propose a simple, scalable solution based on automatically mining information from existing image-caption datasets. We find our method offers a substantial boost over the baselines on GeneCIS, and further improves zero-shot performance on related image retrieval benchmarks. In fact, though evaluated zero-shot, our model surpasses state-of-the-art supervised models on MIT-States.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1246.Learning Semantic Relationship Among Instances for Image-Text Matching</span><br>
                <span class="as">Fu, ZherenandMao, ZhendongandSong, YanandZhang, Yongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15159-15168.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像-文本匹配任务中，如何捕捉样本间和模态间的实例级交互关系，以获取更好的整体嵌入表示。<br>
                    动机：现有的图像-文本匹配方法主要关注片段级别的关系，如图像的显著区域或文本中的单词，而对样本间和模态间的实例级交互关系关注不足。<br>
                    方法：提出一种新颖的分层关系建模框架（HREM），明确捕捉片段级和实例级的关系，以学习区分性和鲁棒的跨模态嵌入。<br>
                    效果：在Flickr30K和MS-COCO数据集上的大量实验表明，该方法在rSum指标上比最先进的方法提高了4%-10%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-text matching, a bridge connecting image and language, is an important task, which generally learns a holistic cross-modal embedding to achieve a high-quality semantic alignment between the two modalities. However, previous studies only focus on capturing fragment-level relation within a sample from a particular modality, e.g., salient regions in an image or text words in a sentence, where they usually pay less attention to capturing instance-level interactions among samples and modalities, e.g., multiple images and texts. In this paper, we argue that sample relations could help learn subtle differences for hard negative instances, and thus transfer shared knowledge for infrequent samples should be promising in obtaining better holistic embeddings. Therefore, we propose a novel hierarchical relation modeling framework (HREM), which explicitly capture both fragment- and instance-level relations to learn discriminative and robust cross-modal embeddings. Extensive experiments on Flickr30K and MS-COCO show our proposed method outperforms the state-of-the-art ones by 4%-10% in terms of rSum.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1247.Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</span><br>
                <span class="as">Schramowski, PatrickandBrack, ManuelandDeiseroth, Bj\&quot;ornandKersting, Kristian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22522-22531.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决文本条件图像生成模型在训练过程中可能产生的退化和偏见问题。<br>
                    动机：由于这些模型高度依赖从互联网随机抓取的大量数据进行训练，因此可能会产生退化和偏见的人为行为，甚至可能加剧这种偏见。<br>
                    方法：为了解决这个问题，作者提出了安全潜在扩散（SLD）方法，通过建立一个新的图像生成测试平台——不适当的图像提示（I2P），来测量未经过滤和不平衡的训练集导致的不适当退化。<br>
                    效果：实验结果表明，引入的SLD能够在扩散过程中去除和抑制不适当的图像部分，无需额外的训练，对整体图像质量和文本对齐没有负面影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed - inappropriate image prompts (I2P) - containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1248.Visual Prompt Tuning for Generative Transfer Learning</span><br>
                <span class="as">Sohn, KihyukandChang, HuiwenandLezama, Jos\&#x27;eandPolania, LuisaandZhang, HanandHao, YuanandEssa, IrfanandJiang, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19840-19851.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从不同领域学习生成图像模型？<br>
                    动机：需要通过在大型数据集上训练的图像合成模型转移知识来学习视觉转换器。<br>
                    方法：采用基于生成视觉转换器的框架，将图像表示为具有自回归或非自回归转换器的一系列视觉标记。为了适应新领域，我们使用提示调优，将可学习的提示（称为提示）添加到图像标记序列的开头，并为任务引入新的提示设计。<br>
                    效果：我们在各种视觉领域中进行了研究，结果显示了知识转移的有效性和明显更好的图像生成质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning generative image models from various domains efficiently needs transferring knowledge from an image synthesis model trained on a large dataset. We present a recipe for learning vision transformers by generative knowledge transfer. We base our framework on generative vision transformers representing an image as a sequence of visual tokens with the autoregressive or non-autoregressive transformers. To adapt to a new domain, we employ prompt tuning, which prepends learnable tokens called prompts to the image token sequence and introduces a new prompt design for our task. We study on a variety of visual domains with varying amounts of training images. We show the effectiveness of knowledge transfer and a significantly better image generation quality. Code is available at https://github.com/google-research/generative_transfer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1249.OmniMAE: Single Model Masked Pretraining on Images and Videos</span><br>
                <span class="as">Girdhar, RohitandEl-Nouby, AlaaeldinandSingh, MannatandAlwala, KalyanVasudevandJoulin, ArmandandMisra, Ishan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10406-10417.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个单一的模型，使其能够处理多种视觉模态（如图像和视频）的任务？<br>
                    动机：现有的方法通常为每种视觉模态分别设计模型，或者使用针对视觉任务定制的架构，导致性能不如单一模态模型。<br>
                    方法：本文提出使用遮蔽自动编码来训练一个简单的视觉Transformer，无需任何标注数据。这种单一模型可以在图像和视频基准测试上学习出与或优于单一模态表示的视觉表示。<br>
                    效果：实验结果表明，该模型在ImageNet上的准确率可以达到86.6%，在具有挑战性的Something Something-v2视频基准测试上可以达到75.5%，创造了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Huge model can be finetuned to achieve 86.6% on ImageNet and 75.5% on the challenging Something Something-v2 video benchmark, setting a new state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1250.Visual Atoms: Pre-Training Vision Transformers With Sinusoidal Waves</span><br>
                <span class="as">Takashima, SoraandHayamizu, RyoandInoue, NakamasaandKataoka, HirokatsuandYokota, Rio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takashima_Visual_Atoms_Pre-Training_Vision_Transformers_With_Sinusoidal_Waves_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18579-18588.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在解决预训练视觉转换器时，轮廓导向的合成数据集为何能达到真实数据集相同的准确度的问题。<br>
                    动机：尽管已有研究表明轮廓导向的合成数据集ExFractalDB-21k在预训练视觉转换器上的效果超过了ImageNet-21k，但关于其设计空间的系统性研究尚未进行，因此存在很大的质疑空间。<br>
                    方法：本研究基于循环谐波开发了一种新的方法论，用于系统地调查轮廓导向的合成数据集的设计空间。通过这种方法，我们能够有效地搜索最优的FDSL参数范围，并最大化数据集中合成图像的多样性。<br>
                    效果：使用新开发的VisualAtom-21k数据集预训练ViT-Base模型后，微调ImageNet-1k数据集时，模型的top-1准确率达到了83.7%，与JFT-300M预训练模型达到的84.2%的top-1准确率仅有0.5%的差距。此外，与静态的JFT-300M数据集不同，合成数据集的质量将不断提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. In the present work, we develop a novel methodology based on circular harmonics for systematically investigating the design space of contour-oriented synthetic datasets. This allows us to efficiently search the optimal range of FDSL parameters and maximize the variety of synthetic images in the dataset, which we found to be a critical factor. When the resulting new dataset VisualAtom-21k is used for pre-training ViT-Base, the top-1 accuracy reached 83.7% when fine-tuning on ImageNet-1k. This is only 0.5% difference from the top-1 accuracy (84.2%) achieved by the JFT-300M pre-training, even though the scale of images is 1/14. Unlike JFT-300M which is a static dataset, the quality of synthetic datasets will continue to improve, and the current work is a testament to this possibility. FDSL is also free of the common issues associated with real images, e.g. privacy/copyright issues, labeling costs/errors, and ethical biases.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1251.Masked Autoencoding Does Not Help Natural Language Supervision at Scale</span><br>
                <span class="as">Weers, FlorisandShankar, VaishaalandKatharopoulos, AngelosandYang, YinfeiandGunter, Tom</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weers_Masked_Autoencoding_Does_Not_Help_Natural_Language_Supervision_at_Scale_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23432-23444.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在研究自我监督和自然语言监督在大规模图像-文本训练中的有效性。<br>
                    动机：尽管已有研究如M3AE和SLIP提出这两种方法可以有效结合，但他们的结果主要基于小数据集（<20M），并未充分反映常用的大数据集（>100M）情况。<br>
                    方法：本研究采用最新的两种方法：掩码自动编码器MAE和对比性语言图像预训练CLIP进行联合训练，并在两个不同规模的数据集上进行实验。<br>
                    效果：结果显示，当在11.3M的图像-文本对数据集上训练时，这种方法比单独使用CLIP有所改进；但在1.4B的图像数据集上训练时，其效果与单独使用CLIP相当。这为大规模图像-文本训练的自我监督提供了一些必要的清晰度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE (Geng et al 2022) and SLIP (Mu et al 2022) have suggested that these approaches can be effectively combined, but most notably their results use small (<20M examples) pre-training datasets and don't effectively reflect the large-scale regime (>100M samples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE (He et al 2021) and contrastive language image pre-training, CLIP (Radford et al 2021) provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1252.Doubly Right Object Recognition: A Why Prompt for Visual Rationales</span><br>
                <span class="as">Mao, ChengzhiandTeotia, RevantandSundar, AmruthaandMenon, SachitandYang, JunfengandWang, XinandVondrick, Carl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mao_Doubly_Right_Object_Recognition_A_Why_Prompt_for_Visual_Rationales_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2722-2732.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探讨计算机视觉模型是否能为其预测提供正确的理由。<br>
                    动机：目前的视觉识别模型仅以分类准确率为评估标准，而忽略了其预测理由的正确性。<br>
                    方法：提出“双重正确”的对象识别基准，要求模型同时产生正确的标签和理由。通过定制的数据集将语言模型中的理由转化为视觉表示，学习一个“为什么提示”，使大型视觉表示能够产生正确的理由。<br>
                    效果：实验结果表明，我们的提示不仅在双重正确对象识别上显著提高了性能，还在未见过的目标任务和数据集上的零样本转移上取得了良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many visual recognition models are evaluated only on their classification accuracy, a metric for which they obtain strong performance. In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions. We propose a "doubly right" object recognition benchmark, where the metric requires the model to simultaneously produce both the right labels as well as the right rationales. We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions. However, by transferring the rationales from language models into visual representations through a tailored dataset, we show that we can learn a "why prompt," which adapts large visual representations to produce correct rationales. Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1253.GLIGEN: Open-Set Grounded Text-to-Image Generation</span><br>
                <span class="as">Li, YuhengandLiu, HaotianandWu, QingyangandMu, FangzhouandYang, JianweiandGao, JianfengandLi, ChunyuanandLee, YongJae</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22511-22521.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有文本到图像扩散模型只能使用文本输入，导致可控性差的问题。<br>
                    动机：现有的预训练文本到图像扩散模型仅使用文本输入，限制了其可控性。<br>
                    方法：提出GLIGEN模型，通过在已有的预训练文本到图像扩散模型基础上增加条件输入层，实现基于场景的文本到图像生成。<br>
                    效果：GLIGEN模型实现了基于场景的文本到图像生成，并在COCO和LVIS数据集上取得了超越现有有监督布局到图像基线模型的优秀性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN: Open-Set Grounded Text-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1254.Q: How To Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!</span><br>
                <span class="as">Khan, ZaidandBG, VijayKumarandSchulter, SamuelandYu, XiangandFu, YunandChandraker, Manmohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Q_How_To_Specialize_Large_Vision-Language_Models_to_Data-Scarce_VQA_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15005-15015.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对大规模视觉语言模型进行微调，以解决专业任务或非自然图像领域的视觉问答问题。<br>
                    动机：针对特定任务或领域的数据集规模远小于通用的视觉问答数据集，而收集额外的标签又具有挑战性，但未标注的图片却很常见。<br>
                    方法：提出SelTDA（自我教学数据增强）策略，通过视觉语言模型和目标数据集构建一个教师模型，该模型可以直接根据图片生成问题-答案伪标签，从而为未标注的图片生成伪标签。然后，在原始数据集上对初始的视觉语言模型进行微调，使用新生成的伪标签进行数据增强。<br>
                    效果：实验表明，这种自我教学的数据增强方法可以提高模型对抗搜索问题、反事实示例和重新表述的鲁棒性，提高领域泛化能力，并更好地保留数值推理能力。这种方法不需要额外的注释或结构修改，并且与任何现代的编码器-解码器多模态转换器兼容。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples, and rephrasings, it improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1255.CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset</span><br>
                <span class="as">Gan, TianandWang, QingandDong, XingningandRen, XiangyuanandNie, LiqiangandGuo, Qingpei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14815-14824.png><br>
            
            <span class="tt"><span class="t0">研究问题：中文视频-文本预训练的研究受限于缺乏大规模的公共数据集和基准。<br>
                    动机：现有的大规模英文视频-文本数据集无法满足中文视频-文本预训练的需求，而现有的中文预训练模型又基于私有数据集，限制了其研究和实际应用。<br>
                    方法：构建了一个包含超过350万对中文视频-文本的公共跨模态数据集CNVid-3.5M，并通过过滤弱配对的视频提高了数据质量。同时，使用三种主流的像素级预训练架构进行基准测试，并提出了硬样本课程学习策略来提升预训练性能。<br>
                    效果：CNVid-3.5M是迄今为止最大的中文视频-文本公共数据集，为中文视频-文本预训练提供了首个像素级基准测试。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Owing to well-designed large-scale video-text datasets, recent years have witnessed tremendous progress in video-text pre-training. However, existing large-scale video-text datasets are mostly English-only. Though there are certain methods studying the Chinese video-text pre-training, they pre-train their models on private datasets whose videos and text are unavailable. This lack of large-scale public datasets and benchmarks in Chinese hampers the research and downstream applications of Chinese video-text pre-training. Towards this end, we release and benchmark CNVid-3.5M, a large-scale public cross-modal dataset containing over 3.5M Chinese video-text pairs. We summarize our contributions by three verbs, i.e., "Build", "Filter", and "Pre-train": 1) To build a public Chinese video-text dataset, we collect over 4.5M videos from the Chinese websites. 2) To improve the data quality, we propose a novel method to filter out 1M weakly-paired videos, resulting in the CNVid-3.5M dataset. And 3) we benchmark CNVid-3.5M with three mainstream pixel-level pre-training architectures. At last, we propose the Hard Sample Curriculum Learning strategy to promote the pre-training performance. To the best of our knowledge, CNVid-3.5M is the largest public video-text dataset in Chinese, and we provide the first pixel-level benchmarks for Chinese video-text pre-training. The dataset, codebase, and pre-trained models are available at https://github.com/CNVid/CNVid-3.5M.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1256.Efficient Multimodal Fusion via Interactive Prompting</span><br>
                <span class="as">Li, YaoweiandQuan, RuijieandZhu, LinchaoandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_Multimodal_Fusion_via_Interactive_Prompting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2604-2613.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低大规模预训练多模态学习模型的计算成本，并提高其效率和灵活性。<br>
                    动机：随着多模态学习模型规模的增大，其下游任务的微调计算成本也随之增加，急需一种有效且灵活的方法来降低这一成本。<br>
                    方法：提出一种名为PMF的高效灵活的多模态融合方法，该方法针对单模态预训练的转换器进行优化。具体包括构建一个模块化的多模态融合框架，以增强不同模态之间的交互性；将普通提示分为三种类型，以便为多模态学习学习不同的优化目标；仅在单模态转换器的深层添加提示向量，从而显著减少训练内存使用。<br>
                    效果：实验结果表明，该方法在性能上与其它几种多模态微调方法相当，但其可训练参数少于3%，训练内存使用最多可节省66%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale pre-training has brought unimodal fields such as computer vision and natural language processing to a new era. Following this trend, the size of multimodal learning models constantly increases, leading to an urgent need to reduce the massive computational cost of fine-tuning these models for downstream tasks. In this paper, we propose an efficient and flexible multimodal fusion method, namely PMF, tailored for fusing unimodally pretrained transformers. Specifically, we first present a modular multimodal fusion framework that exhibits high flexibility and facilitates mutual interactions among different modalities. In addition, we disentangle vanilla prompts into three types in order to learn different optimizing objectives for multimodal learning. It is also worth noting that we propose to add prompt vectors only on the deep layers of the unimodal transformers, thus significantly reducing the training memory usage. Experiment results show that our proposed method achieves comparable performance to several other multimodal finetuning methods with less than 3% trainable parameters and up to 66% saving of training memory usage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1257.Language Adaptive Weight Generation for Multi-Task Visual Grounding</span><br>
                <span class="as">Su, WeiandMiao, PeihanandDou, HuanzhangandWang, GaoangandQiao, LiangandLi, ZheyangandLi, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10857-10866.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉基础模型在视觉定位任务中的表现？<br>
                    动机：目前的视觉基础模型通常被动地提取特征，这可能导致特征匹配的缺失和冗余，限制了性能的进一步提高。<br>
                    方法：提出一种基于语言适应权重的主动感知视觉定位框架（VG-LAW），使视觉基础模型能够根据不同的表达动态生成权重，作为特定于表达的特征提取器。<br>
                    效果：实验证明，VG-LAW无需额外的跨模态交互模块，就能有效地提取特定且相关的视觉特征，并在四个代表性数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although the impressive performance in visual grounding, the prevailing approaches usually exploit the visual backbone in a passive way, i.e., the visual backbone extracts features with fixed weights without expression-related hints. The passive perception may lead to mismatches (e.g., redundant and missing), limiting further performance improvement. Ideally, the visual backbone should actively extract visual features since the expressions already provide the blueprint of desired visual features. The active perception can take expressions as priors to extract relevant visual features, which can effectively alleviate the mismatches. Inspired by this, we propose an active perception Visual Grounding framework based on Language Adaptive Weights, called VG-LAW. The visual backbone serves as an expression-specific feature extractor through dynamic weights generated for various expressions. Benefiting from the specific and relevant visual features extracted from the language-aware visual backbone, VG-LAW does not require additional modules for cross-modal interaction. Along with a neat multi-task head, VG-LAW can be competent in referring expression comprehension and segmentation jointly. Extensive experiments on four representative datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame, validate the effectiveness of the proposed framework and demonstrate state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1258.Indescribable Multi-Modal Spatial Evaluator</span><br>
                <span class="as">Kong, LingkeandQi, X.SharonandShen, QijinandWang, JiachengandZhang, JingyiandHu, YanleandZhou, Qichao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Indescribable_Multi-Modal_Spatial_Evaluator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9853-9862.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态图像配准的主要挑战之一是来自不同成像机器的图像具有不同的成像分布，这使得难以仅关注图像的空间方面并忽略分布差异。<br>
                    动机：为了解决这个问题，我们开发了一种自我监督的方法——无法形容的多模型空间评估器（IMSE），用于处理多模态图像配准。<br>
                    方法：IMSE创建了一个精确的多模态空间评估器来测量两幅图像之间的空间差异，并通过最小化评估器预测的错误来优化配准。为了优化IMSE的性能，我们还提出了一种新的样式增强方法——洗牌重映射，该方法将图像分布随机分为多个段，然后随机打乱和重新映射这些段，从而改变原始图像的分布。<br>
                    效果：实验结果表明，IMSE在T1-T2和CT-MRI数据集上的配准性能优于现有方法。IMSE还可以轻松集成到传统的配准过程中，并提供一种方便的方式来评估和可视化配准结果。此外，IMSE还有潜力成为一种新的图像到图像转换范式。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-modal image registration spatially aligns two images with different distributions. One of its major challenges is that images acquired from different imaging machines have different imaging distributions, making it difficult to focus only on the spatial aspect of the images and ignore differences in distributions. In this study, we developed a self-supervised approach, Indescribable Multi-model Spatial Evaluator (IMSE), to address multi-modal image registration. IMSE creates an accurate multi-modal spatial evaluator to measure spatial differences between two images, and then optimizes registration by minimizing the error predicted of the evaluator. To optimize IMSE performance, we also proposed a new style enhancement method called Shuffle Remap which randomizes the image distribution into multiple segments, and then randomly disorders and remaps these segments, so that the distribution of the original image is changed. Shuffle Remap can help IMSE to predict the difference in spatial location from unseen target distributions. Our results show that IMSE outperformed the existing methods for registration using T1-T2 and CT-MRI datasets. IMSE also can be easily integrated into the traditional registration process, and can provide a convenient way to evaluate and visualize registration results. IMSE also has the potential to be used as a new paradigm for image-to-image translation. Our code is available at https://github.com/Kid-Liet/IMSE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1259.ImageBind: One Embedding Space To Bind Them All</span><br>
                <span class="as">Girdhar, RohitandEl-Nouby, AlaaeldinandLiu, ZhuangandSingh, MannatandAlwala, KalyanVasudevandJoulin, ArmandandMisra, Ishan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15180-15190.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何学习跨六种不同模态（图像、文本、音频、深度、热和IMU数据）的联合嵌入？<br>
                    动机：现有的方法需要所有成对的数据来训练这样的联合嵌入，而我们的方法只需要图像配对数据。<br>
                    方法：我们提出了ImageBind方法，该方法利用了最新的大规模视觉语言模型，并通过使用它们与图像的自然配对，将其零样本能力扩展到新的模态。<br>
                    效果：实验结果表明，ImageBind在跨模态检索、用算术编写模态、跨模态检测和生成等新兴应用程序方面具有强大的涌现能力，并在跨模态的零样本识别任务上超越了专门的有监督模型。此外，我们还展示了强大的少数样本识别结果，证明ImageBind是评估视觉和非视觉任务的新方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1260.On Data Scaling in Masked Image Modeling</span><br>
                <span class="as">Xie, ZhendaandZhang, ZhengandCao, YueandLin, YutongandWei, YixuanandDai, QiandHu, Han</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_On_Data_Scaling_in_Masked_Image_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10365-10374.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过大量实验，系统地研究了掩蔽图像建模（MIM）的扩展性，以打破其对大规模数据无法受益的偏见。<br>
                    动机：尽管预训练语言模型的成功鼓励了大规模自我监督预训练语言模型的发展，并赋予了它们显著的建模能力，但扩展性似乎在最近的研究中被无意忽视。<br>
                    方法：通过从10% ImageNet-1K到完整的ImageNet-22K的数据范围，从4900万到十亿的模型参数范围，以及从125K到500K次的训练长度范围进行广泛的实验，来系统地研究MIM的扩展行为。<br>
                    效果：主要发现可以总结为两点：1) 为了扩大计算和模型参数，MIM仍然需要大量的数据；2) 在非过拟合的情况下，MIM不能从更多的数据中受益，这与先前在自我监督预训练语言模型或监督预训练视觉模型中的观察结果不同。此外，还揭示了MIM的几个有趣特性，如大型MIM模型的高样本效率和预训练验证损失与转移性能之间的强相关性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scaling properties have been one of the central issues in self-supervised pre-training, especially the data scalability, which has successfully motivated the large-scale self-supervised pre-trained language models and endowed them with significant modeling capabilities. However, scaling properties seem to be unintentionally neglected in the recent trending studies on masked image modeling (MIM), and some arguments even suggest that MIM cannot benefit from large-scale data. In this work, we try to break down these preconceptions and systematically study the scaling behaviors of MIM through extensive experiments, with data ranging from 10% of ImageNet-1K to full ImageNet-22K, model parameters ranging from 49-million to one-billion, and training length ranging from 125K to 500K iterations. And our main findings can be summarized in two folds: 1) masked image modeling remains demanding large-scale data in order to scale up computes and model parameters; 2) masked image modeling cannot benefit from more data under a non-overfitting scenario, which diverges from the previous observations in self-supervised pre-trained language models or supervised pre-trained vision models. In addition, we reveal several intriguing properties in MIM, such as high sample efficiency in large MIM models and strong correlation between pre-training validation loss and transfer performance. We hope that our findings could deepen the understanding of masked image modeling and facilitate future developments on large-scale vision models. Code and models will be available at https://github.com/microsoft/SimMIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1261.Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding</span><br>
                <span class="as">Shaharabany, TalandWolf, Lior</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6925-6934.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过考虑从模型图像编码器的潜表示中提取的自相似性图来优化短语接地模型。<br>
                    动机：现有的短语接地模型在弱监督短语接地任务上的性能与最新技术存在较大差距，且在无文本输入的WWbL任务上也存在类似的问题。<br>
                    方法：提出一种有效的方法，通过结合从模型图像编码器的潜表示中提取的自相似性图来优化短语接地模型，从而获得有用的伪标签进行自我训练。<br>
                    效果：实验结果表明，该方法在弱监督短语接地任务和无文本输入的WWbL任务上的性能均大幅超过现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A phrase grounding model receives an input image and a text phrase and outputs a suitable localization map. We present an effective way to refine a phrase ground model by considering self-similarity maps extracted from the latent representation of the model's image encoder. Our main insights are that these maps resemble localization maps and that by combining such maps, one can obtain useful pseudo-labels for performing self-training. Our results surpass, by a large margin, the state-of-the-art in weakly supervised phrase grounding. A similar gap in performance is obtained for a recently proposed downstream task called WWbL, in which the input image is given without any text. Our code is available as supplementary.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1262.Turning a CLIP Model Into a Scene Text Detector</span><br>
                <span class="as">Yu, WenwenandLiu, YuliangandHua, WeiandJiang, DeqiangandRen, BoandBai, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Turning_a_CLIP_Model_Into_a_Scene_Text_Detector_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6978-6988.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的方法，即TCM，将预训练的CLIP模型直接用于文本检测，而无需进行预训练过程。<br>
                    动机：现有的基于视觉语言模型的预训练方法在文本检测领域取得了有效进展。与这些工作相比，本文提出了一种新的方法，即TCM，重点关注直接利用CLIP模型进行文本检测。<br>
                    方法：通过将CLIP模型转化为现有的场景文本检测方法，实现对现有场景文本检测器的改进。该方法具有少量样本训练能力，例如使用10%的标注数据即可显著提高基线方法的性能。<br>
                    效果：实验结果表明，该方法在4个基准测试上的平均F-measure性能提高了22%。此外，通过将CLIP模型转化为现有的场景文本检测方法，还实现了有前景的领域适应能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent large-scale Contrastive Language-Image Pretraining (CLIP) model has shown great potential in various downstream tasks via leveraging the pretrained vision and language knowledge. Scene text, which contains rich textual and visual information, has an inherent connection with a model like CLIP. Recently, pretraining approaches based on vision language models have made effective progresses in the field of text detection. In contrast to these works, this paper proposes a new method, termed TCM, focusing on Turning the CLIP Model directly for text detection without pretraining process. We demonstrate the advantages of the proposed TCM as follows: (1) The underlying principle of our framework can be applied to improve existing scene text detector. (2) It facilitates the few-shot training capability of existing methods, e.g., by using 10% of labeled data, we significantly improve the performance of the baseline method with an average of 22% in terms of the F-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene text detection methods, we further achieve promising domain adaptation ability. The code will be publicly released at https://github.com/wenwenyu/TCM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1263.Masked Video Distillation: Rethinking Masked Feature Modeling for Self-Supervised Video Representation Learning</span><br>
                <span class="as">Wang, RuiandChen, DongdongandWu, ZuxuanandChen, YinpengandDai, XiyangandLiu, MengchenandYuan, LuandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6312-6322.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的视频表征学习方法主要通过重建低层次特征如原始像素值来从头开始学习表示的问题。<br>
                    动机：虽然受益于被遮罩的视觉建模，自我监督的视频表征学习已经取得了显著的进步，但现有的方法主要集中在从零开始通过重建被遮罩的片段的低层次特征来学习表示。<br>
                    方法：本文提出了一种简单而有效的两阶段被遮罩的特征模型框架——被遮罩的视频蒸馏（MVD）。首先，我们通过恢复被遮罩的片段的低层次特征来预训练一个图像（或视频）模型，然后我们将得到的特征作为目标进行被遮罩的特征建模。<br>
                    效果：实验结果表明，使用空间-时间共教方法预训练的视频转换器在多个视频数据集上优于使用单一教师模型蒸馏的学生模型。我们的MVD与普通的ViT相比，在几个具有挑战性的视频下游任务上达到了最先进的性能。例如，使用ViT-Large模型，我们的MVD在Kinetics-400和Something-Something-v2上分别实现了86.4%和76.7%的Top-1准确率，比VideoMAE高出1.2%和2.4%。当采用更大的ViT-Huge模型时，MVD在Something-Something-v2上实现了77.3%的Top-1准确率，达到了最先进的性能。代码将在https://github.com/ruiwang2021/mvd上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Benefiting from masked visual modeling, self-supervised video representation learning has achieved remarkable progress. However, existing methods focus on learning representations from scratch through reconstructing low-level features like raw pixel values. In this paper, we propose masked video distillation (MVD), a simple yet effective two-stage masked feature modeling framework for video representation learning: firstly we pretrain an image (or video) model by recovering low-level features of masked patches, then we use the resulting features as targets for masked feature modeling. For the choice of teacher models, we observe that students taught by video teachers perform better on temporally-heavy video tasks, while image teachers transfer stronger spatial representations for spatially-heavy video tasks. Visualization analysis also indicates different teachers produce different learned patterns for students. To leverage the advantage of different teachers, we design a spatial-temporal co-teaching method for MVD. Specifically, we distill student models from both video teachers and image teachers by masked feature modeling. Extensive experimental results demonstrate that video transformers pretrained with spatial-temporal co-teaching outperform models distilled with a single teacher on a multitude of video datasets. Our MVD with vanilla ViT achieves state-of-the-art performance compared with previous methods on several challenging video downstream tasks. For example, with the ViT-Large model, our MVD achieves 86.4% and 76.7% Top-1 accuracy on Kinetics-400 and Something-Something-v2, outperforming VideoMAE by 1.2% and 2.4% respectively. When a larger ViT-Huge model is adopted, MVD achieves the state-of-the-art performance with 77.3% Top-1 accuracy on Something-Something-v2. Code will be available at https://github.com/ruiwang2021/mvd.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1264.RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving</span><br>
                <span class="as">Ando, AngelikaandGidaris, SpyrosandBursuc, AndreiandPuy, GillesandBoulch, AlexandreandMarlet, Renaud</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5240-5250.png><br>
            
            <span class="tt"><span class="t0">研究问题：能否通过最新的视觉转换器（ViTs）改进3D语义分割的投影方法？<br>
                    动机：尽管现有的投影方法在户外LiDAR点云的语义分割上取得了不错的效果，但最新的计算机视觉研究表明，视觉转换器（ViTs）在许多基于图像的基准测试中已经达到了最先进的结果。<br>
                    方法：我们提出了一种名为RangeViT的方法，该方法将ViTs与三个关键元素相结合：（a）保留与RGB图像相同的主干架构，以利用大型图像集合的知识；（b）用定制的卷积基替换经典的线性嵌入层，以补偿ViTs缺乏的归纳偏置；（c）使用卷积解码器和从卷积基到ViT编码器的跳跃连接来细化像素级预测。<br>
                    效果：实验结果表明，RangeViT在nuScenes和SemanticKITTI上的表现优于现有的投影方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much cheaper to acquire and annotate than point clouds. We reach our best results with pre-trained ViTs on large image datasets. (b) We compensate ViTs' lack of inductive bias by substituting a tailored convolutional stem for the classical linear embedding layer. (c) We refine pixel-wise predictions with a convolutional decoder and a skip connection from the convolutional stem to combine low-level but fine-grained features of the the convolutional stem with the high-level but coarse predictions of the ViT encoder. With these ingredients, we show that our method, called RangeViT, outperforms existing projection-based methods on nuScenes and SemanticKITTI. The code is available at https://github.com/valeoai/rangevit.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1265.VQACL: A Novel Visual Question Answering Continual Learning Setting</span><br>
                <span class="as">Zhang, XiandZhang, FeifeiandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19102-19112.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决多模态任务如视觉问答（VQA）的持续学习问题。<br>
                    动机：尽管在单模态领域的持续学习研究已经取得了很多成果，但在多模态任务如VQA方面的研究却鲜有关注。<br>
                    方法：本文提出了一种新的VQA持续学习设置（VQACL），包括两个关键组件：一个嵌套了视觉和语言数据的双层级任务序列，以及一个新的组合测试，包含新的技能-概念组合。基于VQACL，我们对五种公认的持续学习方法进行了深入评估，并发现它们存在灾难性遗忘和弱泛化能力的问题。为解决这些问题，我们提出了一种新的表示学习方法，利用样本特定和样本不变的特征来学习具有判别性和泛化性的表示。<br>
                    效果：大量的实验结果表明，我们的方法显著优于现有的模型，证明了所提出方法的有效性和复合性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Research on continual learning has recently led to a variety of work in unimodal community, however little attention has been paid to multimodal tasks like visual question answering (VQA). In this paper, we establish a novel VQA Continual Learning setting named VQACL, which contains two key components: a dual-level task sequence where visual and linguistic data are nested, and a novel composition testing containing new skill-concept combinations. The former devotes to simulating the ever-changing multimodal datastream in real world and the latter aims at measuring models' generalizability for cognitive reasoning. Based on our VQACL, we perform in-depth evaluations of five well-established continual learning methods, and observe that they suffer from catastrophic forgetting and have weak generalizability. To address above issues, we propose a novel representation learning method, which leverages a sample-specific and a sample-invariant feature to learn representations that are both discriminative and generalizable for VQA. Furthermore, by respectively extracting such representation for visual and textual input, our method can explicitly disentangle the skill and concept. Extensive experimental results illustrate that our method significantly outperforms existing models, demonstrating the effectiveness and compositionality of the proposed approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1266.What Can Human Sketches Do for Object Detection?</span><br>
                <span class="as">Chowdhury, PinakiNathandBhunia, AyanKumarandSain, AneeshanandKoley, SubhadeepandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chowdhury_What_Can_Human_Sketches_Do_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15083-15094.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用人类草图的天生表现力进行对象检测？<br>
                    动机：目前，人类草图的表现力在图像检索中的应用已经得到了探索，但在基本视觉任务对象检测中的应用还尚未开发。<br>
                    方法：首次将人类草图的表现力用于对象检测的基本视觉任务中，形成了一种基于草图的对象检测框架。该模型可以在测试时无需知道预期的类别（零样本），也不需要额外的边界框（全监督）和类别标签（弱监督）。<br>
                    效果：在PASCAL-VOC和MS-COCO等标准对象检测数据集上，该框架在零样本设置上优于监督（SOD）和弱监督对象检测器（WSOD）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sketches are highly expressive, inherently capturing subjective and fine-grained visual cues. The exploration of such innate properties of human sketches has, however, been limited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches but for the fundamental vision task of object detection. The end result is a sketch-enabled object detection framework that detects based on what you sketch -- that "zebra" (e.g., one that is eating the grass) in a herd of zebras (instance-aware detection), and only the part (e.g., "head" of a "zebra") that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to expect at testing (zero-shot) and (ii) not requiring additional bounding boxes (as per fully supervised) and class labels (as per weakly supervised). Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), which can already elegantly solve the task -- CLIP to provide model generalisation, and SBIR to bridge the (sketch->photo) gap. In particular, we first perform independent prompting on both sketch and photo branches of an SBIR model to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from SBIR. Evaluating our framework on standard object detection datasets like PASCAL-VOC and MS-COCO outperforms both supervised (SOD) and weakly-supervised object detectors (WSOD) on zero-shot setups. Project Page: https://pinakinathc.github.io/sketch-detect</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1267.All Are Worth Words: A ViT Backbone for Diffusion Models</span><br>
                <span class="as">Bao, FanandNie, ShenandXue, KaiwenandCao, YueandLi, ChongxuanandSu, HangandZhu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22669-22679.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种基于视觉转换器的简单通用架构（命名为U-ViT），用于具有扩散模型的图像生成。<br>
                    动机：虽然视觉转换器在各种视觉任务中显示出潜力，但基于卷积神经网络的U-Net在扩散模型中仍然占主导地位。<br>
                    方法：将包括时间、条件和噪声图像补丁的所有输入都视为令牌，并在浅层和深层之间采用长跳跃连接，设计出一种简单的通用U-ViT架构。<br>
                    效果：在无条件和有条件图像生成以及文本到图像生成任务中评估U-ViT，其表现即使不优于，也至少与类似规模的CNN基U-Net相当。特别是在ImageNet 256x256上的有条件图像生成和MS-COCO上的文本到图像生成中，使用U-ViT的潜在扩散模型实现了FID分数为2.29和5.48的记录，这是在训练生成模型期间未访问大型外部数据集的方法中的最好成绩。这些结果表明，对于基于扩散的图像建模，长跳跃连接至关重要，而CNN基U-Net中的下采样和上采样操作并不总是必要的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1268.Sketch2Saliency: Learning To Detect Salient Objects From Human Drawings</span><br>
                <span class="as">Bhunia, AyanKumarandKoley, SubhadeepandKumar, AmandeepandSain, AneeshanandChowdhury, PinakiNathandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhunia_Sketch2Saliency_Learning_To_Detect_Salient_Objects_From_Human_Drawings_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2733-2743.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索草图在图像理解任务中的价值，特别是其突出性。<br>
                    动机：草图是一种自然的注意过程，具有突出性。作者希望研究如何利用草图作为弱标签来检测图像中的突出对象。<br>
                    方法：提出了一种新颖的方法，通过2D注意力机制生成序列草图坐标，以解释“突出对象”的草图。<br>
                    效果：大量的定量和定性实验证明了作者的假设，并表明了基于草图的突出性检测模型与最先进的技术相比具有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human sketch has already proved its worth in various visual understanding tasks (e.g., retrieval, segmentation, image-captioning, etc). In this paper, we reveal a new trait of sketches -- that they are also salient. This is intuitive as sketching is a natural attentive process at its core. More specifically, we aim to study how sketches can be used as a weak label to detect salient objects present in an image. To this end, we propose a novel method that emphasises on how "salient object" could be explained by hand-drawn sketches. To accomplish this, we introduce a photo-to-sketch generation model that aims to generate sequential sketch coordinates corresponding to a given visual photo through a 2D attention mechanism. Attention maps accumulated across the time steps give rise to salient regions in the process. Extensive quantitative and qualitative experiments prove our hypothesis and delineate how our sketch-based saliency detection model gives a competitive performance compared to the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1269.ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding</span><br>
                <span class="as">Xue, LeandGao, MingfeiandXing, ChenandMart{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1179-1189.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改善当前最先进的3D模型在少量标注数据和预定义类别集上的理解能力。<br>
                    动机：目前，通过语言等其他模态的知识可以显著缓解2D模型的类似问题。受此启发，利用多模态信息进行3D模态的学习可能在数据有限的情况下提高3D理解，但这方面的研究尚不充分。<br>
                    方法：提出ULIP，通过从三种模态的对象三元组中预训练来学习图像、语言和3D点云的统一表示。为了克服训练三元组不足的问题，ULIP利用预先训练好的视觉-语言模型，该模型已经通过大量的图像-文本对学习了共同的视觉和文本空间。然后，ULIP使用自动合成的少量三元组学习与共同的图像-文本空间对齐的3D表示空间。<br>
                    效果：实验表明，只需在ShapeNet55上使用我们的框架对多种最新的3D主干网络进行预训练，ULIP就能有效地提高其性能，在ModelNet40和ScanObjectNN的标准3D分类和零样本3D分类任务上都达到了最先进的性能。此外，ULIP还能使PointMLP在ScanObjectNN上的3D分类性能提高约3%，并在ModelNet40上的零样本3D分类任务上以28.8%的优势超过PointCLIP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recognition capabilities of current state-of-the-art 3D models are limited by datasets with a small number of annotated data and a pre-defined set of categories. In its 2D counterpart, recent advances have shown that similar problems can be significantly alleviated by employing knowledge from other modalities, such as language. Inspired by this, leveraging multimodal information for 3D modality could be promising to improve 3D understanding under the restricted data regime, but this line of research is not well studied. Therefore, we introduce ULIP to learn a unified representation of images, language, and 3D point clouds by pre-training with object triplets from the three modalities. To overcome the shortage of training triplets, ULIP leverages a pre-trained vision-language model that has already learned a common visual and textual space by training with massive image-text pairs. Then, ULIP learns a 3D representation space aligned with the common image-text space, using a small number of automatically synthesized triplets. ULIP is agnostic to 3D backbone networks and can easily be integrated into any 3D architecture. Experiments show that ULIP effectively improves the performance of multiple recent 3D backbones by simply pre-training them on ShapeNet55 using our framework, achieving state-of-the-art performance in both standard 3D classification and zero-shot 3D classification on ModelNet40 and ScanObjectNN. ULIP also improves the performance of PointMLP by around 3% in 3D classification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1 accuracy for zero-shot 3D classification on ModelNet40. Our code and pre-trained models are released at https://github.com/salesforce/ULIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1270.Being Comes From Not-Being: Open-Vocabulary Text-to-Motion Generation With Wordless Training</span><br>
                <span class="as">Lin, JunfanandChang, JianlongandLiu, LingboandLi, GuanbinandLin, LiangandTian, QiandChen, Chang-Wen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Being_Comes_From_Not-Being_Open-Vocabulary_Text-to-Motion_Generation_With_Wordless_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23222-23231.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决文本到运动生成这一新兴且具有挑战性的问题，目标是合成与输入文本语义相同的运动。<br>
                    动机：由于缺乏多样化的标记训练数据，大多数方法要么局限于特定类型的文本注释，要么需要在线优化以适应推理过程中的文本，从而影响效率和稳定性。<br>
                    方法：受NLP中的提示学习启发，我们预训练一个运动生成器，使其能够从被遮蔽的运动中重建完整的运动。在推理过程中，我们的方法不是改变运动生成器，而是将输入文本重新构造为被遮蔽的运动作为运动生成器的“重建”提示。<br>
                    效果：实验结果表明，我们的方法相对于基线方法取得了显著的改进。代码可在https://github.com/junfanlin/oohmg获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-to-motion generation is an emerging and challenging problem, which aims to synthesize motion with the same semantics as the input text. However, due to the lack of diverse labeled training data, most approaches either limit to specific types of text annotations or require online optimizations to cater to the texts during inference at the cost of efficiency and stability. In this paper, we investigate offline open-vocabulary text-to-motion generation in a zero-shot learning manner that neither requires paired training data nor extra online optimization to adapt for unseen texts. Inspired by the prompt learning in NLP, we pretrain a motion generator that learns to reconstruct the full motion from the masked motion. During inference, instead of changing the motion generator, our method reformulates the input text into a masked motion as the prompt for the motion generator to "reconstruct" the motion. In constructing the prompt, the unmasked poses of the prompt are synthesized by a text-to-pose generator. To supervise the optimization of the text-to-pose generator, we propose the first text-pose alignment model for measuring the alignment between texts and 3D poses. And to prevent the pose generator from overfitting to limited training texts, we further propose a novel wordless training mechanism that optimizes the text-to-pose generator without any training texts. The comprehensive experimental results show that our method obtains a significant improvement against the baseline methods. The code is available at https://github.com/junfanlin/oohmg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1271.MetaCLUE: Towards Comprehensive Visual Metaphors Research</span><br>
                <span class="as">Akula, ArjunR.andDriscoll, BrendanandNarayana, PradyumnaandChangpinyo, SoravitandJia, ZhiweiandDamle, SuyashandPruthi, GarimaandBasu, SugatoandGuibas, LeonidasandFreeman, WilliamT.andLi, YuanzhenandJampani, Varun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Akula_MetaCLUE_Towards_Comprehensive_Visual_Metaphors_Research_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23201-23211.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中隐喻理解的问题，即如何通过抽象概念之间的微妙关系来理解和生成创造性的图像。<br>
                    动机：虽然计算机视觉基准和方法是理解和生成图像字面解释的主要方式，但图像的隐喻理解仍然相对未被探索。为了解决这个问题，我们提出了MetaCLUE，一套关于视觉隐喻的视觉任务。<br>
                    方法：我们收集了高质量的丰富隐喻标注（抽象对象、概念、关系以及相应的对象框），因为目前没有任何数据集可以方便地评估这些任务。我们基于我们的标注对当前最先进的视觉和语言模型进行了全面分析，突出了当前方法在视觉隐喻分类、定位、理解和生成（文本到图像合成）任务中的优缺点。<br>
                    效果：实验结果表明，MetaCLUE为开发具有人类般创造力的AI系统提供了具体步骤。我们希望这项工作能推动AI系统的发展，使其具有更强的创新和理解能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Creativity is an indispensable part of human cognition and also an inherent part of how we make sense of the world. Metaphorical abstraction is fundamental in communicating creative ideas through nuanced relationships between abstract concepts such as feelings. While computer vision benchmarks and approaches predominantly focus on understanding and generating literal interpretations of images, metaphorical comprehension of images remains relatively unexplored. Towards this goal, we introduce MetaCLUE, a set of vision tasks on visual metaphor. We also collect high-quality and rich metaphor annotations (abstract objects, concepts, relationships along with their corresponding object boxes) as there do not exist any datasets that facilitate the evaluation of these tasks. We perform a comprehensive analysis of state-of-the-art models in vision and language based on our annotations, highlighting strengths and weaknesses of current approaches in visual metaphor Classification, Localization, Understanding (retrieval, question answering, captioning) and gEneration (text-to-image synthesis) tasks. We hope this work provides a concrete step towards systematically developing AI systems with human-like creative capabilities. Project page: https://metaclue.github.io</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1272.EVA: Exploring the Limits of Masked Visual Representation Learning at Scale</span><br>
                <span class="as">Fang, YuxinandWang, WenandXie, BinhuiandSun, QuanandWu, LedellandWang, XinggangandHuang, TiejunandWang, XinlongandCao, Yue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19358-19369.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索大规模视觉表示的极限，仅使用公开可获取的数据。<br>
                    动机：现有的预训练模型在处理大规模视觉表示时存在限制，需要更强大的模型来提高性能。<br>
                    方法：本文提出了一种名为EVA的视觉基础模型，通过预训练重建被遮盖的图像-文本对齐视觉特征，以实现大规模视觉表示。<br>
                    效果：实验结果表明，EVA在各种代表性视觉下游任务上取得了显著改进，如图像识别、视频动作识别、目标检测、实例分割和语义分割等，并且在迁移学习性能方面也取得了突破性进展。此外，EVA还可以作为连接图像和文本的多模态基础模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVIS dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1273.Gloss Attention for Gloss-Free Sign Language Translation</span><br>
                <span class="as">Yin, AoxiongandZhong, TianyunandTang, LiandJin, WeikeandJin, TaoandZhao, Zhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Gloss_Attention_for_Gloss-Free_Sign_Language_Translation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2551-2562.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前大多数手语翻译方法需要使用词汇注释来提供额外的监督信息，但获取词汇注释并不容易。<br>
                    动机：为了解决这个问题，我们首先对现有模型进行分析，确认了词汇注释如何使手语翻译变得更容易。我们发现，它可以为模型提供两方面的信息，1）帮助模型隐式地学习连续手语视频中的语义边界位置，2）帮助模型全局理解手语视频。<br>
                    方法：我们提出了词汇关注机制，使模型能够将其注意力保持在具有相同局部语义的视频片段内，就像词汇注释帮助现有模型所做的那样。此外，我们将句子到句子的相似性知识从自然语言模型转移到我们的词汇关注手语翻译网络（GASLT）中，以帮助它在句子级别理解手语视频。<br>
                    效果：我们在多个大规模的手语数据集上进行实验，结果显示我们的GASLT模型显著优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most sign language translation (SLT) methods to date require the use of gloss annotations to provide additional supervision information, however, the acquisition of gloss is not easy. To solve this problem, we first perform an analysis of existing models to confirm how gloss annotations make SLT easier. We find that it can provide two aspects of information for the model, 1) it can help the model implicitly learn the location of semantic boundaries in continuous sign language videos, 2) it can help the model understand the sign language video globally. We then propose gloss attention, which enables the model to keep its attention within video segments that have the same semantics locally, just as gloss helps existing models do. Furthermore, we transfer the knowledge of sentence-to-sentence similarity from the natural language model to our gloss attention SLT network (GASLT) to help it understand sign language videos at the sentence level. Experimental results on multiple large-scale sign language datasets show that our proposed GASLT model significantly outperforms existing methods. Our code is provided in https://github.com/YinAoXiong/GASLT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1274.Siamese Image Modeling for Self-Supervised Vision Representation Learning</span><br>
                <span class="as">Tao, ChenxinandZhu, XizhouandSu, WeijieandHuang, GaoandLi, BinandZhou, JieandQiao, YuandWang, XiaogangandDai, Jifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2132-2141.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时解决预训练语言模型对结构化知识的利用不足和现有SSL框架在语义对齐和空间敏感性方面的缺陷。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，而现有的SSL框架在语义对齐和空间敏感性方面存在不足。<br>
                    方法：提出Siamese Image Modeling（SiameseIM）方法，通过匹配不同图像视图的强增广来实现语义对齐，并通过预测带有掩码的图像的密集表示来提高空间敏感性。<br>
                    效果：实验结果表明，SiameseIM在各种下游任务上都能超越现有的ID和MIM框架，尤其在少样本、长尾和鲁棒性关注的场景中，改进效果更为显著。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning (SSL) has delivered superior performance on a variety of downstream vision tasks. Two main-stream SSL frameworks have been proposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM). ID pulls together representations from different views of the same image, while avoiding feature collapse. It lacks spatial sensitivity, which requires modeling the local structure within each image. On the other hand, MIM reconstructs the original content given a masked image. It instead does not have good semantic alignment, which requires projecting semantically similar views into nearby representations. To address this dilemma, we observe that (1) semantic alignment can be achieved by matching different image views with strong augmentations; (2) spatial sensitivity can benefit from predicting dense representations with masked images. Driven by these analysis, we propose Siamese Image Modeling (SiameseIM), which predicts the dense representations of an augmented view, based on another masked view from the same image but with different augmentations. SiameseIM uses a Siamese network with two branches. The online branch encodes the first view, and predicts the second view's representation according to the relative positions between these two views. The target branch produces the target by encoding the second view. SiameseIM can surpass both ID and MIM on a wide range of downstream tasks, including ImageNet finetuning and linear probing, COCO and LVIS detection, and ADE20k semantic segmentation. The improvement is more significant in few-shot, long-tail and robustness-concerned scenarios. Code shall be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1275.Mixed Autoencoder for Self-Supervised Visual Representation Learning</span><br>
                <span class="as">Chen, KaiandLiu, ZhiliandHong, LanqingandXu, HangandLi, ZhenguoandYeung, Dit-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mixed_Autoencoder_for_Self-Supervised_Visual_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22742-22751.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在对Masked Autoencoder（MAE）进行数据增强时，简单混合会降低模型性能的问题。<br>
                    动机：虽然MAE在各种视觉任务上表现出优越的性能，但其有效的数据增强策略仍然是一个开放的问题。与对比学习中最重要的部分不同，简单混合会因为互信息（MI）的增加而导致模型性能下降。<br>
                    方法：我们提出了同源识别这一辅助的预训练任务，不仅通过显式要求每个补丁识别同源补丁来减轻MI的增加，而且还进行了对象感知的自监督预训练以获得更好的下游密集感知性能。<br>
                    效果：实验表明，我们的混合自动编码器（MixedAE）在不同的下游任务上实现了掩蔽图像建模（MIM）增强中最先进的迁移结果，具有显著的效率。具体来说，我们的MixedAE在ImageNet-1K、ADE20K和COCO上分别比MAE提高了+0.3%的准确率、+1.7 mIoU和+0.9 AP，同时在标准的ViT-Base上训练速度提高了2倍。此外，MixedAE超过了结合实例判别的强MIM方法iBOT。据我们所知，这是第一个从预训练任务设计的角度考虑MIM混合的工作。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2x. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1276.MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of Hierarchical Vision Transformers</span><br>
                <span class="as">Liu, JihaoandHuang, XinandZheng, JinliangandLiu, YuandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MixMAE_Mixed_and_Masked_Autoencoder_for_Efficient_Pretraining_of_Hierarchical_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6252-6261.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种适用于各种分层视觉变压器的简单但有效的预训练方法。<br>
                    动机：现有的分层视觉变压器的遮蔽图像建模（MIM）方法使用特殊的[MASK]符号替换输入令牌的随机子集，并试图从损坏的图像中重建原始图像令牌，但这种方法训练速度慢且预训练-微调不一致。<br>
                    方法：通过将一张图像的遮蔽令牌替换为另一张图像的可见令牌，即创建混合图像，然后对两个原始图像进行双重重建，以从混合输入中重建两个原始图像，从而显著提高效率。<br>
                    效果：实验结果表明，MixMAE可以有效地学习高质量的视觉表示。特别地，MixMAE与Swin-B/W14一起在ImageNet-1K上实现了85.1%的top-1准确率，只需预训练600个周期。此外，其在其它6个数据集上的转移性能表明，MixMAE比之前流行的MIM方法具有更好的FLOPs /性能权衡。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose Mixed and Masked AutoEncoder (MixMAE), a simple but efficient pretraining method that is applicable to various hierarchical Vision Transformers. Existing masked image modeling (MIM) methods for hierarchical Vision Transformers replace a random subset of input tokens with a special [MASK] symbol and aim at reconstructing original image tokens from the corrupted image. However, we find that using the [MASK] symbol greatly slows down the training and causes pretraining-finetuning inconsistency, due to the large masking ratio (e.g., 60% in SimMIM). On the other hand, MAE does not introduce [MASK] tokens at its encoder at all but is not applicable for hierarchical Vision Transformers. To solve the issue and accelerate the pretraining of hierarchical models, we replace the masked tokens of one image with visible tokens of another image, i.e., creating a mixed image. We then conduct dual reconstruction to reconstruct the two original images from the mixed input, which significantly improves efficiency. While MixMAE can be applied to various hierarchical Transformers, this paper explores using Swin Transformer with a large window size and scales up to huge model size (to reach 600M parameters). Empirical results demonstrate that MixMAE can learn high-quality visual representations efficiently. Notably, MixMAE with Swin-B/W14 achieves 85.1% top-1 accuracy on ImageNet-1K by pretraining for 600 epochs. Besides, its transfer performances on the other 6 datasets show that MixMAE has better FLOPs / performance tradeoff than previous popular MIM methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1277.Video-Text As Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning</span><br>
                <span class="as">Jin, PengandHuang, JinfaandXiong, PengfeiandTian, ShangxuanandLiu, ChangandJi, XiangyangandYuan, LiandChen, Jie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2472-2482.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决对比学习为基础的视频-语言表示学习方法在细粒度跨模态学习中面临的挑战。<br>
                    动机：目前的模型在进行预定义的视频-文本对的语义交互时，存在粗糙的全局交互问题，需要进一步解决细粒度的交互问题。<br>
                    方法：本文创新地将视频-文本建模为具有多变量合作博弈论的游戏玩家，以智能处理不同粒度、灵活组合和模糊强度的细粒度语义交互中的不确定性。具体来说，提出了分层的班扎夫互动（HBI）来评估视频帧和文本词之间可能的对应关系，实现敏感且可解释的跨模态对比。<br>
                    效果：通过在常用的文本-视频检索和视频-问题回答基准上进行大量实验，证明了该方法的有效性。此外，它还可以作为可视化工具，帮助理解跨模态交互，对社区产生深远影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive learning-based video-language representation learning approaches, e.g., CLIP, have achieved outstanding performance, which pursue semantic interaction upon pre-defined video-text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning. In this paper, we creatively model video-text as game players with multivariate cooperative game theory to wisely handle the uncertainty during fine-grained semantic interaction with diverse granularity, flexible combination, and vague intensity. Concretely, we propose Hierarchical Banzhaf Interaction (HBI) to value possible correspondence between video frames and text words for sensitive and explainable cross-modal contrast. To efficiently realize the cooperative game of multiple video frames and multiple text words, the proposed method clusters the original video frames (text words) and computes the Banzhaf Interaction between the merged tokens. By stacking token merge modules, we achieve cooperative games at different semantic levels. Extensive experiments on commonly used text-video retrieval and video-question answering benchmarks with superior performances justify the efficacy of our HBI. More encouragingly, it can also serve as a visualization tool to promote the understanding of cross-modal interaction, which may have a far-reaching impact on the community. Project page is available at https://jpthu17.github.io/HBI/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1278.All in One: Exploring Unified Video-Language Pre-Training</span><br>
                <span class="as">Wang, JinpengandGe, YixiaoandYan, RuiandGe, YuyingandLin, KevinQinghongandTsutsui, SatoshiandLin, XudongandCai, GuanyuandWu, JianpingandShan, YingandQie, XiaohuandShou, MikeZheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6598-6608.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决主流视频-语言预训练模型在处理多模态信息时效率低下的问题。<br>
                    动机：现有的视频-语言预训练模型存在参数过多、效率低下等问题，影响了其在下游任务中的表现。<br>
                    方法：本文首次提出了一种端到端的视频-语言模型——all-in-one Transformer，该模型通过统一的骨干架构将原始的视频和文本信号嵌入到联合表示中。为了克服视频数据的临时信息对设计模态无关的Transformer的挑战，我们引入了一种新颖而有效的令牌滚动操作来以非参数化的方式编码视频片段的临时表示。<br>
                    效果：我们的预训练all-in-one Transformer被转移到各种下游的视频-文本任务中进行微调，包括文本-视频检索、视频问答、多项选择和视觉常识推理。在九个数据集上，我们的方法在最小的模型FLOPs下实现了最先进的性能，证明了我们的方法优于竞争模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mainstream Video-Language Pre-training models consist of three parts, a video encoder, a text encoder, and a video-text fusion Transformer. They pursue better performance via utilizing heavier unimodal encoders or multimodal fusion Transformers, resulting in increased parameters with lower efficiency in downstream tasks. In this work, we for the first time introduce an end-to-end video-language model, namely all-in-one Transformer, that embeds raw video and textual signals into joint representations using a unified backbone architecture. We argue that the unique temporal information of video data turns out to be a key barrier hindering the design of a modality-agnostic Transformer. To overcome the challenge, we introduce a novel and effective token rolling operation to encode temporal representations from video clips in a non-parametric manner. The careful design enables the representation learning of both video-text multimodal inputs and unimodal inputs using a unified backbone model. Our pre-trained all-in-one Transformer is transferred to various downstream video-text tasks after fine-tuning, including text-video retrieval, video-question answering, multiple choice and visual commonsense reasoning. State-of-the-art performances with the minimal model FLOPs on nine datasets demonstrate the superiority of our method compared to the competitive counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1279.VILA: Learning Image Aesthetics From User Comments With Vision-Language Pretraining</span><br>
                <span class="as">Ke, JunjieandYe, KerenandYu, JiahuiandWu, YonghuiandMilanfar, PeymanandYang, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_VILA_Learning_Image_Aesthetics_From_User_Comments_With_Vision-Language_Pretraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10041-10051.png><br>
            
            <span class="tt"><span class="t0">研究问题：评估图像美学是具有挑战性的，因为它受到包括构图、颜色、风格和高级语义在内的多个因素的影响。<br>
                    动机：现有的图像美学评估（IAA）方法主要依赖于人类标注的评分，这过于简化了人类感知到的视觉美学信息。相反，用户评论提供了更全面的信息，是表达人类对图像美学意见和偏好的自然方式。<br>
                    方法：我们提出从用户评论中学习图像美学，并探索视觉语言预训练方法来学习多模态美学表示。具体来说，我们使用图像-评论对预训练一个图像-文本编码器-解码器模型，使用对比和生成目标来学习丰富和通用的美学语义，而无需人工标签。为了高效地适应下游IAA任务，我们进一步提出了一种轻量级的基于排名的适配器，该适配器使用文本作为锚点来学习美学排名概念。<br>
                    效果：我们的预训练美学视觉语言模型在AVA-Captions数据集上的图像美学描述任务上优于先前的工作，并且它具有强大的零样本能力，可以用于美学任务，如零样本风格分类和零样本IAA，超越了许多监督基线。通过使用提出的适配器模块进行最小的参数微调，我们的模型在AVA数据集上实现了最先进的IAA性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Assessing the aesthetics of an image is challenging, as it is influenced by multiple factors including composition, color, style, and high-level semantics. Existing image aesthetic assessment (IAA) methods primarily rely on human-labeled rating scores, which oversimplify the visual aesthetic information that humans perceive. Conversely, user comments offer more comprehensive information and are a more natural way to express human opinions and preferences regarding image aesthetics. In light of this, we propose learning image aesthetics from user comments, and exploring vision-language pretraining methods to learn multimodal aesthetic representations. Specifically, we pretrain an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics without human labels. To efficiently adapt the pretrained model for downstream IAA tasks, we further propose a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept. Our results show that our pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and it has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines. With only minimal finetuning parameters using the proposed adapter module, our model achieves state-of-the-art IAA performance over the AVA dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1280.Fine-Grained Audible Video Description</span><br>
                <span class="as">Shen, XuyangandLi, DongandZhou, JinxingandQin, ZhenandHe, BowenandHan, XiaodongandLi, AixuanandDai, YuchaoandKong, LingpengandWang, MengandQiao, YuandZhong, Yiran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Fine-Grained_Audible_Video_Description_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10585-10596.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的音频-视觉-语言建模任务，即精细的可听视频描述（FAVD），旨在为给定的可听视频提供详细的文本描述。<br>
                    动机：现有的视觉-语言建模任务往往关注视频中的视觉线索，而低估了语言和音频模态的价值。另一方面，FAVD不仅需要音频-视觉-语言建模技能，还需要段落级的语言生成能力。<br>
                    方法：构建了第一个精细的可听视频描述基准（FAVDBench），并为每个视频片段提供了一段总结（即标题）和4-6段描述视觉细节以及1-2段音频相关描述。同时，创建了两个新的度量标准：EntityScore用于衡量视觉描述中实体的完整性，AudioScore用于评估音频描述。作为解决此任务的初步方法，提出了一种音频-视觉-语言转换器，该转换器通过添加额外的音频分支扩展了现有的视频字幕模型。<br>
                    效果：通过在提出的基准上使用传统的字幕度量标准和提出的度量标准进行评估，证明了模型在音频-视觉-语言建模方面的效率。进一步将基准应用于视频生成模型，证明使用精细的视频描述可以创建比使用字幕更复杂的视频。代码和数据集可在https://github.com/OpenNLPLab/FAVDBench获取，在线基准可在www.avlbench.opennlplab.cn查看。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We explore a new task for audio-visual-language modeling called fine-grained audible video description (FAVD). It aims to provide detailed textual descriptions for the given audible videos, including the appearance and spatial locations of each object, the actions of moving objects, and the sounds in videos. Existing visual-language modeling tasks often concentrate on visual cues in videos while undervaluing the language and audio modalities. On the other hand, FAVD requires not only audio-visual-language modeling skills but also paragraph-level language generation abilities. We construct the first fine-grained audible video description benchmark (FAVDBench) to facilitate this research. For each video clip, we first provide a one-sentence summary of the video, ie, the caption, followed by 4-6 sentences describing the visual details and 1-2 audio-related descriptions at the end. The descriptions are provided in both English and Chinese. We create two new metrics for this task: an EntityScore to gauge the completeness of entities in the visual descriptions, and an AudioScore to assess the audio descriptions. As a preliminary approach to this task, we propose an audio-visual-language transformer that extends existing video captioning model with an additional audio branch. We combine the masked language modeling and auto-regressive language modeling losses to optimize our model so that it can produce paragraph-level descriptions. We illustrate the efficiency of our model in audio-visual-language modeling by evaluating it against the proposed benchmark using both conventional captioning metrics and our proposed metrics. We further put our benchmark to the test in video generation models, demonstrating that employing fine-grained video descriptions can create more intricate videos than using captions. Code and dataset are available at https://github.com/OpenNLPLab/FAVDBench. Our online benchmark is available at www.avlbench.opennlplab.cn.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1281.Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification</span><br>
                <span class="as">Yang, YueandPanagopoulou, ArtemisandZhou, ShenghaoandJin, DanielandCallison-Burch, ChrisandYatskar, Mark</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19187-19197.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何构建高性能的概念瓶颈模型（CBM）以实现与黑箱模型相当的准确性，同时不需要手动指定概念。<br>
                    动机：现有的CBM需要手动指定概念，且性能往往不如黑箱模型，限制了其广泛应用。<br>
                    方法：提出一种语言引导瓶颈（LaBo）的方法，利用GPT-3语言模型定义可能的瓶颈空间，通过生成关于类别的事实句子来形成候选概念，并通过一种新的次模态效用函数高效地搜索可能的瓶颈。<br>
                    效果：实验表明，LaBo是一种高效的视觉识别概念先验，其在11个不同数据集上的评估表现出色，证明了可解释模型可以广泛应用，且性能与黑箱方法相当甚至更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a highly effective prior for concepts important to visual recognition. In the evaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot classification: they are 11.7% more accurate than black box linear probes at 1 shot and comparable with more data. Overall, LaBo demonstrates that inherently interpretable models can be widely applied at similar, or better, performance than black box approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1282.EXIF As Language: Learning Cross-Modal Associations Between Images and Camera Metadata</span><br>
                <span class="as">Zheng, ChenhaoandShrivastava, AyushandOwens, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_EXIF_As_Language_Learning_Cross-Modal_Associations_Between_Images_and_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6945-6956.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练图像补丁和EXIF元数据，学习一种视觉表示来捕捉相机拍摄照片的信息。<br>
                    动机：现有的方法在图像取证和校准任务上的性能不佳，需要更强大的特征。<br>
                    方法：利用转换器处理自动插入到图像文件中的EXIF元数据，将其转换为文本形式，然后与图像补丁进行多模态嵌入训练。<br>
                    效果：该方法学习的特征在图像取证和校准任务上显著优于其他自监督和有监督的特征，能够成功地对拼接图像区域进行"零射"定位。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1283.ANetQA: A Large-Scale Benchmark for Fine-Grained Compositional Reasoning Over Untrimmed Videos</span><br>
                <span class="as">Yu, ZhouandZheng, LixiangandZhao, ZhouandWu, FeiandFan, JianpingandRen, KuiandYu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_ANetQA_A_Large-Scale_Benchmark_for_Fine-Grained_Compositional_Reasoning_Over_Untrimmed_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23191-23200.png><br>
            
            <span class="tt"><span class="t0">研究问题：构建视频问答（VideoQA）模型能力的系统分析基准具有挑战性但至关重要。现有的基准测试常常使用非组合的简单问题，并受到语言偏见的影响，使得难以准确诊断模型的弱点。<br>
                    动机：为了解决这些问题，研究人员提出了AGQA，这是一个从预注释的场景图中自动生成问答对的新基准，能够以精细的控制测量多样化的推理能力。然而，其问题在推理视频中的细粒度语义上存在限制，因为这样的信息在其场景图中是缺失的。<br>
                    方法：因此，研究人员提出了ANetQA，这是一个大规模的基准，支持在ActivityNet的未修剪视频上进行细粒度的组合推理。与AGQA类似，ANetQA中的问答对也是从注释的视频场景图自动生成的。<br>
                    效果：ANetQA的特点体现在：（i）具有细粒度语义的未修剪视频；（ii）具有细粒度分类法的空间-时间场景图；（iii）从细粒度模板生成的多样化问题。ANetQA获得了14亿个不平衡和1340万个平衡的问答对，这比具有相似数量视频的AGQA大一个数量级。对于最先进的方法进行了全面实验，最好的模型达到了44.5%的准确率，而人类的表现最高为84.5%，还有很大的改进空间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Building benchmarks to systemically analyze different capabilities of video question answering (VideoQA) models is challenging yet crucial. Existing benchmarks often use non-compositional simple questions and suffer from language biases, making it difficult to diagnose model weaknesses incisively. A recent benchmark AGQA poses a promising paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control. However, its questions have limitations in reasoning about the fine-grained semantics in videos as such information is absent in its scene graphs. To this end, we present ANetQA, a large-scale benchmark that supports fine-grained compositional reasoning over the challenging untrimmed videos from ActivityNet. Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. The fine-grained properties of ANetQA are reflected in the following: (i) untrimmed videos with fine-grained semantics; (ii) spatio-temporal scene graphs with fine-grained taxonomies; and (iii) diverse questions generated from fine-grained templates. ANetQA attains 1.4 billion unbalanced and 13.4 million balanced QA pairs, which is an order of magnitude larger than AGQA with a similar number of videos. Comprehensive experiments are performed for state-of-the-art methods. The best model achieves 44.5% accuracy while human performance tops out at 84.5%, leaving sufficient room for improvement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1284.CLAMP: Prompt-Based Contrastive Learning for Connecting Language and Animal Pose</span><br>
                <span class="as">Zhang, XuandWang, WenandChen, ZheandXu, YufeiandZhang, JingandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_CLAMP_Prompt-Based_Contrastive_Learning_for_Connecting_Language_and_Animal_Pose_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23272-23281.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像方法在动物姿态估计上面临挑战，因为训练数据有限且物种内和物种间的差异大。<br>
                    动机：受视觉语言研究的启发，我们提出预训练的语言模型（如CLIP）可以通过提供丰富的先验知识来描述文本中的动物关键点，从而帮助进行动物姿态估计。<br>
                    方法：我们引入了一种新颖的基于提示的对比学习方案，用于有效地连接语言和动物姿态（CLAMP）。CLAMP通过在网络训练过程中将文本提示适应到动物关键点，试图弥合这一差距。适应过程被分解为空间感知和特征感知过程，并相应地设计了两种新的对比损失。<br>
                    效果：实验结果表明，我们的方法在有监督、少样本和零样本设置下实现了最先进的性能，大大超过了基于图像的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Animal pose estimation is challenging for existing image-based methods because of limited training data and large intra- and inter-species variances. Motivated by the progress of visual-language research, we propose that pre-trained language models (eg, CLIP) can facilitate animal pose estimation by providing rich prior knowledge for describing animal keypoints in text. However, we found that building effective connections between pre-trained language models and visual animal keypoints is non-trivial since the gap between text-based descriptions and keypoint-based visual features about animal pose can be significant. To address this issue, we introduce a novel prompt-based Contrastive learning scheme for connecting Language and AniMal Pose (CLAMP) effectively. The CLAMP attempts to bridge the gap by adapting the text prompts to the animal keypoints during network training. The adaptation is decomposed into spatial-aware and feature-aware processes, and two novel contrastive losses are devised correspondingly. In practice, the CLAMP enables the first cross-modal animal pose estimation paradigm. Experimental results show that our method achieves state-of-the-art performance under the supervised, few-shot, and zero-shot settings, outperforming image-based methods by a large margin. The code is available at https://github.com/xuzhang1199/CLAMP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1285.Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing With Non-Learnable Primitives</span><br>
                <span class="as">Ding, ChuntaoandLu, ZhichaoandWang, ShangguangandCheng, RanandBoddeti, VishnuNaresh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7756-7765.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的多任务学习模型存在任务干扰问题，如何通过非学习式原语和显式任务路由来减轻任务干扰。<br>
                    动机：现有的多任务学习模型由于任务间的干扰，效果并不理想。因此，本文提出采用非学习式原语和显式任务路由的方法来解决这个问题。<br>
                    方法：通过使用非学习式原语提取一组与任务无关的多样化特征，并将其重组为所有任务共享的分支以及每个任务保留的特定分支。同时，将可学习的参数显式分离为共享和特定于任务的部分，以最小化任务干扰。<br>
                    效果：实验结果表明，该方法在图像级别分类和像素级密集预测等多任务学习问题上，显著优于现有的最佳基线，且具有较少的学习参数和类似的浮点运算次数。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this URL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1286.Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style</span><br>
                <span class="as">Lin, FengyinandLi, MingkangandLi, DaandHospedales, TimothyandSong, Yi-ZheandQi, Yonggang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Zero-Shot_Everything_Sketch-Based_Image_Retrieval_and_in_Explainable_Style_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23349-23358.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了基于零短草图的图像检索（ZS-SBIR）问题，并解决了现有技术的两个显著差异。<br>
                    动机：我们的目标是通过一个网络解决所有ZS-SBIR的变体（跨类别、跨数据集），并理解草图和照片匹配的操作过程。<br>
                    方法：我们将跨模态匹配问题简化为关键局部区域组的比较，类似于"词袋"模式。我们的创新在于实现了这一目标，同时不再需要外部语义知识。<br>
                    效果：实验表明，我们的方法在所有ZS-SBIR设置中都表现出优越的性能。通过可视化跨模态标记对应关系，我们优雅地实现了可解释的目标。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper studies the problem of zero-short sketch-based image retrieval (ZS-SBIR), however with two significant differentiators to prior art (i) we tackle all variants (inter-category, intra-category, and cross datasets) of ZS-SBIR with just one network ("everything"), and (ii) we would really like to understand how this sketch-photo matching operates ("explainable"). Our key innovation lies with the realization that such a cross-modal matching problem could be reduced to comparisons of groups of key local patches -- akin to the seasoned "bag-of-words" paradigm. Just with this change, we are able to achieve both of the aforementioned goals, with the added benefit of no longer requiring external semantic knowledge. Technically, ours is a transformer-based cross-modal network, with three novel components (i) a self-attention module with a learnable tokenizer to produce visual tokens that correspond to the most informative local regions, (ii) a cross-attention module to compute local correspondences between the visual tokens across two modalities, and finally (iii) a kernel-based relation network to assemble local putative matches and produce an overall similarity metric for a sketch-photo pair. Experiments show ours indeed delivers superior performances across all ZS-SBIR settings. The all important explainable goal is elegantly achieved by visualizing cross-modal token correspondences, and for the first time, via sketch to photo synthesis by universal replacement of all matched photo patches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1287.Task Residual for Tuning Vision-Language Models</span><br>
                <span class="as">Yu, TaoandLu, ZhiheandJin, XinandChen, ZhiboandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10899-10909.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将大规模视觉语言模型（VLMs）的知识结构转移到数据有限的下游任务中，同时保留适当的原有知识。<br>
                    动机：现有的高效转移学习方法对于视觉语言模型的知识结构处理不当，可能会造成原有知识的损害或过度偏见。<br>
                    方法：提出一种新的高效调优方法，名为任务残差调优（TaskRes）。该方法直接在基于文本的分类器上执行，并明确解耦预训练模型的原有知识和关于目标任务的新知识。具体来说，TaskRes保持了视觉语言模型原始分类器的权重不变，并通过调整一组与原有分类器无关的参数作为原有分类器的残差来获得目标任务的新分类器。<br>
                    效果：在11个基准数据集上，TaskRes显著优于先前的ETL方法（如PT和AT），同时实现简单且效果显著。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale vision-language models (VLMs) pre-trained on billion-level data have learned general visual representations and broad visual concepts. In principle, the well-learned knowledge structure of the VLMs should be inherited appropriately when being transferred to downstream tasks with limited data. However, most existing efficient transfer learning (ETL) approaches for VLMs either damage or are excessively biased towards the prior knowledge, e.g., prompt tuning (PT) discards the pre-trained text-based classifier and builds a new one while adapter-style tuning (AT) fully relies on the pre-trained features. To address this, we propose a new efficient tuning approach for VLMs named Task Residual Tuning (TaskRes), which performs directly on the text-based classifier and explicitly decouples the prior knowledge of the pre-trained models and new knowledge regarding a target task. Specifically, TaskRes keeps the original classifier weights from the VLMs frozen and obtains a new classifier for the target task by tuning a set of prior-independent parameters as a residual to the original one, which enables reliable prior knowledge preservation and flexible task-specific knowledge exploration. The proposed TaskRes is simple yet effective, which significantly outperforms previous ETL methods (e.g., PT and AT) on 11 benchmark datasets while requiring minimal effort for the implementation. Our code is available at https://github.com/geekyutao/TaskRes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1288.Hierarchical Prompt Learning for Multi-Task Learning</span><br>
                <span class="as">Liu, YajingandLu, YuningandLiu, HaoandAn, YaozuandXu, ZhuoranandYao, ZhuokunandZhang, BaofengandXiong, ZhiweiandGui, Chenguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10888-10898.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决如何有效地将视觉语言模型（VLMs）适应于多种相似但不同的视觉任务。<br>
                    动机：现有的方法需要为每个任务学习一个特定的提示，这限制了利用其他任务中可能共享的信息的能力。<br>
                    方法：提出了分层提示（HiPro）学习，这是一种简单而有效的方法，用于将预训练的VLM共同适应于多个下游任务。该方法量化了任务间的亲和力，并随后构建了一个分层任务树。内部节点学习的任务共享提示探索了相应任务组内的信息，而叶节点学习的任务特定提示获取了针对每个任务的细粒度信息。分层提示的结合提供了不同粒度的高质量内容。<br>
                    效果：在四个多任务学习数据集上评估HiPro，结果表明该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-language models (VLMs) can effectively transfer to various vision tasks via prompt learning. Real-world scenarios often require adapting a model to multiple similar yet distinct tasks. Existing methods focus on learning a specific prompt for each task, limiting the ability to exploit potentially shared information from other tasks. Naively training a task-shared prompt using a combination of all tasks ignores fine-grained task correlations. Significant discrepancies across tasks could cause negative transferring. Considering this, we present Hierarchical Prompt (HiPro) learning, a simple and effective method for jointly adapting a pre-trained VLM to multiple downstream tasks. Our method quantifies inter-task affinity and subsequently constructs a hierarchical task tree. Task-shared prompts learned by internal nodes explore the information within the corresponding task group, while task-individual prompts learned by leaf nodes obtain fine-grained information targeted at each task. The combination of hierarchical prompts provides high-quality content of different granularity. We evaluate HiPro on four multi-task learning datasets. The results demonstrate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1289.Revealing the Dark Secrets of Masked Image Modeling</span><br>
                <span class="as">Xie, ZhendaandGeng, ZigangandHu, JingchengandZhang, ZhengandHu, HanandCao, Yue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14475-14485.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过可视化和实验比较了Masked Image Modeling（MIM）和监督预训练模型在视觉任务中的表现差异。<br>
                    动机：虽然MIM已被证明对许多视觉下游任务有效，但其作用方式和位置尚不清楚。<br>
                    方法：通过可视化和实验，比较了MIM和长期占主导地位的监督预训练模型的关键表示差异。<br>
                    效果：实验结果表明，对于几何、运动任务或具有弱语义或细粒度分类的任务，MIM模型的性能明显优于其监督对应模型。此外，对于类别被监督预训练充分覆盖的语义理解数据集，MIM模型仍能实现高度竞争的迁移性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked image modeling (MIM) as pre-training is shown to be effective for numerous vision downstream tasks, but how and where MIM works remain unclear. In this paper, we compare MIM with the long-dominant supervised pre-trained models from two perspectives, the visualizations and the experiments, to uncover their key representational differences. From the visualizations, we find that MIM brings locality inductive bias to all layers of the trained models, but supervised models tend to focus locally at lower layers but more globally at higher layers. That may be the reason why MIM helps Vision Transformers that have a very large receptive field to optimize. Using MIM, the model can maintain a large diversity on attention heads in all layers. But for supervised models, the diversity on attention heads almost disappears from the last three layers and less diversity harms the fine-tuning performance. From the experiments, we find that MIM models can perform significantly better on geometric and motion tasks with weak semantics or fine-grained classification tasks, than their supervised counterparts. Without bells and whistles, a standard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the categories are sufficiently covered by the supervised pre-training, MIM models can still achieve highly competitive transfer performance. With a deeper understanding of MIM, we hope that our work can inspire new and solid research in this direction. Code will be available at https://github.com/zdaxie/MIM-DarkSecrets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1290.Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network</span><br>
                <span class="as">Pan, ZhengxinandWu, FangyuandZhang, Bailing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19275-19284.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像-文本匹配方法通过交叉注意力机制隐含地对齐视觉语义片段，但研究问题：现有的图像-文本匹配方法通过交叉注意力机制隐含地对齐视觉语义片段，但这种方法可能会产生冗余或不相关的区域-单词对齐，降低检索准确性并限制效率。<br>
                    动机：尽管许多研究者在挖掘有意义的对齐以提高准确性方面取得了进展，但效率低下的问题仍未得到解决。<br>
                    方法：我们提出了一种从信息编码的角度学习细粒度图像-文本匹配的方法。具体来说，我们提出了一个编码框架来解释片段对齐过程，为重新审视交叉注意力机制和分析冗余对齐问题提供了新的视角。基于这个框架，我们设计了一个跨模态硬对齐网络（CHAN），它全面利用最相关的区域-单词对，并消除所有其他对齐。<br>
                    效果：我们在MS-COCO和Flickr30K两个公共数据集上进行的大量实验验证了与图像-文本相似性最相关的单词-区域对的判别力，其在双向图像和文本检索任务上的准确性和效率均优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current state-of-the-art image-text matching methods implicitly align the visual-semantic fragments, like regions in images and words in sentences, and adopt cross-attention mechanism to discover fine-grained cross-modal semantic correspondence. However, the cross-attention mechanism may bring redundant or irrelevant region-word alignments, degenerating retrieval accuracy and limiting efficiency. Although many researchers have made progress in mining meaningful alignments and thus improving accuracy, the problem of poor efficiency remains unresolved. In this work, we propose to learn fine-grained image-text matching from the perspective of information coding. Specifically, we suggest a coding framework to explain the fragments aligning process, which provides a novel view to reexamine the cross-attention mechanism and analyze the problem of redundant alignments. Based on this framework, a Cross-modal Hard Aligning Network (CHAN) is designed, which comprehensively exploits the most relevant region-word pairs and eliminates all other alignments. Extensive experiments conducted on two public datasets, MS-COCO and Flickr30K, verify that the relevance of the most associated word-region pairs is discriminative enough as an indicator of the image-text similarity, with superior accuracy and efficiency over the state-of-the-art approaches on the bidirectional image and text retrieval tasks. Our code will be available at https://github.com/ppanzx/CHAN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1291.Images Speak in Images: A Generalist Painter for In-Context Visual Learning</span><br>
                <span class="as">Wang, XinlongandWang, WenandCao, YueandShen, ChunhuaandHuang, Tiejun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6830-6839.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让计算机视觉模型通过少量的提示和例子快速适应各种任务。<br>
                    动机：在计算机视觉中，由于任务的输出表示差异很大，因此对上下文学习的困难在于如何定义通用的任务提示，使视觉模型可以理解并将其转移到领域外的任务。<br>
                    方法：提出了一种名为Painter的通才模型，其解决方案是以“图像”为中心，即将核心视觉任务的输出重新定义为图像，并将任务提示也指定为图像。训练过程非常简单，只需在输入和输出图像对上执行标准的掩蔽图像建模。这使得模型能够根据可见的图像块执行任务。<br>
                    效果：实验结果表明，Painter可以在七个具有代表性的视觉任务上与成熟的特定任务模型竞争，包括从高级视觉理解到低级图像处理的各种任务。此外，Painter在一些具有挑战性的任务上显著优于最近的通才模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In-context learning, as a new paradigm in NLP, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. But in computer vision, the difficulties for in-context learning lie in that tasks vary significantly in the output representations, thus it is unclear how to define the general-purpose task prompts that the vision model can understand and transfer to out-of-domain tasks. In this work, we present Painter, a generalist model which addresses these obstacles with an "image"-centric solution, that is, to redefine the output of core vision tasks as images, and specify task prompts as also images. With this idea, our training process is extremely simple, which performs standard masked image modeling on the stitch of input and output image pairs. This makes the model capable of performing tasks conditioned on visible image patches. Thus, during inference, we can adopt a pair of input and output images from the same task as the input condition, to indicate which task to perform. Without bells and whistles, our generalist Painter can achieve competitive performance compared to well-established task-specific models, on seven representative vision tasks ranging from high-level visual understanding to low-level image processing. In addition, Painter significantly outperforms recent generalist models on several challenging tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1292.Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language</span><br>
                <span class="as">Li, ChuanhaoandLi, ZhenandJing, ChenchenandJia, YundeandWu, Yuwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19092-19101.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索视觉-语言（V&L）领域中，基本元素如词、图像区域和视频帧对组合泛化效果的影响。<br>
                    动机：组合泛化是模拟人类组合能力的关键，对于理解基本元素如何影响组合泛化能力至关重要。<br>
                    方法：本文提出了一个基于自我监督学习的框架，赋予V&L方法两个特性：语义等变和语义不变性。通过这两个特性，该方法能理解基本元素通过感知基本元素变化对样本语义和真实值的影响。<br>
                    效果：在两个任务上进行实验：时间视频基础和视觉问答，实验结果证明了该框架的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compositionality is one of the fundamental properties of human cognition (Fodor & Pylyshyn, 1988). Compositional generalization is critical to simulate the compositional capability of humans, and has received much attention in the vision-and-language (V&L) community. It is essential to understand the effect of the primitives, including words, image regions, and video frames, to improve the compositional generalization capability. In this paper, we explore the effect of primitives for compositional generalization in V&L. Specifically, we present a self-supervised learning based framework that equips V&L methods with two characteristics: semantic equivariance and semantic invariance. With the two characteristics, the methods understand primitives by perceiving the effect of primitive changes on sample semantics and ground-truth. Experimental results on two tasks: temporal video grounding and visual question answering, demonstrate the effectiveness of our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1293.MAGE: MAsked Generative Encoder To Unify Representation Learning and Image Synthesis</span><br>
                <span class="as">Li, TianhongandChang, HuiwenandMishra, ShlokandZhang, HanandKatabi, DinaandKrishnan, Dilip</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MAGE_MAsked_Generative_Encoder_To_Unify_Representation_Learning_and_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2142-2152.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉中生成模型和表示学习两个关键任务的训练通常独立进行，忽视了两者相互促进的潜力，导致训练和维护开销大的问题。<br>
                    动机：作者提出遮蔽生成编码器（MAGE），这是首个统一图像生成和自监督表示学习的框架。主要思路是使用遮蔽图像建模预训练中的可变遮蔽比，使得在同一训练框架下可以进行高遮蔽比的生成训练和低遮蔽比的表示学习。<br>
                    方法：MAGE利用矢量量化GAN学习的语义标记作为输入和输出，并将其与遮蔽相结合。通过在编码器输出上添加对比损失，可以进一步提高表示能力。<br>
                    效果：在ImageNet-1K上，一个单一的MAGE ViT-L模型在类别无条件图像生成任务上获得了9.10 FID，在线性探测上获得了78.9%的Top-1准确率，在图像生成和表示学习方面都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1294.FashionSAP: Symbols and Attributes Prompt for Fine-Grained Fashion Vision-Language Pre-Training</span><br>
                <span class="as">Han, YunpengandZhang, LisaiandChen, QingcaiandChen, ZhijianandLi, ZhonghuaandYang, JianxinandCao, Zhao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FashionSAP_Symbols_and_Attributes_Prompt_for_Fine-Grained_Fashion_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15028-15038.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉-语言预训练模型对细粒度的领域特征关注不足，而这些特征在区分特定领域任务和通用任务中非常重要。<br>
                    动机：提出一种基于时尚符号和属性提示（FashionSAP）的细粒度时尚视觉-语言预训练方法，以建模细粒度的多模态时尚属性和特征。<br>
                    方法：首先，提出时尚符号，一种新的抽象时尚概念层，用于表示不同的时尚项目并泛化各种细粒度的时尚特征，使建模细粒度属性更有效。其次，提出属性提示方法，使模型能够显式学习时尚项目的具体属性。根据时尚数据的形式设计适当的提示模板。<br>
                    效果：在两个公共时尚基准测试集FashionGen和FashionIQ上进行综合实验，FashionSAP在四个流行的时尚任务上取得了最先进的性能。消融研究还表明，提出的抽象时尚符号和属性提示方法使模型能够有效地获取时尚领域的细粒度语义。FashionSAP的显著性能提升为未来的时尚任务研究提供了新的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes and characteristics. Firstly, we propose the fashion symbols, a novel abstract fashion concept layer, to represent different fashion items and to generalize various kinds of fine-grained fashion features, making modelling fine-grained attributes more effective. Secondly, the attributes prompt method is proposed to make the model learn specific attributes of fashion items explicitly. We design proper prompt templates according to the format of fashion data. Comprehensive experiments are conducted on two public fashion benchmarks, i.e., FashionGen and FashionIQ, and FashionSAP gets SOTA performances for four popular fashion tasks. The ablation study also shows the proposed abstract fashion symbols, and the attribute prompt method enables the model to acquire fine-grained semantics in the fashion domain effectively. The obvious performance gains from FashionSAP provide a new baseline for future fashion task research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1295.PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models</span><br>
                <span class="as">Liu, MinghuaandZhu, YinhaoandCai, HongandHan, ShizhongandLing, ZhanandPorikli, FatihandSu, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21736-21746.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现低成本、高泛化的三维部分分割？<br>
                    动机：传统的监督学习方法需要大量精细标注的三维数据集，但收集这些数据成本高昂。<br>
                    方法：利用预训练的图像-语言模型GLIP进行三维点云的部分检测，通过2D到3D的标签提升算法将2D的知识转移到3D，并使用多视图3D先验和少样本提示调优来提高性能。<br>
                    效果：在PartNet和PartNet-Mobility数据集上的广泛评估表明，该方法可以实现优秀的零样本三维部分分割，其少样本版本不仅大幅超越了现有的少样本方法，而且与全监督版本相比也具有竞争力。此外，该方法可以直接应用于iPhone扫描的点云，无明显领域差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalizable 3D part segmentation is important but challenging in vision and robotics. Training deep models via conventional supervised methods requires large-scale 3D datasets with fine-grained part annotations, which are costly to collect. This paper explores an alternative way for low-shot part segmentation of 3D point clouds by leveraging a pretrained image-language model, GLIP, which achieves superior performance on open-vocabulary 2D detection. We transfer the rich knowledge from 2D to 3D through GLIP-based part detection on point cloud rendering and a novel 2D-to-3D label lifting algorithm. We also utilize multi-view 3D priors and few-shot prompt tuning to boost performance significantly. Extensive evaluation on PartNet and PartNet-Mobility datasets shows that our method enables excellent zero-shot 3D part segmentation. Our few-shot version not only outperforms existing few-shot approaches by a large margin but also achieves highly competitive results compared to the fully supervised counterpart. Furthermore, we demonstrate that our method can be directly applied to iPhone-scanned point clouds without significant domain gaps.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1296.MAGVLT: Masked Generative Vision-and-Language Transformer</span><br>
                <span class="as">Kim, SungwoongandJo, DaejinandLee, DonghoonandKim, Jongmin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_MAGVLT_Masked_Generative_Vision-and-Language_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23338-23348.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索一种能同时生成图像和文本序列的统一生成视觉-语言（VL）模型。<br>
                    动机：尽管在大规模配对数据集上进行多模态图像-文本数据生成建模的工作已经积极展开，但通过单一模型生成两种固定模态的数据，而非一种模态的条件生成另一种模态的数据的研究却十分有限。<br>
                    方法：我们提出了一种基于非自回归掩码预测的生成性VL变换器，命名为MAGVLT，并将其与自回归生成性VL变换器（ARGVLT）进行了比较。相比于ARGVLT，我们提出的MAGVLT能够实现双向上下文编码、快速解码以及扩展编辑能力，如图像和文本填充。<br>
                    效果：实验结果表明，我们的MAGVLT在各种VL基准测试的下游生成任务中表现优于ARGVLT，即使推理速度大大提高，也具有显著的优势。特别是在MS-COCO的零样本图像到文本和文本到图像生成任务上，MAGVLT甚至能在没有使用单模态数据和网络的情况下，仅用一个中等规模的模型（参数少于5亿）就取得了有竞争力的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to image, and joint image-and-text mask prediction tasks. Moreover, we devise two additional tasks based on the step-unrolled mask prediction and the selective prediction on the mixture of two image-text pairs. Experimental results on various downstream generation tasks of VL benchmarks show that our MAGVLT outperforms ARGVLT by a large margin even with significant inference speedup. Particularly, MAGVLT achieves competitive results on both zero-shot image-to-text and text-to-image generation tasks from MS-COCO by one moderate-sized model (fewer than 500M parameters) even without the use of monomodal data and networks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1297.DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment</span><br>
                <span class="as">Yao, LeweiandHan, JianhuaandLiang, XiaodanandXu, DanandZhang, WeiandLi, ZhenguoandXu, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23497-23506.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种高效可扩展的训练框架，通过整合大规模图像-文本对实现开放词汇对象检测（OVD）。<br>
                    动机：现有的OVD框架通常依赖于预训练的视觉语言模型或通过伪标签过程利用图像-文本对，而DetCLIPv2直接从大规模的图像-文本对中以端到端的方式学习精细的词-区域对齐。<br>
                    方法：DetCLIPv2采用最大词-区域相似性来引导对比目标，同时使用检测、基础和图像-文本对数据的混合监督进行训练，以提高模型的定位能力并学习广泛的概念。<br>
                    效果：通过交替方案进行联合训练，并采用低分辨率输入的图像-文本对，DetCLIPv2有效地利用了图像-文本对数据。在13M图像-文本对的预训练下，DetCLIPv2显示出优越的开放词汇检测性能，例如，基于Swin-T主干的DetCLIPv2在LVIS基准测试上实现了40.4%的零射击AP，超过了之前的工作GLIP/GLIPv2/DetCLIP 14.4/11.4/4.5%的AP，甚至大幅度超越了其全监督的对应模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents DetCLIPv2, an efficient and scalable training framework that incorporates large-scale image-text pairs to achieve open-vocabulary object detection (OVD). Unlike previous OVD frameworks that typically rely on a pre-trained vision-language model (e.g., CLIP) or exploit image-text pairs via a pseudo labeling process, DetCLIPv2 directly learns the fine-grained word-region alignment from massive image-text pairs in an end-to-end manner. To accomplish this, we employ a maximum word-region similarity between region proposals and textual words to guide the contrastive objective. To enable the model to gain localization capability while learning broad concepts, DetCLIPv2 is trained with a hybrid supervision from detection, grounding and image-text pair data under a unified data formulation. By jointly training with an alternating scheme and adopting low-resolution input for image-text pairs, DetCLIPv2 exploits image-text pair data efficiently and effectively: DetCLIPv2 utilizes 13x more image-text pairs than DetCLIP with a similar training time and improves performance. With 13M image-text pairs for pre-training, DetCLIPv2 demonstrates superior open-vocabulary detection performance, e.g., DetCLIPv2 with Swin-T backbone achieves 40.4% zero-shot AP on the LVIS benchmark, which outperforms previous works GLIP/GLIPv2/DetCLIP by 14.4/11.4/4.5% AP, respectively, and even beats its fully-supervised counterpart by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1298.Affordance Grounding From Demonstration Video To Target Image</span><br>
                <span class="as">Chen, JoyaandGao, DifeiandLin, KevinQinghongandShou, MikeZheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6799-6808.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让智能机器人和助手，如AR眼镜，通过观看示范视频学习人类手部交互行为，并将其应用到用户AR眼镜视图的目标图像上。<br>
                    动机：由于需要预测精细的交互行为以及训练数据有限且无法充分覆盖视频-图像差异，因此将人类手部交互从示范视频中落实到目标图像上的任务具有挑战性。<br>
                    方法：提出了一种名为Affordance Transformer（Afformer）的方法，该方法使用基于转换器的精细解码器逐步改进交互行为的落实。同时引入了Mask Affordance Hand（MaskAHand）自我监督预训练技术，用于合成视频-图像数据并模拟上下文变化，以增强跨越视频-图像差异的交互行为落实。<br>
                    效果：Afformer结合MaskAHand预训练在多个基准测试中实现了最先进的性能，包括在OPRA数据集上取得了37%的显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans excel at learning from expert demonstrations and solving their own problems. To equip intelligent robots and assistants, such as AR glasses, with this ability, it is essential to ground human hand interactions (i.e., affordances) from demonstration videos and apply them to a target image like a user's AR glass view. The video-to-image affordance grounding task is challenging due to (1) the need to predict fine-grained affordances, and (2) the limited training data, which inadequately covers video-image discrepancies and negatively impacts grounding. To tackle them, we propose Affordance Transformer (Afformer), which has a fine-grained transformer-based decoder that gradually refines affordance grounding. Moreover, we introduce Mask Affordance Hand (MaskAHand), a self-supervised pretraining technique for synthesizing video-image data and simulating context changes, enhancing affordance grounding across video-image discrepancies. Afformer with MaskAHand pre-training achieves state-of-the-art performance on multiple benchmarks, including a substantial 37% improvement on the OPRA dataset. Code is made available at https://github.com/showlab/afformer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1299.Unifying Vision, Text, and Layout for Universal Document Processing</span><br>
                <span class="as">Tang, ZinengandYang, ZiyiandWang, GuoxinandFang, YuweiandLiu, YangandZhu, ChenguangandZeng, MichaelandZhang, ChaandBansal, Mohit</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19254-19264.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种通用文档处理（UDOP）模型，该模型将文本、图像和布局模态以及各种任务格式统一起来，包括文档理解和生成。<br>
                    动机：目前的文档AI模型往往只关注单一的文本或图像模态，缺乏对多种模态和任务格式的统一处理。<br>
                    方法：通过引入创新的视觉-文本-布局转换器，UDOP模型将预训练和多领域下游任务统一为基于提示的序列生成方案。同时，利用自监督目标和多样化的标记数据在大规模无标签文档语料库上进行预训练。<br>
                    效果：实验结果表明，UDOP模型在文档理解、问答等8个文档AI任务上取得了显著改进，并在金融报告、学术论文和网站等不同数据领域的文档理解基准测试中排名第一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Document AI tasks, e.g., document understanding and QA, across diverse data domains like finance reports, academic papers, and websites. UDOP ranks first on the leaderboard of the Document Understanding Benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1300.HOICLIP: Efficient Knowledge Transfer for HOI Detection With Vision-Language Models</span><br>
                <span class="as">Ning, ShanandQiu, LongtianandLiu, YongfeiandHe, Xuming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ning_HOICLIP_Efficient_Knowledge_Transfer_for_HOI_Detection_With_Vision-Language_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23507-23517.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决人类-物体交互（HOI）检测的问题，特别是在少量或零次场景下的性能下降。<br>
                    动机：尽管对比性语言-图像预训练（CLIP）在提供HOI检测器交互先验方面具有巨大潜力，但这种方法通常依赖于大规模训练数据，并且在少量或零次场景下性能较差。<br>
                    方法：本文提出了一种新的HOI检测框架，该框架能有效地从CLIP中提取先验知识并实现更好的泛化。具体来说，我们首先引入了一个新的交互解码器，通过交叉注意力机制在CLIP的视觉特征图中提取信息丰富的区域，然后通过知识整合模块与检测主干进行融合，以进行更准确的人体-物体对检测。此外，我们还利用CLIP文本编码器的先验知识，通过嵌入HOI描述来生成分类器。为了区分细粒度的交互，我们通过视觉语义算术和轻量级动词表示适配器从训练数据中构建了一个动词分类器。此外，我们还提出了一种无需训练的增强方法，利用CLIP的全局HOI预测。<br>
                    效果：大量实验表明，我们的方法在各种设置下均大幅超越了现有技术，例如在HICO-Det上提高了+4.04 mAP。源代码可在https://github.com/Artanic30/HOICLIP获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-Object Interaction (HOI) detection aims to localize human-object pairs and recognize their interactions. Recently, Contrastive Language-Image Pre-training (CLIP) has shown great potential in providing interaction prior for HOI detectors via knowledge distillation. However, such approaches often rely on large-scale training data and suffer from inferior performance under few/zero-shot scenarios. In this paper, we propose a novel HOI detection framework that efficiently extracts prior knowledge from CLIP and achieves better generalization. In detail, we first introduce a novel interaction decoder to extract informative regions in the visual feature map of CLIP via a cross-attention mechanism, which is then fused with the detection backbone by a knowledge integration block for more accurate human-object pair detection. In addition, prior knowledge in CLIP text encoder is leveraged to generate a classifier by embedding HOI descriptions. To distinguish fine-grained interactions, we build a verb classifier from training data via visual semantic arithmetic and a lightweight verb representation adapter. Furthermore, we propose a training-free enhancement to exploit global HOI predictions from CLIP. Extensive experiments demonstrate that our method outperforms the state of the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det. The source code is available in https://github.com/Artanic30/HOICLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1301.SmallCap: Lightweight Image Captioning Prompted With Retrieval Augmentation</span><br>
                <span class="as">Ramos, RitaandMartins, BrunoandElliott, DesmondandKementchedjhieva, Yova</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2840-2849.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像描述任务中，大规模数据和模型规模增加导致的预训练和微调成本过高的问题。<br>
                    动机：针对大型模型的替代方案，本文提出了SmallCap，它根据输入图像和从数据存储库中检索的相关描述生成标题。<br>
                    方法：SmallCap模型轻量且易于训练，因为唯一需要学习的参数是新引入的CLIP编码器和GPT-2解码器之间的交叉注意力层。<br>
                    效果：实验表明，仅在COCO上训练的SmallCap在该基准测试中具有竞争力的性能，并且无需重新训练即可转移到其他领域，只需通过目标领域数据的检索即可实现。通过对多样化的人工标记和网络数据进行无训练的数据利用，可以进一步提高其性能，这在包括nocaps基准测试在内的一系列领域中均有效，该基准测试旨在测试对未见过视觉概念的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, which proves effective for a range of domains, including the nocaps benchmark, designed to test generalization to unseen visual concepts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1302.Probing Sentiment-Oriented Pre-Training Inspired by Human Sentiment Perception Mechanism</span><br>
                <span class="as">Feng, TingleiandLiu, JiaxuanandYang, Jufeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Probing_Sentiment-Oriented_Pre-Training_Inspired_by_Human_Sentiment_Perception_Mechanism_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2850-2860.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度卷积神经网络在视觉情感分析中的性能。<br>
                    动机：目前的预训练方法主要依赖大规模对象分类数据集（如ImageNet），虽然这大大提高了性能，但可能导致模型过度关注物体识别，而忽视了情感的高层次概念。<br>
                    方法：提出一种基于人类视觉情感感知机制的情感导向预训练方法。将视觉情感感知过程分为刺激接收、整体组织和高级感知三个步骤，通过模拟这三个步骤进行预训练，以挖掘情感区分表示。<br>
                    效果：实验结果表明，该方法在单标签学习、多标签学习和标签分布学习等主流视觉情感分析任务上均有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-training of deep convolutional neural networks (DCNNs) plays a crucial role in the field of visual sentiment analysis (VSA). Most proposed methods employ the off-the-shelf backbones pre-trained on large-scale object classification datasets (i.e., ImageNet). While it boosts performance for a big margin against initializing model states from random, we argue that DCNNs simply pre-trained on ImageNet may excessively focus on recognizing objects, but failed to provide high-level concepts in terms of sentiment. To address this long-term overlooked problem, we propose a sentiment-oriented pre-training method that is built upon human visual sentiment perception (VSP) mechanism. Specifically, we factorize the process of VSP into three steps, namely stimuli taking, holistic organizing, and high-level perceiving. From imitating each VSP step, a total of three models are separately pre-trained via our devised sentiment-aware tasks that contribute to excavating sentiment-discriminated representations. Moreover, along with our elaborated multi-model amalgamation strategy, the prior knowledge learned from each perception step can be effectively transferred into a single target model, yielding substantial performance gains. Finally, we verify the superiorities of our proposed method over extensive experiments, covering mainstream VSA tasks from single-label learning (SLL), multi-label learning (MLL), to label distribution learning (LDL). Experiment results demonstrate that our proposed method leads to unanimous improvements in these downstream tasks. Our code is released on https://github.com/tinglyfeng/sentiment_pretraining</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1303.TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition</span><br>
                <span class="as">Yu, HaoandCheng, XuandPeng, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3541-3550.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何克服异构图像之间巨大的视觉差异，实现可见光-红外识别（VI recognition）？<br>
                    动机：现有的方法主要通过预训练和先进的神经网络架构如ResNet和ViT来实现，但这些方法忽视了预训练的颜色先验知识对结果的负面影响，且计算负担重，难以在资源有限的实际场景中部署。<br>
                    方法：本文提出了一种针对任务的轻量级预训练神经网络（TOPLight），通过模拟领域冲突和样本变化来引导网络学习如何处理这些困难，从而为异构图像学习更通用的模态共享特征表示。此外，还开发了一种有效的细粒度依赖关系重建模块（FDR）来发现两种模态中共享的重要模式依赖关系。<br>
                    效果：在VI人物识别和VI面部识别数据集上的大量实验表明，提出的TOPLight方法优于当前最先进的方法，同时需要的计算资源更少。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visible-infrared recognition (VI recognition) is a challenging task due to the enormous visual difference across heterogeneous images. Most existing works achieve promising results by transfer learning, such as pretraining on the ImageNet, based on advanced neural architectures like ResNet and ViT. However, such methods ignore the negative influence of the pretrained colour prior knowledge, as well as their heavy computational burden makes them hard to deploy in actual scenarios with limited resources. In this paper, we propose a novel task-oriented pretrained lightweight neural network (TOPLight) for VI recognition. Specifically, the TOPLight method simulates the domain conflict and sample variations with the proposed fake domain loss in the pretraining stage, which guides the network to learn how to handle those difficulties, such that a more general modality-shared feature representation is learned for the heterogeneous images. Moreover, an effective fine-grained dependency reconstruction module (FDR) is developed to discover substantial pattern dependencies shared in two modalities. Extensive experiments on VI person re-identification and VI face recognition datasets demonstrate the superiority of the proposed TOPLight, which significantly outperforms the current state of the arts while demanding fewer computational resources.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1304.Where We Are and What We&#x27;re Looking At: Query Based Worldwide Image Geo-Localization Using Hierarchies and Scenes</span><br>
                <span class="as">Clark, BrandonandKerrigan, AlecandKulkarni, ParthParagandCepeda, VicenteVivancoandShah, Mubarak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Clark_Where_We_Are_and_What_Were_Looking_At_Query_Based_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23182-23190.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管计算机视觉技术的进步，但确定照片的精确经纬度仍然是一项困难的任务。<br>
                    动机：大部分先前的方法都选择学习查询图像的单一表示形式，然后在不同地理粒度级别进行分类，这种方法未能利用不同的视觉线索来提供不同层次的上下文信息。<br>
                    方法：我们引入了一种基于变压器的端到端架构，通过分层交叉注意力来挖掘图像中不同地理层次（我们称之为层次）和相应视觉场景信息之间的关系。<br>
                    效果：我们在4个标准的地理定位数据集上取得了最先进的精度，包括Im2GPS、Im2GPS3k、YFCC4k和YFCC26k，同时定性地展示了我们的方法如何学习不同的视觉层次和场景的不同表示，这是先前的方法所没有展示过的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Determining the exact latitude and longitude that a photo was taken is a useful and widely applicable task, yet it remains exceptionally difficult despite the accelerated progress of other computer vision tasks. Most previous approaches have opted to learn single representations of query images, which are then classified at different levels of geographic granularity. These approaches fail to exploit the different visual cues that give context to different hierarchies, such as the country, state, and city level. To this end, we introduce an end-to-end transformer-based architecture that exploits the relationship between different geographic levels (which we refer to as hierarchies) and the corresponding visual scene information in an image through hierarchical cross-attention. We achieve this by learning a query for each geographic hierarchy and scene type. Furthermore, we learn a separate representation for different environmental scenes, as different scenes in the same location are often defined by completely different visual features. We achieve state of the art accuracy on 4 standard geo-localization datasets : Im2GPS, Im2GPS3k, YFCC4k, and YFCC26k, as well as qualitatively demonstrate how our method learns different representations for different visual hierarchies and scenes, which has not been demonstrated in the previous methods. Above previous testing datasets mostly consist of iconic landmarks or images taken from social media, which makes the dataset a simple memory task, or makes it biased towards certain places. To address this issue we introduce a much harder testing dataset, Google-World-Streets-15k, comprised of images taken from Google Streetview covering the whole planet and present state of the art results. Our code can be found at https://github.com/AHKerrigan/GeoGuessNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1305.FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks</span><br>
                <span class="as">Han, XiaoandZhu, XiatianandYu, LichengandZhang, LiandSong, Yi-ZheandXiang, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2669-2680.png><br>
            
            <span class="tt"><span class="t0">研究问题：在时尚领域，存在各种视觉和语言（V+L）任务，如跨模态检索、文本引导的图像检索、多模态分类和图像描述。这些任务在输入/输出格式和数据集大小上差异巨大，通常需要设计特定任务的模型并进行独立微调，这导致参数效率低下且无法利用任务间的相关性。<br>
                    动机：为了解决这些问题，本文提出了一种针对时尚领域的多任务高效学习法FAME-ViL。<br>
                    方法：FAME-ViL采用单一模型处理多种异构时尚任务，因此具有更高的参数效率。其实现依赖于两个新的组件：（1）一个集成了跨注意力适配器和任务特定适配器的统一V+L模型的任务通用架构；（2）一种稳定而有效的多任务训练策略，支持从异构数据中学习并防止负迁移。<br>
                    效果：在四个时尚任务上的大量实验表明，FAME-ViL可以比替代方案节省61.5%的参数，同时显著优于传统独立训练的单任务模型。代码可在https://github.com/BrandonHanx/FAME-ViL获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In the fashion domain, there exists a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning. They differ drastically in each individual input/output format and dataset size. It has been common to design a task-specific model and fine-tune it independently from a pre-trained V+L model (e.g., CLIP). This results in parameter inefficiency and inability to exploit inter-task relatedness. To address such issues, we propose a novel FAshion-focused Multi-task Efficient learning method for Vision-and-Language tasks (FAME-ViL) in this work. Compared with existing approaches, FAME-ViL applies a single model for multiple heterogeneous fashion tasks, therefore being much more parameter-efficient. It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer. Extensive experiments on four fashion tasks show that our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models. Code is available at https://github.com/BrandonHanx/FAME-ViL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1306.Open-Vocabulary Attribute Detection</span><br>
                <span class="as">Bravo, Mar{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7041-7050.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在介绍开放词汇属性检测（OVAD）任务和相应的OVAD基准，以探索视觉语言模型学习到的对象级属性信息。<br>
                    动机：由于缺乏可靠的属性聚焦评估基准，现有的开放词汇任务主要关注对象类别，而对对象属性的研究有限。<br>
                    方法：创建了一个覆盖MS COCO 80个对象类别的117个属性类别的干净、密集注释的测试集，包括正负注释，实现开放词汇评估。<br>
                    效果：通过研究几种基础模型的属性检测性能，证明了该基准的价值。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by studying the attribute detection performance of several foundation models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1307.Test of Time: Instilling Video-Language Models With a Sense of Time</span><br>
                <span class="as">Bagad, PiyushandTapaswi, MakarandandSnoek, CeesG.M.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bagad_Test_of_Time_Instilling_Video-Language_Models_With_a_Sense_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2503-2516.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前视频理解模型在时间建模和理解方面存在挑战，如何让基础的视频-语言模型具有时间感知能力。<br>
                    动机：语言是实现强大泛化的关键，因此基础的视频-语言模型需要有对时间的理解。<br>
                    方法：本文提出了一种基于VideoCLIP模型的临时适应方案，通过少量的视频-文本数据进行后预训练，以赋予模型时间感知能力。<br>
                    效果：实验结果表明，经过适应性训练的模型在需要更高时间感知的任务上表现出色，为在无需大量数据和计算密集型重新训练的情况下，向现有视频-语言模型注入时间感知提供了初步步骤。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIP, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1308.OpenScene: 3D Scene Understanding With Open Vocabularies</span><br>
                <span class="as">Peng, SongyouandGenova, KyleandJiang, Chiyu{\textquotedblleft</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/815-824.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种替代方法OpenScene，通过在CLIP特征空间中对3研究问题：本文旨在提出一种替代方法OpenScene，通过在CLIP特征空间中对3D场景点进行联合嵌入文本和图像像素，使模型预测密集特征，实现零样本学习。<br>
                    动机：传统的3D场景理解方法依赖于标记的3D数据集进行单一任务的监督训练，而OpenScene则提出了一种新的思路，即通过在CLIP特征空间中对3D场景点进行联合嵌入文本和图像像素，使模型预测密集特征，实现零样本学习和开放词汇查询。<br>
                    方法：OpenScene首先为每个3D点推断出CLIP特征，然后根据与任意类别标签嵌入的相似性对其进行分类。这种方法无需任何标记的3D数据，可以有效地识别复杂3D场景中的对象、材料、功能、活动和房间类型。<br>
                    效果：实验结果表明，OpenScene在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Traditional 3D scene understanding approaches rely on labeled 3D datasets to train a model for a single task with supervision. We propose OpenScene, an alternative approach where a model predicts dense features for 3D scene points that are co-embedded with text and image pixels in CLIP feature space. This zero-shot approach enables task-agnostic training and open-vocabulary queries. For example, to perform SOTA zero-shot 3D semantic segmentation it first infers CLIP features for every 3D point and later classifies them based on similarities to embeddings of arbitrary class labels. More interestingly, it enables a suite of open-vocabulary scene understanding applications that have never been done before. For example, it allows a user to enter an arbitrary text query and then see a heat map indicating which parts of a scene match. Our approach is effective at identifying objects, materials, affordances, activities, and room types in complex 3D scenes, all using a single model trained without any labeled 3D data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1309.KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation</span><br>
                <span class="as">Huang, ZhongzhenandZhang, XiaofanandZhang, Shaoting</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19809-19818.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Radiology report generation aims to automatically generate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including vision, language, and medical terminology. We propose a Knowledge-injected U-Transformer (KiUT) to learn multi-level visual representation and adaptively distill the information with contextual and clinical knowledge for word prediction. In detail, a U-connection schema between the encoder and decoder is designed to model interactions between different modalities. And a symptom graph and an injected knowledge distiller are developed to assist the report generation. Experimentally, we outperform state-of-the-art methods on two widely used benchmark datasets: IU-Xray and MIMIC-CXR. Further experimental results prove the advantages of our architecture and the complementary benefits of the injected knowledge.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1310.ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations</span><br>
                <span class="as">Achlioptas, PanosandHuang, IanandSung, MinhyukandTulyakov, SergeyandGuibas, Leonidas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12685-12694.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过自然语言编辑3D模型的几何形状。<br>
                    动机：现有的技术需要专门的技能来编辑3D模型的几何形状，我们希望通过自然语言的使用来简化这个过程。<br>
                    方法：我们创建了最全面的自然语言描述形状差异的语料库ShapeTalk，并开发了一个通用框架ChangeIt3D，该框架使用任意3D形状生成模型来产生与编辑或变形描述更一致的输出。<br>
                    效果：我们的框架可以直接使用3D到语言的数据进行训练，避免了像神经渲染这样的2D到3D提升方法，大大提高了编辑效率和准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Editing 3D geometry is a challenging task requiring specialized skills. In this work, we aim to facilitate the task of editing the geometry of 3D models through the use of natural language. For example, we may want to modify a 3D chair model to "make its legs thinner" or to "open a hole in its back". To tackle this problem in a manner that promotes open-ended language use and enables fine-grained shape edits, we introduce the most extensive existing corpus of natural language utterances describing shape differences: ShapeTalk. ShapeTalk contains over half a million discriminative utterances produced by contrasting the shapes of common 3D objects for a variety of object classes and degrees of similarity. We also introduce a generic framework, ChangeIt3D, which builds on ShapeTalk and can use an arbitrary 3D generative model of shapes to produce edits that align the output better with the edit or deformation description. Finally, we introduce metrics for the quantitative evaluation of language-assisted shape editing methods that reflect key desiderata within this editing setup. We note that ShapeTalk allows methods to be trained with explicit 3D-to-language data, bypassing the necessity of "lifting" 2D to 3D using methods like neural rendering, as required by extant 2D image-language foundation models. Our code and data are publicly available at https://changeit3d.github.io/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1311.Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers</span><br>
                <span class="as">Kim, DahunandAngelova, AneliaandKuo, Weicheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11144-11154.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何弥合图像级预训练和开放词汇对象检测之间的差距。<br>
                    动机：目前的图像-文本预训练方法无法有效应用于开放词汇对象检测。<br>
                    方法：提出了区域感知的开放词汇视觉变换器（RO-ViT），在预训练阶段，对位置嵌入的区域进行随机裁剪和调整大小，以更好地匹配检测微调阶段的区域级别位置嵌入的使用。同时，使用焦点损失替换对比学习中的常见 softmax 交叉熵损失，以更好地学习有信息但困难的例子。最后，利用最新的新奇目标提议改进开放词汇检测微调。<br>
                    效果：在LVIS和COCO开放词汇检测基准测试以及零样本转移中评估了完整的模型。RO-ViT在LVIS上实现了32.1 APr的先进性能，比现有最佳方法高出5.8个百分点，并且在竞争性的零样本转移检测中也表现出色。令人惊讶的是，RO-ViT还改善了图像级别的表示，并在COCO和Flickr图像-文本检索基准测试中的9个指标上实现了最先进的性能，超过了具有更大模型的竞争性方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) -- a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 APr on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detection. Surprisingly, RO-ViT improves the image-level representation as well and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr image-text retrieval benchmarks, outperforming competitive approaches with larger models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1312.Learning Transferable Spatiotemporal Representations From Natural Script Knowledge</span><br>
                <span class="as">Zeng, ZiyunandGe, YuyingandLiu, XihuiandChen, BinandLuo, PingandXia, Shu-TaoandGe, Yixiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23079-23089.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的预训练视频模型主要在高度策划的数据集上进行，无法很好地捕捉到时空语义，限制了视频理解的进步。<br>
                    动机：受图像-文本预训练成功的启发，作者提出利用语言语义来提升可转移的时空表示学习。<br>
                    方法：引入新的预训练任务——"转向视频进行转录排序"（TVTS），通过关注学到的视频表示对打乱的自动语音识别脚本进行排序。该方法不依赖描述性字幕，只从视频中学习，即利用自然转录的语音知识提供有用的时空语义。<br>
                    效果：该方法在各种基准测试中表现出强大的初始时空表示能力，例如，在SSV2上比VideoMAE提高了+13.6%。代码可在https://github.com/TencentARC/TVTS获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-training on large-scale video data has become a common recipe for learning transferable spatiotemporal representations in recent years. Despite some progress, existing methods are mostly limited to highly curated datasets (e.g., K400) and exhibit unsatisfactory out-of-the-box representations. We argue that it is due to the fact that they only capture pixel-level knowledge rather than spatiotemporal semantics, which hinders further progress in video understanding. Inspired by the great success of image-text pre-training (e.g., CLIP), we take the first step to exploit language semantics to boost transferable spatiotemporal representation learning. We introduce a new pretext task, Turning to Video for Transcript Sorting (TVTS), which sorts shuffled ASR scripts by attending to learned video representations. We do not rely on descriptive captions and learn purely from video, i.e., leveraging the natural transcribed speech knowledge to provide noisy but useful semantics over time. Our method enforces the vision model to contextualize what is happening over time so that it can re-organize the narrative transcripts, and can seamlessly apply to large-scale uncurated video data in the real world. Our method demonstrates strong out-of-the-box spatiotemporal representations on diverse benchmarks, e.g., +13.6% gains over VideoMAE on SSV2 via linear probing. The code is available at https://github.com/TencentARC/TVTS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1313.3D Concept Learning and Reasoning From Multi-View Images</span><br>
                <span class="as">Hong, YiningandLin, ChunruandDu, YilunandChen, ZhenfangandTenenbaum, JoshuaB.andGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9202-9212.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过多视角图像进行三维视觉推理。<br>
                    动机：人类能够通过收集周围世界的多视角观察来准确推理三维空间，受此启发，研究人员提出了一个新的大规模3D多视角视觉问答（3DMV-VQA）基准测试。<br>
                    方法：研究人员使用Habitat模拟器在环境中主动移动并捕获RGB图像，创建了一个包含大约5000个场景、60万个图像和5万个问题的数据集。他们评估了各种最先进的视觉推理模型，并提出了一种从多视角图像推断世界紧凑3D表示的方法，该方法进一步基于开放词汇语义概念，并在这些3D表示上执行推理。<br>
                    效果：实验结果表明，他们的框架比基线模型有大幅度的改进，但这个挑战在很大程度上仍未解决。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world. Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator. In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models for visual reasoning on our benchmark and find that they all perform poorly. We suggest that a principled approach for 3D reasoning from multi-view images should be to infer a compact 3D representation of the world from the multi-view images, which is further grounded on open-vocabulary semantic concepts, and then to execute reasoning on these 3D representations. As the first step towards this approach, we propose a novel 3D concept learning and reasoning (3D-CLR) framework that seamlessly combines these components via neural fields, 2D pre-trained vision-language models, and neural reasoning operators. Experimental results suggest that our framework outperforms baseline models by a large margin, but the challenge remains largely unsolved. We further perform an in-depth analysis of the challenges and highlight potential future directions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1314.Integrally Pre-Trained Transformer Pyramid Networks</span><br>
                <span class="as">Tian, YunjieandXie, LingxiandWang, ZhaozhiandWei, LonghuiandZhang, XiaopengandJiao, JianbinandWang, YaoweiandTian, QiandYe, Qixiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Integrally_Pre-Trained_Transformer_Pyramid_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18610-18620.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种基于掩蔽图像建模（MIM）的预训练框架，以最小化MIM与下游识别任务之间的转移差距。<br>
                    动机：目前的预训练模型在处理视觉识别任务时存在一定的转移差距。作者认为，通过联合预训练主干网络和颈部网络，可以减小这种差距。<br>
                    方法：作者提出了两个技术贡献。首先，他们在预训练阶段通过插入特征金字塔来统一重建和识别颈部网络。其次，他们用多阶段监督的掩蔽特征建模（MFM）补充了掩蔽图像建模（MIM）。预训练后的模型被称为整体预训练变换金字塔网络（iTPNs），是强大的基础模型用于视觉识别。<br>
                    效果：实验结果表明，基础/大型级别的iTPN在ImageNet-1K上达到了86.2%/87.8%的Top-1准确率，使用Mask-RCNN在COCO对象检测上达到了53.2%/55.6%的box AP，使用UPerHead在ADE20K语义分割上达到了54.7%/57.7%的mIoU——所有这些结果都创造了新的记录。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present an integral pre-training framework based on masked image modeling (MIM). We advocate for pre-training the backbone and neck jointly so that the transfer gap between MIM and downstream recognition tasks is minimal. We make two technical contributions. First, we unify the reconstruction and recognition necks by inserting a feature pyramid into the pre-training stage. Second, we complement mask image modeling (MIM) with masked feature modeling (MFM) that offers multi-stage supervision to the feature pyramid. The pre-trained models, termed integrally pre-trained transformer pyramid networks (iTPNs), serve as powerful foundation models for visual recognition. In particular, the base/large-level iTPN achieves an 86.2%/87.8% top-1 accuracy on ImageNet-1K, a 53.2%/55.6% box AP on COCO object detection with 1x training schedule using Mask-RCNN, and a 54.7%/57.7% mIoU on ADE20K semantic segmentation using UPerHead -- all these results set new records. Our work inspires the community to work on unifying upstream pre-training and downstream fine-tuning tasks. Code is available at https://github.com/sunsmarterjie/iTPN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1315.Open-Vocabulary Point-Cloud Object Detection Without 3D Annotation</span><br>
                <span class="as">Lu, YuhengandXu, ChenfengandWei, XiaobaoandXie, XiaodongandTomizuka, MasayoshiandKeutzer, KurtandZhang, Shanghang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1190-1199.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放词汇的3D点云检测问题，即通过文本描述识别新的对象。<br>
                    动机：目前的3D点云检测方法需要大量的标注数据，而我们的目标是在没有3D标注的情况下进行开放词汇的3D对象检测。<br>
                    方法：我们采用分治策略，首先开发一个可以学习通用表示的点云检测器来定位各种对象，然后将文本和点云表示连接起来，使检测器能够根据文本提示对新的物体类别进行分类。具体来说，我们利用丰富的图像预训练模型，让点云检测器在2D预训练检测器预测的2D边界框的指导下学习定位物体。此外，我们还提出了一种新的去偏三元组跨模态对比学习法，将图像、点云和文本的模态连接起来，使点云检测器能够从视觉语言预训练模型（如CLIP）中受益。<br>
                    效果：实验结果表明，我们的方法在ScanNet和SUN RGB-D数据集上分别比一系列基线提高了至少3.03点和7.47点。此外，我们还提供了全面分析来解释我们的方法为何有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of open-vocabulary detection is to identify novel objects based on arbitrary textual descriptions. In this paper, we address open-vocabulary 3D point-cloud detection by a dividing-and-conquering strategy, which involves: 1) developing a point-cloud detector that can learn a general representation for localizing various objects, and 2) connecting textual and point-cloud representations to enable the detector to classify novel object categories based on text prompting. Specifically, we resort to rich image pre-trained models, by which the point-cloud detector learns localizing objects under the supervision of predicted 2D bounding boxes from 2D pre-trained detectors. Moreover, we propose a novel de-biased triplet cross-modal contrastive learning to connect the modalities of image, point-cloud and text, thereby enabling the point-cloud detector to benefit from vision-language pre-trained models, i.e., CLIP. The novel use of image and vision-language pre-trained models for point-cloud detectors allows for open-vocabulary 3D object detection without the need for 3D annotations. Experiments demonstrate that the proposed method improves at least 3.03 points and 7.47 points over a wide range of baselines on the ScanNet and SUN RGB-D datasets, respectively. Furthermore, we provide a comprehensive analysis to explain why our approach works.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1316.Detecting Backdoors in Pre-Trained Encoders</span><br>
                <span class="as">Feng, ShiweiandTao, GuanhongandCheng, SiyuanandShen, GuangyuandXu, XiangzheandLiu, YingqiandZhang, KaiyuanandMa, ShiqingandZhang, Xiangyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Detecting_Backdoors_in_Pre-Trained_Encoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16352-16362.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的后门攻击方法主要针对有监督学习设置，无法处理预训练的编码器，尤其是当输入标签不可用时。<br>
                    动机：计算机视觉中的自我监督学习在无标签数据上进行训练，以获取高质量的输入数据嵌入。新兴的后门攻击对编码器暴露了自我监督学习的关键技术漏洞，因为下游分类器（即使在干净的数据上进一步训练）可能会继承编码器的后门行为。<br>
                    方法：本文提出了DECREE，这是第一个针对预训练编码器的后门检测方法，既不需要分类器头信息也不需要输入标签。<br>
                    效果：我们在超过400个编码器上进行了评估，这些编码器是在3种范式下植入后门的。我们的方法是有效的，即使在只有有限的或没有访问预训练数据集的情况下，也能在ImageNet和OpenAI的CLIP 4亿张图像-文本对上预训练的图像编码器上保持高检测精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning in computer vision trains on unlabeled data, such as images or (image, text) pairs, to obtain an image encoder that learns high-quality embeddings for input data. Emerging backdoor attacks towards encoders expose crucial vulnerabilities of self-supervised learning, since downstream classifiers (even further trained on clean data) may inherit backdoor behaviors from encoders. Existing backdoor detection methods mainly focus on supervised learning settings and cannot handle pre-trained encoders especially when input labels are not available. In this paper, we propose DECREE, the first backdoor detection approach for pre-trained encoders, requiring neither classifier headers nor input labels. We evaluate DECREE on over 400 encoders trojaned under 3 paradigms. We show the effectiveness of our method on image encoders pre-trained on ImageNet and OpenAI's CLIP 400 million image-text pairs. Our method consistently has a high detection accuracy even if we have only limited or no access to the pre-training dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1317.CREPE: Can Vision-Language Foundation Models Reason Compositionally?</span><br>
                <span class="as">Ma, ZixianandHong, JerryandGul, MustafaOmerandGandhi, MonaandGao, IrenaandKrishna, Ranjay</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CREPE_Can_Vision-Language_Foundation_Models_Reason_Compositionally_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10910-10921.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管大型视觉和语言预训练模型在性能上有所提升，但它们在组合性方面仍存在困难。<br>
                    动机：为了解决这个问题，研究人员引入了一个新的组合性评估基准CREPE，以测量认知科学文献中确定的组合性的两个重要方面：系统性和生产力。<br>
                    方法：CREPE测试集包含超过370K的图像-文本对和三个不同的可见-不可见分割。同时，还生成了325K、316K和309K的难负样本标题。<br>
                    效果：实验结果表明，当新颖的组合在检索集中占主导地位时，模型性能会持续下降，Recall@1最高下降9%。随着复杂性的增加，模型的检索成功率也会降低，高复杂度时常常接近随机概率。这些结果不受模型和训练数据集大小的影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that--across 7 architectures trained with 4 algorithms on massive datasets--they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, CREPE, which measures two important aspects of compositionality identified by cognitive science literature: systematicity and productivity. To measure systematicity, CREPE consists of a test dataset containing over 370K image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate 325K, 316K, and 309K hard negative captions for a subset of the pairs. To test productivity, CREPE contains 17K image-text pairs with nine different complexities plus 278K hard negative captions with atomic, swapping, and negation foils. The datasets are generated by repurposing the Visual Genome scene graphs and region descriptions and applying handcrafted templates and GPT-3. For systematicity, we find that model performance decreases consistently when novel compositions dominate the retrieval set, with Recall@1 dropping by up to 9%. For productivity, models' retrieval success decays as complexity increases, frequently nearing random chance at high complexity. These results hold regardless of model and training dataset size.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1318.Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks</span><br>
                <span class="as">Wang, WenhuiandBao, HangboandDong, LiandBjorck, JohanandPeng, ZhiliangandLiu, QiangandAggarwal, KritiandMohammed, OwaisKhanandSinghal, SakshamandSom, SubhojitandWei, Furu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19175-19186.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在介绍一种通用的多模态基础模型BEiT-3，该模型在视觉和视觉语言任务上均取得了优秀的迁移性能。<br>
                    动机：目前，语言、视觉和多模态预训练正在逐渐融合。我们通过三个方向推进这一大融合：主干架构、预训练任务和模型扩展。<br>
                    方法：我们使用Multiway Transformers进行通用建模，其模块化架构可以实现深度融合和模态特定编码。基于共享的主干，我们对图像（Imglish）、文本（英语）和图像-文本对（“平行句子”）进行统一的掩码“语言”建模。<br>
                    效果：实验结果表明，BEiT-3在物体检测（COCO）、语义分割（ADE20K）、图像分类（ImageNet）、视觉推理（NLVR2）、视觉问答（VQAv2）、图像描述（COCO）和跨模态检索（Flickr30K，COCO）等任务上表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves excellent transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We use Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked "language" modeling on images (Imglish), texts (English), and image-text pairs ("parallel sentences") in a unified manner. Experimental results show that BEiT-3 obtains remarkable performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1319.Weakly Supervised Posture Mining for Fine-Grained Classification</span><br>
                <span class="as">Tang, ZhenchaoandYang, HualinandChen, CalvinYu-Chian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Weakly_Supervised_Posture_Mining_for_Fine-Grained_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23735-23744.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高细粒度分类任务的准确性，特别是在鸟类等常见视觉类别的子类别之间存在微妙差异的情况下。<br>
                    动机：过去的工作主要关注图像中单个判别区域的独立特征，而忽视了整个图像中不同判别区域之间的联系。然而，不同判别区域之间的关系包含了丰富的姿势信息，通过加入姿势信息，模型可以学习对象的行为，从而提高分类性能。<br>
                    方法：提出了一种名为PMRC（姿势挖掘和反向交叉熵）的新型细粒度框架，该框架可以与不同的骨干网络结合使用。在PMRC中，我们使用Deep Navigator从图像中生成判别区域，然后使用它们构建图。通过消息传递聚合图并获取分类结果。为了迫使PMRC学习如何挖掘姿势信息，我们设计了一种新的训练范式，使Deep Navigator和消息传递能够相互通信和共同训练。此外，我们还提出了反向交叉熵（RCE），并证明与交叉熵（CE）相比，RCE不仅可以提高模型的准确性，还可以推广到提高其他类型的细粒度分类模型的准确性。<br>
                    效果：在基准数据集上的实验结果表明，PMRC可以实现最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Because the subtle differences between the different sub-categories of common visual categories such as bird species, fine-grained classification has been seen as a challenging task for many years. Most previous works focus towards the features in the single discriminative region isolatedly, while neglect the connection between the different discriminative regions in the whole image. However, the relationship between different discriminative regions contains rich posture information and by adding the posture information, model can learn the behavior of the object which attribute to improve the classification performance. In this paper, we propose a novel fine-grained framework named PMRC (posture mining and reverse cross-entropy), which is able to combine with different backbones to good effect. In PMRC, we use the Deep Navigator to generate the discriminative regions from the images, and then use them to construct the graph. We aggregate the graph by message passing and get the classification results. Specifically, in order to force PMRC to learn how to mine the posture information, we design a novel training paradigm, which makes the Deep Navigator and message passing communicate and train together. In addition, we propose the reverse cross-entropy (RCE) and demomenstate that compared to the cross-entropy (CE), RCE can not only promote the accurracy of our model but also generalize to promote the accuracy of other kinds of fine-grained classification models. Experimental results on benchmark datasets confirm that PMRC can achieve state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1320.LAVENDER: Unifying Video-Language Understanding As Masked Language Modeling</span><br>
                <span class="as">Li, LinjieandGan, ZheandLin, KevinandLin, Chung-ChingandLiu, ZichengandLiu, CeandWang, Lijuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23119-23129.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种统一的视频-语言（VidL）框架，以简化模型架构并实现跨任务的统一。<br>
                    动机：现有的VidL模型在模型架构和训练目标上需要针对每个任务进行特定设计，缺乏统一性。<br>
                    方法：本文提出了一种名为LAVENDER的统一的VidL框架，其中使用Masked Language Modeling (MLM)作为所有预训练和下游任务的通用接口。这种统一化使得只需要一个轻量级的MLM头，而不是具有更多参数的解码器，放在多模态编码器的顶部。<br>
                    效果：实验结果表明，这个统一的框架在14个VidL基准测试中取得了有竞争力的性能，包括视频问答、文本到视频检索和视频字幕等。进一步的广泛分析表明，LAVENDER可以无缝支持所有下游任务，只需一组参数值进行多任务微调；可以在有限的训练样本下推广到各种下游任务；并能够在视频问答任务上实现零样本评估。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unified vision-language frameworks have greatly advanced in recent years, most of which adopt an encoder-decoder architecture to unify image-text tasks as sequence-to-sequence generation. However, existing video-language (VidL) models still require task-specific designs in model architecture and training objectives for each task. In this work, we explore a unified VidL framework LAVENDER, where Masked Language Modeling (MLM) is used as the common interface for all pre-training and downstream tasks. Such unification leads to a simplified model architecture, where only a lightweight MLM head, instead of a decoder with much more parameters, is needed on top of the multimodal encoder. Surprisingly, experimental results show that this unified framework achieves competitive performance on 14 VidL benchmarks, covering video question answering, text-to-video retrieval and video captioning. Extensive analyses further demonstrate LAVENDER can (i) seamlessly support all downstream tasks with just a single set of parameter values when multi-task finetuned; (ii) generalize to various downstream tasks with limited training samples; and (iii) enable zero-shot evaluation on video question answering tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1321.Shifted Diffusion for Text-to-Image Generation</span><br>
                <span class="as">Zhou, YufanandLiu, BingchenandZhu, YizheandYang, XiaoandChen, ChangyouandXu, Jinhui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10157-10166.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种名为Corgi的新的文本到图像生成方法。<br>
                    动机：现有的DALL-E 2模型在文本到图像生成方面存在不足，本研究希望通过改进其扩散模型来提高图像嵌入生成的效率和效果。<br>
                    方法：基于提出的移位扩散模型，通过设计新的初始分布和扩散的转换步骤，将预训练的CLIP模型的知识无缝地编码到扩散过程中。<br>
                    效果：实验结果表明，该方法在从文本生成图像嵌入方面比强大的DALL-E 2基线更有效，从而产生更好的文本到图像生成效果。此外，该方法还实现了半监督和无语言的训练，即使只有部分或没有图像在训练数据集中具有关联的标题，也能进行有效的文本到图像生成。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Corgi, a novel method for text-to-image generation. Corgi is based on our proposed shifted diffusion model, which achieves better image embedding generation from input text. Different from the baseline diffusion model used in DALL-E 2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model in its diffusion process by designing a new initialization distribution and a new transition step of the diffusion. Compared to the strong DALL-E 2 baseline, our method performs better in generating image embedding from the text in terms of both efficiency and effectiveness, which consequently results in better text-to-image generation. Extensive large-scale experiments are conducted and evaluated in terms of both quantitative measures and human evaluation, indicating a stronger generation ability of our method compared to existing ones. Furthermore, our model enables semi-supervised and language-free training for text-to-image generation, where only part or none of the images in the training dataset have an associated caption. Trained with only 1.7% of the images being captioned, our semi-supervised model obtains FID results comparable to DALL-E 2 on zero-shot text-to-image generation evaluated on MS-COCO. Corgi also achieves new state-of-the-art results across different datasets on downstream language-free text-to-image generation tasks, outperforming the previous method, Lafite, by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1322.OvarNet: Towards Open-Vocabulary Object Attribute Recognition</span><br>
                <span class="as">Chen, KeyanandJiang, XiaolongandHu, YaoandTang, XuandGao, YanandChen, JianqiandXie, Weidi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_OvarNet_Towards_Open-Vocabulary_Object_Attribute_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23518-23527.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在训练阶段没有手动标注的情况下，如何在图像中同时检测对象并推断其视觉属性的问题。<br>
                    动机：目前的模型大多将目标检测和属性分类分开处理，缺乏对开放词汇下的目标检测和属性分类的研究。<br>
                    方法：首先采用两阶段策略CLIP-Attr进行开放词汇的目标检测和属性分类；然后通过联合所有可用数据集进行联邦学习来微调CLIP模型，使视觉表示与属性对齐；最后，为了提高效率，使用知识蒸馏训练了一个端到端的Faster-RCNN类型模型，该模型在语义类别和属性上执行类别无关的对象提议和分类。<br>
                    效果：实验结果表明，识别语义类别和属性对于视觉场景理解是互补的，即联合训练目标检测和属性预测在很大程度上优于将这两个任务独立处理的现有方法，显示出对新属性和新类别的强大泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario. To achieve this goal, we make the following contributions: (i) we start with a naive two-stage approach for open-vocabulary object detection and attribute classification, termed CLIP-Attr. The candidate objects are first proposed with an offline RPN and later classified for semantic category and attributes; (ii) we combine all available datasets and train with a federated strategy to finetune the CLIP model, aligning the visual representation with attributes, additionally, we investigate the efficacy of leveraging freely available online image-caption pairs under weakly supervised learning; (iii) in pursuit of efficiency, we train a Faster-RCNN type model end-to-end with knowledge distillation, that performs class-agnostic object proposals and classification on semantic categories and attributes with classifiers generated from a text encoder; Finally, (iv) we conduct extensive experiments on VAW, MS-COCO, LSA, and OVAD datasets, and show that recognition of semantic category and attributes is complementary for visual scene understanding, i.e., jointly training object detection and attributes prediction largely outperform existing approaches that treat the two tasks independently, demonstrating strong generalization ability to novel attributes and categories.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1323.TarViS: A Unified Approach for Target-Based Video Segmentation</span><br>
                <span class="as">Athar, AliandHermans, AlexanderandLuiten, JonathonandRamanan, DevaandLeibe, Bastian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Athar_TarViS_A_Unified_Approach_for_Target-Based_Video_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18738-18748.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频分割领域目前被分割成不同的任务，跨越多个基准。尽管最先进的技术取得了快速进展，但当前的方法绝大多数都是针对特定任务的，无法在概念上泛化到其他任务。<br>
                    动机：受到最近具有多任务能力的方法和架构的启发，我们提出了TarViS：一种新的、统一的网络架构，可以应用于任何需要在视频中分割一组任意定义的“目标”的任务。<br>
                    方法：我们的方法是灵活的，因为任务如何定义这些目标，它将这些目标建模为抽象的“查询”，然后用于预测像素精确的目标掩码。一个单一的TarViS模型可以在不同的数据集集合上进行联合训练，并在推理过程中无需任何特定任务的再训练就可以在任务之间进行热交换。<br>
                    效果：为了证明其有效性，我们将TarViS应用于四个不同的任务，即视频实例分割（VIS）、视频全景分割（VPS）、视频对象分割（VOS）和点样本引导跟踪（PET）。我们统一、联合训练的模型在这四个任务中的五个基准上实现了最先进的性能，在其余两个基准上也有竞争力的表现。代码和模型权重可在以下链接获取：https://github.com/Ali2500/TarViS</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), Video Object Segmentation (VOS) and Point Exemplar-guided Tracking (PET). Our unified, jointly trained model achieves state-of-the-art performance on 5/7 benchmarks spanning these four tasks, and competitive performance on the remaining two. Code and model weights are available at: https://github.com/Ali2500/TarViS</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1324.EC2: Emergent Communication for Embodied Control</span><br>
                <span class="as">Mu, YaoandYao, ShunyuandDing, MingyuandLuo, PingandGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6704-6714.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用多模态预训练快速学习新环境中的行动，并实现视频演示和语言指令的协同学习。<br>
                    动机：现有的方法通过对比学习强制对齐两种模态，但更好的模拟它们的互补差异可以导致更全面的表示以进行下游适应。<br>
                    方法：提出Emergent Communication for Embodied Control (EC^2)，一种用于少样本具身控制的新颖视频-语言表示预训练方案。主要思想是通过涌现交流学习视频的“无监督语言”，连接视频细节的语义和自然语言的结构。<br>
                    效果：在Metaworld和Franka Kitchen具身基准测试中，EC^2在作为任务输入的视频和文本方面始终优于先前的对比学习方法。进一步的消融实验确认了涌现语言的重要性，这对视频和语言学习都有利，明显优于使用预训练的视频字幕。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Embodied control requires agents to leverage multi-modal pre-training to quickly learn how to act in new environments, where video demonstrations contain visual and motion details needed for low-level perception and control, and language instructions support generalization with abstract, symbolic structures. While recent approaches apply contrastive learning to force alignment between the two modalities, we hypothesize better modeling their complementary differences can lead to more holistic representations for downstream adaption. To this end, we propose Emergent Communication for Embodied Control (EC^2), a novel scheme to pre-train video-language representations for few-shot embodied control. The key idea is to learn an unsupervised "language" of videos via emergent communication, which bridges the semantics of video details and structures of natural language. We learn embodied representations of video trajectories, emergent language, and natural language using a language model, which is then used to finetune a lightweight policy network for downstream control. Through extensive experiments in Metaworld and Franka Kitchen embodied benchmarks, EC^2 is shown to consistently outperform previous contrastive learning methods for both videos and texts as task inputs. Further ablations confirm the importance of the emergent language, which is beneficial for both video and language learning, and significantly superior to using pre-trained video captions. We also present a quantitative and qualitative analysis of the emergent language and discuss future directions toward better understanding and leveraging emergent communication in embodied tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1325.I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification</span><br>
                <span class="as">Naeem, MuhammadFerjadandKhan, MuhammadGulZainAliandXian, YongqinandAfzal, MuhammadZeshanandStricker, DidierandVanGool, LucandTombari, Federico</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15169-15179.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模语言模型为零样本图像分类提供文本监督。<br>
                    动机：现有的方法需要访问高质量的信息源，且仅限于单一信息源，而训练在网络级文本上的大型语言模型具有将所学知识用于多种任务的惊人能力。<br>
                    方法：通过让大型语言模型根据不同注释者的几个文本描述生成每个类别的多个文本描述（称为视图），然后使用这些类视图学习多视图语义嵌入进行零样本图像分类。<br>
                    效果：实验表明，每个类别的文本视图提供了互补的信息，使模型能够学习高度判别性的类别嵌入。此外，I2MVFormer在利用来自大型语言模型的多视图文本监督方面优于基线模型，并在三个公共基准数据集上建立了新的无监督语义嵌入的零样本图像分类的最先进的状态。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent works have shown that unstructured text (documents) from online sources can serve as useful auxiliary information for zero-shot image classification. However, these methods require access to a high-quality source like Wikipedia and are limited to a single source of information. Large Language Models (LLM) trained on web-scale text show impressive abilities to repurpose their learned knowledge for a multitude of tasks. In this work, we provide a novel perspective on using an LLM to provide text supervision for a zero-shot image classification model. The LLM is provided with a few text descriptions from different annotators as examples. The LLM is conditioned on these examples to generate multiple text descriptions for each class (referred to as views). Our proposed model, I2MVFormer, learns multi-view semantic embeddings for zero-shot image classification with these class views. We show that each text view of a class provides complementary information allowing a model to learn a highly discriminative class embedding. Moreover, we show that I2MVFormer is better at consuming the multi-view text supervision from LLM compared to baseline models. I2MVFormer establishes a new state-of-the-art on three public benchmark datasets for zero-shot image classification with unsupervised semantic embeddings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1326.Context-Aware Alignment and Mutual Masking for 3D-Language Pre-Training</span><br>
                <span class="as">Jin, ZhaoandHayat, MunawarandYang, YuweiandGuo, YulanandLei, Yinjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10984-10994.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D视觉语言推理在有效人机交互中的重要性，以及当前方法对特定任务的依赖性和缺乏可迁移的通用表示的问题。<br>
                    动机：尽管图像-文本数据的视觉语言预训练取得了令人鼓舞的进步，但由于点云的高度稀疏和不规则结构以及3D对象空间关系的视点变化引起的模糊性，3D语言预训练仍然是一个开放的问题。<br>
                    方法：本文提出了一种通用的3D语言预训练方法，通过学习通用表示来解决3D语言推理的多个方面。该方法包括两个主要部分：1）上下文感知的空间语义对齐，用于建立点云和文本之间的细粒度对应关系；2）互信息3D语言掩码建模，以实现跨模态信息交换。<br>
                    效果：实验结果表明，所提出的3D语言预训练方法一旦适应各种下游任务，包括3D视觉定位、3D密集字幕生成和3D问答，就能取得良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D visual language reasoning plays an important role in effective human-computer interaction. The current approaches for 3D visual reasoning are task-specific, and lack pre-training methods to learn generic representations that can transfer across various tasks. Despite the encouraging progress in vision-language pre-training for image-text data, 3D-language pre-training is still an open issue due to limited 3D-language paired data, highly sparse and irregular structure of point clouds and ambiguities in spatial relations of 3D objects with viewpoint changes. In this paper, we present a generic 3D-language pre-training approach, that tackles multiple facets of 3D-language reasoning by learning universal representations. Our learning objective constitutes two main parts. 1) Context aware spatial-semantic alignment to establish fine-grained correspondence between point clouds and texts. It reduces relational ambiguities by aligning 3D spatial relationships with textual semantic context. 2) Mutual 3D-Language Masked modeling to enable cross-modality information exchange. Instead of reconstructing sparse 3D points for which language can hardly provide cues, we propose masked proposal reasoning to learn semantic class and mask-invariant representations. Our proposed 3D-language pre-training method achieves promising results once adapted to various downstream tasks, including 3D visual grounding, 3D dense captioning and 3D question answering. Our codes are available at https://github.com/leolyj/3D-VLP</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1327.Generalized Decoding for Pixel, Image, and Language</span><br>
                <span class="as">Zou, XueyanandDou, Zi-YiandYang, JianweiandGan, ZheandLi, LinjieandLi, ChunyuanandDai, XiyangandBehl, HarkiratandWang, JianfengandYuan, LuandPeng, NanyunandWang, LijuanandLee, YongJaeandGao, Jianfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15116-15127.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种通用解码器X-Decoder，能够无缝预测像素级分割和语言标记。<br>
                    动机：现有的模型无法同时处理图像分割和视觉语言任务，而X-Decoder通过在相同的语义空间中解码不同的像素级和标记级输出，解决了这一问题。<br>
                    方法：X-Decoder接受两种类型的查询输入：(i)非语义的通用查询，(ii)从文本输入中产生的语义查询，以在同一语义空间中解码不同类型的像素级和标记级输出。<br>
                    效果：实验结果显示，X-Decoder在多种下游任务上具有强大的迁移能力，并在开放词汇分割和引用分割等任务上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly. X-Decoder takes as input two types of queries: (i) generic non-semantic queries and (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks. Further, our design enables seamless interactions across tasks at different granularities and brings mutual benefits by learning a common and rich pixel-level visual-semantic understanding space, without any pseudo-labeling. After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. Notably, it achieves (1) state-of-the-art results on open-vocabulary segmentation and referring segmentation on eight datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and VL tasks; and (3) flexibility for efficient finetuning and novel task composition. Code, demo, video and visualization are available at: https://x-decoder-vl.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1328.Towards Unified Scene Text Spotting Based on Sequence Generation</span><br>
                <span class="as">Kil, TaehoandKim, SeonghyeonandSeo, SukminandKim, YoonsikandKim, Daehee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kil_Towards_Unified_Scene_Text_Spotting_Based_on_Sequence_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15223-15232.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自动回归模型在端到端文本检测任务中的限制，如特定检测格式、忽略不同文本形状以及最大可检测文本数量有限等问题。<br>
                    动机：目前的自动回归模型在端到端文本检测任务上取得了一些进展，但它们使用特定的检测格式，忽视了各种文本形状，并且其能检测的文本数量有限。<br>
                    方法：我们提出了一种名为UNITS的统一场景文本检测器，该模型统一了各种检测格式（包括四边形和多边形），使其能够检测任意形状的文本。此外，我们还应用了起点提示技术，使模型可以从任意起点提取文本，从而提取出比训练实例更多的文本。<br>
                    效果：实验结果表明，我们的方法在性能上与最先进的方法相当。进一步的分析显示，UNITS可以提取出比训练实例更多的文本。我们的代码可以在https://github.com/clovaai/units获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sequence generation models have recently made significant progress in unifying various vision tasks. Although some auto-regressive models have demonstrated promising results in end-to-end text spotting, they use specific detection formats while ignoring various text shapes and are limited in the maximum number of text instances that can be detected. To overcome these limitations, we propose a UNIfied scene Text Spotter, called UNITS. Our model unifies various detection formats, including quadrilaterals and polygons, allowing it to detect text in arbitrary shapes. Additionally, we apply starting-point prompting to enable the model to extract texts from an arbitrary starting point, thereby extracting more texts beyond the number of instances it was trained on. Experimental results demonstrate that our method achieves competitive performance compared to state-of-the-art methods. Further analysis shows that UNITS can extract a larger number of texts than it was trained on. We provide the code for our method at https://github.com/clovaai/units.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1329.SpaText: Spatio-Textual Representation for Controllable Image Generation</span><br>
                <span class="as">Avrahami, OmriandHayes, ThomasandGafni, OranandGupta, SonalandTaigman, YanivandParikh, DeviandLischinski, DaniandFried, OhadandYin, Xi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18370-18380.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的文本到图像扩散模型无法精细地控制不同区域/对象的形状或布局。<br>
                    动机：为了解决这个问题，我们提出了SpaText，一种使用开放词汇场景控制的文本到图像生成新方法。<br>
                    方法：用户除了提供描述整个场景的全局文本提示外，还需要提供一个分割图，其中每个感兴趣区域都由自由形式的自然语言描述进行注释。由于缺乏具有图像中每个区域的详细文本描述的大规模数据集，我们选择利用现有的大规模文本到图像数据集，并基于一种新的基于CLIP的空间-文本表示法来构建我们的方法。<br>
                    效果：实验结果表明，该方法在两种最先进的扩散模型（像素基和潜在基）上均取得了显著的效果。此外，我们还展示了如何将无分类器引导方法扩展到多条件情况，并提出了一种新的加速推理算法。最后，我们提供了几种自动评估指标，并通过FID分数和用户研究来评估我们的方法，结果显示该方法在自由形式文本场景控制下的图像生成方面达到了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText --- a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1330.Leveraging per Image-Token Consistency for Vision-Language Pre-Training</span><br>
                <span class="as">Gou, YunhaoandKo, TomandYang, HansiandKwok, JamesandZhang, YuandWang, Mingxuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gou_Leveraging_per_Image-Token_Consistency_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19155-19164.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉-语言预训练（VLP）方法主要采用跨模态的掩码语言模型（CMLM）来学习视觉-语言的关联，但作者发现这种方法存在缺陷。<br>
                    动机：CMLM在处理视觉-语言预训练时存在一些问题，如模态偏见和未充分利用未被遮蔽的标记。<br>
                    方法：为解决这些问题，作者提出了EPIC（lEveraging Per Image-Token Consistency for vision-language pre-training）方法。EPIC通过为每张图片和句子对遮蔽与图片相关的标记（即基于显著性的遮蔽策略），并用语言模型生成的替代词替换它们，然后要求模型判断句子中的每个标记是否与图片一致（即图像-标记一致性任务）。<br>
                    效果：实验表明，EPIC方法与当前最先进的预训练方法（包括ViLT、ALBEF、METER和X-VLM）结合后，可以在下游任务上取得显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence whether it is consistent with the image (i.e., Image-Token Consistency Task). The proposed EPIC method is easily combined with pre-training methods. Extensive experiments show that the combination of the EPIC method and state-of-the-art pre-training approaches, including ViLT, ALBEF, METER, and X-VLM, leads to significant improvements on downstream tasks. Our coude is released at https://github.com/gyhdog99/epic</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1331.Neural Congealing: Aligning Images to a Joint Semantic Atlas</span><br>
                <span class="as">Ofri-Amar, DolevandGeyer, MichalandKasten, YoniandDekel, Tali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ofri-Amar_Neural_Congealing_Aligning_Images_to_a_Joint_Semantic_Atlas_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19403-19412.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种零射击自监督框架，用于检测和联合对齐一组给定图像中语义上常见的内容。<br>
                    动机：目前的预训练模型在处理图像集合时，往往需要大量的训练数据和复杂的输入信息，如分割掩码等。<br>
                    方法：采用预先训练的DINO-ViT特征，学习一个联合语义图册（捕获输入集中DINO-ViT特征的模式）和从统一图册到每个输入图像的密集映射。通过优化图册表示和每张图像的映射，只需要少量真实世界图像作为输入即可。<br>
                    效果：实验结果表明，该方法在各种具有挑战性的图像集上表现良好，包括混合领域的图像集（如描绘雕塑和猫艺术品的图像），描绘相关但不同类别的对象的图像集（如狗和虎），或训练数据稀缺的领域（如咖啡杯）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Neural Congealing -- a zero-shot self-supervised framework for detecting and jointly aligning semantically-common content across a given set of images. Our approach harnesses the power of pre-trained DINO-ViT features to learn: (i) a joint semantic atlas -- a 2D grid that captures the mode of DINO-ViT features in the input set, and (ii) dense mappings from the unified atlas to each of the input images. We derive a new robust self-supervised framework that optimizes the atlas representation and mappings per image set, requiring only a few real-world images as input without any additional input information (e.g., segmentation masks). Notably, we design our losses and training paradigm to account only for the shared content under severe variations in appearance, pose, background clutter or other distracting objects. We demonstrate results on a plethora of challenging image sets including sets of mixed domains (e.g., aligning images depicting sculpture and artwork of cats), sets depicting related yet different object categories (e.g., dogs and tigers), or domains for which large-scale training data is scarce (e.g., coffee mugs). We thoroughly evaluate our method and show that our test-time optimization approach performs favorably compared to a state-of-the-art method that requires extensive training on large-scale datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1332.Learning Expressive Prompting With Residuals for Vision Transformers</span><br>
                <span class="as">Das, RajshekharandDukler, YonatanandRavichandran, AvinashandSwaminathan, Ashwin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Das_Learning_Expressive_Prompting_With_Residuals_for_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3366-3377.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种有效的视觉变换器适应方法，通过在预训练模型的输入和中间表示中插入一组可学习的参数。<br>
                    动机：现有的视觉变换器适应方法通常需要大量的计算资源，并且效果不尽如人意。<br>
                    方法：本文提出了一种名为“表达提示残差”（EXPRES）的方法，该方法通过学习“输出”令牌来构建下游表示，类似于ViT的学习类令牌。此外，为了更好地控制由冻结的变换器处理的下游表示，我们在各种计算的输出中引入了残差可学习令牌。<br>
                    效果：实验结果表明，EXPRES在图像分类、少样本学习和语义分割等任务上均取得了最先进的提示调优效果。此外，我们的方法比现有的视觉提示基线效率高一个数量级。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prompt learning is an efficient approach to adapt transformers by inserting learnable set of parameters into the input and intermediate representations of a pre-trained model. In this work, we present Expressive Prompts with Residuals (EXPRES) which modifies the prompt learning paradigm specifically for effective adaptation of vision transformers (ViT). Out method constructs downstream representations via learnable "output" tokens, that are akin to the learned class tokens of the ViT. Further for better steering of the downstream representation processed by the frozen transformer, we introduce residual learnable tokens that are added to the output of various computations. We apply EXPRES for image classification, few shot learning, and semantic segmentation, and show our method is capable of achieving state of the art prompt tuning on 3/3 categories of the VTAB benchmark. In addition to strong performance, we observe that our approach is an order of magnitude more prompt efficient than existing visual prompting baselines. We analytically show the computational benefits of our approach over weight space adaptation techniques like finetuning. Lastly we systematically corroborate the architectural design of our method via a series of ablation experiments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1333.Learning To Generate Text-Grounded Mask for Open-World Semantic Segmentation From Only Image-Text Pairs</span><br>
                <span class="as">Cha, JunbumandMun, JonghwanandRoh, Byungseok</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cha_Learning_To_Generate_Text-Grounded_Mask_for_Open-World_Semantic_Segmentation_From_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11165-11174.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何仅使用图像-文本对进行开放世界语义分割，而无需密集注释。<br>
                    动机：现有的开放世界分割方法通过对比学习来学习多样的视觉概念，并将学到的图像级理解转移到分割任务上，但存在训练和测试之间的差异。<br>
                    方法：提出了一种新的基于文本的对比学习（TCL）框架，使模型能够直接学习区域-文本对齐。该方法为给定的文本生成分割掩码，从掩码区域提取基于文本的图像嵌入，并通过TCL将其与文本嵌入对齐。<br>
                    效果：通过直接学习区域-文本对齐，该框架鼓励模型直接提高生成的分割掩码的质量。在统一的评估协议下，TCL在所有数据集上都实现了最先进的零样本分割性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We tackle open-world semantic segmentation, which aims at learning to segment arbitrary visual concepts in images, by using only image-text pairs without dense annotations. Existing open-world segmentation methods have shown impressive advances by employing contrastive learning (CL) to learn diverse visual concepts and transferring the learned image-level understanding to the segmentation task. However, these CL-based methods suffer from a train-test discrepancy, since it only considers image-text alignment during training, whereas segmentation requires region-text alignment during testing. In this paper, we proposed a novel Text-grounded Contrastive Learning (TCL) framework that enables a model to directly learn region-text alignment. Our method generates a segmentation mask for a given text, extracts text-grounded image embedding from the masked region, and aligns it with text embedding via TCL. By learning region-text alignment directly, our framework encourages a model to directly improve the quality of generated segmentation masks. In addition, for a rigorous and fair comparison, we present a unified evaluation protocol with widely used 8 semantic segmentation datasets. TCL achieves state-of-the-art zero-shot segmentation performances with large margins in all datasets. Code is available at https://github.com/kakaobrain/tcl.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1334.Learning Video Representations From Large Language Models</span><br>
                <span class="as">Zhao, YueandMisra, IshanandKr\&quot;ahenb\&quot;uhl, PhilippandGirdhar, Rohit</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6586-6597.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大型语言模型学习视频-语言表示。<br>
                    动机：现有的预训练语言模型缺乏对视觉输入的处理，我们的目标是通过重新利用预训练的语言模型来创建自动视频叙述器。<br>
                    方法：我们将预训练的语言模型进行条件设置以处理视觉输入，并对其进行微调以生成自动视频叙述。然后，我们使用这些叙述进行对比学习以获取视频-语言嵌入。<br>
                    效果：实验结果表明，我们的方法在多个第一人称和第三人称的视频任务上超越了先前的最佳状态，无论是在零射击还是微调设置中。特别是在EGTEA分类和Epic-Kitchens-100多实例检索基准测试中，LAVILA获得了10.1%和5.9%的绝对增益。此外，使用Ego4D数据集一半的叙述进行训练的LAVILA表现优于使用完整数据集进行训练的模型，并且在增加预训练数据和模型大小时表现出积极的比例行为。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce LAVILA, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-language embedding learned contrastively with these narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, LAVILA obtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LAVILA trained with only half the narrations from the Ego4D dataset outperforms models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1335.MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining</span><br>
                <span class="as">Dong, XiaoyiandBao, JianminandZheng, YinglinandZhang, TingandChen, DongdongandYang, HaoandZeng, MingandZhang, WeimingandYuan, LuandChen, DongandWen, FangandYu, Nenghai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10995-11005.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种简单有效的框架MaskCLIP，将一种新的被提出的遮蔽自我蒸馏方法融入到对比语言-图像预训练中。<br>
                    动机：遮蔽自我蒸馏的核心思想是将完整图像的表示提炼为从遮蔽图像预测的表示。这种结合有两个重要的优点。首先，遮蔽自我蒸馏针对局部补丁表示学习，这与关注文本相关表示的视觉-语言对比是互补的。其次，遮蔽自我蒸馏在训练目标方面也与视觉-语言对比一致，因为它们都使用视觉编码器进行特征对齐，因此能够学习到从语言中获得间接监督的局部语义。<br>
                    方法：通过设计专门的实验和全面分析来验证这两个优点。同时，我们还在文本分支中引入了局部语义监督，进一步提高了预训练性能。<br>
                    效果：大量实验表明，MaskCLIP在各种具有挑战性的下游任务中，当应用语言编码器的指导时，在线性探测、微调和零样本性能方面都取得了优越的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a simple yet effective framework MaskCLIP, which incorporates a newly proposed masked self-distillation into contrastive language-image pretraining. The core idea of masked self-distillation is to distill representation from a full image to the representation predicted from a masked image. Such incorporation enjoys two vital benefits. First, masked self-distillation targets local patch representation learning, which is complementary to vision-language contrastive focusing on text-related representation. Second, masked self-distillation is also consistent with vision-language contrastive from the perspective of training objective as both utilize the visual encoder for feature aligning, and thus is able to learn local semantics getting indirect supervision from the language. We provide specially designed experiments with a comprehensive analysis to validate the two benefits. Symmetrically, we also introduce the local semantic supervision into the text branch, which further improves the pretraining performance. With extensive experiments, we show that MaskCLIP, when applied to various challenging downstream tasks, achieves superior results in linear probing, finetuning, and zero-shot performance with the guidance of the language encoder. We will release the code and data after the publication.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1336.Open-Vocabulary Semantic Segmentation With Mask-Adapted CLIP</span><br>
                <span class="as">Liang, FengandWu, BichenandDai, XiaoliangandLi, KunpengandZhao, YinanandZhang, HangandZhang, PeizhaoandVajda, PeterandMarculescu, Diana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7061-7070.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放词汇语义分割中的性能瓶颈，即预训练的CLIP模型在处理被遮盖的图像时表现不佳的问题。<br>
                    动机：目前的两阶段方法首先生成类别无关的遮盖建议，然后利用预训练的视觉语言模型（如CLIP）对遮盖区域进行分类。但这种方法的性能瓶颈在于预训练的CLIP模型在处理被遮盖的图像时表现不佳。<br>
                    方法：我们提出通过微调CLIP模型来解决这一问题，方法是使用一组被遮盖的图像区域及其对应的文本描述作为训练数据。我们通过挖掘现有的图像-字幕数据集（如COCO Captions），并使用CLIP将被遮盖的图像区域匹配到图像字幕中的名词来收集训练数据。<br>
                    效果：实验表明，我们的“空白”区域方法可以在不修改CLIP的任何权重的情况下带来显著的改进，并且可以进一步改善完全微调的模型。特别是在COCO上训练并在ADE20K-150上评估时，我们的最佳模型实现了29.6%的mIoU，比之前最先进的方法高出+8.5%。这是第一次，开放词汇通用模型在没有特定数据集适应的情况下，其性能与2017年的有监督专业模型相匹配。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the "blank" areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1337.Supervised Masked Knowledge Distillation for Few-Shot Transformers</span><br>
                <span class="as">Lin, HanandHan, GuangxingandMa, JiaweiandHuang, ShiyuanandLin, XudongandChang, Shih-Fu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19649-19659.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器在少量学习设置下，由于缺乏类似卷积神经网络的归纳偏置，容易过拟合和性能下降的问题。<br>
                    动机：尽管视觉转换器在数据丰富的计算机视觉任务中取得了令人印象深刻的性能，但在只有少量标记数据的小型数据集上的少量学习设置下，视觉转换器往往会过拟合并遭受严重性能下降。<br>
                    方法：受最近自我监督知识蒸馏和掩蔽图像建模（MIM）的进展启发，我们提出了一种新的监督掩蔽知识蒸馏模型（SMKD），用于少量学习的视觉转换器，该模型将标签信息纳入自我蒸馏框架。<br>
                    效果：实验结果表明，与之前的自我监督方法相比，我们的模型在四个少量分类基准数据集上的表现优于以往的方法，并达到了新的最先进的水平。详细的消融研究证实了我们模型的每个组件的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) emerge to achieve impressive performance on many data-abundant computer vision tasks by capturing long-range dependencies among local features. However, under few-shot learning (FSL) settings on small datasets with only a few labeled data, ViT tends to overfit and suffers from severe performance degradation due to its absence of CNN-alike inductive bias. Previous works in FSL avoid such problem either through the help of self-supervised auxiliary losses, or through the dextile uses of label information under supervised settings. But the gap between self-supervised and supervised few-shot Transformers is still unfilled. Inspired by recent advances in self-supervised knowledge distillation and masked image modeling (MIM), we propose a novel Supervised Masked Knowledge Distillation model (SMKD) for few-shot Transformers which incorporates label information into self-distillation frameworks. Compared with previous self-supervised methods, we allow intra-class knowledge distillation on both class and patch tokens, and introduce the challenging task of masked patch tokens reconstruction across intra-class images. Experimental results on four few-shot classification benchmark datasets show that our method with simple design outperforms previous methods by a large margin and achieves a new start-of-the-art. Detailed ablation studies confirm the effectiveness of each component of our model. Code for this paper is available here: https://github.com/HL-hanlin/SMKD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1338.Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning</span><br>
                <span class="as">Yu, YoungjaeandChung, JiwanandYun, HeeseungandHessel, JackandPark, JaeSungandLu, XimingandZellers, RowanandAmmanabrolu, PrithvirajandLeBras, RonanandKim, GunheeandChoi, Yejin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10845-10856.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将文本预训练模型的知识扩展到多模态输入，如图像和音频，而无需配对的领域数据？<br>
                    动机：目前的模型可以执行常识推理，但需要扩展其知识以处理多模态任务，如视觉常识推理。<br>
                    方法：提出ESPER，使用强化学习来对齐多模态输入和语言模型生成，无需直接监督。例如，奖励优化仅依赖于从CLIP派生的余弦相似度，无需额外的配对（图像，文本）数据。<br>
                    效果：实验表明，ESPER在各种多模态文本生成任务上优于基线和先前的工作，包括一个新的基准测试——ESP数据集，该数据集要求模型为每张图片生成多个不同领域的文本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6], ethical norms [25]), and larger models like GPT-3 manifest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use reinforcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1339.Meta-Personalizing Vision-Language Models To Find Named Instances in Video</span><br>
                <span class="as">Yeh, Chun-HsiaoandRussell, BryanandSivic, JosefandHeilbron, FabianCabaandJenni, Simon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yeh_Meta-Personalizing_Vision-Language_Models_To_Find_Named_Instances_in_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19123-19132.png><br>
            
            <span class="tt"><span class="t0">研究问题：大型视觉语言模型在视频搜索应用中，虽然可以进行类别级别的查询，但目前还无法进行特定对象实例的个性化搜索。<br>
                    动机：为了解决这一问题，我们提出了一种元个性化预训练视觉语言模型的方法，即学习如何在测试时个性化搜索视频中的特定对象实例。<br>
                    方法：我们扩展了视觉语言模型的词汇表，通过学习每个实例特有的词嵌入来个性化模型。同时，我们将每个实例的嵌入表示为共享和学习的全局类别特征的组合，以捕获仅与实例相关的特征。<br>
                    效果：我们在"This-Is-My"个人视频实例检索基准测试中评估了我们的方法，并在DeepFashion2数据集上取得了比现有技术提高15%的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale vision-language models (VLM) have shown impressive results for language-guided search applications. While these models allow category-level queries, they currently struggle with personalized searches for moments in a video where a specific object instance such as "My dog Biscuit" appears. We present the following three contributions to address this problem. First, we describe a method to meta-personalize a pre-trained VLM, i.e., learning how to learn to personalize a VLM at test time to search in video. Our method extends the VLM's token vocabulary by learning novel word embeddings specific to each instance. To capture only instance-specific features, we represent each instance embedding as a combination of shared and learned global category features. Second, we propose to learn such personalization without explicit human supervision. Our approach automatically identifies moments of named visual instances in video using transcripts and vision-language similarity in the VLM's embedding space. Finally, we introduce This-Is-My, a personal video instance retrieval benchmark. We evaluate our approach on This-Is-My and DeepFashion2 and show that we obtain a 15% relative improvement over the state of the art on the latter dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1340.Soft-Landing Strategy for Alleviating the Task Discrepancy Problem in Temporal Action Localization Tasks</span><br>
                <span class="as">Kang, HyolimandKim, HanjungandAn, JoungbinandCho, MinsuandKim, SeonJoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Soft-Landing_Strategy_for_Alleviating_the_Task_Discrepancy_Problem_in_Temporal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6514-6523.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何缓解预训练编码器和下游任务之间的转移性差距。<br>
                    动机：现有的时序动作定位（TAL）方法在处理任务不匹配问题上，要么需要大量的内存和计算资源进行重新训练或端到端微调，要么通过预定义任务进行缓解，但都存在效率低下的问题。<br>
                    方法：我们提出了软着陆（SoLa）策略，通过在冻结的编码器上添加一个轻量级的神经网络模块——SoLa模块，有效地连接预训练编码器和下游任务。同时，我们还提出了一种无监督的训练方案，让SoLa模块通过帧间隔作为监督信号进行学习，从而消除了对时间标注的需求。<br>
                    效果：我们在多个下游TAL任务基准上进行了实验评估，结果显示我们的方法在显著提高计算效率的同时，有效地缓解了任务不匹配的问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal Action Localization (TAL) methods typically operate on top of feature sequences from a frozen snippet encoder that is pretrained with the Trimmed Action Classification (TAC) tasks, resulting in a task discrepancy problem. While existing TAL methods mitigate this issue either by retraining the encoder with a pretext task or by end-to-end finetuning, they commonly require an overload of high memory and computation. In this work, we introduce Soft-Landing (SoLa) strategy, an efficient yet effective framework to bridge the transferability gap between the pretrained encoder and the downstream tasks by incorporating a light-weight neural network, i.e., a SoLa module, on top of the frozen encoder. We also propose an unsupervised training scheme for the SoLa module; it learns with inter-frame Similarity Matching that uses the frame interval as its supervisory signal, eliminating the need for temporal annotations. Experimental evaluation on various benchmarks for downstream TAL tasks shows that our method effectively alleviates the task discrepancy problem with remarkable computational efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1341.Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images</span><br>
                <span class="as">Lu, MingY.andChen, BowenandZhang, AndrewandWilliamson, DrewF.K.andChen, RichardJ.andDing, TongandLe, LongPhiandChuang, Yung-SungandMahmood, Faisal</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19764-19775.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的对比视觉语言预训练方法主要针对图像-文本对的大型数据集进行，且研究问题：现有的对比视觉语言预训练方法主要针对图像-文本对的大型数据集进行，且主要用于小型到中型图像的下游任务，不适用于计算病理学领域。<br>
                    动机：计算病理学领域缺乏公开的成对图像-文本数据集，且每个图像的维度可能高达100,000 x 100,000像素。<br>
                    方法：提出MI-Zero框架，通过对比对齐图像和文本模型的零样本转移能力，实现对十亿像素的组织病理学全幻灯片图像的处理。<br>
                    效果：在超过55万份病理报告和其他可用的领域内文本语料库上预训练文本编码器，并在超过3.3万个组织病理学图像-标题对上进行预训练，实现了三种不同的真实世界癌症亚型任务的平均中位数零样本准确率为70.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive visual language pretraining has emerged as a powerful method for either training new language-aware image encoders or augmenting existing pretrained models with zero-shot visual recognition capabilities. However, existing works typically train on large datasets of image-text pairs and have been designed to perform downstream tasks involving only small to medium sized-images, neither of which are applicable to the emerging field of computational pathology where there are limited publicly available paired image-text datasets and each image can span up to 100,000 x 100,000 pixels in dimensions. In this paper we present MI-Zero, a simple and intuitive framework for unleashing the zero-shot transfer capabilities of contrastively aligned image and text models to gigapixel histopathology whole slide images, enabling multiple downstream diagnostic tasks to be carried out by pretrained encoders without requiring any additional labels. MI-Zero reformulates zero-shot transfer under the framework of multiple instance learning to overcome the computational challenge of inference on extremely large images. We used over 550k pathology reports and other available in-domain text corpora to pretrain our text encoder. By effectively leveraging strong pretrained encoders, our best model pretrained on over 33k histopathology image-caption pairs achieves an average median zero-shot accuracy of 70.2% across three different real-world cancer subtyping tasks. Our code is available at: https://github.com/mahmoodlab/MI-Zero.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1342.PMR: Prototypical Modal Rebalance for Multimodal Learning</span><br>
                <span class="as">Fan, YunfengandXu, WenchaoandWang, HaozhaoandWang, JunxiaoandGuo, Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20029-20038.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态学习（MML）旨在联合利用不同模态的共同先验来弥补其内在局限性，但现有的MML方法往往对不同模态优化统一的学习目标，导致“模态失衡”问题和反效果的MML性能。<br>
                    动机：为了解决上述问题，我们提出了原型模态再平衡（PMR）方法，通过刺激特定慢速学习的模态，而不受到其他模态的干扰，以更好地利用多模态的特性。<br>
                    方法：我们引入了代表每个类别一般特征的原型，用于构建非参数分类器进行单模态性能评估。然后，我们尝试通过增强向原型的聚类来加速慢速学习的模态。此外，为了防止过早收敛，我们在早期训练阶段引入了一个基于原型的熵正则化项，以减轻主导模态的抑制作用。<br>
                    效果：我们的PMR方法仅依赖于每个模态的表示，没有模型结构和融合方法的限制，因此在各种场景中具有巨大的应用潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal learning (MML) aims to jointly exploit the common priors of different modalities to compensate for their inherent limitations. However, existing MML methods often optimize a uniform objective for different modalities, leading to the notorious "modality imbalance" problem and counterproductive MML performance. To address the problem, some existing methods modulate the learning pace based on the fused modality, which is dominated by the better modality and eventually results in a limited improvement on the worse modal. To better exploit the features of multimodal, we propose Prototypical Modality Rebalance (PMR) to perform stimulation on the particular slow-learning modality without interference from other modalities. Specifically, we introduce the prototypes that represent general features for each class, to build the non-parametric classifiers for uni-modal performance evaluation. Then, we try to accelerate the slow-learning modality by enhancing its clustering toward prototypes. Furthermore, to alleviate the suppression from the dominant modality, we introduce a prototype-based entropy regularization term during the early training stage to prevent premature convergence. Besides, our method only relies on the representations of each modality and without restrictions from model structures and fusion methods, making it with great application potential for various scenarios. The source code is available here.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1343.Trainable Projected Gradient Method for Robust Fine-Tuning</span><br>
                <span class="as">Tian, JunjiaoandHe, ZechengandDai, XiaoliangandMa, Chih-YaoandLiu, Yen-ChengandKira, Zsolt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Trainable_Projected_Gradient_Method_for_Robust_Fine-Tuning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7836-7845.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高预训练模型对分布外数据的鲁棒性并保持泛化能力。<br>
                    动机：目前的迁移学习方法大多采用手动设计的启发式方法或昂贵的超参数搜索，这限制了它们在大数据集和神经网络上的扩展性。<br>
                    方法：提出可训练投影梯度法（TPGM），将微调视为双层约束优化问题，自动学习每一层施加的约束。<br>
                    效果：实验结果表明，TPGM在分布外性能上优于现有的微调方法，同时在最佳内部性能上与之匹配。例如，在DomainNet-Real和ImageNet上进行微调时，与普通微调相比，TPGM在其草图对应物上分别表现出22%和10%的相对分布外改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies on transfer learning have shown that selectively fine-tuning a subset of layers or customizing different learning rates for each layer can greatly improve robustness to out-of-distribution (OOD) data and retain generalization capability in the pre-trained models. However, most of these methods employ manually crafted heuristics or expensive hyper-parameter search, which prevent them from scaling up to large datasets and neural networks. To solve this problem, we propose Trainable Projected Gradient Method (TPGM) to automatically learn the constraint imposed for each layer for a fine-grained fine-tuning regularization. This is motivated by formulating fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM maintains a set of projection radii, i.e., distance constraints between the fine-tuned model and the pre-trained model, for each layer, and enforces them through weight projections. To learn the constraints, we propose a bi-level optimization to automatically learn the best set of projection radii in an end-to-end manner. Theoretically, we show that the bi-level optimization formulation is the key to learn different constraints for each layer. Empirically, with little hyper-parameter search cost, TPGM outperforms existing fine-tuning methods in OOD performance while matching the best in-distribution (ID) performance. For example, when fine-tuned on DomainNet-Real and ImageNet, compared to vanilla fine-tuning, TPGM shows 22% and 10% relative OOD improvement respectively on their sketch counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1344.Are Deep Neural Networks SMARTer Than Second Graders?</span><br>
                <span class="as">Cherian, AnoopandPeng, Kuan-ChuanandLohit, SuhasandSmith, KevinA.andTenenbaum, JoshuaB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cherian_Are_Deep_Neural_Networks_SMARTer_Than_Second_Graders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10834-10844.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在解决需要广泛技能的问题上的可泛化性如何？<br>
                    动机：为了解决这个问题，我们提出了SMART和SMART-101数据集，用于评估神经网络在解决视觉语言难题上的抽象、演绎和泛化能力。<br>
                    方法：我们设计了专门针对6-8岁儿童的视觉语言谜题，并创建了一个包含101个独特谜题的数据集。我们还开发了一种可以整合各种最先进的神经骨干的视觉和语言元学习模型。<br>
                    效果：实验表明，虽然强大的深度模型在监督设置下的谜题上表现良好，但当分析其泛化能力时，它们并不比随机准确率更好。填补这一空白可能需要新的多模态学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent times have witnessed an increasing number of applications of deep neural networks towards solving tasks that require superior cognitive abilities, e.g., playing Go, generating art, question answering (such as ChatGPT), etc. Such a dramatic progress raises the question: how generalizable are neural networks in solving problems that demand broad skills? To answer this question, we propose SMART: a Simple Multimodal Algorithmic Reasoning Task and the associated SMART-101 dataset, for evaluating the abstraction, deduction, and generalization abilities of neural networks in solving visuo-linguistic puzzles designed specifically for children in the 6--8 age group. Our dataset consists of 101 unique puzzles; each puzzle comprises a picture and a question, and their solution needs a mix of several elementary skills, including arithmetic, algebra, and spatial reasoning, among others. To scale our dataset towards training deep neural networks, we programmatically generate entirely new instances for each puzzle while retaining their solution algorithm. To benchmark the performance on the SMART-101 dataset, we propose a vision-and-language meta-learning model that can incorporate varied state-of-the-art neural backbones. Our experiments reveal that while powerful deep models offer reasonable performances on puzzles in a supervised setting, they are not better than random accuracy when analyzed for generalization -- filling this gap may demand new multimodal learning approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1345.Multi-Modal Learning With Missing Modality via Shared-Specific Feature Modelling</span><br>
                <span class="as">Wang, HuandChen, YuanhongandMa, CongboandAvery, JodieandHull, LouiseandCarneiro, Gustavo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15878-15887.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决多模态模型中缺失模态的问题。<br>
                    动机：当前方法处理多模态任务中的缺失模态问题，要么仅在评估期间处理缺失的模态，要么训练单独的模型来处理特定的缺失模态设置，且这些模型是为特定任务设计的，不易适应其他任务。<br>
                    方法：提出共享-特定特征建模（ShaSpec）方法，通过学习共享和特定特征以更好地表示输入数据，利用所有可用的输入模态进行训练和评估。<br>
                    效果：实验结果表明，ShaSpec在医学图像分割和计算机视觉分类等任务上的表现优于竞争方法，例如在BraTS2018数据集上，ShaSpec提高了肿瘤、肿瘤核心和整个肿瘤的3%以上的SOTA。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The missing modality issue is critical but non-trivial to be solved by multi-modal models. Current methods aiming to handle the missing modality problem in multi-modal tasks, either deal with missing modalities only during evaluation or train separate models to handle specific missing modality settings. In addition, these models are designed for specific tasks, so for example, classification models are not easily adapted to segmentation tasks and vice versa. In this paper, we propose the Shared-Specific Feature Modelling (ShaSpec) method that is considerably simpler and more effective than competing approaches that address the issues above. ShaSpec is designed to take advantage of all available input modalities during training and evaluation by learning shared and specific features to better represent the input data. This is achieved from a strategy that relies on auxiliary tasks based on distribution alignment and domain classification, in addition to a residual feature fusion procedure. Also, the design simplicity of ShaSpec enables its easy adaptation to multiple tasks, such as classification and segmentation. Experiments are conducted on both medical image segmentation and computer vision classification, with results indicating that ShaSpec outperforms competing methods by a large margin. For instance, on BraTS2018, ShaSpec improves the SOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole tumour.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1346.Stare at What You See: Masked Image Modeling Without Reconstruction</span><br>
                <span class="as">Xue, HongweiandGao, PengandLi, HongyangandQiao, YuandSun, HaoandLi, HouqiangandLuo, Jiebo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Stare_at_What_You_See_Masked_Image_Modeling_Without_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22732-22741.png><br>
            
            <span class="tt"><span class="t0">研究问题：在有语义丰富的教师模型的掩蔽图像建模（MIM）中，重建是否必要？<br>
                    动机：强大的教师模型提取的特征已经编码了完整图像中的跨区域丰富语义相关性，因此提出了一个问题：在有教师模型的MIM中，重建是否是必要的？<br>
                    方法：本文提出了一种有效的MIM范式MaskAlign，它简单地学习了学生模型提取的可见补丁特征和教师模型提取的完整图像特征的一致性。为了进一步提高性能并解决学生和教师模型之间的输入不一致问题，我们提出了动态对齐（DA）模块来应用可学习的对齐。<br>
                    效果：实验结果表明，即使在没有重建被遮蔽区域的情况下，掩蔽建模也不会失去效果。结合动态对齐，MaskAlign可以实现具有更高效率的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Autoencoders (MAE) have been prevailing paradigms for large-scale vision representation pre-training. By reconstructing masked image patches from a small portion of visible image regions, MAE forces the model to infer semantic correlation within an image. Recently, some approaches apply semantic-rich teacher models to extract image features as the reconstruction target, leading to better performance. However, unlike the low-level features such as pixel values, we argue the features extracted by powerful teacher models already encode rich semantic correlation across regions in an intact image. This raises one question: is reconstruction necessary in Masked Image Modeling (MIM) with a teacher model? In this paper, we propose an efficient MIM paradigm named MaskAlign. MaskAlign simply learns the consistency of visible patch feature extracted by the student model and intact image features extracted by the teacher model. To further advance the performance and tackle the problem of input inconsistency between the student and teacher model, we propose a Dynamic Alignment (DA) module to apply learnable alignment. Our experimental results demonstrate that masked modeling does not lose effectiveness even without reconstruction on masked regions. Combined with Dynamic Alignment, MaskAlign can achieve state-of-the-art performance with much higher efficiency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1347.Joint Visual Grounding and Tracking With Natural Language Specification</span><br>
                <span class="as">Zhou, LiandZhou, ZikunandMao, KaigeandHe, Zhenyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23151-23160.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的视觉跟踪算法中，视觉基础和跟踪被分开处理，忽视了自然语言描述为两个步骤提供全局语义线索的问题。<br>
                    动机：目前的视觉跟踪算法将视觉基础和跟踪分为两个步骤进行处理，这种分离的框架忽略了自然语言描述为这两个步骤提供全局语义线索的联系，并且难以进行端到端的训练。<br>
                    方法：本文提出了一个联合视觉基础和跟踪的框架，将基础和跟踪重新定义为一个统一的任务：基于给定的视觉-语言参考来定位目标。具体来说，我们设计了一个多源关系模型模块来有效地建立视觉-语言参考和测试图像之间的关系，并设计了一个时间建模模块，以全局语义信息为指导提供时间线索，以提高模型对目标外观变化的适应性。<br>
                    效果：在TNL2K、LaSOT、OTB99和RefCOCOg等数据集上的大量实验结果表明，我们的方法在跟踪和基础方面都优于最先进的算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Tracking by natural language specification aims to locate the referred target in a sequence based on the natural language description. Existing algorithms solve this issue in two steps, visual grounding and tracking, and accordingly deploy the separated grounding model and tracking model to implement these two steps, respectively. Such a separated framework overlooks the link between visual grounding and tracking, which is that the natural language descriptions provide global semantic cues for localizing the target for both two steps. Besides, the separated framework can hardly be trained end-to-end. To handle these issues, we propose a joint visual grounding and tracking framework, which reformulates grounding and tracking as a unified task: localizing the referred target based on the given visual-language references. Specifically, we propose a multi-source relation modeling module to effectively build the relation between the visual-language references and the test image. In addition, we design a temporal modeling module to provide a temporal clue with the guidance of the global semantic information for our model, which effectively improves the adaptability to the appearance variations of the target. Extensive experimental results on TNL2K, LaSOT, OTB99, and RefCOCOg demonstrate that our method performs favorably against state-of-the-art algorithms for both tracking and grounding. Code is available at https://github.com/lizhou-cs/JointNLT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1348.Fake It Till You Make It: Learning Transferable Representations From Synthetic ImageNet Clones</span><br>
                <span class="as">Sar{\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8011-8021.png><br>
            
            <span class="tt"><span class="t0">研究问题：最近的图像生成模型如Stable Diffusion是否能够完全取代真实图像来训练图像预测模型？<br>
                    动机：探索仅使用类名，无需真实图像，能否训练出有效的ImageNet分类模型。<br>
                    方法：利用Stable Diffusion生成ImageNet的合成克隆，并以此训练分类模型。<br>
                    效果：结果显示，通过最小化和类别无关的提示工程，合成克隆能够大大缩小合成图像模型与真实图像训练模型之间的差距，且在几个标准的分类基准测试中表现良好。更重要的是，基于合成图像训练的模型展现出强大的泛化能力，其转移学习性能与真实数据训练的模型相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhibit strong generalization properties and perform on par with models trained on real data for transfer. Project page: https://europe.naverlabs.com/imagenet-sd</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1349.HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization</span><br>
                <span class="as">Kim, SungyeonandJeong, BoseungandKwak, Suha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_HIER_Metric_Learning_Beyond_Class_Labels_via_Hierarchical_Regularization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19903-19912.png><br>
            
            <span class="tt"><span class="t0">研究问题：传统的监督学习方式限制了度量学习的进步。<br>
                    动机：提出一种新的正则化方法，通过发现训练数据的隐语义层次结构，提供比常见度量学习损失函数诱导的类间可分性更丰富、更精细的监督。<br>
                    方法：提出了一种名为HIER的新方法，无需对语义层次结构进行注释，而是通过在双曲空间中学习分层代理来实现。<br>
                    效果：在四个标准基准测试中，HIER始终优于传统方法，并在几乎所有设置中实现了最佳记录，甚至超过了现有的双曲度量学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Supervision for metric learning has long been given in the form of equivalence between human-labeled classes. Although this type of supervision has been a basis of metric learning for decades, we argue that it hinders further advances in the field. In this regard, we propose a new regularization method, dubbed HIER, to discover the latent semantic hierarchy of training data, and to deploy the hierarchy to provide richer and more fine-grained supervision than inter-class separability induced by common metric learning losses. HIER achieves this goal with no annotation for the semantic hierarchy but by learning hierarchical proxies in hyperbolic spaces. The hierarchical proxies are learnable parameters, and each of them is trained to serve as an ancestor of a group of data or other proxies to approximate the semantic hierarchy among them. HIER deals with the proxies along with data in hyperbolic space since the geometric properties of the space are well-suited to represent their hierarchical structure. The efficacy of HIER is evaluated on four standard benchmarks, where it consistently improved the performance of conventional methods when integrated with them, and consequently achieved the best records, surpassing even the existing hyperbolic metric learning technique, in almost all settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1350.Interactive and Explainable Region-Guided Radiology Report Generation</span><br>
                <span class="as">Tanida, TimandM\&quot;uller, PhilipandKaissis, GeorgiosandRueckert, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tanida_Interactive_and_Explainable_Region-Guided_Radiology_Report_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7433-7442.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成放射学报告，帮助放射科医生减轻报告撰写的负担。<br>
                    动机：现有的方法从图像级别特征生成完整的报告，未能明确关注图像中的解剖区域。<br>
                    方法：提出了一种简单而有效的区域引导报告生成模型，该模型检测解剖区域，然后描述各个显著的区域以形成最终报告。<br>
                    效果：实验结果表明，该方法在报告生成方面非常有效，优于先前最先进的模型，并突出了其交互能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The automatic generation of radiology reports has the potential to assist radiologists in the time-consuming task of report writing. Existing methods generate the full report from image-level features, failing to explicitly focus on anatomical regions in the image. We propose a simple yet effective region-guided report generation model that detects anatomical regions and then describes individual, salient regions to form the final report. While previous methods generate reports without the possibility of human intervention and with limited explainability, our method opens up novel clinical use cases through additional interactive capabilities and introduces a high degree of transparency and explainability. Comprehensive experiments demonstrate our method's effectiveness in report generation, outperforming previous state-of-the-art models, and highlight its interactive capabilities. The code and checkpoints are available at https://github.com/ttanida/rgrg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1351.Benchmarking Self-Supervised Learning on Diverse Pathology Datasets</span><br>
                <span class="as">Kang, MinguandSong, HeonandPark, SeonwookandYoo, DonggeunandPereira, S\&#x27;ergio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Benchmarking_Self-Supervised_Learning_on_Diverse_Pathology_Datasets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3344-3354.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用未标记的病理图像数据进行预训练，以提高下游任务的性能。<br>
                    动机：标注病理图像数据成本高昂，而自监督学习是一种有效的利用未标记数据的方法。然而，目前还没有关于如何将其应用于病理学领域的系统性研究。<br>
                    方法：在本文中，我们对四种代表性的自监督学习方法进行了大规模的实验，并在各种下游任务上进行了评估。我们还提出了一套针对病理学领域的特定技术，并对其进行了实验验证。<br>
                    效果：实验结果表明，在标准的自监督学习设置（如线性和微调评估）以及低标签环境下，大规模领域对齐的病理学预训练始终优于ImageNet预训练。此外，我们提出的领域特定技术也带来了显著的性能提升。最后，我们首次将自监督学习应用于具有挑战性的核实例分割任务，并在各种设置下实现了显著且一致的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL to the challenging task of nuclei instance segmentation and show large and consistent performance improvements under diverse settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1352.From Images to Textual Prompts: Zero-Shot Visual Question Answering With Frozen Large Language Models</span><br>
                <span class="as">Guo, JiaxianandLi, JunnanandLi, DongxuandTiong, AnthonyMengHuatandLi, BoyangandTao, DachengandHoi, Steven</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10867-10877.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用大型语言模型进行零样本视觉问答。<br>
                    动机：大型语言模型与视觉问答任务之间存在模态和任务的断开，直接端到端训练既不灵活又计算量大。<br>
                    方法：提出Img2Prompt模块，通过LLM无关模型提供描述图像内容和自构建的问题-答案对的提示，以指导大型语言模型执行零样本视觉问答任务。<br>
                    效果：Img2Prompt能灵活应用于各种大型语言模型进行VQA，无需端到端训练，大大降低了部署成本，且在性能上比依赖端到端训练的方法更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnection and task disconnection between LLM and VQA task. End-to-end training on vision and language data may bridge the disconnections, but is inflexible and computationally expensive. To address this issue, we propose Img2Prompt, a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training. In order to provide such prompts, we further employ LLM-agnostic models to provide prompts that can describe image content and self-constructed question-answer pairs, which can effectively guide LLM to perform zero-shot VQA tasks. Img2Prompt offers the following benefits: 1) It can flexibly work with various LLMs to perform VQA. 2) Without the needing of end-to-end training, it significantly reduces the cost of deploying LLM for zero-shot VQA tasks. 3) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method even outperforms few-shot methods by as much as 20%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1353.Neuralizer: General Neuroimage Analysis Without Re-Training</span><br>
                <span class="as">Czolbe, SteffenandDalca, AdrianV.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Czolbe_Neuralizer_General_Neuroimage_Analysis_Without_Re-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6217-6230.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何解决神经影像处理任务中需要大量时间和专业知识来训练和调整深度学习模型的问题。<br>
                    动机：现有的深度学习策略和架构在解决神经影像处理任务时，当面临新任务或具有不同视觉特征的数据集时，通常需要重新训练或微调模型，这对缺乏资源或机器学习专业知识的神经科学家和临床研究人员构成了巨大的障碍。<br>
                    方法：我们提出了Neuralizer，这是一个可以推广到以前未见过的神经影像任务和模态而无需重新训练或微调的单一模型。该模型可以在单个前向传递过程中进行一般化，并在推理期间完成任务。<br>
                    效果：我们在冠状切片上的实验表明，当我们只有少数标注的受试者可用时，我们的多任务网络在未训练任务的情况下优于特定任务的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neuroimage processing tasks like segmentation, reconstruction, and registration are central to the study of neuroscience. Robust deep learning strategies and architectures used to solve these tasks are often similar. Yet, when presented with a new task or a dataset with different visual characteristics, practitioners most often need to train a new model, or fine-tune an existing one. This is a time-consuming process that poses a substantial barrier for the thousands of neuroscientists and clinical researchers who often lack the resources or machine-learning expertise to train deep learning models. In practice, this leads to a lack of adoption of deep learning, and neuroscience tools being dominated by classical frameworks. We introduce Neuralizer, a single model that generalizes to previously unseen neuroimaging tasks and modalities without the need for re-training or fine-tuning. Tasks do not have to be known a priori, and generalization happens in a single forward pass during inference. The model can solve processing tasks across multiple image modalities, acquisition methods, and datasets, and generalize to tasks and modalities it has not been trained on. Our experiments on coronal slices show that when few annotated subjects are available, our multi-task network outperforms task-specific baselines without training on the task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1354.Visual Prompt Multi-Modal Tracking</span><br>
                <span class="as">Zhu, JiawenandLai, SimiaoandChen, XinandWang, DongandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9516-9526.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用预训练的基础模型进行多模态跟踪。<br>
                    动机：全微调在RGB参数上的方式虽然有效，但由于下游数据稀缺和转移性差等问题，这种方式并非最优。<br>
                    方法：受语言模型中提示学习的启发，开发了视觉提示多模态跟踪（ViPT），通过学习模态相关的提示来适应各种下游多模态跟踪任务的预训练基础模型。<br>
                    效果：ViPT在RGB+Depth、RGB+Thermal、RGB+Event等多种下游跟踪任务上优于全微调模式，同时只引入少量可训练参数（小于1%的模型参数），实现了高效的参数使用并取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full fine-tuning on the RGB-based parameters. Albeit effective, this manner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multimodal tracking tasks. ViPT finds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable parameters (less than 1% of model parameters). ViPT outperforms the full fine-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, and RGB+Event tracking. Extensive experiments show the potential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efficiency. Code and models are available at https://github.com/jiawen-zhu/ViPT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1355.GIVL: Improving Geographical Inclusivity of Vision-Language Models With Pre-Training Methods</span><br>
                <span class="as">Yin, DaandGao, FengandThattai, GovindandJohnston, MichaelandChang, Kai-Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_GIVL_Improving_Geographical_Inclusivity_of_Vision-Language_Models_With_Pre-Training_Methods_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10951-10961.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何开发一种服务于所有社区，而不仅仅是某个特定地区的AI技术。<br>
                    动机：由于文化差异，某些地区的知识可能不适用于其他地区，如果模型对地域特性不了解，可能导致在不同地区的表现差异，并对未被充分代表的群体产生偏见。<br>
                    方法：提出GIVL模型，这是一种地理包容性视觉和语言预训练模型。设计了新的预训练目标——图像知识匹配（IKM）和图像编辑检查（IEC），以预训练GIVL模型。<br>
                    效果：与在相似规模数据上进行预训练的类似大小模型相比，GIVL在地理多样性视觉和语言任务上取得了最先进的、更平衡的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A key goal for the advancement of AI is to develop technologies that serve the needs not just of one group but of all communities regardless of their geographical region. In fact, a significant proportion of knowledge is locally shared by people from certain regions but may not apply equally in other regions because of cultural differences. If a model is unaware of regional characteristics, it may lead to performance disparity across regions and result in bias against underrepresented groups. We propose GIVL, a Geographically Inclusive Vision-and-Language Pre-trained model. There are two attributes of geo-diverse visual concepts which can help to learn geo-diverse knowledge: 1) concepts under similar categories have unique knowledge and visual characteristics, 2) concepts with similar visual features may fall in completely different categories. Motivated by the attributes, we design new pre-training objectives Image-Knowledge Matching (IKM) and Image Edit Checking (IEC) to pre-train GIVL. Compared with similar-size models pre-trained with similar scale of data, GIVL achieves state-of-the-art (SOTA) and more balanced performance on geo-diverse V&L tasks. Code and data are released at https://github.com/WadeYin9712/GIVL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1356.Towards Fast Adaptation of Pretrained Contrastive Models for Multi-Channel Video-Language Retrieval</span><br>
                <span class="as">Lin, XudongandTiwari, SimranandHuang, ShiyuanandLi, ManlingandShou, MikeZhengandJi, HengandChang, Shih-Fu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Towards_Fast_Adaptation_of_Pretrained_Contrastive_Models_for_Multi-Channel_Video-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14846-14855.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将视频和文本信息进行联合，以实现多通道视频-语言检索。<br>
                    动机：现有的对比性多模态模型在图像/视频和文本的实体对齐上表现出色，但在有限的数据和资源下，如何快速适应多通道视频-语言检索仍不清楚。<br>
                    方法：本文通过两个维度（如何表示视频和如何融合视频和文本信息）来设计原则性的模型空间，并基于最近的方法进行分类，探讨了使用连续特征向量或离散文本令牌表示视频，以及使用多模态转换器或预训练对比文本模型进行融合的可能性。<br>
                    效果：实验结果表明，离散文本令牌与预训练对比文本模型的结合能产生最佳性能，甚至能在无需额外训练数百万视频-文本数据的情况下超越最新的iVQA和How2QA数据集上的最先进技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-channel video-language retrieval require models to understand information from different channels (e.g. video+question, video+speech) to correctly link a video with a textual response or query. Fortunately, contrastive multimodal models are shown to be highly effective at aligning entities in images/videos and text, e.g., CLIP; text contrastive models are extensively studied recently for their strong ability of producing discriminative sentence embeddings, e.g., SimCSE. However, there is not a clear way to quickly adapt these two lines to multi-channel video-language retrieval with limited data and resources. In this paper, we identify a principled model design space with two axes: how to represent videos and how to fuse video and text information. Based on categorization of recent methods, we investigate the options of representing videos using continuous feature vectors or discrete text tokens; for the fusion method, we explore the use of a multimodal transformer or a pretrained contrastive text model. We extensively evaluate the four combinations on five video-language datasets. We surprisingly find that discrete text tokens coupled with a pretrained contrastive text model yields the best performance, which can even outperform state-of-the-art on the iVQA and How2QA datasets without additional training on millions of video-text data. Further analysis shows that this is because representing videos as text tokens captures the key visual information and text tokens are naturally aligned with text models that are strong retrievers after the contrastive pretraining process. All the empirical analysis establishes a solid foundation for future research on affordable and upgradable multimodal intelligence. The code will be released at https://github.com/XudongLinthu/upgradable-multimodal-intelligence to facilitate future research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1357.Hierarchical Discriminative Learning Improves Visual Representations of Biomedical Microscopy</span><br>
                <span class="as">Jiang, ChengandHou, XinhaiandKondepudi, AkhilandChowdury, AsadurandFreudiger, ChristianW.andOrringer, DanielA.andLee, HonglakandHollon, ToddC.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Hierarchical_Discriminative_Learning_Improves_Visual_Representations_of_Biomedical_Microscopy_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19798-19808.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高计算机视觉在生物医学显微镜和临床医学中的作用。<br>
                    动机：现有的自监督表示学习方法主要针对实例判别，并将其直接应用于从用于癌症诊断的千兆像素全幅图像（WSIs）中采样的图像补丁或视场，这种方法存在局限性。<br>
                    方法：提出HiDisc方法，利用临床生物医学显微镜中固有的患者-幻灯片-补丁层次结构定义了一个分层判别学习任务，该任务隐式地学习了潜在诊断的特征。HiDisc使用自监督对比学习框架，其中正补丁对是根据数据层次结构中的共同祖先定义的，并使用统一的补丁、幻灯片和患者判别学习目标进行视觉SSL。<br>
                    效果：通过两个生物医学显微镜数据集上的两个视觉任务对HiDisc视觉表示进行基准测试，结果表明：（1）HiDisc预训练在癌症诊断和基因突变预测方面优于当前最先进的自监督预训练方法；（2）HiDisc在学习高质量视觉表示时使用了自然补丁多样性，而无需强大的数据增强。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning high-quality, self-supervised, visual representations is essential to advance the role of computer vision in biomedical microscopy and clinical medicine. Previous work has focused on self-supervised representation learning (SSL) methods developed for instance discrimination and applied them directly to image patches, or fields-of-view, sampled from gigapixel whole-slide images (WSIs) used for cancer diagnosis. However, this strategy is limited because it (1) assumes patches from the same patient are independent, (2) neglects the patient-slide-patch hierarchy of clinical biomedical microscopy, and (3) requires strong data augmentations that can degrade downstream performance. Importantly, sampled patches from WSIs of a patient's tumor are a diverse set of image examples that capture the same underlying cancer diagnosis. This motivated HiDisc, a data-driven method that leverages the inherent patient-slide-patch hierarchy of clinical biomedical microscopy to define a hierarchical discriminative learning task that implicitly learns features of the underlying diagnosis. HiDisc uses a self-supervised contrastive learning framework in which positive patch pairs are defined based on a common ancestry in the data hierarchy, and a unified patch, slide, and patient discriminative learning objective is used for visual SSL. We benchmark HiDisc visual representations on two vision tasks using two biomedical microscopy datasets, and demonstrate that (1) HiDisc pretraining outperforms current state-of-the-art self-supervised pretraining methods for cancer diagnosis and genetic mutation prediction, and (2) HiDisc learns high-quality visual representations using natural patch diversity without strong data augmentations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1358.ProD: Prompting-To-Disentangle Domain Knowledge for Cross-Domain Few-Shot Image Classification</span><br>
                <span class="as">Ma, TianyiandSun, YifanandYang, ZongxinandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19754-19763.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决跨领域少样本图像分类中训练测试领域差距对分类准确度的影响。<br>
                    动机：现有的多领域训练方案和卷积神经网络提取主干特征的方法无法有效解决训练测试领域的鸿沟。<br>
                    方法：提出一种通过提示机制进行解耦（ProD）的方法。该方法采用流行的多领域训练方案，并使用标准的卷积神经网络提取主干特征。其关键在于在变换器中使用提示机制从主干特征中解耦领域一般（DG）和领域特定（DS）的知识。具体来说，ProD将一个DG和一个DS提示连接到主干特征上，并将它们输入到一个轻量级的变换器中。DG提示是所有训练领域共享的可学习提示，而DS提示是根据感兴趣的领域实时生成的。结果，变换器并行输出DG和DS特征以及两个提示，产生了解耦效果。<br>
                    效果：实验表明，1) 只需为所有训练领域共享一个DG提示，就可以提高对新测试领域的泛化能力；2) 通过使DG提示对训练领域保持中性，可以进一步加强跨领域的泛化能力；3) 当进行推理时，可以从支持样本中生成DS提示，并通过提示机制捕获测试领域的知识。综合以上三点优势，ProD显著提高了跨领域的少样本分类性能。例如，在CUB数据集上，ProD将5类5次采样的准确率从73.56%（基线）提高到79.19%，创造了新的最先进的状态。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper considers few-shot image classification under the cross-domain scenario, where the train-to-test domain gap compromises classification accuracy. To mitigate the domain gap, we propose a prompting-to-disentangle (ProD) method through a novel exploration with the prompting mechanism. ProD adopts the popular multi-domain training scheme and extracts the backbone feature with a standard Convolutional Neural Network. Based on these two common practices, the key point of ProD is using the prompting mechanism in the transformer to disentangle the domain-general (DG) and domain-specific (DS) knowledge from the backbone feature. Specifically, ProD concatenates a DG and a DS prompt to the backbone feature and feeds them into a lightweight transformer. The DG prompt is learnable and shared by all the training domains, while the DS prompt is generated from the domain-of-interest on the fly. As a result, the transformer outputs DG and DS features in parallel with the two prompts, yielding the disentangling effect. We show that: 1) Simply sharing a single DG prompt for all the training domains already improves generalization towards the novel test domain. 2) The cross-domain generalization can be further reinforced by making the DG prompt neutral towards the training domains. 3) When inference, the DS prompt is generated from the support samples and can capture test domain knowledge through the prompting mechanism. Combining all three benefits, ProD significantly improves cross-domain few-shot classification. For instance, on CUB, ProD improves the 5-way 5-shot accuracy from 73.56% (baseline) to 79.19%, setting a new state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1359.ViLEM: Visual-Language Error Modeling for Image-Text Retrieval</span><br>
                <span class="as">Chen, YuxinandMa, ZongyangandZhang, ZiqiandQi, ZhongangandYuan, ChunfengandShan, YingandLi, BingandHu, WeimingandQie, XiaohuandWu, Jianping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11018-11027.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的图像-文本检索预训练模型采用"双编码器"架构进行高效的全局对齐，但忽略了图像和文本之间的详细语义关联。<br>
                    动机：为了解决上述问题，我们提出了一种新的代理任务——视觉-语言错误建模（ViLEM），通过“校对”文本中的每个词与相应的图像，将详细的图像-文本关联注入到“双编码器”模型中。<br>
                    方法：首先，我们使用预训练的语言模型自动生成多样化的合理负样本文本。然后，ViLEM强制模型区分这些合理负样本文本中每个词的正确性，并通过参考图像信息进一步纠正错误的词。此外，我们还提出了一个多粒度交互框架，通过将文本特征与全局和局部图像特征进行交互，实现ViLEM，使局部文本语义与高层视觉上下文和多级局部视觉信息相关联。<br>
                    效果：实验结果表明，我们的方法在图像-文本检索任务上大幅超越了最先进的"双编码器"方法，并且显著提高了对局部文本语义的判别能力。我们的模型还可以很好地泛化到视频-文本检索任务上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dominant pre-training works for image-text retrieval adopt "dual-encoder" architecture to enable high efficiency, where two encoders are used to extract image and text representations and contrastive learning is employed for global alignment. However, coarse-grained global alignment ignores detailed semantic associations between image and text. In this work, we propose a novel proxy task, named Visual-Language Error Modeling (ViLEM), to inject detailed image-text association into "dual-encoder" model by "proofreading" each word in the text against the corresponding image. Specifically, we first edit the image-paired text to automatically generate diverse plausible negative texts with pre-trained language models. ViLEM then enforces the model to discriminate the correctness of each word in the plausible negative texts and further correct the wrong words via resorting to image information. Furthermore, we propose a multi-granularity interaction framework to perform ViLEM via interacting text features with both global and local image features, which associates local text semantics with both high-level visual context and multi-level local visual information. Our method surpasses state-of-the-art "dual-encoder" methods by a large margin on the image-text retrieval task and significantly improves discriminativeness to local textual semantics. Our model can also generalize well to video-text retrieval.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1360.Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens</span><br>
                <span class="as">Chen, YuxiaoandYuan, JianboandTian, YuandGeng, ShijieandLi, XinyuandZhou, DingandMetaxas, DimitrisN.andYang, Hongxia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Revisiting_Multimodal_Representation_in_Contrastive_Learning_From_Patch_and_Token_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15095-15104.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练大规模文本语料库和知识图谱来训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive learning-based vision-language pre-training approaches, such as CLIP, have demonstrated great success in many vision-language tasks. These methods achieve cross-modal alignment by encoding a matched image-text pair with similar feature embeddings, which are generated by aggregating information from visual patches and language tokens. However, direct aligning cross-modal information using such representations is challenging, as visual patches and text tokens differ in semantic levels and granularities. To alleviate this issue, we propose a Finite Discrete Tokens (FDT) based multimodal representation. FDT is a set of learnable tokens representing certain visual-semantic concepts. Both images and texts are embedded using shared FDT by first grounding multimodal inputs to FDT space and then aggregating the activated FDT representations. The matched visual and semantic concepts are enforced to be represented by the same set of discrete tokens by a sparse activation constraint. As a result, the granularity gap between the two modalities is reduced. Through both quantitative and qualitative analyses, we demonstrate that using FDT representations in CLIP-style models improves cross-modal alignment and performance in visual recognition and vision-language downstream tasks. Furthermore, we show that our method can learn more comprehensive representations, and the learned FDT capture meaningful cross-modal correspondence, ranging from objects to actions and attributes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1361.HumanBench: Towards General Human-Centric Perception With Projector Assisted Pretraining</span><br>
                <span class="as">Tang, ShixiangandChen, ChengandXie, QingsongandChen, MeilinandWang, YizhouandCi, YuanzhengandBai, LeiandZhu, FengandYang, HaiyangandYi, LiandZhao, RuiandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21970-21982.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何开发一种通用的预训练模型，以适应多样化的以人为中心的下游任务。<br>
                    动机：以人为中心的视觉任务在工业应用中广泛存在，如监控、自动驾驶和元宇宙等，因此需要一种能适应这些任务的通用预训练模型。<br>
                    方法：提出了基于现有数据集的HumanBench，用于全面评估不同预训练方法在19个数据集上的泛化能力，这些数据集来自6种不同的下游任务。同时，为了学习人体中的粗粒度和细粒度知识，进一步提出了Projector AssisTed Hierarchical预训练方法（PATH）。<br>
                    效果：在HumanBench的综合评估中，PATH在17个下游数据集上取得了新的最先进成果，在其他2个数据集上达到了同等水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-centric perceptions include a variety of vision tasks, which have widespread industrial applications, including surveillance, autonomous driving, and the metaverse. It is desirable to have a general pretrain model for versatile human-centric downstream tasks. This paper forges ahead along this path from the aspects of both benchmark and pretraining methods. Specifically, we propose a HumanBench based on existing datasets to comprehensively evaluate on the common ground the generalization abilities of different pretraining methods on 19 datasets from 6 diverse downstream tasks, including person ReID, pose estimation, human parsing, pedestrian attribute recognition, pedestrian detection, and crowd counting. To learn both coarse-grained and fine-grained knowledge in human bodies, we further propose a Projector AssisTed Hierarchical pretraining method (PATH) to learn diverse knowledge at different granularity levels. Comprehensive evaluations on HumanBench show that our PATH achieves new state-of-the-art results on 17 downstream datasets and on-par results on the other 2 datasets. The code will be publicly at https://github.com/OpenGVLab/HumanBench.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1362.PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers</span><br>
                <span class="as">Grainger, RyanandPaniagua, ThomasandSong, XiandCuntoor, NareshandLee, MunWaiandWu, Tianfu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Grainger_PaCa-ViT_Learning_Patch-to-Cluster_Attention_in_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18568-18578.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器（ViTs）中将图像块视为“视觉令牌”并学习补丁到补丁的注意力的问题，以及补丁嵌入标记器的语义差距和二次复杂性问题。<br>
                    动机：目前的视觉转换器存在语义差距、二次复杂度问题以及难以解释的模型等问题。<br>
                    方法：本文提出了在ViT中学习补丁到集群注意力（PaCa）的方法。我们的PaCa-ViT查询从补丁开始，而键和值直接基于聚类（预定义的小数量的集群）。这些集群是端到端学习的，从而产生更好的标记器，并引导联合聚类关注和关注聚类以获得更好且可解释的模型。二次复杂度被放宽为线性复杂度。<br>
                    效果：实验结果表明，与现有技术相比，该方法在ImageNet-1k图像分类、MS-COCO对象检测和实例分割以及MIT-ADE20k语义分割的所有三个基准测试中都获得了比SWin和PVTs更好的性能。由于线性复杂度，它在MS-COCO和MIT-ADE20k上比PVT模型更高效。此外，学习到的集群在语义上具有意义。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) are built on the assumption of treating image patches as "visual tokens" and learn patch-to-patch attention. The patch embedding based tokenizer has a semantic gap with respect to its counterpart, the textual tokenizer. The patch-to-patch attention suffers from the quadratic complexity issue, and also makes it non-trivial to explain learned ViTs. To address these issues in ViT, this paper proposes to learn Patch-to-Cluster attention (PaCa) in ViT. Queries in our PaCa-ViT starts with patches, while keys and values are directly based on clustering (with a predefined small number of clusters). The clusters are learned end-to-end, leading to better tokenizers and inducing joint clustering-for-attention and attention-for-clustering for better and interpretable models. The quadratic complexity is relaxed to linear complexity. The proposed PaCa module is used in designing efficient and interpretable ViT backbones and semantic segmentation head networks. In experiments, the proposed methods are tested on ImageNet-1k image classification, MS-COCO object detection and instance segmentation and MIT-ADE20k semantic segmentation. Compared with the prior art, it obtains better performance in all the three benchmarks than the SWin and the PVTs by significant margins in ImageNet-1k and MIT-ADE20k. It is also significantly more efficient than PVT models in MS-COCO and MIT-ADE20k due to the linear complexity. The learned clusters are semantically meaningful. Code and model checkpoints are available at https://github.com/iVMCL/PaCaViT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1363.Vision Transformers Are Parameter-Efficient Audio-Visual Learners</span><br>
                <span class="as">Lin, Yan-BoandSung, Yi-LinandLei, JieandBansal, MohitandBertasius, Gedas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2299-2309.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在探索仅在视觉数据上预训练的冻结视觉转换器（ViTs）在无需微调任何原始参数的情况下，对视听数据的泛化能力。<br>
                    动机：现有的视听方法需要依赖昂贵的音频预训练或外部音频编码器，且需调整的参数较多。因此，本研究提出一种潜在视听混合适配器（LAVISH adapter），以减少参数数量并提高性能。<br>
                    方法：通过将少量可训练参数注入每个冻结的ViT层，使预训练的ViT适应视听任务。为了有效融合视觉和听觉提示，该适配器使用一组形成注意力瓶颈的潜在令牌，从而消除了标准交叉注意力的二次成本。<br>
                    效果：与现有的模态特定视听方法相比，该方法在各种视听任务上实现了竞争甚至更好的性能，同时使用的可调参数更少，且无需依赖昂贵的音频预训练或外部音频编码器。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers (ViTs) have achieved impressive results on various computer vision tasks in the last several years. In this work, we study the capability of frozen ViTs, pretrained only on visual data, to generalize to audio-visual data without finetuning any of its original parameters. To do so, we propose a latent audio-visual hybrid (LAVISH) adapter that adapts pretrained ViTs to audio-visual tasks by injecting a small number of trainable parameters into every layer of a frozen ViT. To efficiently fuse visual and audio cues, our LAVISH adapter uses a small set of latent tokens, which form an attention bottleneck, thus, eliminating the quadratic cost of standard cross-attention. Compared to the existing modality-specific audio-visual methods, our approach achieves competitive or even better performance on various audio-visual tasks while using fewer tunable parameters and without relying on costly audio pretraining or external audio encoders. Our code is available at https://genjib.github.io/project_page/LAVISH/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1364.Towards Modality-Agnostic Person Re-Identification With Descriptive Query</span><br>
                <span class="as">Chen, CuiqunandYe, MangandJiang, Ding</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15128-15137.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决跨模态和多模态的人物重识别问题，特别是在没有照片查询的情况下，如何利用文本或草图进行人物重识别。<br>
                    动机：现有的人物重识别方法通常只关注单一模态的匹配，如文本到图像或草图到照片。然而，在实际应用中，我们往往无法确定是否有文本或草图可用。因此，本文提出了一种新的、具有挑战性的模态无关的人物重识别问题。<br>
                    方法：为了解决这个问题，本文提出了一种统一的人物重识别（UNIReID）架构，该架构可以有效地适应跨模态和多模态任务。具体来说，UNIReID采用了一个简单的双编码器和特定于任务的模态学习来挖掘和融合视觉和文本模态信息。此外，为了解决UNIReID中不同任务训练不平衡的问题，本文还提出了一种基于任务难度的任务感知动态训练策略，以自适应地调整训练重点。<br>
                    效果：通过收集照片对应的草图，构建了三个多模态人物重识别数据集。实验结果表明，UNIReID在不同的任务和未见过的场景上大大提高了检索准确性和泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Person re-identification (ReID) with descriptive query (text or sketch) provides an important supplement for general image-image paradigms, which is usually studied in a single cross-modality matching manner, e.g., text-to-image or sketch-to-photo. However, without a camera-captured photo query, it is uncertain whether the text or sketch is available or not in practical scenarios. This motivates us to study a new and challenging modality-agnostic person re-identification problem. Towards this goal, we propose a unified person re-identification (UNIReID) architecture that can effectively adapt to cross-modality and multi-modality tasks. Specifically, UNIReID incorporates a simple dual-encoder with task-specific modality learning to mine and fuse visual and textual modality information. To deal with the imbalanced training problem of different tasks in UNIReID, we propose a task-aware dynamic training strategy in terms of task difficulty, adaptively adjusting the training focus. Besides, we construct three multi-modal ReID datasets by collecting the corresponding sketches from photos to support this challenging task. The experimental results on three multi-modal ReID datasets show that our UNIReID greatly improves the retrieval accuracy and generalization ability on different tasks and unseen scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1365.Learning To Exploit Temporal Structure for Biomedical Vision-Language Processing</span><br>
                <span class="as">Bannur, ShruthiandHyland, StephanieandLiu, QianchuandP\&#x27;erez-Garc{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bannur_Learning_To_Exploit_Temporal_Structure_for_Biomedical_Vision-Language_Processing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15016-15027.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更好地利用视觉和文本模态之间的语义对齐进行自我监督学习，特别是在生物医学领域。<br>
                    动机：现有的生物医学视觉语言处理（VLP）工作主要依赖于单个图像和报告对的对齐，忽略了临床笔记通常引用的先前图像，这既引入了模态间的不良对齐，也错过了通过数据中现有时间内容进行丰富自我监督的机会。<br>
                    方法：我们的方法名为BioViL-T，使用一个CNN-Transformer混合多图像编码器与文本模型联合训练。当有可用的先前图像和报告时，我们在训练和微调阶段都明确考虑它们。设计用于应对诸如姿势变化和跨时间缺失输入图像等挑战。<br>
                    效果：在单图像和多图像设置中，我们的模型在下游任务上都表现出色，在进展分类、短语基础和报告生成方面达到最先进的性能，同时在疾病分类和句子相似性任务上提供持续改进。我们发布了一个新的多模态时间基准数据集CXR-T，以量化视觉语言表示的质量。实验结果表明，引入先前的图像和报告可以最大限度地利用数据，具有显著优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning in vision--language processing (VLP) exploits semantic alignment between imaging and text modalities. Prior work in biomedical VLP has mostly relied on the alignment of single image and report pairs even though clinical notes commonly refer to prior images. This does not only introduce poor alignment between the modalities but also a missed opportunity to exploit rich self-supervision through existing temporal content in the data. In this work, we explicitly account for prior images and reports when available during both training and fine-tuning. Our approach, named BioViL-T, uses a CNN--Transformer hybrid multi-image encoder trained jointly with a text model. It is designed to be versatile to arising challenges such as pose variations and missing input images across time. The resulting model excels on downstream tasks both in single- and multi-image setups, achieving state-of-the-art (SOTA) performance on (I) progression classification, (II) phrase grounding, and (III) report generation, whilst offering consistent improvements on disease classification and sentence-similarity tasks. We release a novel multi-modal temporal benchmark dataset, CXR-T, to quantify the quality of vision--language representations in terms of temporal semantics. Our experimental results show the significant advantages of incorporating prior images and reports to make most use of the data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1366.Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval</span><br>
                <span class="as">Jiang, DingandYe, Mang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2787-2797.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决基于文本描述查询的目标人物检索问题，即如何将视觉和文本模态映射到共同的潜在空间中。<br>
                    动机：现有的方法主要依赖于预训练的单模态模型来提取视觉和文本特征，但这些方法缺乏必要的底层对齐能力，无法有效地匹配多模态数据。此外，这些方法使用先验信息探索显式的部分对齐，可能导致模内信息的扭曲。<br>
                    方法：本文提出了IRRA框架，这是一种跨模态的隐含关系推理和对齐框架，通过学习局部视觉-文本标记之间的关系，增强全局图像-文本匹配，而无需额外的先验监督。具体来说，首先设计了一个在掩码语言建模范式中的隐含关系推理模块，通过一个跨模态的多模态交互编码器将视觉线索整合到文本标记中，实现跨模态交互。其次，为了全局对齐视觉和文本嵌入，提出了相似性分布匹配，以最小化图像-文本相似性分布与归一化的标签匹配分布之间的KL散度。<br>
                    效果：所提出的方法在所有三个公开数据集上都取得了新的最先进的结果，与现有方法相比，Rank-1准确率的显著差距为3%-9%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Text-to-image person retrieval aims to identify the target person based on a given textual description query. The primary challenge is to learn the mapping of visual and textual modalities into a common latent space. Prior works have attempted to address this challenge by leveraging separately pre-trained unimodal models to extract visual and textual features. However, these approaches lack the necessary underlying alignment capabilities required to match multimodal data effectively. Besides, these works use prior information to explore explicit part alignments, which may lead to the distortion of intra-modality information. To alleviate these issues, we present IRRA: a cross-modal Implicit Relation Reasoning and Aligning framework that learns relations between local visual-textual tokens and enhances global image-text matching without requiring additional prior supervision. Specifically, we first design an Implicit Relation Reasoning module in a masked language modeling paradigm. This achieves cross-modal interaction by integrating the visual cues into the textual tokens with a cross-modal multimodal interaction encoder. Secondly, to globally align the visual and textual embeddings, Similarity Distribution Matching is proposed to minimize the KL divergence between image-text similarity distributions and the normalized label matching distributions. The proposed method achieves new state-of-the-art results on all three public datasets, with a notable margin of about 3%-9% for Rank-1 accuracy compared to prior methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1367.REVEAL: Retrieval-Augmented Visual-Language Pre-Training With Multi-Source Multimodal Knowledge Memory</span><br>
                <span class="as">Hu, ZiniuandIscen, AhmetandSun, ChenandWang, ZiruiandChang, Kai-WeiandSun, YizhouandSchmid, CordeliaandRoss, DavidA.andFathi, Alireza</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23369-23379.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种端到端的检索增强视觉语言模型（REVEAL），用于研究问题：本文旨在提出一种端到端的检索增强视觉语言模型（REVEAL），用于将世界知识编码到大规模记忆中，并从中检索以回答知识密集型查询。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. REVEAL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REVEAL achieves state-of-the-art results on visual question answering and image captioning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1368.Generic-to-Specific Distillation of Masked Autoencoders</span><br>
                <span class="as">Huang, WeiandPeng, ZhiliangandDong, LiandWei, FuruandJiao, JianbinandYe, Qixiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Generic-to-Specific_Distillation_of_Masked_Autoencoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15996-16005.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何充分利用预训练的大型视觉转换器（ViTs）模型的知识，提升小型ViT模型的性能。<br>
                    动机：尽管大型的视觉转换器通过自我监督的预训练机制取得了显著的进步，但受限于模型容量的小型ViT模型却无法从中受益。<br>
                    方法：提出了通用到特定的蒸馏（G2SD）方法，通过在大型模型（教师模型）的监督下进行预训练，将大型模型的知识转移到小型模型中。在通用蒸馏阶段，鼓励小型模型的解码器将其特征预测与大型模型的隐藏表示对齐，以转移任务无关的知识；在特定蒸馏阶段，限制小型模型的预测结果与大型模型保持一致，以转移保证任务性能的任务特定特征。<br>
                    效果：使用G2SD方法，小型ViT-Small模型在图像分类、目标检测和语义分割任务上的性能分别达到了其教师模型ViT-Base的98.7%、98.1%和99.3%，为两阶段视觉蒸馏设定了坚实的基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large vision Transformers (ViTs) driven by self-supervised pre-training mechanisms achieved unprecedented progress. Lightweight ViT models limited by the model capacity, however, benefit little from those pre-training mechanisms. Knowledge distillation defines a paradigm to transfer representations from large (teacher) models to small (student) ones. However, the conventional single-stage distillation easily gets stuck on task-specific transfer, failing to retain the task-agnostic knowledge crucial for model generalization. In this study, we propose generic-to-specific distillation (G2SD), to tap the potential of small ViT models under the supervision of large models pre-trained by masked autoencoders. In generic distillation, decoder of the small model is encouraged to align feature predictions with hidden representations of the large model, so that task-agnostic knowledge can be transferred. In specific distillation, predictions of the small model are constrained to be consistent with those of the large model, to transfer task-specific features which guarantee task performance. With G2SD, the vanilla ViT-Small model respectively achieves 98.7%, 98.1% and 99.3% the performance of its teacher (ViT-Base) for image classification, object detection, and semantic segmentation, setting a solid baseline for two-stage vision distillation. Code will be available at https://github.com/pengzhiliang/G2SD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1369.Improving Cross-Modal Retrieval With Set of Diverse Embeddings</span><br>
                <span class="as">Kim, DongwonandKim, NamyupandKwak, Suha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Improving_Cross-Modal_Retrieval_With_Set_of_Diverse_Embeddings_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23422-23431.png><br>
            
            <span class="tt"><span class="t0">研究问题：跨模态检索是一个挑战性的任务，因为图像和文本模态之间存在固有的模糊性。<br>
                    动机：为了解决这个问题，我们提出了一种新的基于集合的嵌入方法。<br>
                    方法：我们提出了一种新的相似度函数——平滑Chamfer相似度，以及一种新的集合预测模块，通过槽位注意力机制有效地捕获输入的多样化语义。<br>
                    效果：在COCO和Flickr30K数据集上进行评估，该方法优于现有的方法，包括那些在推理时需要大量计算的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cross-modal retrieval across image and text modalities is a challenging task due to its inherent ambiguity: An image often exhibits various situations, and a caption can be coupled with diverse images. Set-based embedding has been studied as a solution to this problem. It seeks to encode a sample into a set of different embedding vectors that capture different semantics of the sample. In this paper, we present a novel set-based embedding method, which is distinct from previous work in two aspects. First, we present a new similarity function called smooth-Chamfer similarity, which is designed to alleviate the side effects of existing similarity functions for set-based embedding. Second, we propose a novel set prediction module to produce a set of embedding vectors that effectively captures diverse semantics of input by the slot attention mechanism. Our method is evaluated on the COCO and Flickr30K datasets across different visual backbones, where it outperforms existing methods including ones that demand substantially larger computation at inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1370.Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning</span><br>
                <span class="as">Lu, XiaochengandGuo, SongandLiu, ZimingandGuo, Jingcai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Decomposed_Soft_Prompt_Guided_Fusion_Enhancing_for_Compositional_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23560-23569.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决组合零样本学习（CZSL）中识别训练期间由已知状态和对象形成的新型概念的问题。<br>
                    动机：现有的方法要么学习结合的状态-对象表示，挑战未见过的组合的泛化能力，要么设计两个分类器分别从图像特征中识别状态和对象，忽略了它们之间的固有关系。<br>
                    方法：提出了一种名为分解融合软提示（DFSP）的新框架，通过引入视觉语言模型（VLMs）进行未见过的组合识别。具体来说，DFSP构建了可学习的软提示与状态和对象的向量组合，以建立它们的联合表示。此外，还在语言和图像分支之间设计了一个跨模态分解融合模块，该模块在语言特征中分解状态和对象，而不是在图像特征中。值得注意的是，与分解后的特征融合后，图像特征可以更具表现力地学习与状态和对象的关系，从而改善对对空间中未见过的组合的反应，缩小已见和未见集合之间的领域差距。<br>
                    效果：在三个具有挑战性的基准测试上进行的实验结果表明，我们的方法比其他最先进的方法有显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel concepts formed by known states and objects during training. Existing methods either learn the combined state-object representation, challenging the generalization of unseen compositions, or design two classifiers to identify state and object separately from image features, ignoring the intrinsic relationship between them. To jointly eliminate the above issues and construct a more robust CZSL system, we propose a novel framework termed Decomposed Fusion with Soft Prompt (DFSP), by involving vision-language models (VLMs) for unseen composition recognition. Specifically, DFSP constructs a vector combination of learnable soft prompts with state and object to establish the joint representation of them. In addition, a cross-modal decomposed fusion module is designed between the language and image branches, which decomposes state and object among language features instead of image features. Notably, being fused with the decomposed features, the image features can be more expressive for learning the relationship with states and objects, respectively, to improve the response of unseen compositions in the pair space, hence narrowing the domain gap between seen and unseen sets. Experimental results on three challenging benchmarks demonstrate that our approach significantly outperforms other state-of-the-art methods by large margins.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1371.Uncurated Image-Text Datasets: Shedding Light on Demographic Bias</span><br>
                <span class="as">Garcia, NoaandHirota, YusukeandWu, YankunandNakashima, Yuta</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6957-6966.png><br>
            
            <span class="tt"><span class="t0">研究问题：大规模未标注数据集用于训练视觉-语言模型可能导致公平性问题。<br>
                    动机：手动标注的小型数据集，如MSCOCO，也受到社会偏见的影响，而从互联网上爬取的数据缺乏控制，可能使这个问题变得更糟。<br>
                    方法：对广泛用于训练视觉-语言模型的Google概念描述数据集进行部分注释，包括四个人口统计和两个上下文属性。<br>
                    效果：通过评估三种主流的视觉-语言任务（图像描述、文本-图像CLIP嵌入和文本到图像生成），发现社会偏见在所有任务中都是一个持续存在的问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The increasing tendency to collect large and uncurated datasets to train vision-and-language models has raised concerns about fair representations. It is known that even small but manually annotated datasets, such as MSCOCO, are affected by societal bias. This problem, far from being solved, may be getting worse with data crawled from the Internet without much control. In addition, the lack of tools to analyze societal bias in big collections of images makes addressing the problem extremely challenging. Our first contribution is to annotate part of the Google Conceptual Captions dataset, widely used for training vision-and-language models, with four demographic and two contextual attributes. Our second contribution is to conduct a comprehensive analysis of the annotations, focusing on how different demographic groups are represented. Our last contribution lies in evaluating three prevailing vision-and-language tasks: image captioning, text-image CLIP embeddings, and text-to-image generation, showing that societal bias is a persistent problem in all of them.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1372.FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation</span><br>
                <span class="as">Qin, JieandWu, JieandYan, PengxiangandLi, MingandYuxi, RenandXiao, XuefengandWang, YitongandWang, RuiandWen, ShileiandPan, XinandWang, Xingang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_FreeSeg_Unified_Universal_and_Open-Vocabulary_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19446-19455.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有方法在特定分割任务上需要设计专门的架构或参数，导致各种分割任务之间的碎片化，阻碍了分割模型的统一性。<br>
                    动机：为了实现更通用的应用场景，开放词汇学习已经出现，用于完成基于文本描述的任意类别的分割，这使得分割系统更加普遍化。然而，现有的方法致力于为特定的分割任务设计专门的架构或参数。<br>
                    方法：本文提出了FreeSeg，一个通用的框架来完成统一、通用和开放词汇的图像分割。FreeSeg通过一次训练优化一个一体化的网络，并在推理过程中使用相同的架构和参数无缝处理不同的分割任务。此外，自适应提示学习有助于统一的模型捕获任务感知和类别敏感的概念，提高模型在多任务和不同场景中的鲁棒性。<br>
                    效果：大量的实验结果表明，FreeSeg在性能和泛化性方面建立了新的最先进的结果，在三个分割任务上都超过了最好的特定架构的任务：在COCO数据集上的语义分割提高了5.5% mIoU，实例分割提高了17.6% mAP，全景分割对未见类别提高了20.1% PQ。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, open-vocabulary learning has emerged to accomplish segmentation for arbitrary categories of text-based descriptions, which popularizes the segmentation system to more general-purpose application scenarios. However, existing methods devote to designing specialized architectures or parameters for specific segmentation tasks. These customized design paradigms lead to fragmentation between various segmentation tasks, thus hindering the uniformity of segmentation models. Hence in this paper, we propose FreeSeg, a generic framework to accomplish Unified, Universal and Open-Vocabulary Image Segmentation. FreeSeg optimizes an all-in-one network via one-shot training and employs the same architecture and parameters to handle diverse segmentation tasks seamlessly in the inference procedure. Additionally, adaptive prompt learning facilitates the unified model to capture task-aware and category-sensitive concepts, improving model robustness in multi-task and varied scenarios. Extensive experimental results demonstrate that FreeSeg establishes new state-of-the-art results in performance and generalization on three segmentation tasks, which outperforms the best task-specific architectures by a large margin: 5.5% mIoU on semantic segmentation, 17.6% mAP on instance segmentation, 20.1% PQ on panoptic segmentation for the unseen class on COCO. Project page: https://FreeSeg.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1373.AVFormer: Injecting Vision Into Frozen Speech Models for Zero-Shot AV-ASR</span><br>
                <span class="as">Seo, PaulHongsuckandNagrani, ArshaandSchmid, Cordelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seo_AVFormer_Injecting_Vision_Into_Frozen_Speech_Models_for_Zero-Shot_AV-ASR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22922-22931.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过整合视觉信息，提高语音识别系统的鲁棒性。<br>
                    动机：完全从零开始训练有监督的多模态模型需要大量的标记视听数据集，这在实际应用中受到限制。<br>
                    方法：提出了AVFormer，一种简单的方式，通过使用轻量级的可训练适配器将视觉嵌入注入到冻结的ASR模型中，以增强音频模型的视觉信息。<br>
                    效果：实验结果表明，该方法在三个不同的AV-ASR基准测试（How2，VisSpeech和Ego4D）上取得了最先进的零样本结果，同时在传统的纯音频语音识别基准测试（LibriSpeech）上也保持了良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Audiovisual automatic speech recognition (AV-ASR) aims to improve the robustness of a speech recognition system by incorporating visual information. Training fully supervised multimodal models for this task from scratch, however is limited by the need for large labelled audiovisual datasets (in each downstream domain of interest). We present AVFormer, a simple method for augmenting audioonly models with visual information, at the same time performing lightweight domain adaptation. We do this by (i) injecting visual embeddings into a frozen ASR model using lightweight trainable adaptors. We show that these can be trained on a small amount of weakly labelled video data with minimum additional training time and parameters. (ii) We also introduce a simple curriculum scheme during training which we show is crucial to enable the model to jointly process audio and visual information effectively; and finally (iii) we show that our model achieves state of the art zero-shot results on three different AV-ASR benchmarks (How2, VisSpeech and Ego4D), while also crucially preserving decent performance on traditional audio-only speech recognition benchmarks (LibriSpeech). Qualitative results show that our model effectively leverages visual information for robust speech recognition.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1374.CLIP2: Contrastive Language-Image-Point Pretraining From Real-World Point Cloud Data</span><br>
                <span class="as">Zeng, YihanandJiang, ChenhanandMao, JiagengandHan, JianhuaandYe, ChaoqiangandHuang, QingqiuandYeung, Dit-YanandYang, ZhenandLiang, XiaodanandXu, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_CLIP2_Contrastive_Language-Image-Point_Pretraining_From_Real-World_Point_Cloud_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15244-15253.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将2D视觉语言模型的成功应用于3D空间，以实现开放世界的3D视觉理解。<br>
                    动机：由于文本-3D数据对的数量有限，现有的方法通常通过构造中间的2D表示来处理3D数据，但这会丢失3D几何信息。<br>
                    方法：提出对比性语言-图像-点云预训练（CLIP^2），利用自然存在的2D和3D场景对应关系，构建了良好对齐的实例化文本-图像-点云代理，并提出了跨模态对比目标来学习语义和实例级对齐的点云表示。<br>
                    效果：实验结果表明，我们学习的3D表示在包括零样本和少样本3D识别在内的下游任务中具有强大的迁移能力，大幅提高了现有方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive Language-Image Pre-training, benefiting from large-scale unlabeled text-image pairs, has demonstrated great performance in open-world vision understanding tasks. However, due to the limited Text-3D data pairs, adapting the success of 2D Vision-Language Models (VLM) to the 3D space remains an open problem. Existing works that leverage VLM for 3D understanding generally resort to constructing intermediate 2D representations for the 3D data, but at the cost of losing 3D geometry information. To take a step toward open-world 3D vision understanding, we propose Contrastive Language-Image-Point Cloud Pretraining (CLIP^2) to directly learn the transferable 3D point cloud representation in realistic scenarios with a novel proxy alignment mechanism. Specifically, we exploit naturally-existed correspondences in 2D and 3D scenarios, and build well-aligned and instance-based text-image-point proxies from those complex scenarios. On top of that, we propose a cross-modal contrastive objective to learn semantic and instance-level aligned point cloud representation. Experimental results on both indoor and outdoor scenarios show that our learned 3D representation has great transfer ability in downstream tasks, including zero-shot and few-shot 3D recognition, which boosts the state-of-the-art methods by large margins. Furthermore, we provide analyses of the capability of different representations in real scenarios and present the optional ensemble scheme.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1375.CLIPPO: Image-and-Language Understanding From Pixels Only</span><br>
                <span class="as">Tschannen, MichaelandMustafa, BasilandHoulsby, Neil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11006-11017.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单一的编码器处理图像和文本，以实现图像、文本和多模态任务的统一？<br>
                    动机：目前的多模态模型往往由许多特定于任务和模态的组件组成，训练过程复杂。<br>
                    方法：提出一种纯像素模型CLIP-Pixels Only（CLIPPO），该模型使用单个编码器处理常规图像和作为图像渲染的文本，仅通过对比损失进行训练。<br>
                    效果：实验结果显示，CLIPPO在图像检索和零射击图像分类等图像任务上表现良好，参数数量仅为CLIP风格模型的一半，无需特定的文本塔或嵌入。当通过图像-文本对比学习和下一句对比学习联合训练时，CLIPPO可以在自然语言理解任务上表现良好，无需任何词级损失（语言建模或掩码语言建模），性能优于基于像素的前人工作。令人惊讶的是，CLIPPO只需将问题和图像一起渲染，就能在视觉问答任务上获得良好的准确性。最后，由于CLIPPO不需要分词器，证明了它可以在无需修改的情况下实现强大的多语言多模态检索性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal models are becoming increasingly effective, in part due to unified components, such as the Transformer architecture. However, multimodal models still often consist of many task- and modality-specific pieces and training procedures. For example, CLIP (Radford et al., 2021) trains independent text and image towers via a contrastive loss. We explore an additional unification: the use of a pure pixel-based model to perform image, text, and multimodal tasks. Our model is trained with contrastive loss alone, so we call it CLIP-Pixels Only (CLIPPO). CLIPPO uses a single encoder that processes both regular images and text rendered as images. CLIPPO performs image-based tasks such as retrieval and zero-shot image classification almost as well as CLIP-style models, with half the number of parameters and no text-specific tower or embedding. When trained jointly via image-text contrastive learning and next-sentence contrastive learning, CLIPPO can perform well on natural language understanding tasks, without any word-level loss (language modelling or masked language modelling), outperforming pixel-based prior work. Surprisingly, CLIPPO can obtain good accuracy in visual question answering, simply by rendering the question and image together. Finally, we exploit the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multimodal retrieval without modifications. Code and pretrained models are available at https://github.com/google-research/big_vision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1376.Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond</span><br>
                <span class="as">Fei, ZhengcongandFan, MingyuanandZhu, LiandHuang, JunshiandWei, XiaomingandWei, Xiaolin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fei_Masked_Auto-Encoders_Meet_Generative_Adversarial_Networks_and_Beyond_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24449-24459.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用生成对抗网络（GAN）提高预训练的视觉变换器模型的效率和性能。<br>
                    动机：虽然Masked Auto-Encoder (MAE)方法在图像任务上表现优秀，但需要大量的训练资源。<br>
                    方法：提出一种名为GAN-MAE的新框架，其中生成器根据可见的图像块生成被遮蔽的图像块，判别器则用于判断图像块是否由生成器合成。判别器和生成器的视觉变换器主干共享参数。<br>
                    效果：实验表明，与标准的MAE相比，GAN-MAE框架的对抗训练更有效，且在相同的模型大小、训练数据和计算资源下表现更好。此外，该方法在迁移下游任务时也表现出良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Auto-Encoder (MAE) pretraining methods randomly mask image patches and then train a vision Transformer to reconstruct the original pixels based on the unmasked patches. While they demonstrates impressive performance for downstream vision tasks, it generally requires a large amount of training resource. In this paper, we introduce a novel Generative Adversarial Networks alike framework, referred to as GAN-MAE, where a generator is used to generate the masked patches according to the remaining visible patches, and a discriminator is employed to predict whether the patch is synthesized by the generator. We believe this capacity of distinguishing whether the image patch is predicted or original is benefit to representation learning. Another key point lies in that the parameters of the vision Transformer backbone in the generator and discriminator are shared. Extensive experiments demonstrate that adversarial training of GAN-MAE framework is more efficient and accordingly outperforms the standard MAE given the same model size, training data, and computation resource. The gains are substantially robust for different model sizes and datasets, in particular, a ViT-B model trained with GAN-MAE for 200 epochs outperforms the MAE with 1600 epochs on fine-tuning top-1 accuracy of ImageNet-1k with much less FLOPs. Besides, our approach also works well at transferring downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1377.iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition</span><br>
                <span class="as">Wei, YixuanandCao, YueandZhang, ZhengandPeng, HouwenandYao, ZhuliangandXie, ZhendaandHu, HanandGuo, Baining</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2776-2786.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地结合图像分类和对比语言-图像预训练这两种常见的视觉识别方法。<br>
                    动机：现有的多任务学习方法在处理图像分类和语言-图像预训练时，通常使用两个独立的头部进行处理，这种方法存在一定局限性。<br>
                    方法：本文提出了一种名为iCLIP的方法，该方法将图像分类和语言-图像预训练深度融合，使图像分类与语言-图像预训练共享相同的公式和模型权重。同时，通过利用外部知识（如字典中的描述）增强图像分类任务的类别名称，进一步连接这两个任务。<br>
                    效果：实验表明，该方法很好地结合了两种任务的优点：由于清晰的类别标签，图像分类任务具有强大的判别能力；而由于文本描述中的丰富语义，CLIP任务具有良好的零样本能力。在IN-1K上，该方法达到了82.9%的Top-1准确率，并在Kornblith 12数据集基准的零样本识别上超越了CLIP 1.8%，且模型大小相似。代码和模型已公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a method that effectively combines two prevalent visual recognition methods, i.e., image classification and contrastive language-image pre-training, dubbed iCLIP. Instead of naive multi-task learning that use two separate heads for each task, we fuse the two tasks in a deep fashion that adapts the image classification to share the same formula and the same model weights with the language-image pre-training. To further bridge these two tasks, we propose to enhance the category names in image classification tasks using external knowledge, such as their descriptions in dictionaries. Extensive experiments show that the proposed method combines the advantages of two tasks well: the strong discrimination ability in image classification tasks due to the clear and clean category labels, and the good zero-shot ability in CLIP tasks ascribed to the richer semantics in the text descriptions. In particular, it reaches 82.9% top-1 accuracy on IN-1K, and surpasses CLIPby 1.8%, with similar model size, on zero-shot recognition of Kornblith 12-dataset benchmark. The code and models are publicly available at https://github.com/weiyx16/iCLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1378.CapDet: Unifying Dense Captioning and Open-World Detection Pretraining</span><br>
                <span class="as">Long, YanxinandWen, YoupengandHan, JianhuaandXu, HangandRen, PengzhenandZhang, WeiandZhao, ShenandLiang, Xiaodan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_CapDet_Unifying_Dense_Captioning_and_Open-World_Detection_Pretraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15233-15243.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的开放世界检测方法在推理阶段需要预定义类别空间，仅能预测属于该空间的对象。<br>
                    动机：为了实现真正的开放世界检测，本文提出了一种名为CapDet的新方法，可以预测给定的类别列表或直接生成预测边界框的类别。<br>
                    方法：通过引入额外的密集字幕头，将开放世界检测和密集字幕任务统一到一个有效框架中，生成基于区域的字幕。<br>
                    效果：实验结果表明，通过统一密集字幕任务，CapDet在LVIS（1203类）上的性能比基线方法提高了2.1% mAP，并在VG V1.2和VG-COCO数据集上实现了最先进的密集字幕性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Benefiting from large-scale vision-language pre-training on image-text pairs, open-world detection methods have shown superior generalization ability under the zero-shot or few-shot detection settings. However, a pre-defined category space is still required during the inference stage of existing methods and only the objects belonging to that space will be predicted. To introduce a "real" open-world detector, in this paper, we propose a novel method named CapDet to either predict under a given category list or directly generate the category of predicted bounding boxes. Specifically, we unify the open-world detection and dense caption tasks into a single yet effective framework by introducing an additional dense captioning head to generate the region-grounded captions. Besides, adding the captioning task will in turn benefit the generalization of detection performance since the captioning dataset covers more concepts. Experiment results show that by unifying the dense caption task, our CapDet has obtained significant performance improvements (e.g., +2.1% mAP on LVIS rare classes) over the baseline method on LVIS (1203 classes). Besides, our CapDet also achieves state-of-the-art performance on dense captioning tasks, e.g., 15.44% mAP on VG V1.2 and 13.98% on the VG-COCO dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1379.RILS: Masked Visual Reconstruction in Language Semantic Space</span><br>
                <span class="as">Yang, ShushengandGe, YixiaoandYi, KunandLi, DianandShan, YingandQie, XiaohuandWang, Xinggang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_RILS_Masked_Visual_Reconstruction_in_Language_Semantic_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23304-23314.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索图像遮蔽模型（MIM）和自然语言监督两种范式的协同作用，并研究它们结合时产生的新特性。<br>
                    动机：通过将图像遮蔽模型与自然语言监督相结合，使视觉模型能够捕获结构化信息，并通过预测被遮蔽标记的适当语义来改善文本编码器。<br>
                    方法：提出了一种新的遮蔽视觉重建语言语义空间（RILS）预训练框架，其中句子表示作为原型，将仅视觉信号转换为具有语义意义的遮蔽模型重建目标的补丁-句子概率。<br>
                    效果：实验结果表明，该方法不仅拥有之前MIM和CLIP的最佳性能，而且在各种任务上由于它们的互惠互利而取得了进一步的改进。RILS在下游分类、检测和分割任务上表现出优越的可转移性，特别是在低样本量的情况下。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Both masked image modeling (MIM) and natural language supervision have facilitated the progress of transferable visual pre-training. In this work, we seek the synergy between two paradigms and study the emerging properties when MIM meets natural language supervision. To this end, we present a novel masked visual Reconstruction In Language semantic Space (RILS) pre-training framework, in which sentence representations, encoded by the text encoder, serve as prototypes to transform the vision-only signals into patch-sentence probabilities as semantically meaningful MIM reconstruction targets. The vision models can therefore capture useful components with structured information by predicting proper semantic of masked tokens. Better visual representations could, in turn, improve the text encoder via the image-text alignment objective, which is essential for the effective MIM target transformation. Extensive experimental results demonstrate that our method not only enjoys the best of previous MIM and CLIP but also achieves further improvements on various tasks due to their mutual benefits. RILS exhibits advanced transferability on downstream classification, detection, and segmentation, especially for low-shot regimes. Code is available at https://github.com/hustvl/RILS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1380.Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations</span><br>
                <span class="as">Yang, ZiyanandKafle, KushalandDernoncourt, FranckandOrdonez, Vicente</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Improving_Visual_Grounding_by_Encouraging_Consistent_Gradient-Based_Explanations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19165-19174.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何调整联合视觉-语言模型，使其基于梯度的解释与人类提供的较小基础数据集的区域级注释一致？<br>
                    动机：目前的方法依赖于视觉-语言模型对物体检测器输出的评分，而我们的方法则通过使模型的解释与人类注释一致来提高视觉基础效果。<br>
                    方法：我们提出了一种基于边界的损失函数来调整联合视觉-语言模型，使其解释与区域级注释一致，这种方法被称为注意力掩码一致性（AMC）。<br>
                    效果：实验结果表明，使用AMC训练的标准视觉-语言模型在Flickr30k视觉基础基准测试中获得了86.49%的准确率，比之前在同一监督级别下训练的最佳模型提高了5.38%。此外，该方法在公认的引用表达式理解基准测试中也表现出色，RefCOCO+的简单测试中准确率为80.34%，困难分割中为64.55%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a margin-based loss for tuning joint vision-language models so that their gradient-based explanations are consistent with region-level annotations provided by humans for relatively smaller grounding datasets. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding results than previous methods that rely on using vision-language models to score the outputs of object detectors. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.49% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when compared to the best previous model trained under the same level of supervision. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension where it obtains 80.34% accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC is effective, easy to implement, and is general as it can be adopted by any vision-language model, and can use any type of region annotations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1381.Learning Visual Representations via Language-Guided Sampling</span><br>
                <span class="as">ElBanani, MohamedandDesai, KaranandJohnson, Justin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Banani_Learning_Visual_Representations_via_Language-Guided_Sampling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19208-19220.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过语言相似性来采样语义相似的图像对进行对比学习，以改进视觉表示学习。<br>
                    动机：现有的视觉表示学习方法主要依赖于手工制作的增强或学习到的簇，而本文提出的方法则使用语言相似性来采样视图对，从而更好地捕捉语义信息。<br>
                    方法：该方法利用预训练的语言模型指导学习，而不是直接最小化跨模态损失。具体来说，它通过使用语言相似性来采样语义相似的图像对进行对比学习。<br>
                    效果：实验结果表明，语言引导的学习比基于图像和图像-文本的表示学习方法产生更好的特征。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although an object may appear in numerous contexts, we often describe it in a limited number of ways. Language allows us to abstract away visual variation to represent and communicate concepts. Building on this intuition, we propose an alternative approach to visual representation learning: using language similarity to sample semantically similar image pairs for contrastive learning. Our approach diverges from image-based contrastive learning by sampling view pairs using language similarity instead of hand-crafted augmentations or learned clusters. Our approach also differs from image-text contrastive learning by relying on pre-trained language models to guide the learning rather than directly minimizing a cross-modal loss. Through a series of experiments, we show that language-guided learning yields better features than image-based and image-text representation learning approaches.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1382.Logical Implications for Visual Question Answering Consistency</span><br>
                <span class="as">Tascon-Morales, SergioandM\&#x27;arquez-Neila, PabloandSznitman, Raphael</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tascon-Morales_Logical_Implications_for_Visual_Question_Answering_Consistency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6725-6735.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管视觉问答（VQA）模型在近期取得了显著进展，但其不一致或矛盾的答案仍对其真正的推理能力产生质疑。<br>
                    动机：大多数现有的VQA方法采用间接策略或对问题和答案对进行强假设来强制模型的一致性，而我们提出了一种新的策略，通过直接减少逻辑不一致性来提高模型性能。<br>
                    方法：我们引入了一个新的一致性损失项，可以广泛应用于各种VQA模型，该损失项依赖于了解问题和答案对之间的逻辑关系。当这种信息在VQA数据集中通常不可用时，我们提出使用专门的语言模型来推断这些逻辑关系，并将其用于我们提出的一致性损失函数中。<br>
                    效果：我们在VQA Introspect和DME数据集上进行了广泛的实验，结果显示，我们的方法在改进最先进的VQA模型的同时，在不同的架构和设置下都表现出稳健性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite considerable recent progress in Visual Question Answering (VQA) models, inconsistent or contradictory answers continue to cast doubt on their true reasoning capabilities. However, most proposed methods use indirect strategies or strong assumptions on pairs of questions and answers to enforce model consistency. Instead, we propose a novel strategy intended to improve model performance by directly reducing logical inconsistencies. To do this, we introduce a new consistency loss term that can be used by a wide range of the VQA models and which relies on knowing the logical relation between pairs of questions and answers. While such information is typically not available in VQA datasets, we propose to infer these logical relations using a dedicated language model and use these in our proposed consistency loss function. We conduct extensive experiments on the VQA Introspect and DME datasets and show that our method brings improvements to state-of-the-art VQA models while being robust across different architectures and settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1383.AdaMAE: Adaptive Masking for Efficient Spatiotemporal Learning With Masked Autoencoders</span><br>
                <span class="as">Bandara, WeleGedaraChamindaandPatel, NamanandGholami, AliandNikkhah, MehdiandAgrawal, MotilalandPatel, VishalM.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bandara_AdaMAE_Adaptive_Masking_for_Efficient_Spatiotemporal_Learning_With_Masked_Autoencoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14507-14517.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种自适应掩蔽策略，用于通过重建被遮蔽的输入数据来学习图像、文本、音频、视频等的可泛化表示。<br>
                    动机：目前的掩蔽自动编码器（MAEs）方法在处理视频时，依赖于随机的补丁、管道或帧为基础的遮蔽策略来选择这些标记。这种方法需要大量的内存和计算资源，且预训练速度慢。<br>
                    方法：本文提出了AdaMAE，一种端到端可训练的自适应遮蔽策略。这种策略使用辅助采样网络根据语义上下文采样可见标记。这个网络估计一个空间-时间补丁标记的分类分布。那些增加预期重构误差的标记会被奖励并被选为可见标记，这是受强化学习中的策略梯度算法的启发。<br>
                    效果：实验结果表明，AdaMAE从高时空信息区域采样了更多的标记，从而允许我们遮蔽95%的标记，降低了内存需求并加快了预训练速度。在Something-Something v2数据集上进行的消融研究表明，我们的自适应采样方法有效，并在Kinetics-400动作分类数据集上取得了最先进的结果，准确率分别为70.0%和81.7%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Autoencoders (MAEs) learn generalizable representations for image, text, audio, video, etc., by reconstructing masked input data from tokens of the visible data. Current MAE approaches for videos rely on random patch, tube, or frame based masking strategies to select these tokens. This paper proposes AdaMAE, an adaptive masking strategy for MAEs that is end-to-end trainable. Our adaptive masking strategy samples visible tokens based on the semantic context using an auxiliary sampling network. This network estimates a categorical distribution over spacetime-patch tokens. The tokens that increase the expected reconstruction error are rewarded and selected as visible tokens, motivated by the policy gradient algorithm in reinforcement learning. We show that AdaMAE samples more tokens from the high spatiotemporal information regions, thereby allowing us to mask 95% of tokens, resulting in lower memory requirements and faster pre-training. We conduct ablation studies on the Something-Something v2 (SSv2) dataset to demonstrate the efficacy of our adaptive sampling approach and report state-of-the-art results of 70.0% and 81.7% in top-1 accuracy on SSv2 and Kinetics-400 action classification datasets with a ViT-Base backbone and 800 pre-training epochs. Code and pre-trained models are available at: https://github.com/wgcban/adamae.git</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1384.Towards Flexible Multi-Modal Document Models</span><br>
                <span class="as">Inoue, NaotoandKikuchi, KotaroandSimo-Serra, EdgarandOtani, MayuandYamaguchi, Kota</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Inoue_Towards_Flexible_Multi-Modal_Document_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14287-14296.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在构建一个能够同时解决多种设计任务的全面模型。<br>
                    动机：在生成图形文档的创新工作流程中，存在许多复杂的相互关联的任务，如对齐元素、选择适当的字体或使用美观和谐的颜色等。<br>
                    方法：我们建立了一个名为FlexDM的模型，将矢量图形文档视为一组多模态元素，并使用统一的架构学习预测被遮盖的字段，如元素类型、位置、样式属性、图像或文本。通过显式多任务学习和领域内预训练，我们的模型能更好地捕捉不同文档字段之间的多模态关系。<br>
                    效果：实验结果表明，我们的单一FlexDM能够成功解决多种不同的设计任务，同时实现的性能与特定任务和昂贵的基线相竞争。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Creative workflows for generating graphical documents involve complex inter-related tasks, such as aligning elements, choosing appropriate fonts, or employing aesthetically harmonious colors. In this work, we attempt at building a holistic model that can jointly solve many different design tasks. Our model, which we denote by FlexDM, treats vector graphic documents as a set of multi-modal elements, and learns to predict masked fields such as element type, position, styling attributes, image, or text, using a unified architecture. Through the use of explicit multi-task learning and in-domain pre-training, our model can better capture the multi-modal relationships among the different document fields. Experimental results corroborate that our single FlexDM is able to successfully solve a multitude of different design tasks, while achieving performance that is competitive with task-specific and costly baselines.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1385.DegAE: A New Pretraining Paradigm for Low-Level Vision</span><br>
                <span class="as">Liu, YihaoandHe, JingwenandGu, JinjinandKong, XiangtaoandQiao, YuandDong, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_DegAE_A_New_Pretraining_Paradigm_for_Low-Level_Vision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23292-23303.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练在低级视觉中的应用模糊和未完全确立的问题，以及回答预训练的初衷和核心问题。<br>
                    动机：尽管自监督预训练在高级视觉中取得了显著的成功，但在低级视觉中的应用仍然不明确。作者认为预训练对于数据获取困难的高成本任务更为重要。<br>
                    方法：通过考察高级和低级视觉中的先前预训练方法，将现有的低级视觉任务分为两组：低成本和高成本任务。针对高成本任务，提出了一种新的预训练范式——退化自动编码器（DegAE）。<br>
                    效果：实验结果表明，使用DegAE预训练后，SwinIR在图像去雾任务上的性能提高了6.88dB，Uformer在去雾和去雨任务上分别提高了3.22dB和0.54dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised pretraining has achieved remarkable success in high-level vision, but its application in low-level vision remains ambiguous and not well-established. What is the primitive intention of pretraining? What is the core problem of pretraining in low-level vision? In this paper, we aim to answer these essential questions and establish a new pretraining scheme for low-level vision. Specifically, we examine previous pretraining methods in both high-level and low-level vision, and categorize current low-level vision tasks into two groups based on the difficulty of data acquisition: low-cost and high-cost tasks. Existing literature has mainly focused on pretraining for low-cost tasks, where the observed performance improvement is often limited. However, we argue that pretraining is more significant for high-cost tasks, where data acquisition is more challenging. To learn a general low-level vision representation that can improve the performance of various tasks, we propose a new pretraining paradigm called degradation autoencoder (DegAE). DegAE follows the philosophy of designing pretext task for self-supervised pretraining and is elaborately tailored to low-level vision. With DegAE pretraining, SwinIR achieves a 6.88dB performance gain on image dehaze task, while Uformer obtains 3.22dB and 0.54dB improvement on dehaze and derain tasks, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1386.ScaleDet: A Scalable Multi-Dataset Object Detector</span><br>
                <span class="as">Chen, YanbeiandWang, ManchenandMittal, AbhayandXu, ZhenlinandFavaro, PaoloandTighe, JosephandModolo, Davide</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7288-7297.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用异构大规模数据集进行无额外标注成本的多数据集训练。<br>
                    动机：现有的多数据集学习者大多依赖人工重新标记或复杂的优化来统一跨数据集的标签，我们提出了一种简单且可扩展的公式来为多数据集训练生成统一的语义标签空间。<br>
                    方法：我们提出了一个可扩展的多数据集检测器（ScaleDet），通过视觉-文本对齐学习跨数据集的标签分配和标签语义相似性。<br>
                    效果：我们在LVIS、COCO、Objects365、OpenImages等上游数据集以及ODinW的13个下游数据集上进行了广泛的实验，结果显示，ScaleDet在相同的主干网络上取得了令人信服的强大模型性能，mAP达到了50.7（LVIS）、58.8（COCO）、46.8（Objects365）、76.2（OpenImages）和71.8（ODinW），超过了最先进的检测器。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-dataset training provides a viable solution for exploiting heterogeneous large-scale datasets without extra annotation cost. In this work, we propose a scalable multi-dataset detector (ScaleDet) that can scale up its generalization across datasets when increasing the number of training datasets. Unlike existing multi-dataset learners that mostly rely on manual relabelling efforts or sophisticated optimizations to unify labels across datasets, we introduce a simple yet scalable formulation to derive a unified semantic label space for multi-dataset training. ScaleDet is trained by visual-textual alignment to learn the label assignment with label semantic similarities across datasets. Once trained, ScaleDet can generalize well on any given upstream and downstream datasets with seen and unseen classes. We conduct extensive experiments using LVIS, COCO, Objects365, OpenImages as upstream datasets, and 13 datasets from Object Detection in the Wild (ODinW) as downstream datasets. Our results show that ScaleDet achieves compelling strong model performance with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the same backbone.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1387.Language-Guided Music Recommendation for Video via Prompt Analogies</span><br>
                <span class="as">McKee, DanielandSalamon, JustinandSivic, JosefandRussell, Bryan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/McKee_Language-Guided_Music_Recommendation_for_Video_via_Prompt_Analogies_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14784-14793.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种方法，在用户用自由形式的自然语言指导音乐选择的同时，为输入视频推荐音乐。<br>
                    动机：现有音乐视频数据集提供了所需的（视频，音乐）训练对，但缺乏音乐的文本描述。<br>
                    方法：我们提出了一个文本合成方法，该方法依赖于基于类比的提示过程，从预训练的音乐标记器输出和少量的人工文本描述中生成自然语言音乐描述。然后使用这些合成的音乐描述来训练一个新的三模态模型，该模型融合了文本和视频输入表示以查询音乐样本。<br>
                    效果：通过收集YT8M-MusicVideo数据集的一个子集4k片段进行标注，并公开可用的自然语言音乐描述作为我们的测试数据集，我们的方法可以在视频到音乐检索方面匹配或超过先前方法的性能，同时在使用文本指导时显著提高检索准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this problem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descriptions of the music. This work addresses this challenge with the following three contributions. First, we propose a text-synthesis approach that relies on an analogy-based prompting procedure to generate natural language music descriptions from a large-scale language model (BLOOM-176B) given pre-trained music tagger outputs and a small number of human text descriptions. Second, we use these synthesized music descriptions to train a new trimodal model, which fuses text and video input representations to query music samples. For training, we introduce a text dropout regularization mechanism which we show is critical to model performance. Our model design allows for the retrieved music audio to agree with the two input modalities by matching visual style depicted in the video and musical genre, mood, or instrumentation described in the natural language query. Third, to evaluate our approach, we collect a testing dataset for our problem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset with natural language music descriptions which we make publicly available. We show that our approach can match or exceed the performance of prior methods on video-to-music retrieval while significantly improving retrieval accuracy when using text guidance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1388.LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision \&amp; Language Models</span><br>
                <span class="as">Bulat, AdrianandTzimiropoulos, Georgios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23232-23241.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的软提示学习方法在训练数据上过度拟合，导致在相同领域的未见过类别上准确率大幅下降。<br>
                    动机：为了缓解基本类别的过拟合问题，提高提示表示能力，并解决由提示学习和LASP引入的视觉语言不匹配问题。<br>
                    方法：提出了一种新的语言感知软提示（LASP）学习方法，通过最大化学习提示与预定义的手工文本提示正确分类的概率来减轻基础类别过拟合的问题；提出了分组LASP，其中每组提示都根据单独的文本提示子集进行优化；识别了由提示学习和LASP引入的视觉语言不匹配问题，并提出了一种重新校准机制来解决它。<br>
                    效果：实验结果表明，该方法在所有11个数据集上都显著优于所有先前的软提示学习方法，并在8个测试数据集上首次匹配和超过了手工提示和CLIP获得的新颖类别准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1389.AutoAD: Movie Description in Context</span><br>
                <span class="as">Han, TengdaandBain, MaxandNagrani, ArshaandVarol, G\&quot;ulandXie, WeidiandZisserman, Andrew</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_AutoAD_Movie_Description_in_Context_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18930-18940.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种自动音频描述（AD）模型，该模型可以接收电影输入并输出文本形式的音频描述。<br>
                    动机：生成高质量的电影音频描述具有挑战性，因为描述依赖于上下文，并且可用的训练数据有限。<br>
                    方法：我们利用预训练的基础模型（如GPT和CLIP）的力量，只训练一个映射网络来桥接这两个模型进行视觉条件文本生成。我们还从电影片段、前一段的音频描述以及字幕中获取上下文信息，并在大规模数据集上进行预训练以解决缺乏训练数据的问题。<br>
                    效果：通过去除MAD数据集中的标签噪声并添加字符命名信息，我们对现有的音频描述数据集进行了改进。实验结果表明，我们的模型在电影音频描述任务上的表现优于先前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The objective of this paper is an automatic Audio Description (AD) model that ingests movies and outputs AD in text form. Generating high-quality movie AD is challenging due to the dependency of the descriptions on context, and the limited amount of training data available. In this work, we leverage the power of pretrained foundation models, such as GPT and CLIP, and only train a mapping network that bridges the two models for visually-conditioned text generation. In order to obtain high-quality AD, we make the following four contributions: (i) we incorporate context from the movie clip, AD from previous clips, as well as the subtitles; (ii) we address the lack of training data by pretraining on large-scale datasets, where visual or contextual information is unavailable, e.g. text-only AD without movies or visual captioning datasets without context; (iii) we improve on the currently available AD datasets, by removing label noise in the MAD dataset, and adding character naming information; and (iv) we obtain strong results on the movie AD task compared with previous methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1390.MaPLe: Multi-Modal Prompt Learning</span><br>
                <span class="as">Khattak, MuhammadUzairandRasheed, HanoonaandMaaz, MuhammadandKhan, SalmanandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19113-19122.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练的视觉-语言（V-L）模型如CLIP对输入文本提示的选择敏感，需要仔细选择提示模板以执行良好。<br>
                    动机：受自然语言处理（NLP）文献的启发，最近的CLIP适应方法学习提示作为文本输入来微调CLIP进行下游任务。作者注意到，只在CLIP的一个分支（语言或视觉）中使用提示调整表示是次优的，因为它不允许在下游任务上动态调整两个表示空间的灵活性。<br>
                    方法：本文提出了多模态提示学习（MaPLe）用于视觉和语言分支，以提高视觉和语言表示之间的对齐。设计促进了视觉-语言提示的强大耦合，以确保互惠互利，并阻止学习独立的单模解决方案。此外，在不同的早期阶段学习单独的提示，逐步模拟阶段特征关系，以允许丰富的上下文学习。<br>
                    效果：在三个代表性任务上评估了该方法的有效性，包括推广到新的类别、新的目标任务数据集和未见过的区域转移。与最先进的Co-CoOp方法相比，MaPLe表现出良好的性能，并在11个不同的图像识别数据集上实现了3.45%的绝对增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to fine-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the flexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method Co-CoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodal-prompt-learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1391.Multi-Modal Representation Learning With Text-Driven Soft Masks</span><br>
                <span class="as">Park, JaeyooandHan, Bohyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Multi-Modal_Representation_Learning_With_Text-Driven_Soft_Masks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2798-2807.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种视觉语言表示学习方法，通过引入新的操作、损失和数据增强策略。<br>
                    动机：目前的预训练模型主要依赖图像-标题对进行训练，缺乏对精细标注的利用，同时存在过拟合和偏见问题。<br>
                    方法：首先，我们通过软掩膜的方式生成与特定单词最相关的图像区域，以产生多样化的特征。然后，我们使用多模态编码器计算词条件视觉注意力，以确定每个单词的相关区域。接着，我们提出了焦点损失函数，以鼓励模型关注更难但更多样化的例子。最后，我们通过掩蔽文本和渲染图像畸变来进行多模态数据增强。<br>
                    效果：实验结果表明，这三种创新的结合对于学习预训练模型非常有效，并在多个视觉语言下游任务上取得了优异的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a visual-linguistic representation learning approach within a self-supervised learning framework by introducing a new operation, loss, and data augmentation strategy. First, we generate diverse features for the image-text matching (ITM) task via soft-masking the regions in an image, which are most relevant to a certain word in the corresponding caption, instead of completely removing them. Since our framework relies only on image-caption pairs with no fine-grained annotations, we identify the relevant regions to each word by computing the word-conditional visual attention using multi-modal encoder. Second, we encourage the model to focus more on hard but diverse examples by proposing a focal loss for the image-text contrastive learning (ITC) objective, which alleviates the inherent limitations of overfitting and bias issues. Last, we perform multi-modal data augmentations for self-supervised learning via mining various examples by masking texts and rendering distortions on images. We show that the combination of these three innovations is effective for learning a pretrained model, leading to outstanding performance on multiple vision-language downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1392.VindLU: A Recipe for Effective Video-and-Language Pretraining</span><br>
                <span class="as">Cheng, FengandWang, XiziandLei, JieandCrandall, DavidandBansal, MohitandBertasius, Gedas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10739-10750.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频和语言理解（VidL）模型设计中最重要的因素是什么？<br>
                    动机：现代VidL方法使用复杂且专门的模型架构和高级预训练协议，使得这些框架的可复制性、分析和比较变得困难。<br>
                    方法：通过实证研究，调查了VidL模型设计中的多个因素，包括时空架构设计、多模态融合方案、预训练目标、预训练数据选择、预训练和微调协议以及数据集和模型规模。<br>
                    效果：研究发现，最重要的设计因素包括：时间建模、视频到文本的多模态融合、掩蔽建模目标以及图像和视频的联合训练。基于这些实证洞察，开发了一个名为VindLU的分步式预训练方法。该方法在不依赖外部CLIP预训练的情况下，在几个VidL任务上取得了与最先进的结果相当甚至更好的结果。特别是在文本到视频检索任务上，该方法在DiDeMo上获得了61.2%的成绩，在ActivityNet上获得了55.0%的成绩，比当前最先进的方法分别高出7.8%和6.1%。此外，该方法还在ActivityNet-QA、MSRVTT-QA、MSRVTT-MC和TVQA上获得了最先进的视频问答结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The last several years have witnessed remarkable progress in video-and-language (VidL) understanding. However, most modern VidL approaches use complex and specialized model architectures and sophisticated pretraining protocols, making the reproducibility, analysis and comparisons of these frameworks difficult. Hence, instead of proposing yet another new VidL model, this paper conducts a thorough empirical study demystifying the most important factors in the VidL model design. Among the factors that we investigate are (i) the spatiotemporal architecture design, (ii) the multimodal fusion schemes, (iii) the pretraining objectives, (iv) the choice of pretraining data, (v) pretraining and finetuning protocols, and (vi) dataset and model scaling. Our empirical study reveals that the most important design factors include: temporal modeling, video-to-text multimodal fusion, masked modeling objectives, and joint training on images and videos. Using these empirical insights, we then develop a step-by-step recipe, dubbed VindLU, for effective VidL pretraining. Our final model trained using our recipe achieves comparable or better than state-of-the-art results on several VidL tasks without relying on external CLIP pretraining. In particular, on the text-to-video retrieval task, our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and TVQA. Our code and pretrained models are publicly available at: https://github.com/klauscc/VindLU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1393.Scaling Language-Image Pre-Training via Masking</span><br>
                <span class="as">Li, YanghaoandFan, HaoqiandHu, RonghangandFeichtenhofer, ChristophandHe, Kaiming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23390-23400.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种更简单、更有效的CLIP训练方法，即快速语言-图像预训练（FLIP）。<br>
                    动机：现有的语言-图像预训练模型在训练过程中需要处理大量的图像块，这既耗时又消耗内存。<br>
                    方法：FLIP方法在训练过程中随机屏蔽并移除大部分图像块，通过这种方式，我们可以在相同的时间下学习更多的图像-文本对，并在每次迭代中对比相似内存占用的更多样本。<br>
                    效果：实验结果表明，FLIP在准确性和训练速度上都优于无屏蔽的基线方法。在各种下游任务上，FLIP也显著优于在同一数据上训练的CLIP模型。此外，我们还探索了增加模型大小、数据大小或训练长度的扩展行为，并报告了令人鼓舞的结果和比较。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP. Our method randomly masks out and removes a large portion of image patches during training. Masking allows us to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint. It leads to a favorable trade-off between accuracy and training time. In our experiments on 400 million image-text pairs, FLIP improves both accuracy and speed over the no-masking baseline. On a large diversity of downstream tasks, FLIP dominantly outperforms the CLIP counterparts trained on the same data. Facilitated by the speedup, we explore the scaling behavior of increasing the model size, data size, or training length, and report encouraging results and comparisons. We hope that our work will foster future research on scaling vision-language learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1394.Vita-CLIP: Video and Text Adaptive CLIP via Multimodal Prompting</span><br>
                <span class="as">Wasim, SyedTalalandNaseer, MuzammalandKhan, SalmanandKhan, FahadShahbazandShah, Mubarak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wasim_Vita-CLIP_Video_and_Text_Adaptive_CLIP_via_Multimodal_Prompting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23034-23044.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何平衡预训练模型在有监督和零样本动作识别任务上的性能。<br>
                    动机：目前的工作在有监督性能和零样本泛化之间存在权衡，因为提升一个方面会导致另一个方面的性能下降。<br>
                    方法：提出一种多模态提示学习方案，通过统一的训练来平衡有监督和零样本性能。视觉提示包括全局视频级提示、局部帧级提示和总结提示；文本提示用于增强文本上下文。<br>
                    效果：在Kinetics-600, HMDB51和UCF101上实现了最先进的零样本性能，同时在有监督设置中保持竞争力。通过冻结预训练的骨干网络，优化了更少的参数并保留了现有的通用表示，从而实现了强大的零样本性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Adopting contrastive image-text pretrained models like CLIP towards video classification has gained attention due to its cost-effectiveness and competitive performance. However, recent works in this area face a trade-off. Finetuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Similarly, freezing the backbone to retain zero-shot capability causes significant drop in supervised accuracy. Because of this, recent works in literature typically train separate models for supervised and zero-shot action recognition. In this work, we propose a multimodal prompt learning scheme that works to balance the supervised and zero-shot performance under a single unified training. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Additionally, we define a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 and UCF101 while remaining competitive in the supervised setting. By keeping the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing general representation which helps achieve the strong zero-shot performance. Our codes and models will be publicly released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1395.Learning Attribute and Class-Specific Representation Duet for Fine-Grained Fashion Analysis</span><br>
                <span class="as">Jiao, YangandGao, YanandMeng, JingjingandShang, JinandSun, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_Learning_Attribute_and_Class-Specific_Representation_Duet_for_Fine-Grained_Fashion_Analysis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11050-11059.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有时尚表示学习中，只关注细粒度属性级别的问题，而忽视了不同类别之间属性的关系和依赖性。<br>
                    动机：通过利用关于时尚属性和类别分类学的先验知识，更好地建模这些属性关系和依赖性。<br>
                    方法：提出一个包含两个子网络的嵌入网络，分别用于处理属性和类别，通过多粒度损失函数引入适当的归纳偏置，以跨不同的时尚表示粒度进行学习。<br>
                    效果：在三个基准数据集上的实验结果表明，该方法优于最先进的方法，具有很大的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fashion representation learning involves the analysis and understanding of various visual elements at different granularities and the interactions among them. Existing works often learn fine-grained fashion representations at the attribute-level without considering their relationships and inter-dependencies across different classes. In this work, we propose to learn an attribute and class specific fashion representation duet to better model such attribute relationships and inter-dependencies by leveraging prior knowledge about the taxonomy of fashion attributes and classes. Through two sub-networks for the attributes and classes, respectively, our proposed an embedding network progressively learn and refine the visual representation of a fashion image to improve its robustness for fashion retrieval. A multi-granularity loss consisting of attribute-level and class-level losses is proposed to introduce appropriate inductive bias to learn across different granularities of the fashion representations. Experimental results on three benchmark datasets demonstrate the effectiveness of our method, which outperforms the state-of-the-art methods with a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1396.Clover: Towards a Unified Video-Language Alignment and Fusion Model</span><br>
                <span class="as">Huang, JingjiaandLi, YinanandFeng, JiashiandWu, XinglongandSun, XiaoshuaiandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14856-14866.png><br>
            
            <span class="tt"><span class="t0">研究问题：构建一个通用的视频-语言模型来解决各种视频理解任务，如文本-视频检索、视频问答。<br>
                    动机：目前大多数方法通过堆叠单模态和跨模态特征编码器并使用成对对比预训练任务进行训练，虽然具有吸引力的通用性，但结果模型在效率和性能之间需要折衷，且大多采用不同的架构来处理不同的下游任务。<br>
                    方法：提出Clover——一种相关视频-语言预训练方法，通过一种新的三模态对齐预训练任务来提高跨模态特征的对齐和融合，并通过引入学习语义掩蔽样本和新成对排序损失来增强三模态对齐。<br>
                    效果：Clover在多个下游任务上建立了新的最先进水平，包括三种零射击和微调设置的检索任务以及八个视频问答任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Building a universal video-language model for solving various video understanding tasks (e.g., text-video retrieval, video question answering) is an open challenge to the machine learning field. Towards this goal, most recent works build the model by stacking uni-modal and cross-modal feature encoders and train it with pair-wise contrastive pre-text tasks. Though offering attractive generality, the resulted models have to compromise between efficiency and performance. They mostly adopt different architectures to deal with different downstream tasks. We find this is because the pair-wise training cannot well align and fuse features from different modalities. We then introduce Clover--a Correlated Video-Language pre-training method--towards a universal video-language model for solving multiple video understanding tasks with neither performance nor efficiency compromise. It improves cross-modal feature alignment and fusion via a novel tri-modal alignment pre-training task. Additionally, we propose to enhance the tri-modal alignment via incorporating learning from semantic masked samples and a new pair-wise ranking loss. Clover establishes new state-of-the-arts on multiple downstream tasks, including three retrieval tasks for both zero-shot and fine-tuning settings, and eight video question answering tasks. Codes and pre-trained models will be released at https://github.com/LeeYN-43/Clover.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1397.Self-Supervised Learning From Images With a Joint-Embedding Predictive Architecture</span><br>
                <span class="as">Assran, MahmoudandDuval, QuentinandMisra, IshanandBojanowski, PiotrandVincent, PascalandRabbat, MichaelandLeCun, YannandBallas, Nicolas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15619-15629.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何不依赖手工制作的数据增强来学习高度语义的图像表示。<br>
                    动机：现有的方法需要依赖手工制作的数据增强，而本文提出的方法不需要。<br>
                    方法：介绍了一种名为Image-based Joint-Embedding Predictive Architecture (I-JEPA)的非生成式自我监督学习方法，通过预测同一图像中不同目标块的表示来进行学习。<br>
                    效果：实验证明，当与视觉变压器结合使用时，I-JEPA具有很高的可扩展性。例如，使用16个A100 GPU在72小时内训练ViT-Huge/14模型，在从线性分类到对象计数和深度预测的各种任务上都能取得强大的下游性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1398.DeAR: Debiasing Vision-Language Models With Additive Residuals</span><br>
                <span class="as">Seth, AshishandHemani, MayurandAgarwal, Chirag</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6820-6829.png><br>
            
            <span class="tt"><span class="t0">研究问题：大型预训练视觉-语言模型（VLMs）在各种基于视觉的下游任务中提供了丰富的、可适应的图像和文本表示，但由于训练数据中不同身份群体的分布不均，这些模型存在社会偏见。<br>
                    动机：由于特定文本概念与不同身份群体的图像表示之间的相似性偏差，这些偏见限制了此类模型在现实世界高风险应用中的实用性。<br>
                    方法：我们提出了DeAR（使用附加残差进行去偏），这是一种新的去偏方法，通过学习附加残差图像表示来抵消原始表示，确保公平的输出表示。<br>
                    效果：通过多个数据集进行的公平性和零射性能保持的实验结果表明，我们的框架是有效的。我们还引入了一个新的基于上下文的偏见基准测试数据集——保护属性标签关联（PATA）数据集，用于评估大型预训练VLMs的公平性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large pre-trained vision-language models (VLMs) reduce the time for developing predictive models for various vision-grounded language downstream tasks by providing rich, adaptable image and text representations. However, these models suffer from societal biases owing to the skewed distribution of various identity groups in the training data. These biases manifest as the skewed similarity between the representations for specific text concepts and images of people of different identity groups and, therefore, limit the usefulness of such models in real-world high-stakes applications. In this work, we present DeAR (Debiasing with Additive Residuals), a novel debiasing method that learns additive residual image representations to offset the original representations, ensuring fair output representations. In doing so, it reduces the ability of the representations to distinguish between the different identity groups. Further, we observe that the current fairness tests are performed on limited face image datasets that fail to indicate why a specific text concept should/should not apply to them. To bridge this gap and better evaluate DeAR, we introduce a new context-based bias benchmarking dataset - the Protected Attribute Tag Association (PATA) dataset for evaluating the fairness of large pre-trained VLMs. Additionally, PATA provides visual context for a diverse human population in different scenarios with both positive and negative connotations. Experimental results for fairness and zero-shot performance preservation using multiple datasets demonstrate the efficacy of our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1399.Understanding Masked Image Modeling via Learning Occlusion Invariant Feature</span><br>
                <span class="as">Kong, XiangwenandZhang, Xiangyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6241-6251.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前，虽然掩蔽图像建模（MIM）在自监督视觉识别方面取得了巨大成功，但其作为基于重建的框架，其工作机制仍然是一个未解的问题。<br>
                    动机：由于MIM与先前研究较多的siamese方法（如对比学习）差异较大，因此理解MIM如何工作仍是一个开放性问题。<br>
                    方法：本文提出了一个新的观点：MIM隐式地学习了遮挡不变特征，这与其它siamese方法类似，只不过后者学习的是不同的不变性。通过将MIM的公式放松为等效的siamese形式，可以将MIM方法解释为与传统方法统一的框架，其中只有a)数据转换，即要学习的不变性，和b)相似度测量不同。<br>
                    效果：以MAE（He等人，2021）为例，我们发现MIM模型的成功与选择的相似度函数关系不大，但由被遮蔽的图像引入的被学习的遮挡不变特征——它被证明是视觉变换器的优选初始化，尽管学到的特征可能语义较少。我们希望我们的发现能激发计算机视觉社区中的研究人员开发出更强大的自监督方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, Masked Image Modeling (MIM) achieves great success in self-supervised visual recognition. However, as a reconstruction-based framework, it is still an open question to understand how MIM works, since MIM appears very different from previous well-studied siamese approaches such as contrastive learning. In this paper, we propose a new viewpoint: MIM implicitly learns occlusion-invariant features, which is analogous to other siamese methods while the latter learns other invariance. By relaxing MIM formulation into an equivalent siamese form, MIM methods can be interpreted in a unified framework with conventional methods, among which only a) data transformations, i.e. what invariance to learn, and b) similarity measurements are different. Furthermore, taking MAE (He et al., 2021) as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image -- it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic. We hope our findings could inspire researchers to develop more powerful self-supervised methods in computer vision community.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1400.Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space</span><br>
                <span class="as">Kim, SiwonandOh, JinohandLee, SungjinandYu, SeunghakandDo, JaeyoungandTaghavi, Tara</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10942-10950.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有基于概念的解释方法需要大量手动收集的概念标注图像的问题。<br>
                    动机：手动收集的概念标注图像既昂贵又存在人为偏见的风险。<br>
                    方法：提出一种利用文本驱动的概念进行反事实解释的方法（CounTEX），通过预训练的多模态联合嵌入空间定义概念，无需额外的概念标注数据集。<br>
                    效果：CounTEX能生成忠实的解释，提供对模型决策理由的语义理解，且不受人为偏见影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Concept-based explanation aims to provide concise and human-understandable explanations of an image classifier. However, existing concept-based explanation methods typically require a significant amount of manually collected concept-annotated images. This is costly and runs the risk of human biases being involved in the explanation. In this paper, we propose counterfactual explanation with text-driven concepts (CounTEX), where the concepts are defined only from text by leveraging a pre-trained multi-modal joint embedding space without additional concept-annotated datasets. A conceptual counterfactual explanation is generated with text-driven concepts. To utilize the text-driven concepts defined in the joint embedding space to interpret target classifier outcome, we present a novel projection scheme for mapping the two spaces with a simple yet effective implementation. We show that CounTEX generates faithful explanations that provide a semantic understanding of model decision rationale robust to human bias.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1401.Fine-Tuned CLIP Models Are Efficient Video Learners</span><br>
                <span class="as">Rasheed, HanoonaandKhattak, MuhammadUzairandMaaz, MuhammadandKhan, SalmanandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6545-6554.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地将图像级的CLIP表示转移到视频上？<br>
                    动机：由于在大规模上对视频进行训练是不可行的，因此最近的一些方法主要关注如何将基于图像的CLIP有效转移到视频领域。<br>
                    方法：本文提出了一种简单的视频微调CLIP（ViFi-CLIP）基线，通过帧级别的处理，从CLIP的图像编码器开始，然后进行特征池化和与相应文本嵌入的相似性匹配，以隐式地在ViFi-CLIP中建模时间线索。<br>
                    效果：实验结果表明，这种简单而强大的基线在零样本、基本到新颖的泛化、少样本和全监督设置等五种视频基准测试中表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in implicitly modeling the temporal cues within ViFi-CLIP. Such fine-tuning helps the model to focus on scene dynamics, moving objects and inter-object relationships. For low-data regimes where full fine-tuning is not viable, we propose a 'bridge and prompt' approach that first uses finetuning to bridge the domain gap and then learns prompts on language and vision side to adapt CLIP representations. We extensively evaluate this simple yet strong baseline on zero-shot, base-to-novel generalization, few-shot and fully supervised settings across five video benchmarks. Our code and models will be publicly released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1402.Visual Recognition by Request</span><br>
                <span class="as">Tang, ChufengandXie, LingxiandZhang, XiaopengandHu, XiaolinandTian, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Visual_Recognition_by_Request_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15265-15274.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现无限粒度的视觉语义识别，弥补现有视觉识别算法的不足。<br>
                    动机：人类具有无限粒度的视觉语义识别能力，但现有的视觉识别算法无法达到这个目标。<br>
                    方法：提出一种新的视觉识别范式——视觉识别请求（ViRReq），将视觉识别分解为原子任务——请求，并利用知识库、分层文本字典辅助任务定义。<br>
                    效果：ViRReq能够从高度不完整的标注中学习复杂的整体-部分层次结构，并能最小化地插入新概念。在两个具有层次化整体-部分标注的数据集CPP和ADE20K上，ViRReq表现出灵活的识别能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans have the ability of recognizing visual semantics in an unlimited granularity, but existing visual recognition algorithms cannot achieve this goal. In this paper, we establish a new paradigm named visual recognition by request (ViRReq) to bridge the gap. The key lies in decomposing visual recognition into atomic tasks named requests and leveraging a knowledge base, a hierarchical and text-based dictionary, to assist task definition. ViRReq allows for (i) learning complicated whole-part hierarchies from highly incomplete annotations and (ii) inserting new concepts with minimal efforts. We also establish a solid baseline by integrating language-driven recognition into recent semantic and instance segmentation methods, and demonstrate its flexible recognition ability on CPP and ADE20K, two datasets with hierarchical whole-part annotations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1403.MAP: Multimodal Uncertainty-Aware Vision-Language Pre-Training Model</span><br>
                <span class="as">Ji, YataiandWang, JunjieandGong, YuanandZhang, LinandZhu, YanruandWang, HongfaandZhang, JiaxingandSakai, TetsuyaandYang, Yujiu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_MAP_Multimodal_Uncertainty-Aware_Vision-Language_Pre-Training_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23262-23271.png><br>
            
            <span class="tt"><span class="t0">研究问题：多模态语义理解中的不确定性问题，包括模间和模内不确定性，以及研究问题：多模态语义理解中的不确定性问题，包括模间和模内不确定性，以及在未标记数据集上的预训练和特定任务的下游数据集上的微调中对此不确定性进行建模的研究不足。<br>
                    动机：现有的确定性方法无法充分传达丰富的多模态语义信息和复杂的关系，因此需要对这种不确定性进行建模。<br>
                    方法：通过利用序列级别的交互作用，使用概率分布编码器（PDE）将所有模态的表示投影为概率分布。并将不确定性建模与流行的预训练框架相结合，提出了适合的预训练任务：基于分布的视觉语言对比学习（D-VLC）、基于分布的掩码语言建模（D-MLM）和基于分布的图像文本匹配（D-ITM）。<br>
                    效果：在具有挑战性的下游任务中，包括图像-文本检索、视觉问答、视觉推理和视觉蕴含等，经过微调的模型取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the exiting deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned models are applied to challenging downstream tasks, including image-text retrieval, visual question answering, visual reasoning, and visual entailment, and achieve state-of-the-art results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1404.Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?</span><br>
                <span class="as">Wu, WenhaoandLuo, HaipengandFang, BoandWang, JingdongandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Cap4Video_What_Can_Auxiliary_Captions_Do_for_Text-Video_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10704-10713.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用视频的标题、标签和字幕等文本信息进行文本-视频检索。<br>
                    动机：现有的文本-视频检索方法主要关注视频视觉内容与文本查询句子的跨模态匹配，忽视了视频附带的相关文本信息。<br>
                    方法：提出一种新的文本-视频检索方法，通过零样本视频字幕生成从视频中直接生成相关字幕，并利用网络预训练模型（如CLIP和GPT-2）的知识。<br>
                    效果：新提出的Cap4Video框架在四个标准文本-视频检索基准测试中取得了最先进的性能，验证了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most existing text-video retrieval methods focus on cross-modal matching between the visual content of videos and textual query sentences. However, in real-world scenarios, online videos are often accompanied by relevant text information such as titles, tags, and even subtitles, which can be utilized to match textual queries. This insight has motivated us to propose a novel approach to text-video retrieval, where we directly generate associated captions from videos using zero-shot video captioning with knowledge from web-scale pre-trained models (e.g., CLIP and GPT-2). Given the generated captions, a natural question arises: what benefits do they bring to text-video retrieval? To answer this, we introduce Cap4Video, a new framework that leverages captions in three ways: i) Input data: video-caption pairs can augment the training data. ii) Intermediate feature interaction: we perform cross-modal feature interaction between the video and caption to produce enhanced video representations. iii) Output score: the Query-Caption matching branch can complement the original Query-Video matching branch for text-video retrieval. We conduct comprehensive ablation studies to demonstrate the effectiveness of our approach. Without any post-processing, Cap4Video achieves state-of-the-art performance on four standard text-video retrieval benchmarks: MSR-VTT (51.4%), VATEX (66.6%), MSVD (51.8%), and DiDeMo (52.0%). The code is available at https://github.com/whwu95/Cap4Video.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1405.Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles</span><br>
                <span class="as">Ye, ShuquanandXie, YujiaandChen, DongdongandXu, YichongandYuan, LuandZhu, ChenguangandLiao, Jing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Improving_Commonsense_in_Vision-Language_Models_via_Knowledge_Graph_Riddles_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2634-2645.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在分析和改善近期流行的视觉-语言（VL）模型的常识能力。<br>
                    动机：尽管现有的VL模型取得了巨大的成功，但我们发现它们仍然缺乏常识知识和推理能力，这是迈向人工智能的关键组成部分。<br>
                    方法：我们提出了一种名为"Data Augmentation with kNowledge graph linearization for CommonsensE capability" (DANCE)的数据增强策略，通过利用常识知识图谱（如ConceptNet）在训练过程中实时向现有VL数据集注入常识知识。<br>
                    效果：通过在代表性的VL模型上进行大量实验，我们证明DANCE技术能够显著提高模型的常识能力，同时保持其在基本检索任务上的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper focuses on analyzing and improving the commonsense ability of recent popular vision-language (VL) models. Despite the great success, we observe that existing VL-models still lack commonsense knowledge/reasoning ability (e.g., "Lemons are sour"), which is a vital component towards artificial general intelligence. Through our analysis, we find one important reason is that existing large-scale VL datasets do not contain much commonsense knowledge, which motivates us to improve the commonsense of VL-models from the data perspective. Rather than collecting a new VL training dataset, we propose a more scalable strategy, i.e., "Data Augmentation with kNowledge graph linearization for CommonsensE capability" (DANCE). It can be viewed as one type of data augmentation technique, which can inject commonsense knowledge into existing VL datasets on the fly during training. More specifically, we leverage the commonsense knowledge graph (e.g., ConceptNet) and create variants of text description in VL datasets via bidirectional sub-graph sequentialization. For better commonsense evaluation, we further propose the first retrieval-based commonsense diagnostic benchmark. By conducting extensive experiments on some representative VL-models, we demonstrate that our DANCE technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1406.S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning</span><br>
                <span class="as">Suo, WeiandSun, MengyangandLiu, WeisongandGao, YiqiandWang, PengandZhang, YanningandWu, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_S3C_Semi-Supervised_VQA_Natural_Language_Explanation_via_Self-Critical_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2646-2656.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉问答（VQA）模型决策过程的自然语言解释问题，以及现有方法在逻辑一致性和人工标注解释获取上的瓶颈。<br>
                    动机：传统的关注点或梯度分析无法准确反映推理过程，而自由文本理由更易理解且能赢得用户信任。但现有的方法主要使用后验或自我合理化模型来获得合理的解释，存在逻辑不一致和人工标注解释获取困难的问题。<br>
                    方法：本文提出了一种新的半监督VQA-NLE通过自我批判学习（S3C）的方法，通过回答奖励来评估候选解释，以提高答案和理由之间的逻辑一致性。利用半监督学习框架，S3C可以在没有人工标注解释的情况下从大量的样本中受益。<br>
                    效果：大量的自动测量和人工评估都显示了该方法的有效性。同时，该框架在两个VQA-NLE数据集上实现了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>VQA Natural Language Explanation (VQA-NLE) task aims to explain the decision-making process of VQA models in natural language. Unlike traditional attention or gradient analysis, free-text rationales can be easier to understand and gain users' trust. Existing methods mostly use post-hoc or self-rationalization models to obtain a plausible explanation. However, these frameworks are bottlenecked by the following challenges: 1) the reasoning process cannot be faithfully responded to and suffer from the problem of logical inconsistency. 2) Human-annotated explanations are expensive and time-consuming to collect. In this paper, we propose a new Semi-Supervised VQA-NLE via Self-Critical Learning (S3C), which evaluates the candidate explanations by answering rewards to improve the logical consistency between answers and rationales. With a semi-supervised learning framework, the S3C can benefit from a tremendous amount of samples without human-annotated explanations. A large number of automatic measures and human evaluations all show the effectiveness of our method. Meanwhile, the framework achieves a new state-of-the-art performance on the two VQA-NLE datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1407.LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization</span><br>
                <span class="as">Liu, ShengandHuynh, CongPhuocandChen, CongandArap, MaximandHamid, Raffay</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_LEMaRT_Label-Efficient_Masked_Region_Transform_for_Image_Harmonization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18290-18299.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种简单而有效的自我监督预训练方法，用于图像调和，该方法可以利用大规模的未标注图像数据集。<br>
                    动机：现有的图像调和模型需要大量的标注数据进行训练，这在实际应用中是不现实的。因此，作者提出了一种新的自我监督预训练方法，可以在未标注的图像数据集上进行训练。<br>
                    方法：首先，作者使用在线生成的预训练数据，通过标签高效掩蔽区域转换（LEMaRT）管道生成前景掩码，并对指定区域的视觉属性进行一系列变换，如散焦模糊、对比度和饱和度等。然后，通过从被干扰的图像中恢复原始图像来预训练图像调和模型。其次，作者通过将局部和全局自注意力机制相结合，对Swin Transformer进行改进，提出了一种新的图像调和模型SwinIH。<br>
                    效果：实验结果表明，使用LEMaRT预训练的SwinIH在图像调和任务上达到了新的最先进的水平，同时比现有的方法更加标签高效，即在微调阶段需要的标注数据更少。在iHarmony4数据集上，当只使用50%的训练数据进行微调时，SwinIH的性能比最先进的SCS-Co高出0.4 dB；当使用全部训练数据集进行训练时，SwinIH的性能比SCS-Co高出1.0 dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a simple yet effective self-supervised pretraining method for image harmonization which can leverage large-scale unannotated image datasets. To achieve this goal, we first generate pre-training data online with our Label-Efficient Masked Region Transform (LEMaRT) pipeline. Given an image, LEMaRT generates a foreground mask and then applies a set of transformations to perturb various visual attributes, e.g., defocus blur, contrast, saturation, of the region specified by the generated mask. We then pre-train image harmonization models by recovering the original image from the perturbed image. Secondly, we introduce an image harmonization model, namely SwinIH, by retrofitting the Swin Transformer [27] with a combination of local and global self-attention mechanisms. Pretraining SwinIH with LEMaRT results in a new state of the art for image harmonization, while being label-efficient, i.e., consuming less annotated data for fine-tuning than existing methods. Notably, on iHarmony4 dataset [8], SwinIH outperforms the state of the art, i.e., SCS-Co [16] by a margin of 0.4 dB when it is fine-tuned on only 50% of the training data, and by 1.0 dB when it is trained on the full training dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1408.Multi-Concept Customization of Text-to-Image Diffusion</span><br>
                <span class="as">Kumari, NupurandZhang, BingliangandZhang, RichardandShechtman, EliandZhu, Jun-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1931-1941.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让模型快速学习新概念并合成多个新概念？<br>
                    动机：用户希望合成自己的新概念，如家庭、宠物或物品。<br>
                    方法：提出Custom Diffusion方法，通过优化文本到图像的调节机制来表示新概念，同时实现快速调整。可以联合训练多个概念或将多个微调后的模型组合成一个。<br>
                    效果：该方法在定性和定量评估中优于或与几个基线和同时进行的工作相媲美，同时具有内存和计算效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning ( 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1409.Advancing Visual Grounding With Scene Knowledge: Benchmark and Method</span><br>
                <span class="as">Chen, ZhihongandZhang, RuifeiandSong, YibingandWan, XiangandLi, Guanbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Advancing_Visual_Grounding_With_Scene_Knowledge_Benchmark_and_Method_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15039-15049.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉基础任务中的问题，即如何建立视觉和语言之间的精细对齐。<br>
                    动机：现有的视觉基础数据集大多使用简单的描述文本构建，这无法充分测试模型在图像和文本上的理解和推理能力。<br>
                    方法：提出了一种新的场景知识引导的视觉基础（SK-VG）基准，其中图像内容和引用表达式不足以确定目标对象，迫使模型在长篇场景知识上进行推理。为此，我们提出了两种接受三元输入的方法，一种是将知识嵌入到图像特征中，另一种是利用语言结构帮助计算图像-文本匹配。<br>
                    效果：实验结果表明，提出的方法取得了有希望的结果，但仍有改进的空间，包括性能和可解释性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reasoning abilities over their joint space. However, most existing VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study, where a simple LSTM-based text encoder without pretraining can achieve state-of-the-art performance on mainstream VG datasets. Therefore, in this paper, we propose a novel benchmark of Scene Knowledge-guided Visual Grounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept the triple-type input, where the former embeds knowledge into the image features before the image-query interaction; the latter leverages linguistic structure to assist in computing the image-text matching. We conduct extensive experiments to analyze the above methods and show that the proposed approaches achieve promising results but still leave room for improvement, including performance and interpretability.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1410.Multiview Compressive Coding for 3D Reconstruction</span><br>
                <span class="as">Wu, Chao-YuanandJohnson, JustinandMalik, JitendraandFeichtenhofer, ChristophandGkioxari, Georgia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Multiview_Compressive_Coding_for_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9065-9075.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过单张图片理解物体和场景的三维姿态。<br>
                    动机：二维识别由于大规模学习和通用表示取得了巨大进步，但三维姿态识别由于图像中未描绘的遮挡等问题带来了新的挑战。<br>
                    方法：我们提出了一种简单的框架，通过学习自我监督学习的先进进展来获取可泛化的表示。我们的模型“多视图压缩编码（MCC）”从各种RGB-D视频中学习，将输入的外观和几何压缩以预测3D结构。<br>
                    效果：MCC的普适性和效率使其能够从大规模和多样化的数据源进行学习，并对由DALL*E 2想象或在野外用iPhone捕获的新对象具有强大的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A central goal of visual recognition is to understand objects and scenes from a single image. 2D recognition has witnessed tremendous progress thanks to large-scale learning and general-purpose representations. But, 3D poses new challenges stemming from occlusions not depicted in the image. Prior works try to overcome these by inferring from multiple views or rely on scarce CAD models and category-specific priors which hinder scaling to novel settings. In this work, we explore single-view 3D reconstruction by learning generalizable representations inspired by advances in self-supervised learning. We introduce a simple framework that operates on 3D points of single objects or whole scenes coupled with category-agnostic large-scale training from diverse RGB-D videos. Our model, Multiview Compressive Coding (MCC), learns to compress the input appearance and geometry to predict the 3D structure by querying a 3D-aware decoder. MCC's generality and efficiency allow it to learn from large-scale and diverse data sources with strong generalization to novel objects imagined by DALL*E 2 or captured in-the-wild with an iPhone.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1411.Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning</span><br>
                <span class="as">Piergiovanni, AJandKuo, WeichengandAngelova, Anelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2214-2224.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将ViT编码器转化为能同时处理图像和视频输入的高效视频模型。<br>
                    动机：现有的方法需要对两种类型的输入进行单独处理，缺乏效率。<br>
                    方法：通过稀疏采样输入，使模型能够从两种输入中进行训练和推理，无需完全微调，易于扩展。<br>
                    效果：该模型实现了最新的成果，能在大规模预训练ViTs上应用，且无需全量微调。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a simple approach which can turn a ViT encoder into an efficient video model, which can seamlessly work with both image and video inputs. By sparsely sampling the inputs, the model is able to do training and inference from both inputs. The model is easily scalable and can be adapted to large-scale pre-trained ViTs without requiring full finetuning. The model achieves SOTA results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1412.Token Boosting for Robust Self-Supervised Visual Transformer Pre-Training</span><br>
                <span class="as">Li, TianjiaoandFoo, LinGengandHu, PingandShang, XindiandRahmani, HosseinandYuan, ZehuanandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Token_Boosting_for_Robust_Self-Supervised_Visual_Transformer_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24027-24038.png><br>
            
            <span class="tt"><span class="t0">研究问题：预训练视觉转换器（VTs）时，输入数据可能被破坏和不可靠，这在现实世界的场景中是一个挑战。<br>
                    动机：现有的预训练方法往往忽视了输入数据可能的不可靠性，特别是在使用遮蔽自动编码进行预训练时，输入和遮蔽的“真实”目标都可能是不可靠的。<br>
                    方法：提出了一种插拔式组件——Token Boosting Module (TBM)，用于VTs，使其能够在遮蔽自动编码预训练过程中学习提取干净和健壮的特征。<br>
                    效果：通过理论分析和大量实验，证明了TBM能够提高模型预训练的稳健性和泛化性，从而有利于下游任务。在四个损坏的数据集上进行的实验表明，TBM能够持续提升下游任务的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning with large-scale unlabeled data has become a powerful tool for pre-training Visual Transformers (VTs). However, prior works tend to overlook that, in real-world scenarios, the input data may be corrupted and unreliable. Pre-training VTs on such corrupted data can be challenging, especially when we pre-train via the masked autoencoding approach, where both the inputs and masked "ground truth" targets can potentially be unreliable in this case. To address this limitation, we introduce the Token Boosting Module (TBM) as a plug-and-play component for VTs that effectively allows the VT to learn to extract clean and robust features during masked autoencoding pre-training. We provide theoretical analysis to show how TBM improves model pre-training with more robust and generalizable representations, thus benefiting downstream tasks. We conduct extensive experiments to analyze TBM's effectiveness, and results on four corrupted datasets demonstrate that TBM consistently improves performance on downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1413.Probabilistic Prompt Learning for Dense Prediction</span><br>
                <span class="as">Kwon, HyeongjunandSong, TaeyongandJeong, SomiandKim, JinandJang, JinhyunandSohn, Kwanghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Probabilistic_Prompt_Learning_for_Dense_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6768-6777.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的概率提示学习方法，以充分利用视觉语言知识进行密集预测任务。<br>
                    动机：目前的确定性提示学习方法在处理需要处理更复杂和多样化对象的密集预测任务时，由于单一且确定的描述无法充分表示整个图像，因此其性能有限。<br>
                    方法：我们引入了可学习的类别无关属性提示来描述跨对象类别的通用属性。这些属性与类别信息和视觉上下文知识结合，定义了类别特定的文本分布。然后采样文本表示并使用概率像素-文本匹配损失来指导密集预测任务，从而提高了所提出方法的稳定性和泛化能力。<br>
                    效果：我们在不同密集预测任务上进行了大量实验和消融研究，结果证明了我们提出的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent progress in deterministic prompt learning has become a promising alternative to various downstream vision tasks, enabling models to learn powerful visual representations with the help of pre-trained vision-language models. However, this approach results in limited performance for dense prediction tasks that require handling more complex and diverse objects, since a single and deterministic description cannot sufficiently represent the entire image. In this paper, we present a novel probabilistic prompt learning to fully exploit the vision-language knowledge in dense prediction tasks. First, we introduce learnable class-agnostic attribute prompts to describe universal attributes across the object class. The attributes are combined with class information and visual-context knowledge to define the class-specific textual distribution. Text representations are sampled and used to guide the dense prediction task using the probabilistic pixel-text matching loss, enhancing the stability and generalization capability of the proposed method. Extensive experiments on different dense prediction tasks and ablation studies demonstrate the effectiveness of our proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1414.WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</span><br>
                <span class="as">Jeong, JongheonandZou, YangandKim, TaewanandZhang, DongqingandRavichandran, AvinashandDabeer, Onkar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19606-19616.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现工业质量检查的自动视觉异常分类和分割。<br>
                    动机：目前的研究需要为每个质量检查任务训练定制模型，这需要特定于任务的图像和注释。<br>
                    方法：提出基于窗口的CLIP（WinCLIP）和其少数正常样本扩展WinCLIP+。WinCLIP采用状态词和提示模板的组合集成，并提取和聚合与文本对齐的窗口/补丁/图像级特征。WinCLIP+使用来自正常图像的互补信息。<br>
                    效果：在MVTec-AD和VisA上，未经进一步调整的WinCLIP在零样本异常分类和分割中分别达到91.8%/85.1%（78.1%/79.6%）的AUROC，而WinCLIP+在1个正常样本中达到93.1%/95.2%（83.8%/96.4%），大幅超过现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual anomaly classification and segmentation are vital for automating industrial quality inspection. The focus of prior research in the field has been on training custom models for each quality inspection task, which requires task-specific images and annotation. In this paper we move away from this regime, addressing zero-shot and few-normal-shot anomaly classification and segmentation. Recently CLIP, a vision-language model, has shown revolutionary generality with competitive zero/few-shot performance in comparison to full-supervision. But CLIP falls short on anomaly classification and segmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a compositional ensemble on state words and prompt templates and (2) efficient extraction and aggregation of window/patch/image-level features aligned with text. We also propose its few-normal-shot extension WinCLIP+, which uses complementary information from normal images. In MVTec-AD (and VisA), without further tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot anomaly classification and segmentation while WinCLIP+ does 93.1%/95.2% (83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1415.Learning Geometric-Aware Properties in 2D Representation Using Lightweight CAD Models, or Zero Real 3D Pairs</span><br>
                <span class="as">Arsomngern, PattaramaneeandNutanong, SaranaandSuwajanakorn, Supasorn</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Arsomngern_Learning_Geometric-Aware_Properties_in_2D_Representation_Using_Lightweight_CAD_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21371-21381.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用轻量级的3D数据（如CAD模型）提升2D场景理解。<br>
                    动机：大规模场景数据集的需求限制了可扩展性和进一步改进，因此需要寻找替代的学习方式。<br>
                    方法：构建一个具有几何感知对齐的3D空间，通过Chamfer距离反映CAD模型的几何相似性，并将获得的几何感知属性引入到2D特征中。<br>
                    效果：该方法在各种2D理解任务上的表现优于现有的RGB-CAD方法，即使只使用轻量级的CAD模型或伪数据，也能在NYUv2、SUNRGB-D、室内ADE20k和室内/室外COCO等四个任务上达到与最先进的场景扫描方法相当的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cross-modal training using 2D-3D paired datasets, such as those containing multi-view images and 3D scene scans, presents an effective way to enhance 2D scene understanding by introducing geometric and view-invariance priors into 2D features. However, the need for large-scale scene datasets can impede scalability and further improvements. This paper explores an alternative learning method by leveraging a lightweight and publicly available type of 3D data in the form of CAD models. We construct a 3D space with geometric-aware alignment where the similarity in this space reflects the geometric similarity of CAD models based on the Chamfer distance. The acquired geometric-aware properties are then induced into 2D features, which boost performance on downstream tasks more effectively than existing RGB-CAD approaches. Our technique is not limited to paired RGB-CAD datasets. By training exclusively on pseudo pairs generated from CAD-based reconstruction methods, we enhance the performance of SOTA 2D pre-trained models that use ResNet-50 or ViT-B backbones on various 2D understanding tasks. We also achieve comparable results to SOTA methods trained on scene scans on four tasks in NYUv2, SUNRGB-D, indoor ADE20k, and indoor/outdoor COCO, despite using lightweight CAD models or pseudo data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1416.Texts as Images in Prompt Tuning for Multi-Label Image Recognition</span><br>
                <span class="as">Guo, ZixianandDong, BowenandJi, ZhilongandBai, JinfengandGuo, YiwenandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2808-2817.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用文本描述进行提示调优，以适应数据有限或标签有限的下游任务。<br>
                    动机：现有的方法需要视觉数据（如图像）来学习提示，但文本描述易于收集且可直接获取类别标签。<br>
                    方法：提出TaI提示法，将文本描述视为图像进行提示调优，并进一步提出双重粒度提示调优（TaI-DPT）以提升多标签识别性能。<br>
                    效果：实验结果表明，TaI-DPT在多个基准测试中优于零射提示CLIP，并能与现有的图像提示方法结合以提高识别性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Prompt tuning has been employed as an efficient way to adapt large vision-language pre-trained models (e.g. CLIP) to various downstream tasks in data-limited or label-limited settings. Nonetheless, visual data (e.g., images) is by default prerequisite for learning prompts in existing methods. In this work, we advocate that the effectiveness of image-text contrastive learning in aligning the two modalities (for training CLIP) further makes it feasible to treat texts as images for prompt tuning and introduce TaI prompting. In contrast to the visual data, text descriptions are easy to collect, and their class labels can be directly derived. Particularly, we apply TaI prompting to multi-label image recognition, where sentences in the wild serve as alternatives to images for prompt tuning. Moreover, with TaI, double-grained prompt tuning (TaI-DPT) is further presented to extract both coarse-grained and fine-grained embeddings for enhancing the multi-label recognition performance. Experimental results show that our proposed TaI-DPT outperforms zero-shot CLIP by a large margin on multiple benchmarks, e.g., MS-COCO, VOC2007, and NUS-WIDE, while it can be combined with existing methods of prompting from images to improve recognition performance further. The code is released at https://github.com/guozix/TaI-DPT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1417.Finetune Like You Pretrain: Improved Finetuning of Zero-Shot Vision Models</span><br>
                <span class="as">Goyal, SachinandKumar, AnanyaandGarg, SankalpandKolter, ZicoandRaghunathan, Aditi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19338-19347.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像-文本模型（如CLIP）微调过程中的微妙差异可能导致最终性能出现大的差异的问题。<br>
                    动机：最近的研究表明，即使是微调过程的微小差异，也可能导致分布内（ID）和分布外（OOD）数据的最终性能出现大的差异。<br>
                    方法：本文提出了一种模仿对比预训练的自然而简单的方法，即通过将下游类别标签视为文本提示并继续优化图像嵌入和类描述提示嵌入之间的对比损失来进行对比微调。<br>
                    效果：在7个分布转移、6个迁移学习和3个少样本学习基准测试中，该方法始终优于基线。在WILDs-iWILDCam上，该方法FLYP比领导者榜顶的结果提高了2.3% ID和2.7% OOD，达到了最高的报告准确率。在7个OOD数据集（2个WILDs和5个与ImageNet相关的转移）上，FLYP比标准微调提高了4.2% OOD，比当前最先进的LP-FT高出1%以上。同样，在3个少样本学习基准测试中，FLYP比标准微调和最先进的方法分别提高了4.6%和4.4%。因此，本文提出的对比微调方法被确立为监督图像-文本模型（如CLIP）微调的一种简单直观的最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Finetuning image-text models such as CLIP achieves state-of-the-art accuracies on a variety of benchmarks. However, recent works (Kumar et al., 2022; Wortsman et al., 2021) have shown that even subtle differences in the finetuning process can lead to surprisingly large differences in the final performance, both for in-distribution (ID) and out-of-distribution (OOD) data. In this work, we show that a natural and simple approach of mimicking contrastive pretraining consistently outperforms alternative finetuning approaches. Specifically, we cast downstream class labels as text prompts and continue optimizing the contrastive loss between image embeddings and class-descriptive prompt embeddings (contrastive finetuning). Our method consistently outperforms baselines across 7 distribution shift, 6 transfer learning, and 3 few-shot learning benchmarks. On WILDS-iWILDCam, our proposed approach FLYP outperforms the top of the leaderboard by 2.3% ID and 2.7% OOD, giving the highest reported accuracy. Averaged across 7 OOD datasets (2 WILDS and 5 ImageNet associated shifts), FLYP gives gains of 4.2% OOD over standard finetuning and outperforms current state-ofthe-art (LP-FT) by more than 1% both ID and OOD. Similarly, on 3 few-shot learning benchmarks, FLYP gives gains up to 4.6% over standard finetuning and 4.4% over the state-of-the-art. Thus we establish our proposed method of contrastive finetuning as a simple and intuitive state-ofthe-art for supervised finetuning of image-text models like CLIP. Code is available at https://github.com/locuslab/FLYP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1418.Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning</span><br>
                <span class="as">Yu, ZhongzhiandWu, ShangandFu, YongganandZhang, ShunyaoandLin, Yingyan(Celine)</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11102-11112.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在数据有限的情况下，充分利用基础视觉变换器（FViTs）的潜力进行下游任务的微调。<br>
                    动机：由于FViTs对数据的高需求，以及在小样本调整中常见的特征有限的问题，使得其在数据有限的场景下充分发挥其潜力成为一项挑战。<br>
                    方法：提出了一种基于提示的数据增强（Hint-Aug）框架，通过使用预训练的FViTs学习到的特征来增强微调样本中的过拟合部分，以提高小样本FViT微调的效果。<br>
                    效果：在五个数据集和三种参数高效微调技术上的大量实验和消融研究均验证了Hint-Aug的有效性，其在各种低样本设置下比最先进的数据增强方法高出0.04%至32.91%的准确率。例如，在Pet数据集上，Hint-Aug在只有一半训练数据的情况下，比最先进的数据增强方法提高了2.22%的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleashing FViTs' potential under data-limited scenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first identify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representative features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tuning. We thus hypothesize that leveraging those learned features to augment the tuning data can boost the effectiveness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by augmenting the over-fitted parts of tuning samples with the learned features of pretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an Attentive Over-fitting Detector (AOD) to detect over-confident patches of foundation ViTs for potentially alleviating their over-fitting on the few-shot tuning data and (2) a Confusion-based Feature Infusion (CFI) module to infuse easy-to-confuse features from the pretrained FViTs with the over-confident patches detected by the above AOD in order to enhance the feature diversity during tuning. Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consistently validate Hint-Aug's effectiveness: 0.04% 32.91% higher accuracy over the state-of-the-art (SOTA) data augmentation method under various low-shot settings. For example, on the Pet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data over SOTA data augmentation methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1419.Explicit Visual Prompting for Low-Level Structure Segmentations</span><br>
                <span class="as">Liu, WeihuangandShen, XiandPun, Chi-ManandCun, Xiaodong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19434-19445.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像中低层次结构的通用检测问题，包括分割被操纵的部分、识别失焦像素、分离阴影区域和检测隐藏物体。<br>
                    动机：虽然每个这样的问题通常都有专门的解决方案，但作者认为统一的处理方法在所有这些问题上都能表现良好。<br>
                    方法：受到NLP中广泛使用的预训练和提示调优协议的启发，作者提出了一种新的视觉提示模型，名为显式视觉提示（EVP）。与以前的视觉提示（通常是数据集级别的隐式嵌入）不同，作者的关键洞察是强制可调参数关注每个单独图像的显式视觉内容，即来自冻结补丁嵌入的特征和输入的高频率组件。<br>
                    效果：所提出的EVP在相同数量的可调参数下显著优于其他参数高效的调优协议（每个任务额外5.7%的可训练参数）。与特定任务的解决方案相比，EVP在各种低层次结构分割任务上也实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1420.PiMAE: Point Cloud and Image Interactive Masked Autoencoders for 3D Object Detection</span><br>
                <span class="as">Chen, AnthonyandZhang, KevinandZhang, RenruiandWang, ZihanandLu, YuhengandGuo, YandongandZhang, Shanghang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5291-5301.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的遮蔽自动编码器在多模态设置中的能力问题，特别是在研究问题：本文旨在解决现有的遮蔽自动编码器在多模态设置中的能力问题，特别是在点云和RGB图像数据这两种经常在现实世界中一起出现的模态上。<br>
                    动机：尽管遮蔽自动编码器已在几种独立模态中学习到强大的视觉表示并取得了最先进的结果，但在多模态设置中的应用却鲜有研究。<br>
                    方法：本文提出了PiMAE，一个自我监督的预训练框架，通过三个方面促进3D和2D的交互。首先，我们注意到两种来源之间的遮蔽策略的重要性，并利用投影模块对两种模态的遮蔽和可见标记进行互补对齐。然后，我们利用精心设计的双分支MAE管道和一个新的共享解码器来促进遮蔽标记中的跨模态交互。最后，我们设计了一个独特的跨模态重建模块，以增强两种模态的表示学习。<br>
                    效果：通过对大规模的RGB-D场景理解基准（SUN RGB-D和ScannetV2）进行大量实验，我们发现交互式地学习点图像特征并非易事。我们的模型大大提高了多种3D检测器、2D检测器和少样本分类器的精度，分别提高了2.9%、6.7%和2.4%。代码可在https://github.com/BLVLab/PiMAE获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Autoencoders learn strong visual representations and achieve state-of-the-art results in several independent modalities, yet very few works have addressed their capabilities in multi-modality settings. In this work, we focus on point cloud and RGB image data, two modalities that are often presented together in the real world and explore their meaningful interactions. To improve upon the cross-modal synergy in existing works, we propose PiMAE, a self-supervised pre-training framework that promotes 3D and 2D interaction through three aspects. Specifically, we first notice the importance of masking strategies between the two sources and utilize a projection module to complementarily align the mask and visible tokens of the two modalities. Then, we utilize a well-crafted two-branch MAE pipeline with a novel shared decoder to promote cross-modality interaction in the mask tokens. Finally, we design a unique cross-modal reconstruction module to enhance representation learning for both modalities. Through extensive experiments performed on large-scale RGB-D scene understanding benchmarks (SUN RGB-D and ScannetV2), we discover it is nontrivial to interactively learn point-image features, where we greatly improve multiple 3D detectors, 2D detectors and few-shot classifiers by 2.9%, 6.7%, and 2.4%, respectively. Code is available at https://github.com/BLVLab/PiMAE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1421.Referring Image Matting</span><br>
                <span class="as">Li, JizhiziandZhang, JingandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Referring_Image_Matting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22448-22457.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的任务，名为参考图像蒙版（RIM），旨在提取与给定研究问题：本文提出了一种新的任务，名为参考图像蒙版（RIM），旨在提取与给定自然语言描述最匹配的特定对象的详细阿尔法蒙版，从而实现对图像蒙版的更自然、更简单的指令。<br>
                    动机：与传统的图像蒙版不同，它要么需要用户定义的涂鸦/ trimap来提取特定的前景对象，要么直接无序地提取图像中的所有前景对象，本文引入了一个新的任务，即参考图像蒙版（RIM）。<br>
                    方法：我们首先设计了一个全面的图像合成和表达生成引擎，以基于公共数据集自动产生高质量图像和多样化的文字属性，从而建立了一个大型的具有挑战性的数据集RefMatte。然后，我们构建了一个真实世界的测试集，包含100张高分辨率的自然图像，并手动注释复杂的短语，以评估RIM方法的领域外泛化能力。最后，我们提出了一种名为CLIPMat的新基线方法用于RIM，包括上下文嵌入提示、文本驱动的语义弹出窗口和多级细节提取器。<br>
                    效果：在RefMatte上的大量实验，无论是关键词设置还是表达设置，都验证了CLIPMat优于代表性的方法。我们希望这项工作能为图像蒙版提供新的见解，并鼓励更多的后续研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Different from conventional image matting, which either requires user-defined scribbles/trimap to extract a specific foreground object or directly extracts all the foreground objects in the image indiscriminately, we introduce a new task named Referring Image Matting (RIM) in this paper, which aims to extract the meticulous alpha matte of the specific object that best matches the given natural language description, thus enabling a more natural and simpler instruction for image matting. First, we establish a large-scale challenging dataset RefMatte by designing a comprehensive image composition and expression generation engine to automatically produce high-quality images along with diverse text attributes based on public datasets. RefMatte consists of 230 object categories, 47,500 images, 118,749 expression-region entities, and 474,996 expressions. Additionally, we construct a real-world test set with 100 high-resolution natural images and manually annotate complex phrases to evaluate the out-of-domain generalization abilities of RIM methods. Furthermore, we present a novel baseline method CLIPMat for RIM, including a context-embedded prompt, a text-driven semantic pop-up, and a multi-level details extractor. Extensive experiments on RefMatte in both keyword and expression settings validate the superiority of CLIPMat over representative methods. We hope this work could provide novel insights into image matting and encourage more follow-up studies. The dataset, code and models are available at https://github.com/JizhiziLi/RIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1422.ConStruct-VL: Data-Free Continual Structured VL Concepts Learning</span><br>
                <span class="as">Smith, JamesSealeandCascante-Bonilla, PaolaandArbelle, AssafandKim, DonghyunandPanda, RameswarandCox, DavidandYang, DiyiandKira, ZsoltandFeris, RogerioandKarlinsky, Leonid</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_ConStruct-VL_Data-Free_Continual_Structured_VL_Concepts_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14994-15004.png><br>
            
            <span class="tt"><span class="t0">研究问题：大型预训练视觉-语言（VL）基础模型在零样本下游任务中表现出色，但在结构化视觉-语言概念（SVLC）推理方面仍存在脆弱性。<br>
                    动机：解决VL模型在识别对象属性、状态和对象间关系等SVLC推理能力上的缺陷，避免因错误推理而需要使用私有数据进行修正的问题。<br>
                    方法：提出首个持续无任务标记的结构化VL概念学习（ConStruct-VL）基准，并设计了一种基于对抗性伪重播（APR）的数据自由方法，通过生成过去任务模型的对抗性提醒来提高模型性能。同时，提出一种持续参数高效的分层LoRA（LaLo）神经网络架构，允许在训练时无需记忆成本地访问所有过去的模型。<br>
                    效果：该方法在所有数据自由方法中表现最好，提高了7%的性能，甚至在某些经验回放水平上取得了匹配，这对于必须保护数据隐私的应用来说是不可能的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as   7% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved). Our code is publicly available at https://github.com/jamessealesmith/ConStruct-VL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1423.Delivering Arbitrary-Modal Semantic Segmentation</span><br>
                <span class="as">Zhang, JiamingandLiu, RuipingandShi, HaoandYang, KailunandRei{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1136-1147.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何融合任意数量的模态以提高语义分割的鲁棒性？<br>
                    动机：目前的多模态融合方法在处理任意数量的模态上仍有待探索。<br>
                    方法：创建了DeLiVER任意模态分割基准，包括深度、激光雷达、多视图、事件和RGB等五种模态。同时，提供了四种恶劣天气条件和五种传感器故障案例的数据，以利用模态互补性和解决部分停机问题。提出了一种名为CMNeXt的任意跨模态分割模型，该模型包含一个自查询中心(SQ-Hub)，用于从任何模态中提取有效信息并与RGB表示进行后续融合，每增加一个附加模态仅增加微量的参数（0.01M）。此外，为了高效灵活地从辅助模态中获取判别性线索，引入了简单的并行池化混合器(PPX)。<br>
                    效果：在六个基准测试集上进行了大量实验，CMNeXt实现了最先进的性能，可以在DeLiVER、KITTI-360、MFNet、NYU Depth V2、UrbanLF和MCubeS数据集上从1个模态扩展到81个模态。在最新收集的DeLiVER数据集中，四模态CMNeXt的mIoU达到了66.30%，比单模态基线提高了9.10%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multimodal fusion can make semantic segmentation more robust. However, fusing an arbitrary number of modalities remains underexplored. To delve into this problem, we create the DeLiVER arbitrary-modal segmentation benchmark, covering Depth, LiDAR, multiple Views, Events, and RGB. Aside from this, we provide this dataset in four severe weather conditions as well as five sensor failure cases to exploit modal complementarity and resolve partial outages. To facilitate this data, we present the arbitrary cross-modal segmentation model CMNeXt. It encompasses a Self-Query Hub (SQ-Hub) designed to extract effective information from any modality for subsequent fusion with the RGB representation and adds only negligible amounts of parameters ( 0.01M) per additional modality. On top, to efficiently and flexibly harvest discriminative cues from the auxiliary modalities, we introduce the simple Parallel Pooling Mixer (PPX). With extensive experiments on a total of six benchmarks, our CMNeXt achieves state-of-the-art performance, allowing to scale from 1 to 81 modalities on the DeLiVER, KITTI-360, MFNet, NYU Depth V2, UrbanLF, and MCubeS datasets. On the freshly collected DeLiVER, the quad-modal CMNeXt reaches up to 66.30% in mIoU with a +9.10% gain as compared to the mono-modal baseline.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1424.Hyperbolic Contrastive Learning for Visual Representations Beyond Objects</span><br>
                <span class="as">Ge, SongweiandMishra, ShlokandKornblith, SimonandLi, Chun-LiangandJacobs, David</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6840-6849.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自监督/无监督方法在视觉表示学习中取得了快速进展，但这些方法通常使用相同的视角来处理对象和场景。<br>
                    动机：观察到视觉上相似的对象在表示空间中接近，我们主张场景和对象应该根据它们的组成性遵循一种分层结构。<br>
                    方法：提出一个对比学习框架，其中欧几里得损失用于学习对象表示，双曲损失用于鼓励场景的表示接近其构成对象的表示。这种新的双曲目标通过优化其范数的大小来鼓励表示之间的场景-对象超价关系。<br>
                    效果：在COCO和OpenImages数据集上预训练时，双曲损失提高了多个基线在多个数据集和任务（包括图像分类、对象检测和语义分割）的下游性能。我们还发现，学到的表示的性质使我们能够以零样本的方式解决涉及场景和对象之间交互的各种视觉任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although self-/un-supervised methods have led to rapid progress in visual representation learning, these methods generally treat objects and scenes using the same lens. In this paper, we focus on learning representations of objects and scenes that preserve the structure among them. Motivated by the observation that visually similar objects are close in the representation space, we argue that the scenes and objects should instead follow a hierarchical structure based on their compositionality. To exploit such a structure, we propose a contrastive learning framework where a Euclidean loss is used to learn object representations and a hyperbolic loss is used to encourage representations of scenes to lie close to representations of their constituent objects in hyperbolic space. This novel hyperbolic objective encourages the scene-object hypernymy among the representations by optimizing the magnitude of their norms. We show that when pretraining on the COCO and OpenImages datasets, the hyperbolic loss improves the downstream performance of several baselines across multiple datasets and tasks, including image classification, object detection, and semantic segmentation. We also show that the properties of the learned representations allow us to solve various vision tasks that involve the interaction between scenes and objects in a zero-shot fashion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1425.Non-Contrastive Learning Meets Language-Image Pre-Training</span><br>
                <span class="as">Zhou, JinghaoandDong, LiandGan, ZheandWang, LijuanandWei, Furu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Non-Contrastive_Learning_Meets_Language-Image_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11028-11038.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索非对比语言-图像预训练（nCLIP）的有效性，并研究视觉自监督模型中是否会出现良好的特性。<br>
                    动机：虽然对比语言-图像预训练（CLIP）已成为将图像和文本对齐的事实标准，但网络爬取的数据中图像和文本之间的松散关联使得对比目标数据效率低下，且需要大的批量训练。<br>
                    方法：通过实验观察非对比目标在表征学习中的作用，同时研究其在零样本识别下的不足。基于上述研究，进一步引入了xCLIP，这是一个结合CLIP和nCLIP的多任务框架，并展示了nCLIP如何帮助CLIP增强特征语义。两种目标之间的协同作用使xCLIP在零样本转移和表征学习方面都表现出色。<br>
                    效果：通过一系列广泛的下游任务进行系统评估，包括零样本分类、跨领域分类、检索、视觉表示学习和文本表示学习，展示了一致的性能提升，验证了xCLIP的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive language-image pre-training (CLIP) serves as a de-facto standard to align images and texts. Nonetheless, the loose correlation between images and texts of web-crawled data renders the contrastive objective data inefficient and craving for a large training batch size. In this work, we explore the validity of non-contrastive language-image pre-training (nCLIP) and study whether nice properties exhibited in visual self-supervised models can emerge. We empirically observe that the non-contrastive objective nourishes representation learning while sufficiently underperforming under zero-shot recognition. Based on the above study, we further introduce xCLIP, a multi-tasking framework combining CLIP and nCLIP, and show that nCLIP aids CLIP in enhancing feature semantics. The synergy between two objectives lets xCLIP enjoy the best of both worlds: superior performance in both zero-shot transfer and representation learning. Systematic evaluation is conducted spanning a wide variety of downstream tasks including zero-shot classification, out-of-domain classification, retrieval, visual representation learning, and textual representation learning, showcasing a consistent performance gain and validating the effectiveness of xCLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1426.Teaching Structured Vision \&amp; Language Concepts to Vision \&amp; Language Models</span><br>
                <span class="as">Doveh, SivanandArbelle, AssafandHarary, SivanandSchwartz, EliandHerzig, RoeiandGiryes, RajaandFeris, RogerioandPanda, RameswarandUllman, ShimonandKarlinsky, Leonid</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2657-2668.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉语言模型在理解复杂语言结构（如对象属性、关系和状态）方面的挑战。<br>
                    动机：尽管视觉语言模型在各种任务上表现出色，但在理解复杂的语言结构方面仍有困难。现有的方法需要收集专门的数据集来教授每种语言结构，这既昂贵又耗时。<br>
                    方法：本文提出了一种基于语言结构理解的数据驱动方法，利用现有的视觉语言预训练数据集，无需额外的数据。通过操纵现有配对的视觉语言数据集的文本部分，训练出的视觉语言模型在理解复杂语言结构方面有显著提高。<br>
                    效果：实验结果表明，使用更新的数据训练的视觉语言模型在理解复杂语言结构方面提高了15%，同时在零样本能力方面只有轻微的下降。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision and Language (VL) models have demonstrated remarkable zero-shot performance in a variety of tasks. However, some aspects of complex language understanding still remain a challenge. We introduce the collective notion of Structured Vision & Language Concepts (SVLC) which includes object attributes, relations, and states which are present in the text and visible in the image. Recent studies have shown that even the best VL models struggle with SVLC. A possible way of fixing this issue is by collecting dedicated datasets for teaching each SVLC type, yet this might be expensive and time-consuming. Instead, we propose a more elegant data-driven approach for enhancing VL models' understanding of SVLCs that makes more effective use of existing VL pre-training datasets and does not require any additional data. While automatic understanding of image structure still remains largely unsolved, language structure is much better modeled and understood, allowing for its effective utilization in teaching VL models. In this paper, we propose various techniques based on language structure understanding that can be used to manipulate the textual part of off-the-shelf paired VL datasets. VL models trained with the updated data exhibit a significant improvement of up to 15% in their SVLC understanding with only a mild degradation in their zero-shot capabilities both when training from scratch or fine-tuning a pre-trained model. Our code and pretrained models are available at: https://github.com/SivanDoveh/TSVLC</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1427.Open-Set Representation Learning Through Combinatorial Embedding</span><br>
                <span class="as">Kim, GeehoandKang, JunohandHan, Bohyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Open-Set_Representation_Learning_Through_Combinatorial_Embedding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19744-19753.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉识别任务通常只能处理一小部分类别，因为剩余类别的标签不可用。我们希望通过基于有标签和无标签示例的表示学习来识别数据集中的新概念，并将识别范围扩展到已知和新的类别。<br>
                    动机：目前的视觉识别任务由于标签的限制，往往只能处理部分类别，对于剩余的类别无法进行有效的识别。因此，我们希望通过结合有标签和无标签的示例，通过表示学习来发现新的类别。<br>
                    方法：我们提出了一种组合学习方法，该方法利用多个监督元分类器在异构标签空间上给出的组合知识，自然地将未见过类别的示例进行聚类。通过无监督的成对关系学习，使组合嵌入给出的表示更加鲁棒。<br>
                    效果：我们在公开数据集上进行了广泛的实验，结果表明，我们的方法在图像检索和图像分类任务中取得了显著的性能提升，能够有效地发现新的类别。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual recognition tasks are often limited to dealing with a small subset of classes simply because the labels for the remaining classes are unavailable. We are interested in identifying novel concepts in a dataset through representation learning based on both labeled and unlabeled examples, and extending the horizon of recognition to both known and novel classes. To address this challenging task, we propose a combinatorial learning approach, which naturally clusters the examples in unseen classes using the compositional knowledge given by multiple supervised meta-classifiers on heterogeneous label spaces. The representations given by the combinatorial embedding are made more robust by unsupervised pairwise relation learning. The proposed algorithm discovers novel concepts via a joint optimization for enhancing the discrimitiveness of unseen classes as well as learning the representations of known classes generalizable to novel ones. Our extensive experiments demonstrate remarkable performance gains by the proposed approach on public datasets for image retrieval and image categorization with novel class discovery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1428.Top-Down Visual Attention From Analysis by Synthesis</span><br>
                <span class="as">Shi, BaifengandDarrell, TrevorandWang, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Top-Down_Visual_Attention_From_Analysis_by_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2102-2112.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的视觉注意力算法主要刺激驱动，而人类智能体往往能根据高级任务来引导注意力。<br>
                    动机：探索一种基于分析-合成（AbS）理论的自上而下的注意力机制，使模型能够更好地模拟人类的视觉注意力。<br>
                    方法：提出一种分析-合成视觉变换器（AbSViT），通过优化稀疏重构目标并加入自上而下的信号，实现可控的自上而下注意力。<br>
                    效果：实验表明，AbSViT在视觉语言任务上优于基线模型，并在分类、语义分割和模型鲁棒性等方面表现良好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current attention algorithms (e.g., self-attention) are stimulus-driven and highlight all the salient objects in an image. However, intelligent agents like humans often guide their attention based on the high-level task at hand, focusing only on task-related objects. This ability of task-guided top-down attention provides task-adaptive representation and helps the model generalize to various tasks. In this paper, we consider top-down attention from a classic Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a functional equivalence between visual attention and sparse reconstruction; we show that an AbS visual system that optimizes a similar sparse reconstruction objective modulated by a goal-directed top-down signal naturally simulates top-down attention. We further propose Analysis-by-Synthesis Vision Transformer (AbSViT), which is a top-down modulated ViT model that variationally approximates AbS, and achieves controllable top-down attention. For real-world applications, AbSViT consistently improves over baselines on Vision-Language tasks such as VQA and zero-shot retrieval where language guides the top-down attention. AbSViT can also serve as a general backbone, improving performance on classification, semantic segmentation, and model robustness. Project page: https://sites.google.com/view/absvit.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>