<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
  </div>
  </div>

  <div id="sortoptions">
    To search the paper by title:
    <div class="pagination_p"> 
      <a href="search.html" >search</a> 
    </div>
    </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic7</h2>
      <b>Topic words : &ensp;</b>performance, &ensp;time, &ensp;training, &ensp;neural, &ensp;efficient, &ensp;network, &ensp;memory, &ensp;parameters</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1565.Frame Flexible Network</span><br>
                <span class="as">Zhang, YitianandBai, YueandLiu, ChangandWang, HuanandLi, ShengandFu, Yun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Frame_Flexible_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10504-10513.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视频识别算法对不同帧数的输入进行不同的训练流程，需要重复的训练操作和大量的存储成本。<br>
                    动机：如果使用未在训练中使用的其他帧来评估模型，性能会显著下降（如图1所示，总结为“时间频率偏差”现象）。为了解决这个问题，我们提出了一个通用框架，名为“灵活帧网络”（FFN），它不仅允许模型在不同的帧上进行评估以调整其计算，而且显著降低了存储多个模型的内存成本。<br>
                    方法：具体来说，FFN整合了多组训练序列，通过多频对齐（MFAL）学习时间频率不变的表示，并利用多频适应（MFAD）进一步增强表示能力。<br>
                    效果：通过各种架构和流行基准的综合实证验证，坚实地证明了FFN的有效性和泛化性（例如，在Something-Something V1数据集的帧4/8/16上比Uniformer有7.08/5.15/2.17%的性能提升）。代码可在https://github.com/BeSpontaneous/FFN获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing video recognition algorithms always conduct different training pipelines for inputs with different frame numbers, which requires repetitive training operations and multiplying storage costs. If we evaluate the model using other frames which are not used in training, we observe the performance will drop significantly (see Fig.1, which is summarized as Temporal Frequency Deviation phenomenon. To fix this issue, we propose a general framework, named Frame Flexible Network (FFN), which not only enables the model to be evaluated at different frames to adjust its computation, but also reduces the memory costs of storing multiple models significantly. Concretely, FFN integrates several sets of training sequences, involves Multi-Frequency Alignment (MFAL) to learn temporal frequency invariant representations, and leverages Multi-Frequency Adaptation (MFAD) to further strengthen the representation abilities. Comprehensive empirical validations using various architectures and popular benchmarks solidly demonstrate the effectiveness and generalization of FFN (e.g., 7.08/5.15/2.17% performance gain at Frame 4/8/16 on Something-Something V1 dataset over Uniformer). Code is available at https://github.com/BeSpontaneous/FFN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1566.Minimizing the Accumulated Trajectory Error To Improve Dataset Distillation</span><br>
                <span class="as">Du, JiaweiandJiang, YidiandTan, VincentY.F.andZhou, JoeyTianyiandLi, Haizhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3749-3758.png><br>
            
            <span class="tt"><span class="t0">研究问题：大规模真实世界数据的处理需要大量的计算、存储和训练，以及寻找优秀的神经网络架构。<br>
                    动机：为了解决这个问题，提出了数据集蒸馏的方法，将大量真实世界数据的信息提炼成小而紧凑的合成数据集，以减少数据处理的负担。<br>
                    方法：现有的方法主要依赖于通过匹配在真实数据和合成数据训练过程中获得的梯度来学习合成数据集。然而，这些梯度匹配方法受到由于蒸馏和后续评估之间的差异导致的累积轨迹误差的影响。为此，我们提出了一种新的方法，鼓励优化算法寻求平坦的轨迹。<br>
                    效果：我们的新方法被称为平坦轨迹蒸馏（FTD），它通过向平坦轨迹进行正则化，使得在合成数据上训练的权重能够抵抗累积误差的干扰。实验结果表明，FTD方法可以显著提高梯度匹配方法的性能，在ImageNet数据集的高分辨率图像子集上提高了4.7%。此外，我们还验证了该方法在不同分辨率数据集上的有效性和通用性，并展示了其在神经架构搜索中的应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model-based deep learning has achieved astounding successes due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, storage, training and the search for good neural architectures. Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients obtained during training between the real and synthetic data. However, these gradient-matching methods suffer from the accumulated trajectory error caused by the discrepancy between the distillation and subsequent evaluation. To alleviate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages the optimization algorithm to seek a flat trajectory. We show that the weights trained on synthetic data are robust against the accumulated errors perturbations with the regularization towards the flat trajectory. Our method, called Flat Trajectory Distillation (FTD), is shown to boost the performance of gradient-matching methods by up to 4.7% on a subset of images of the ImageNet dataset with higher resolution images. We also validate the effectiveness and generalizability of our method with datasets of different resolutions and demonstrate its applicability to neural architecture search.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1567.Fast Point Cloud Generation With Straight Flows</span><br>
                <span class="as">Wu, LemengandWang, DilinandGong, ChengyueandLiu, XingchaoandXiong, YunyangandRanjan, RakeshandKrishnamoorthi, RaghuramanandChandra, VikasandLiu, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Fast_Point_Cloud_Generation_With_Straight_Flows_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9445-9454.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的扩散模型在生成高质量点云样本时需要数千步的迭代去噪，这限制了其在许多3D实时应用中的使用。<br>
                    动机：为了解决这个问题，我们提出了一种新的模型——Point Straight Flow（PSF），它通过将复杂的学习轨迹优化为一条直线，只需要一步就可以实现优秀的性能。<br>
                    方法：我们对标准的扩散模型进行了重新设计，将其优化的学习路径从曲线变为直线。此外，我们还开发了一种蒸馏策略，可以在不损失性能的情况下将直线路径缩短为一步，使其能够应用于具有延迟约束的3D实时应用。<br>
                    效果：我们在多个3D任务上进行评估，发现PSF的性能与标准的扩散模型相当，甚至超过了其他高效的3D点云生成方法。在实际的3D应用中，如点云补全和低延迟设置下的无训练文本引导生成，PSF表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have emerged as a powerful tool for point cloud generation. A key component that drives the impressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of learning steps has limited its applications to many 3D real-world. To address this limitation, we propose Point Straight Flow (PSF), a model that exhibits impressive performance using one step. Our idea is based on the reformulation of the standard diffusion model, which optimizes the curvy learning trajectory into a straight path. Further, we develop a distillation strategy to shorten the straight path into one step without a performance loss, enabling applications to 3D real-world with latency constraints. We perform evaluations on multiple 3D tasks and find that our PSF performs comparably to the standard diffusion model, outperforming other efficient 3D point cloud generation methods. On real-world applications such as point cloud completion and training-free text-guided generation in a low-latency setup, PSF performs favorably.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1568.Achieving a Better Stability-Plasticity Trade-Off via Auxiliary Networks in Continual Learning</span><br>
                <span class="as">Kim, SanghwanandNoci, LorenzoandOrvieto, AntonioandHofmann, Thomas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Achieving_a_Better_Stability-Plasticity_Trade-Off_via_Auxiliary_Networks_in_Continual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11930-11939.png><br>
            
            <span class="tt"><span class="t0">研究问题：与人类能够以连续的方式学习新任务的自然能力相比，神经网络存在灾难性遗忘的问题，即在优化新任务后，模型在旧任务上的表现会大幅下降。<br>
                    动机：为了解决神经网络的灾难性遗忘问题，持续学习社区提出了几种解决方案，旨在让神经网络具备学习当前任务的能力（可塑性）的同时，仍能保持对以前任务的高准确性（稳定性）。然而，可塑性-稳定性权衡问题仍然远未解决，其背后的机制也尚未得到充分理解。<br>
                    方法：本文提出了一种名为辅助网络持续学习的新颖方法，该方法为主要关注稳定性的持续学习模型添加了一个促进可塑性的额外辅助网络。具体来说，所提出的框架通过一个在可塑性和稳定性之间自然插值的正则化器来实现。<br>
                    效果：实验结果表明，ANCL在任务增量和类别增量场景中超越了强大的基线。通过对ANCL解决方案的广泛分析，我们发现了稳定性-可塑性权衡问题背后的一些基本原则。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved, and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and stability, surpassing strong baselines on task incremental and class incremental scenarios. Through extensive analyses on ANCL solutions, we identify some essential principles beneath the stability-plasticity trade-off.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1569.Power Bundle Adjustment for Large-Scale 3D Reconstruction</span><br>
                <span class="as">Weber, SimonandDemmel, NikolausandChan, TinChonandCremers, Daniel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weber_Power_Bundle_Adjustment_for_Large-Scale_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/281-289.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种解决大规模束调整问题的扩展算法——电力束调整。<br>
                    动机：现有的迭代方法在解决大规模的束调整问题上存在速度慢和精度低的问题。<br>
                    方法：通过逆舒尔补的幂级数展开，提出了一种新的求解器——逆展开方法，并证明其收敛性。<br>
                    效果：实验结果表明，该方法在解决正规方程的速度和精度上优于现有的迭代方法，并且可以作为分布式束调整框架的子问题求解器，显著提高分布式优化的速度和精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Power Bundle Adjustment as an expansion type algorithm for solving large-scale bundle adjustment problems. It is based on the power series expansion of the inverse Schur complement and constitutes a new family of solvers that we call inverse expansion methods. We theoretically justify the use of power series and we prove the convergence of our approach. Using the real-world BAL dataset we show that the proposed solver challenges the state-of-the-art iterative methods and significantly accelerates the solution of the normal equation, even for reaching a very high accuracy. This easy-to-implement solver can also complement a recently presented distributed bundle adjustment framework. We demonstrate that employing the proposed Power Bundle Adjustment as a sub-problem solver significantly improves speed and accuracy of the distributed optimization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1570.Boosting Verified Training for Robust Image Classifications via Abstraction</span><br>
                <span class="as">Zhang, ZhaodiandXue, ZhiyiandChen, YangandLiu, SiandZhang, YuelingandLiu, JingandZhang, Min</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Boosting_Verified_Training_for_Robust_Image_Classifications_via_Abstraction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16251-16260.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高图像分类器的鲁棒性？<br>
                    动机：现有的图像分类器对输入的微小变化敏感，导致分类结果的不稳定性。<br>
                    方法：提出一种基于抽象的新的训练方法，通过将扰动图像映射到区间进行训练，减小训练集的方差和模型的损失函数的复杂度，从而提高模型的鲁棒性。同时，该方法还提供了一种可验证的、与神经网络类型和规模无关的黑盒验证方法。<br>
                    效果：在多个基准测试中，该方法比现有技术表现更好，能减少95.64%的已训练模型的错误，实现高达602.50倍的速度提升，并能处理高达1.38亿个可训练参数的更大模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a novel, abstraction-based, certified training method for robust image classifiers. Via abstraction, all perturbed images are mapped into intervals before feeding into neural networks for training. By training on intervals, all the perturbed images that are mapped to the same interval are classified as the same label, rendering the variance of training sets to be small and the loss landscape of the models to be smooth. Consequently, our approach significantly improves the robustness of trained models. For the abstraction, our training method also enables a sound and complete black-box verification approach, which is orthogonal and scalable to arbitrary types of neural networks regardless of their sizes and architectures. We evaluate our method on a wide range of benchmarks in different scales. The experimental results show that our method outperforms state of the art by (i) reducing the verified errors of trained models up to 95.64%; (ii) totally achieving up to 602.50x speedup; and (iii) scaling up to larger models with up to 138 million trainable parameters. The demo is available at https://github.com/zhangzhaodi233/ABSCERT.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1571.Post-Training Quantization on Diffusion Models</span><br>
                <span class="as">Shang, YuzhangandYuan, ZhihangandXie, BinandWu, BingzheandYan, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1972-1981.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的去噪扩散模型生成过程缓慢，因为迭代噪声估计需要依赖复杂的神经网络。<br>
                    动机：为了解决这一问题，本文提出了一种从压缩噪声估计网络的角度加速生成的方法。<br>
                    方法：通过引入后训练量化（PTQ）来加速去噪扩散模型的生成过程。针对去噪扩散模型多时间步结构的特点，对量化操作、校准数据集和校准指标进行了探索和改进。<br>
                    效果：实验表明，该方法可以直接将全精度去噪扩散模型量化为8位模型，同时在无需训练的情况下保持甚至提高其性能。此外，该方法还可以作为其他快速采样方法（如DDIM）的即插即用模块。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Denoising diffusion (score-based) generative models have recently achieved significant accomplishments in generating realistic and diverse data. These approaches define a forward diffusion process for transforming data into noise and a backward denoising process for sampling data from noise. Unfortunately, the generation process of current denoising diffusion models is notoriously slow due to the lengthy iterative noise estimations, which rely on cumbersome neural networks. It prevents the diffusion models from being widely deployed, especially on edge devices. Previous works accelerate the generation process of diffusion model (DM) via finding shorter yet effective sampling trajectories. However, they overlook the cost of noise estimation with a heavy network in every iteration. In this work, we accelerate generation from the perspective of compressing the noise estimation network. Due to the difficulty of retraining DMs, we exclude mainstream training-aware compression paradigms and introduce post-training quantization (PTQ) into DM acceleration. However, the output distributions of noise estimation networks change with time-step, making previous PTQ methods fail in DMs since they are designed for single-time step scenarios. To devise a DM-specific PTQ method, we explore PTQ on DM in three aspects: quantized operations, calibration dataset, and calibration metric. We summarize and use several observations derived from all-inclusive investigations to formulate our method, which especially targets the unique multi-time-step structure of DMs. Experimentally, our method can directly quantize full-precision DMs into 8-bit models while maintaining or even improving their performance in a training-free manner. Importantly, our method can serve as a plug-and-play module on other fast-sampling methods, such as DDIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1572.X-Pruner: eXplainable Pruning for Vision Transformers</span><br>
                <span class="as">Yu, LuandXiang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24355-24363.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器模型在各种任务中占据主导地位，但计算成本高、内存需求大，不适合在边缘平台上部署。<br>
                    动机：现有的模型剪枝方法无法解释模型内部单元与目标类别之间的关系，导致性能下降。<br>
                    方法：提出一种新的可解释性剪枝框架X-Pruner，通过设计一种可解释的剪枝标准来测量每个可剪单元对预测每个目标类别的贡献。<br>
                    效果：实验结果表明，X-Pruner在减少计算成本的同时，性能只有轻微下降，优于现有的黑盒方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently vision transformer models have become prominent models for a range of tasks. These models, however, usually suffer from intensive computational costs and heavy memory requirements, making them impractical for deployment on edge platforms. Recent studies have proposed to prune transformers in an unexplainable manner, which overlook the relationship between internal units of the model and the target class, thereby leading to inferior performance. To alleviate this problem, we propose a novel explainable pruning framework dubbed X-Pruner, which is designed by considering the explainability of the pruning criterion. Specifically, to measure each prunable unit's contribution to predicting each target class, a novel explainability-aware mask is proposed and learned in an end-to-end manner. Then, to preserve the most informative units and learn the layer-wise pruning rate, we adaptively search the layer-wise threshold that differentiates between unpruned and pruned units based on their explainability-aware mask values. To verify and evaluate our method, we apply the X-Pruner on representative transformer models including the DeiT and Swin Transformer. Comprehensive simulation results demonstrate that the proposed X-Pruner outperforms the state-of-the-art black-box methods with significantly reduced computational costs and slight performance degradation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1573.Hard Sample Matters a Lot in Zero-Shot Quantization</span><br>
                <span class="as">Li, HuantongandWu, XiangmiaoandLv, FanbingandLiao, DaihaiandLi, ThomasH.andZhang, YonggangandHan, BoandTan, Mingkui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Hard_Sample_Matters_a_Lot_in_Zero-Shot_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24417-24426.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用零散量化（ZSQ）压缩和加速深度神经网络，特别是在无法获取全精度模型训练数据的情况下。<br>
                    动机：现有的ZSQ方法中，由于网络量化使用的是合成样本，因此量化模型的性能严重依赖于合成样本的质量。然而，我们发现这些方法中构造的合成样本很容易被模型拟合，导致在这些样本上性能显著下降。<br>
                    方法：我们提出了HArd sample Synthesizing and Training (HAST)方法。具体来说，HAST在合成样本时更关注困难样本，并在训练量化模型时使合成样本难以拟合。同时，HAST对齐了全精度和量化模型提取的特征，以确保这两种模型提取的特征的相似性。<br>
                    效果：大量实验表明，HAST显著优于现有的ZSQ方法，其性能与使用真实数据进行量化的模型相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Zero-shot quantization (ZSQ) is promising for compressing and accelerating deep neural networks when the data for training full-precision models are inaccessible. In ZSQ, network quantization is performed using synthetic samples, thus, the performance of quantized models depends heavily on the quality of synthetic samples. Nonetheless, we find that the synthetic samples constructed in existing ZSQ methods can be easily fitted by models. Accordingly, quantized models obtained by these methods suffer from significant performance degradation on hard samples. To address this issue, we propose HArd sample Synthesizing and Training (HAST). Specifically, HAST pays more attention to hard samples when synthesizing samples and makes synthetic samples hard to fit when training quantized models. HAST aligns features extracted by full-precision and quantized models to ensure the similarity between features extracted by these two models. Extensive experiments show that HAST significantly outperforms existing ZSQ methods, achieving performance comparable to models that are quantized with real data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1574.Neural Rate Estimator and Unsupervised Learning for Efficient Distributed Image Analytics in Split-DNN Models</span><br>
                <span class="as">Ahuja, NileshandDatta, ParualandKanzariya, BhavyaandSomayazulu, V.SrinivasaandTickoo, Omesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ahuja_Neural_Rate_Estimator_and_Unsupervised_Learning_for_Efficient_Distributed_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2022-2030.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化图像分析的压缩和任务性能。<br>
                    动机：传统的压缩方法在压缩图像数据时会引入影响下游分析任务性能的伪影，而分块深度神经网络（Split-DNN）可以解决这个问题。<br>
                    方法：提出一种高质量的“神经速率估计器”，将低维瓶颈输出解释为中间特征的潜在表示，并将率失真优化问题视为训练一个等效的变分自动编码器的问题。<br>
                    效果：通过使用蒸馏基于的损失代替监督损失项（如交叉熵损失），可以在无需显式训练标签的情况下进行无监督训练瓶颈单元，从而在图像分类和语义分割任务上以更低的比特率获得更好的任务准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Thanks to advances in computer vision and AI, there has been a large growth in the demand for cloud-based visual analytics in which images captured by a low-powered edge device are transmitted to the cloud for analytics. Use of conventional codecs (JPEG, MPEG, HEVC, etc.) for compressing such data introduces artifacts that can seriously degrade the performance of the downstream analytic tasks. Split-DNN computing has emerged as a paradigm to address such usages, in which a DNN is partitioned into a client-side portion and a server side portion. Low-complexity neural networks called 'bottleneck units' are introduced at the split point to transform the intermediate layer features into a lower-dimensional representation better suited for compression and transmission. Optimizing the pipeline for both compression and task-performance requires high-quality estimates of the information-theoretic rate of the intermediate features. Most works on compression for image analytics use heuristic approaches to estimate the rate, leading to suboptimal performance. We propose a high-quality 'neural rate-estimator' to address this gap. We interpret the lower-dimensional bottleneck output as a latent representation of the intermediate feature and cast the rate-distortion optimization problem as one of training an equivalent variational auto-encoder with an appropriate loss function. We show that this leads to improved rate-distortion outcomes. We further show that replacing supervised loss terms (such as cross-entropy loss) by distillation-based losses in a teacher-student framework allows for unsupervised training of bottleneck units without the need for explicit training labels. This makes our method very attractive for real world deployments where access to labeled training data is difficult or expensive. We demonstrate that our method outperforms several state-of-the-art methods by obtaining improved task accuracy at lower bitrates on image classification and semantic segmentation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1575.1\% VS 100\%: Parameter-Efficient Low Rank Adapter for Dense Predictions</span><br>
                <span class="as">Yin, DongshuoandYang, YiranandWang, ZhechaoandYu, HongfengandWei, KaiwenandSun, Xian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_1_VS_100_Parameter-Efficient_Low_Rank_Adapter_for_Dense_Predictions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20116-20126.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地微调大规模预训练的视觉模型以适应下游任务，同时减少可训练参数的数量。<br>
                    动机：虽然微调整个大模型可以获得最先进的性能，但效率低下且需要为每个任务存储一个相同大小的新模型副本。<br>
                    方法：提出LoRand方法，通过生成低秩合成的小适配器结构，保持原始主干参数固定，实现高参数共享，仅对预训练主干参数的1%到3%进行训练。<br>
                    效果：在物体检测、语义分割和实例分割任务上进行大量实验，结果显示LoRand在COCO和ADE20K数据集上的性能与标准微调相当，并在资源较少的PASCAL VOC数据集上优于微调。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Fine-tuning large-scale pre-trained vision models to downstream tasks is a standard technique for achieving state-of-the-art performance on computer vision benchmarks. However, fine-tuning the whole model with millions of parameters is inefficient as it requires storing a same-sized new model copy for each task. In this work, we propose LoRand, a method for fine-tuning large-scale vision models with a better trade-off between task performance and the number of trainable parameters. LoRand generates tiny adapter structures with low-rank synthesis while keeping the original backbone parameters fixed, resulting in high parameter sharing. To demonstrate LoRand's effectiveness, we implement extensive experiments on object detection, semantic segmentation, and instance segmentation tasks. By only training a small percentage (1% to 3%) of the pre-trained backbone parameters, LoRand achieves comparable performance to standard fine-tuning on COCO and ADE20K and outperforms fine-tuning in low-resource PASCAL VOC dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1576.ResFormer: Scaling ViTs With Multi-Resolution Training</span><br>
                <span class="as">Tian, RuiandWu, ZuxuanandDai, QiandHu, HanandQiao, YuandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_ResFormer_Scaling_ViTs_With_Multi-Resolution_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22721-22731.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器在处理未见过的训练输入分辨率时性能下降的问题。<br>
                    动机：提出一个改进的框架，通过多分辨率训练来提高对各种（大部分未见过）测试分辨率的性能。<br>
                    方法：构建ResFormer框架，在不同的分辨率上复制图像并实施规模一致性损失以获取不同尺度的交互信息。同时，提出一种全局-局部位置嵌入策略，根据输入大小进行平滑变化。<br>
                    效果：在ImageNet上进行的大量实验表明，ResFormer具有强大的可扩展性，能有效地处理广泛的分辨率。例如，当评估相对较低和较高的分辨率时，ResFormer-B-MR的Top-1准确率分别为75.86%和81.72%，比DeiT-B高出48%和7.49%。此外，还证明ResFormer具有灵活性，可以容易地扩展到语义分割、目标检测和视频动作识别等领域。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViTs) have achieved overwhelming success, yet they suffer from vulnerable resolution scalability, i.e., the performance drops drastically when presented with input resolutions that are unseen during training. We introduce, ResFormer, a framework that is built upon the seminal idea of multi-resolution training for improved performance on a wide spectrum of, mostly unseen, testing resolutions. In particular, ResFormer operates on replicated images of different resolutions and enforces a scale consistency loss to engage interactive information across different scales. More importantly, to alternate among varying resolutions effectively, especially novel ones in testing, we propose a global-local positional embedding strategy that changes smoothly conditioned on input sizes. We conduct extensive experiments for image classification on ImageNet. The results provide strong quantitative evidence that ResFormer has promising scaling abilities towards a wide range of resolutions. For instance, ResFormer- B-MR achieves a Top-1 accuracy of 75.86% and 81.72% when evaluated on relatively low and high resolutions respectively (i.e., 96 and 640), which are 48% and 7.49% better than DeiT-B. We also demonstrate, moreover, ResFormer is flexible and can be easily extended to semantic segmentation, object detection and video action recognition.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1577.You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model</span><br>
                <span class="as">Tang, ShengkunandWang, YaqingandKong, ZhenglunandZhang, TianchiandLi, YaoandDing, CaiwenandWang, YanzhiandLiang, YiandXu, Dongkuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_You_Need_Multiple_Exiting_Dynamic_Early_Exiting_for_Accelerating_Unified_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10781-10791.png><br>
            
            <span class="tt"><span class="t0">研究问题：大型转换器模型虽然在各种下游视觉语言任务上取得了显著改进，但其庞大的模型规模导致推理速度慢和服务器成本增加。<br>
                    动机：并非所有输入都需要相同数量的计算来进行，这可能导致计算资源浪费。因此，需要一种方法来动态分配计算资源以提高推理效率。<br>
                    方法：提出一种新的早期退出策略，允许在编码器和解码器中同时根据输入层相似性动态跳过多个层的早期退出，即MuE。通过分解编码器中的图像和文本模态，MuE具有灵活性，可以在模态方面跳过不同的层，提高推理效率的同时最小化性能下降。<br>
                    效果：实验结果表明，该方法在SNLI-VE和MS COCO数据集上可以将推理时间减少50%和40%，同时保持99%和96%的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large-scale transformer models bring significant improvements for various downstream vision language tasks with a unified architecture. The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing. While some certain predictions benefit from the full complexity of the large-scale model, not all of input need the same amount of computation to conduct, potentially leading to computation resource waste. To handle this challenge, early exiting is proposed to adaptively allocate computational power in term of input complexity to improve inference efficiency. The existing early exiting strategies usually adopt output confidence based on intermediate layers as a proxy of input complexity to incur the decision of skipping following layers. However, such strategies cannot apply to encoder in the widely-used unified architecture with both encoder and decoder due to difficulty of output confidence estimation in the encoder. It is suboptimal in term of saving computation power to ignore the early exiting in encoder component. To handle this challenge, we propose a novel early exiting strategy for unified visual language models, which allows dynamically skip the layers in encoder and decoder simultaneously in term of input layer-wise similarities with multiple times of early exiting, namely MuE. By decomposing the image and text modalities in the encoder, MuE is flexible and can skip different layers in term of modalities, advancing the inference efficiency while minimizing performance drop. Experiments on the SNLI-VE and MS COCO datasets show that the proposed approach MuE can reduce inference time by up to 50% and 40% while maintaining 99% and 96% performance respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1578.PD-Quant: Post-Training Quantization Based on Prediction Difference Metric</span><br>
                <span class="as">Liu, JiaweiandNiu, LinandYuan, ZhihangandYang, DaweiandWang, XinggangandLiu, Wenyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24427-24437.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何确定合适的量化参数，以降低神经网络的尺寸和计算成本，同时保持预测精度。<br>
                    动机：现有的后训练量化方法只考虑局部信息，可能无法得到最优的量化参数。<br>
                    方法：提出PD-Quant方法，通过使用网络预测前后的差异信息来确定量化参数，并调整激活分布以解决因校准集数量少而导致的过拟合问题。<br>
                    效果：实验表明，PD-Quant能产生更好的量化参数，提高量化模型的预测精度，尤其在低位设置中表现优秀。例如，在权重2位、激活2位的情况下，PD-Quant将ResNet-18的准确率提升至53.14%，RegNetX-600MF的准确率提升至40.67%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Post-training quantization (PTQ) is a neural network compression technique that converts a full-precision model into a quantized model using lower-precision data types. Although it can help reduce the size and computational cost of deep neural networks, it can also introduce quantization noise and reduce prediction accuracy, especially in extremely low-bit settings. How to determine the appropriate quantization parameters (e.g., scaling factors and rounding of weights) is the main problem facing now. Existing methods attempt to determine these parameters by minimize the distance between features before and after quantization, but such an approach only considers local information and may not result in the most optimal quantization parameters. We analyze this issue and propose PD-Quant, a method that addresses this limitation by considering global information. It determines the quantization parameters by using the information of differences between network prediction before and after quantization. In addition, PD-Quant can alleviate the overfitting problem in PTQ caused by the small number of calibration sets by adjusting the distribution of activations. Experiments show that PD-Quant leads to better quantization parameters and improves the prediction accuracy of quantized models, especially in low-bit settings. For example, PD-Quant pushes the accuracy of ResNet-18 up to 53.14% and RegNetX-600MF up to 40.67% in weight 2-bit activation 2-bit. The code is released at https://github.com/hustvl/PD-Quant.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1579.Ultra-High Resolution Segmentation With Ultra-Rich Context: A Novel Benchmark</span><br>
                <span class="as">Ji, DeyiandZhao, FengandLu, HongtaoandTao, MingyuanandYe, Jieping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ji_Ultra-High_Resolution_Segmentation_With_Ultra-Rich_Context_A_Novel_Benchmark_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23621-23630.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前需要一种大规模、高分辨率的细粒度密集标注数据集来推动超高清分割（UHR）方法的发展。<br>
                    动机：现有的UHR数据集在图像数量、场景复杂度和上下文丰富度等方面存在不足，因此需要开发一种新的数据集来满足需求。<br>
                    方法：研究者提出了URUR数据集，它包含大量的高分辨率图像（3008张，大小为5120x5120），覆盖了63个城市的复杂场景，丰富的上下文信息（1百万个实例，8个类别），以及精细的标注（约800亿个手动标注像素）。同时，研究者还提出了WSDNet框架，该框架通过集成多级离散小波变换（DWT）和波形平滑损失（WSL）函数，有效降低了计算负担并保留了更多的空间细节。<br>
                    效果：实验证明，URUR数据集在多个UHR数据集上表现出了最先进的性能，而WSDNet框架也显示出了高效和出色的分割效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the increasing interest and rapid development of methods for Ultra-High Resolution (UHR) segmentation, a large-scale benchmark covering a wide range of scenes with full fine-grained dense annotations is urgently needed to facilitate the field. To this end, the URUR dataset is introduced, in the meaning of Ultra-High Resolution dataset with Ultra-Rich Context. As the name suggests, URUR contains amounts of images with high enough resolution (3,008 images of size 5,120x5,120), a wide range of complex scenes (from 63 cities), rich-enough context (1 million instances with 8 categories) and fine-grained annotations (about 80 billion manually annotated pixels), which is far superior to all the existing UHR datasets including DeepGlobe, Inria Aerial, UDD, etc.. Moreover, we also propose WSDNet, a more efficient and effective framework for UHR segmentation especially with ultra-rich context. Specifically, multi-level Discrete Wavelet Transform (DWT) is naturally integrated to release computation burden while preserve more spatial details, along with a Wavelet Smooth Loss (WSL) to reconstruct original structured context and texture with a smooth constrain. Experiments on several UHR datasets demonstrate its state-of-the-art performance. The dataset is available at https://github.com/jankyee/URUR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1580.Towards High-Quality and Efficient Video Super-Resolution via Spatial-Temporal Data Overfitting</span><br>
                <span class="as">Li, GenandJi, JieandQin, MinghaiandNiu, WeiandRen, BinandAfghah, FatemehandGuo, LinkeandMa, Xiaolong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Towards_High-Quality_and_Efficient_Video_Super-Resolution_via_Spatial-Temporal_Data_Overfitting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10259-10269.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习进行视频分辨率提升，同时减少存储和带宽资源的消耗。<br>
                    动机：现有的视频传输系统通过将视频分割成块并使用超分辨率模型对每个块进行过拟合来提高视频质量，但这需要大量的块来保证过拟合的质量，从而增加了存储和数据传输的负担。<br>
                    方法：提出一种新的方法，通过利用空间-时间信息精确地将视频分割成块，从而将块的数量和模型大小保持在最低限度。并通过数据感知的联合训练技术将该方法提升为一个单一的过拟合模型，进一步减少了存储需求。<br>
                    效果：在实验中，该方法在实时视频超分辨率方面取得了良好的效果，与最先进的方法相比，该方法在直播视频分辨率提升任务上实现了28 fps的流速度和41.60 dB的PSNR，速度快了14倍，质量提高了2.29 dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>As deep convolutional neural networks (DNNs) are widely used in various fields of computer vision, leveraging the overfitting ability of the DNN to achieve video resolution upscaling has become a new trend in the modern video delivery system. By dividing videos into chunks and overfitting each chunk with a super-resolution model, the server encodes videos before transmitting them to the clients, thus achieving better video quality and transmission efficiency. However, a large number of chunks are expected to ensure good overfitting quality, which substantially increases the storage and consumes more bandwidth resources for data transmission. On the other hand, decreasing the number of chunks through training optimization techniques usually requires high model capacity, which significantly slows down execution speed. To reconcile such, we propose a novel method for high-quality and efficient video resolution upscaling tasks, which leverages the spatial-temporal information to accurately divide video into chunks, thus keeping the number of chunks as well as the model size to a minimum. Additionally, we advance our method into a single overfitting model by a data-aware joint training technique, which further reduces the storage requirement with negligible quality drop. We deploy our proposed overfitting models on an off-the-shelf mobile phone, and experimental results show that our method achieves real-time video super-resolution with high video quality. Compared with the state-of-the-art, our method achieves 28 fps streaming speed with 41.60 PSNR, which is 14 times faster and 2.29 dB better in the live video resolution upscaling tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1581.Class-Incremental Exemplar Compression for Class-Incremental Learning</span><br>
                <span class="as">Luo, ZilinandLiu, YaoyaoandSchiele, BerntandSun, Qianru</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11371-11380.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行基于范例的类别增量学习（CIL）？<br>
                    动机：目前的CIL方法在每个增量阶段都需要存储旧类别的少量范例，这限制了范例的数量。<br>
                    方法：提出了一种名为类别增量掩蔽（CIM）的自适应掩模生成模型，通过从类激活映射（CAM）中生成0-1掩码来压缩非判别性像素的范例，从而突破了“少量”的限制。<br>
                    效果：在Food-101、ImageNet-100和ImageNet-1000等高分辨率CIL基准测试中，使用CIM压缩的范例实现了新的最先进的CIL准确性，比FOSTER在10阶段ImageNet-1000上高出4.8个百分点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Exemplar-based class-incremental learning (CIL) finetunes the model with all samples of new classes but few-shot exemplars of old classes in each incremental phase, where the "few-shot" abides by the limited memory budget. In this paper, we break this "few-shot" limit based on a simple yet surprisingly effective idea: compressing exemplars by downsampling non-discriminative pixels and saving "many-shot" compressed exemplars in the memory. Without needing any manual annotation, we achieve this compression by generating 0-1 masks on discriminative pixels from class activation maps (CAM). We propose an adaptive mask generation model called class-incremental masking (CIM) to explicitly resolve two difficulties of using CAM: 1) transforming the heatmaps of CAM to 0-1 masks with an arbitrary threshold leads to a trade-off between the coverage on discriminative pixels and the quantity of exemplars, as the total memory is fixed; and 2) optimal thresholds vary for different object classes, which is particularly obvious in the dynamic environment of CIL. We optimize the CIM model alternatively with the conventional CIL model through a bilevel optimization problem. We conduct extensive experiments on high-resolution CIL benchmarks including Food-101, ImageNet-100, and ImageNet-1000, and show that using the compressed exemplars by CIM can achieve a new state-of-the-art CIL accuracy, e.g., 4.8 percentage points higher than FOSTER on 10-Phase ImageNet-1000. Our code is available at https://github.com/xfflzl/CIM-CIL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1582.Boost Vision Transformer With GPU-Friendly Sparsity and Quantization</span><br>
                <span class="as">Yu, ChongandChen, TaoandGan, ZhongxueandFan, Jiayuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Boost_Vision_Transformer_With_GPU-Friendly_Sparsity_and_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22658-22668.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何充分利用GPU硬件对视觉变换器进行加速部署。<br>
                    动机：由于视觉变换器中的许多堆叠自注意力和交叉注意力块涉及许多高维张量乘法操作，因此其GPU加速部署具有挑战性且鲜有研究。<br>
                    方法：设计了一种压缩方案，最大限度地利用了GPU友好的2:4细粒度结构化稀疏和量化。首先通过2:4结构化剪枝将原始大模型稀疏化，然后通过稀疏蒸馏感知量化训练将浮点稀疏模型进一步量化为定点模型。<br>
                    效果：实验结果表明，该压缩方案在减少视觉变换器模型大小6.4-12.7倍、FLOPs 30.3-62倍的同时，在ImageNet分类、COCO检测和ADE20K分割基准任务上几乎没有精度下降。此外，该方案还可以在实际部署性能上提高1.39-1.79倍和3.22-3.43倍的延迟和吞吐量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The transformer extends its success from the language to the vision domain. Because of the numerous stacked self-attention and cross-attention blocks in the transformer, which involve many high-dimensional tensor multiplication operations, the acceleration deployment of vision transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compression scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which considers GPU can provide an extra speedup of 2:4 sparse calculation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization process. The proposed compression scheme is flexible to support supervised and unsupervised learning styles. Experiment results show GPUSQ-ViT scheme achieves state-of-the-art compression by reducing vision transformer models 6.4-12.7 times on model size and 30.3-62 times on FLOPs with negligible accuracy degradation on ImageNet classification, COCO detection and ADE20K segmentation benchmarking tasks. Moreover, GPUSQ-ViT can boost actual deployment performance by 1.39-1.79 times and 3.22-3.43 times of latency and throughput on A100 GPU, and 1.57-1.69 times and 2.11-2.51 times improvement of latency and throughput on AGX Orin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1583.Bilateral Memory Consolidation for Continual Learning</span><br>
                <span class="as">Nie, XingandXu, ShixiongandLiu, XiyanandMeng, GaofengandHuo, ChunleiandXiang, Shiming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nie_Bilateral_Memory_Consolidation_for_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16026-16035.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度模型在处理长期任务序列时的记忆能力，防止灾难性遗忘？<br>
                    动机：人类能够持续获取并整合新知识，而深度模型在处理长期任务时会出现记忆丧失的问题。<br>
                    方法：提出一种新的双向记忆整合（BiMeCo）框架，将模型参数分为短期记忆模块和长期记忆模块，通过知识蒸馏和基于动量的更新来增强两个记忆模块之间的动态交互，形成通用知识以防止遗忘。<br>
                    效果：实验表明，BiMeCo显著提高了现有连续学习方法的性能，例如，在CIFAR-100上使用ResNet-18时，与最先进的CwD方法结合使用，BiMeCo的参数减少了2倍，性能提高了约2%到6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans are proficient at continuously acquiring and integrating new knowledge. By contrast, deep models forget catastrophically, especially when tackling highly long task sequences. Inspired by the way our brains constantly rewrite and consolidate past recollections, we propose a novel Bilateral Memory Consolidation (BiMeCo) framework that focuses on enhancing memory interaction capabilities. Specifically, BiMeCo explicitly decouples model parameters into short-term memory module and long-term memory module, responsible for representation ability of the model and generalization over all learned tasks, respectively. BiMeCo encourages dynamic interactions between two memory modules by knowledge distillation and momentum-based updating for forming generic knowledge to prevent forgetting. The proposed BiMeCo is parameter-efficient and can be integrated into existing methods seamlessly. Extensive experiments on challenging benchmarks show that BiMeCo significantly improves the performance of existing continual learning methods. For example, combined with the state-of-the-art method CwD, BiMeCo brings in significant gains of around 2% to 6% while using 2x fewer parameters on CIFAR-100 under ResNet-18.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1584.FlexiViT: One Model for All Patch Sizes</span><br>
                <span class="as">Beyer, LucasandIzmailov, PavelandKolesnikov, AlexanderandCaron, MathildeandKornblith, SimonandZhai, XiaohuaandMinderer, MatthiasandTschannen, MichaelandAlabdulmohsin, IbrahimandPavetic, Filip</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14496-14506.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用图像转换器（ViT）在不同的计算预算下进行训练？<br>
                    动机：改变图像转换器的切片大小会影响速度和准确性，但通常需要重新训练模型。<br>
                    方法：在训练时随机化切片大小，使得模型能够适应不同的计算预算。<br>
                    效果：通过广泛的任务评估，发现这种方法可以匹配甚至超越在单一切片大小下训练的标准ViT模型，为依赖ViT主干架构的大多数模型提供了易于添加的计算适应性能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, openworld detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pretrained models are available at github.com/googleresearch/big_vision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1585.Wavelet Diffusion Models Are Fast and Scalable Image Generators</span><br>
                <span class="as">Phung, HaoandDao, QuanandTran, Anh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Phung_Wavelet_Diffusion_Models_Are_Fast_and_Scalable_Image_Generators_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10199-10208.png><br>
            
            <span class="tt"><span class="t0">研究问题：扩散模型在高保真图像生成方面表现出色，但训练和推理速度慢是其应用于实时应用的主要障碍。<br>
                    动机：为了解决扩散模型运行速度慢的问题，本文提出了一种基于小波的扩散方案。<br>
                    方法：通过小波分解从图像和特征层面提取高低频率成分，并对其进行自适应处理以加快处理速度，同时保持良好的生成质量。此外，还提出了使用重建项来有效提高模型训练的收敛性。<br>
                    效果：在CelebA-HQ、CIFAR-10、LSUN-Church和STL-10数据集上的实验结果表明，该方法为实现实时高保真扩散模型迈出了重要一步。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models are rising as a powerful solution for high-fidelity image generation, which exceeds GANs in quality in many circumstances. However, their slow training and inference speed is a huge bottleneck, blocking them from being used in real-time applications. A recent DiffusionGAN method significantly decreases the models' running time by reducing the number of sampling steps from thousands to several, but their speeds still largely lag behind the GAN counterparts. This paper aims to reduce the speed gap by proposing a novel wavelet-based diffusion scheme. We extract low-and-high frequency components from both image and feature levels via wavelet decomposition and adaptively handle these components for faster processing while maintaining good generation quality. Furthermore, we propose to use a reconstruction term, which effectively boosts the model training convergence. Experimental results on CelebA-HQ, CIFAR-10, LSUN-Church, and STL-10 datasets prove our solution is a stepping-stone to offering real-time and high-fidelity diffusion models. Our code and pre-trained checkpoints are available at https://github.com/VinAIResearch/WaveDiff.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1586.NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers</span><br>
                <span class="as">Liu, YijiangandYang, HuanruiandDong, ZhenandKeutzer, KurtandDu, LiandZhang, Shanghang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_NoisyQuant_Noisy_Bias-Enhanced_Post-Training_Activation_Quantization_for_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20321-20330.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉变换器的后训练量化效果。<br>
                    动机：视觉变换器复杂的架构和高昂的训练成本促使了对后训练量化的探索，但其激活分布的重尾性阻碍了现有后训练量化方法的效果。<br>
                    方法：本文提出了NoisyQuant，一种针对视觉变换器后训练激活量化性能的量化器无关增强方法。通过在被量化的值上添加固定的均匀噪声偏置，可以在可证明的条件下显著降低量化误差。<br>
                    效果：大量实验表明，NoisyQuant在最小化计算开销的同时，大大提高了视觉变换器的后训练量化性能。例如，在线性均匀6位激活量化中，NoisyQuant分别将ViT、DeiT和Swin Transformer在ImageNet上的SOTA top-1准确率提高了1.7%、1.1%和0.5%，实现了与先前的非线性、混合精度量化相媲美甚至更高的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The complicated architecture and high training cost of vision transformers urge the exploration of post-training quantization. However, the heavy-tailed distribution of vision transformer activations hinders the effectiveness of previous post-training quantization methods, even with advanced quantizer designs. Instead of tuning the quantizer to better fit the complicated activation distribution, this paper proposes NoisyQuant, a quantizer-agnostic enhancement for the post-training activation quantization performance of vision transformers. We make a surprising theoretical discovery that for a given quantizer, adding a fixed Uniform noisy bias to the values being quantized can significantly reduce the quantization error under provable conditions. Building on the theoretical insight, NoisyQuant achieves the first success on actively altering the heavy-tailed activation distribution with additive noisy bias to fit a given quantizer. Extensive experiments show NoisyQuant largely improves the post-training quantization performance of vision transformer with minimal computation overhead. For instance, on linear uniform 6-bit activation quantization, NoisyQuant improves SOTA top-1 accuracy on ImageNet by up to 1.7%, 1.1% and 0.5% for ViT, DeiT, and Swin Transformer respectively, achieving on-par or even higher performance than previous nonlinear, mixed-precision quantization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1587.Video Compression With Entropy-Constrained Neural Representations</span><br>
                <span class="as">Gomes, CarlosandAzevedo, RobertoandSchroers, Christopher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gomes_Video_Compression_With_Entropy-Constrained_Neural_Representations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18497-18506.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过神经网络对视频进行编码，以实现新的视频处理形式。<br>
                    动机：传统的视频压缩技术在性能上仍然优于最新的神经网络视频表示（NVR）方法。<br>
                    方法：提出了一种新的卷积架构用于视频表示，可以更好地表示时空信息，并设计了一种能够同时优化码率和失真的训练策略。<br>
                    效果：在UVG数据集上的实验结果表明，该方法在视频压缩方面取得了新的最先进的结果，并且是第一个在性能上超过常用的HEVC基准的NVR-based视频压缩方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Encoding videos as neural networks is a recently proposed approach that allows new forms of video processing. However, traditional techniques still outperform such neural video representation (NVR) methods for the task of video compression. This performance gap can be explained by the fact that current NVR methods: i) use architectures that do not efficiently obtain a compact representation of temporal and spatial information; and ii) minimize rate and distortion disjointly (first overfitting a network on a video and then using heuristic techniques such as post-training quantization or weight pruning to compress the model). We propose a novel convolutional architecture for video representation that better represents spatio-temporal information and a training strategy capable of jointly optimizing rate and distortion. All network and quantization parameters are jointly learned end-to-end, and the post-training operations used in previous works are unnecessary. We evaluate our method on the UVG dataset, achieving new state-of-the-art results for video compression with NVRs. Moreover, we deliver the first NVR-based video compression method that improves over the typically adopted HEVC benchmark (x265, disabled b-frames, "medium" preset), closing the gap to autoencoder-based video compression techniques.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1588.A General Regret Bound of Preconditioned Gradient Method for DNN Training</span><br>
                <span class="as">Yong, HongweiandSun, YingandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yong_A_General_Regret_Bound_of_Preconditioned_Gradient_Method_for_DNN_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7866-7875.png><br>
            
            <span class="tt"><span class="t0">研究问题：优化深度神经网络的自适应学习率方法只考虑了全预条件矩阵的对角元素，而全矩阵预条件梯度方法虽然理论上具有较低的遗憾界，但由于其高复杂性，不适用于训练深度神经网络。<br>
                    动机：提出一种带有约束的全矩阵预条件梯度的一般遗憾界，并展示预处理器的更新公式可以通过解决锥约束优化问题得到。<br>
                    方法：通过最小化引导函数的上界，开发了一种新的深度神经网络优化器AdaBK。同时开发了一系列技术，包括统计信息更新、阻尼、高效的矩阵逆根计算和梯度幅度保持等，使AdaBK能够有效且高效地实施。<br>
                    效果：提出的AdaBK可以方便地嵌入到许多现有的深度神经网络优化器中，如SGDM和AdamW。相应的SGDM_BK和AdamW_BK算法在图像分类、目标检测和分割等基准视觉任务上表现出比现有深度神经网络优化器显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While adaptive learning rate methods, such as Adam, have achieved remarkable improvement in optimizing Deep Neural Networks (DNNs), they consider only the diagonal elements of the full preconditioned matrix. Though the full-matrix preconditioned gradient methods theoretically have a lower regret bound, they are impractical for use to train DNNs because of the high complexity. In this paper, we present a general regret bound with a constrained full-matrix preconditioned gradient and show that the updating formula of the preconditioner can be derived by solving a cone-constrained optimization problem. With the block-diagonal and Kronecker-factorized constraints, a specific guide function can be obtained. By minimizing the upper bound of the guide function, we develop a new DNN optimizer, termed AdaBK. A series of techniques, including statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation, are developed to make AdaBK effective and efficient to implement. The proposed AdaBK can be readily embedded into many existing DNN optimizers, e.g., SGDM and AdamW, and the corresponding SGDM_BK and AdamW_BK algorithms demonstrate significant improvements over existing DNN optimizers on benchmark vision tasks, including image classification, object detection and segmentation. The source code will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1589.Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields</span><br>
                <span class="as">Park, SungheonandSon, MinjungandJang, SeokhwanandAhn, YoungChunandKim, Ji-YeonandKang, Nahyup</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Temporal_Interpolation_Is_All_You_Need_for_Dynamic_Neural_Radiance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4212-4221.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练动态场景的有意义的时空表示。<br>
                    动机：在动态场景中，时间插值在学习有意义的表示中起着关键作用。<br>
                    方法：提出一种基于特征向量的时间插值的新方法来训练动态场景的时空神经辐射场。根据底层表示，提出了两种特征插值方法，神经网络或网格。在神经网络表示中，通过多个神经网络模块从空间-时间输入中提取特征，并根据时间帧进行插值。所提出的多级特征插值网络有效地捕捉了短期和长期时间范围的特征。在网格表示中，通过四维哈希网格学习空间-时间特征，显著减少了训练时间。网格表示比先前的基于神经网络的方法快100倍以上，同时保持了渲染质量。连接静态和动态特征并添加简单的平滑性项进一步提高了我们提出模型的性能。<br>
                    效果：尽管模型架构简单，但我们的方法在神经网络表示的渲染质量和网格表示的训练速度方面都取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Temporal interpolation often plays a crucial role to learn meaningful representations in dynamic scenes. In this paper, we propose a novel method to train spatiotemporal neural radiance fields of dynamic scenes based on temporal interpolation of feature vectors. Two feature interpolation methods are suggested depending on underlying representations, neural networks or grids. In the neural representation, we extract features from space-time inputs via multiple neural network modules and interpolate them based on time frames. The proposed multi-level feature interpolation network effectively captures features of both short-term and long-term time ranges. In the grid representation, space-time features are learned via four-dimensional hash grids, which remarkably reduces training time. The grid representation shows more than 100 times faster training speed than the previous neural-net-based methods while maintaining the rendering quality. Concatenating static and dynamic features and adding a simple smoothness term further improve the performance of our proposed models. Despite the simplicity of the model architectures, our method achieved state-of-the-art performance both in rendering quality for the neural representation and in training speed for the grid representation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1590.PlenVDB: Memory Efficient VDB-Based Radiance Fields for Fast Training and Rendering</span><br>
                <span class="as">Yan, HanandLiu, CelongandMa, ChaoandMei, Xing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_PlenVDB_Memory_Efficient_VDB-Based_Radiance_Fields_for_Fast_Training_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/88-96.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a new representation for neural radiance fields that accelerates both the training and the inference processes with VDB, a hierarchical data structure for sparse volumes. VDB takes both the advantages of sparse and dense volumes for compact data representation and efficient data access, being a promising data structure for NeRF data interpolation and ray marching. Our method, Plenoptic VDB (PlenVDB), directly learns the VDB data structure from a set of posed images by means of a novel training strategy and then uses it for real-time rendering. Experimental results demonstrate the effectiveness and the efficiency of our method over previous arts: First, it converges faster in the training process. Second, it delivers a more compact data format for NeRF data presentation. Finally, it renders more efficiently on commodity graphics hardware. Our mobile PlenVDB demo achieves 30+ FPS, 1280x720 resolution on an iPhone12 mobile phone. Check plenvdb.github.io for details.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1591.Tangentially Elongated Gaussian Belief Propagation for Event-Based Incremental Optical Flow Estimation</span><br>
                <span class="as">Nagata, JunandSekikawa, Yusuke</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nagata_Tangentially_Elongated_Gaussian_Belief_Propagation_for_Event-Based_Incremental_Optical_Flow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21940-21949.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现低延迟的光流估计。<br>
                    动机：现有的方法无法同时实现低延迟和全光流的估计。<br>
                    方法：提出一种切向拉长的高斯（TEG）信念传播（BP）方法，通过消息传递和贝叶斯推断实现全光流的增量估计。<br>
                    效果：在真实世界数据集上，TEGBP方法的效果优于最先进的增量准全流方法，并将在接收后开源。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optical flow estimation is a fundamental functionality in computer vision. An event-based camera, which asynchronously detects sparse intensity changes, is an ideal device for realizing low-latency estimation of the optical flow owing to its low-latency sensing mechanism. An existing method using local plane fitting of events could utilize the sparsity to realize incremental updates for low-latency estimation; however, its output is merely a normal component of the full optical flow. An alternative approach using a frame-based deep neural network could estimate the full flow; however, its intensive non-incremental dense operation prohibits the low-latency estimation. We propose tangentially elongated Gaussian (TEG) belief propagation (BP) that realizes incremental full-flow estimation. We model the probability of full flow as the joint distribution of TEGs from the normal flow measurements, such that the marginal of this distribution with correct prior equals the full flow. We formulate the marginalization using a message-passing based on the BP to realize efficient incremental updates using sparse measurements. In addition to the theoretical justification, we evaluate the effectiveness of the TEGBP in real-world datasets; it outperforms SOTA incremental quasi-full flow method by a large margin. The code will be open-sourced upon acceptance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1592.Fair Scratch Tickets: Finding Fair Sparse Networks Without Weight Training</span><br>
                <span class="as">Tang, PengweiandYao, WeiandLi, ZhicongandLiu, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Fair_Scratch_Tickets_Finding_Fair_Sparse_Networks_Without_Weight_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24406-24416.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决计算机视觉模型可能存在的公平性问题。<br>
                    动机：目前存在大量的工作致力于通过预处理、处理中和处理后的方法来减轻计算机视觉中的不公平现象。<br>
                    方法：本文提出了一种新颖的处理中公平感知学习范式，借鉴了计算机视觉公平性中的彩票假设（LTH）。我们随机初始化一个密集神经网络，并找到合适的权重二值掩码以获得公平的稀疏子网络，而无需进行任何权重训练。<br>
                    效果：实验结果表明，我们在随机初始化的网络中发现具有天生公平性的这种稀疏子网络，其准确性-公平性权衡与现有的公平感知处理中方法训练的密集神经网络相当。我们将这些公平子网络称为“公平刮刮乐”（FSTs）。我们还从理论上为它们提供了公平性和准确性的保证。在我们的实验中，我们在各种数据集、目标属性、随机初始化方法、稀疏模式和公平代理上研究了FSTs的存在性。我们还发现FSTs可以跨数据集转移，并研究了FSTs的其他性质。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies suggest that computer vision models come at the risk of compromising fairness. There are extensive works to alleviate unfairness in computer vision using pre-processing, in-processing, and post-processing methods. In this paper, we lead a novel fairness-aware learning paradigm for in-processing methods through the lens of the lottery ticket hypothesis (LTH) in the context of computer vision fairness. We randomly initialize a dense neural network and find appropriate binary masks for the weights to obtain fair sparse subnetworks without any weight training. Interestingly, to the best of our knowledge, we are the first to discover that such sparse subnetworks with inborn fairness exist in randomly initialized networks, achieving an accuracy-fairness trade-off comparable to that of dense neural networks trained with existing fairness-aware in-processing approaches. We term these fair subnetworks as Fair Scratch Tickets (FSTs). We also theoretically provide fairness and accuracy guarantees for them. In our experiments, we investigate the existence of FSTs on various datasets, target attributes, random initialization methods, sparsity patterns, and fairness surrogates. We also find that FSTs can transfer across datasets and investigate other properties of FSTs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1593.The Resource Problem of Using Linear Layer Leakage Attack in Federated Learning</span><br>
                <span class="as">Zhao, JoshuaC.andElkordy, AhmedRoushdyandSharma, AtulandEzzeldin, YahyaH.andAvestimehr, SalmanandBagchi, Saurabh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_The_Resource_Problem_of_Using_Linear_Layer_Leakage_Attack_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3974-3983.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在联邦学习中提高隐私保护水平，使服务器只能访问解密后的聚合更新？<br>
                    动机：线性层泄漏方法是目前唯一能够扩展并实现高泄漏率的数据重建攻击方法，但这种方法的资源开销随着客户端数量的增加而增加。<br>
                    方法：通过增加注入的全连接层的大小来提高泄漏率。同时，通过将聚合视为多个个体更新的组合，应用稀疏性来减轻资源开销。<br>
                    效果：与最先进的方法相比，使用稀疏性可以将模型大小开销减少327倍，计算时间减少3.34倍，同时保持相同的总泄漏率，即使在1000个客户端的聚合中，泄漏率仍为77%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Secure aggregation promises a heightened level of privacy in federated learning, maintaining that a server only has access to a decrypted aggregate update. Within this setting, linear layer leakage methods are the only data reconstruction attacks able to scale and achieve a high leakage rate regardless of the number of clients or batch size. This is done through increasing the size of an injected fully-connected (FC) layer. We show that this results in a resource overhead which grows larger with an increasing number of clients. We show that this resource overhead is caused by an incorrect perspective in all prior work that treats an attack on an aggregate update in the same way as an individual update with a larger batch size. Instead, by attacking the update from the perspective that aggregation is combining multiple individual updates, this allows the application of sparsity to alleviate resource overhead. We show that the use of sparsity can decrease the model size overhead by over 327x and the computation time by 3.34x compared to SOTA while maintaining equivalent total leakage rate, 77% even with 1000 clients in aggregation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1594.Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-Time Mobile Telepresence</span><br>
                <span class="as">Fu, YongganandLi, YuechengandLi, ChenghuiandSaragih, JasonandZhang, PeizhaoandDai, XiaoliangandLin, Yingyan(Celine)</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_Auto-CARD_Efficient_and_Robust_Codec_Avatar_Driving_for_Real-Time_Mobile_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21036-21045.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过减少计算冗余，实现在AR/VR中实时、稳健的逼真头像驱动。<br>
                    动机：当前，对AR/VR中的实时、高逼真度头像驱动有着强烈需求，但高昂的计算成本和设备限制是主要瓶颈。<br>
                    方法：提出了一种名为Auto-CARD的框架，通过最小化两种冗余源来实现这一目标。首先，开发了一种专用的神经架构搜索技术AVE-NAS，用于AR/VR中的头像编码；其次，利用连续渲染过程中的图像时间冗余性，开发了一种称为LATEX的机制来跳过冗余帧的计算。<br>
                    效果：在Meta Quest 2上进行的实时Codec Avatar驱动测试中，Auto-CARD框架实现了5.05倍的速度提升，同时保持了与最先进的头像编码设计相当甚至更好的动画质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-time and robust photorealistic avatars for telepresence in AR/VR have been highly desired for enabling immersive photorealistic telepresence. However, there still exists one key bottleneck: the considerable computational expense needed to accurately infer facial expressions captured from headset-mounted cameras with a quality level that can match the realism of the avatar's human appearance. To this end, we propose a framework called Auto-CARD, which for the first time enables real-time and robust driving of Codec Avatars when exclusively using merely on-device computing resources. This is achieved by minimizing two sources of redundancy. First, we develop a dedicated neural architecture search technique called AVE-NAS for avatar encoding in AR/VR, which explicitly boosts both the searched architectures' robustness in the presence of extreme facial expressions and hardware friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal redundancy in consecutively captured images during continuous rendering and develop a mechanism dubbed LATEX to skip the computation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space derived by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames. For evaluation, we demonstrate the efficacy of our Auto-CARD framework in real-time Codec Avatar driving settings, where we achieve a 5.05x speed-up on Meta Quest 2 while maintaining a comparable or even better animation quality than state-of-the-art avatar encoder designs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1595.Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners</span><br>
                <span class="as">Chen, ZitianandShen, YikangandDing, MingyuandChen, ZhenfangandZhao, HengshuangandLearned-Miller, ErikG.andGan, Chuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11828-11837.png><br>
            
            <span class="tt"><span class="t0">研究问题：多任务学习（MTL）中的优化比单任务学习（STL）更具挑战性，因为不同任务的梯度可能是矛盾的。<br>
                    动机：当任务相关时，共享一些参数可能有益（合作），但某些任务需要额外的参数以在特定类型的数据或区分上具有专业知识（专业化）。<br>
                    方法：我们提出了Mod-Squad，这是一个模块化为专家小组的新模型。这种结构允许我们将合作和专业化形式化为匹配专家和任务的过程。我们在训练单个模型时优化这个匹配过程。具体来说，我们在一个transformer模型中加入了专家混合（MoE）层，并引入了一种新的损失函数，该函数包含了任务和专家之间的相互依赖性。<br>
                    效果：实验结果显示，我们的方法是优越的。对于每个任务，我们可以将这小部分专家提取出来作为一个独立的模型，其性能与大模型相同。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optimization in multi-task learning (MTL) is more challenging than single-task learning (STL), as the gradient from different tasks can be contradictory. When tasks are related, it can be beneficial to share some parameters among them (cooperation). However, some tasks require additional parameters with expertise in a specific type of data or discrimination (specialization). To address the MTL challenge, we propose Mod-Squad, a new model that is Modularized into groups of experts (a 'Squad'). This structure allows us to formalize cooperation and specialization as the process of matching experts and tasks. We optimize this matching process during the training of a single model. Specifically, we incorporate mixture of experts (MoE) layers into a transformer model, with a new loss that incorporates the mutual dependence between tasks and experts. As a result, only a small set of experts are activated for each task. This prevents the sharing of the entire backbone model between all tasks, which strengthens the model, especially when the training set size and the number of tasks scale up. More interestingly, for each task, we can extract the small set of experts as a standalone model that maintains the same performance as the large model. Extensive experiments on the Taskonomy dataset with 13 vision tasks and the PASCALContext dataset with 5 vision tasks show the superiority of our approach. The project page can be accessed at https://vis-www.cs.umass.edu/mod-squad.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1596.Run, Don&#x27;t Walk: Chasing Higher FLOPS for Faster Neural Networks</span><br>
                <span class="as">Chen, JierunandKao, Shiu-hongandHe, HaoandZhuo, WeipengandWen, SongandLee, Chul-HoandChan, S.-H.Gary</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12021-12031.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计更快的神经网络？<br>
                    动机：减少浮点运算数量（FLOPs）并不能直接导致网络延迟的降低，因为操作效率低下导致的低浮点运算每秒（FLOPS）。<br>
                    方法：重新审视流行的操作符，发现其低FLOPS主要是由于频繁的内存访问，特别是深度卷积。因此，提出一种新的部分卷积（PConv），通过减少冗余计算和内存访问来更有效地提取空间特征。基于PConv，进一步提出了FasterNet，这是一种新的神经网络家族，在各种设备上都能显著提高运行速度，同时保持对各种视觉任务的准确性。<br>
                    效果：例如，在ImageNet-1k上，我们的小型FasterNet-T0比MobileViT-XXS在GPU、CPU和ARM处理器上的速度快2.8倍、3.3倍和2.4倍，同时准确率高出2.9%。我们的大型FasterNet-L实现了令人印象深刻的83.5%的top-1准确率，与新兴的Swin-B相当，同时在GPU上的推理吞吐量高36%，在CPU上的计算时间节省了37%。代码可在https://github.com/JierunChen/FasterNet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is 2.8x, 3.3x, and 2.4x faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being 2.9% more accurate. Our large FasterNet-L achieves impressive 83.5% top-1 accuracy, on par with the emerging Swin-B, while having 36% higher inference throughput on GPU, as well as saving 37% compute time on CPU. Code is available at https://github.com/JierunChen/FasterNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1597.Adaptive Plasticity Improvement for Continual Learning</span><br>
                <span class="as">Liang, Yan-ShuoandLi, Wu-Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Adaptive_Plasticity_Improvement_for_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7816-7825.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决持续学习中的灾难性遗忘问题，同时考虑评估和提高模型在新任务上的可塑性。<br>
                    动机：现有的方法在解决遗忘问题时可能会损害模型对新任务的适应性。<br>
                    方法：提出一种名为自适应可塑性改进的新方法，通过评估并自适应地提高模型在新任务上的可塑性。<br>
                    效果：实验结果表明，该方法在准确性和内存使用方面优于其他最先进的基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many works have tried to solve the catastrophic forgetting (CF) problem in continual learning (lifelong learning). However, pursuing non-forgetting on old tasks may damage the model's plasticity for new tasks. Although some methods have been proposed to achieve stability-plasticity trade-off, no methods have considered evaluating a model's plasticity and improving plasticity adaptively for a new task. In this work, we propose a new method, called adaptive plasticity improvement (API), for continual learning. Besides the ability to overcome CF on old tasks, API also tries to evaluate the model's plasticity and then adaptively improve the model's plasticity for learning a new task if necessary. Experiments on several real datasets show that API can outperform other state-of-the-art baselines in terms of both accuracy and memory usage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1598.Towards Better Decision Forests: Forest Alternating Optimization</span><br>
                <span class="as">Carreira-Perpi\~n\&#x27;an, Miguel\&#x27;A.andGabidolla, MagzhanandZharmagambetov, Arman</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Carreira-Perpinan_Towards_Better_Decision_Forests_Forest_Alternating_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7589-7598.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化决策森林模型以提高其准确性？<br>
                    动机：尽管决策森林是机器学习中最准确的模型之一，但其训练方式高度依赖启发式方法，且优化过程复杂。<br>
                    方法：提出一种新的优化算法——森林交替优化（Forest Alternating Optimization），通过在所有树和参数上联合优化期望损失和正则化来学习森林。<br>
                    效果：实验证明，该方法训练出的森林在准确率上超过了最先进的模型，同时使用的树更少、更小。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Decision forests are among the most accurate models in machine learning. This is remarkable given that the way they are trained is highly heuristic: neither the individual trees nor the overall forest optimize any well-defined loss. While diversity mechanisms such as bagging or boosting have been until now critical in the success of forests, we think that a better optimization should lead to better forests---ideally eliminating any need for an ensembling heuristic. However, unlike for most other models, such as neural networks, optimizing forests or trees is not easy, because they define a non-differentiable function. We show, for the first time, that it is possible to learn a forest by optimizing a desirable loss and regularization jointly over all its trees and parameters. Our algorithm, Forest Alternating Optimization, is based on defining a forest as a parametric model with a fixed number of trees and structure (rather than adding trees indefinitely as in bagging or boosting). It then iteratively updates each tree in alternation so that the objective function decreases monotonically. The algorithm is so effective at optimizing that it easily overfits, but this can be corrected by averaging. The result is a forest that consistently exceeds the accuracy of the state-of-the-art while using fewer, smaller trees.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1599.DA Wand: Distortion-Aware Selection Using Neural Mesh Parameterization</span><br>
                <span class="as">Liu, RichardandAigerman, NoamandKim, VladimirG.andHanocka, Rana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_DA_Wand_Distortion-Aware_Selection_Using_Neural_Mesh_Parameterization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16739-16749.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种神经技术，用于学习在一点周围选择局部子区域进行网格参数化。<br>
                    动机：我们的框架是由用于表面去马赛克、纹理或绘画的交互式工作流程驱动的。<br>
                    方法：我们的主要想法是以数据驱动的方式学习局部参数化，使用神经网络框架中的新型可微分参数化层。我们训练一个分割网络来选择参数化为二维的3D区域，并对由此产生的畸变进行惩罚，从而产生对畸变敏感的分割。训练完成后，用户可以在我们的系统中交互地选择网格上的一点并获得该点周围的大型有意义区域，该区域会产生低畸变的参数化。<br>
                    效果：实验结果表明，我们的系统可以有效地进行网格参数化，并且用户可以通过交互方式获得满意的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a neural technique for learning to select a local sub-region around a point which can be used for mesh parameterization. The motivation for our framework is driven by interactive workflows used for decaling, texturing, or painting on surfaces. Our key idea to to learn a local parameterization in a data-driven manner, using a novel differentiable parameterization layer within a neural network framework. We train a segmentation network to select 3D regions that are parameterized into 2D and penalized by the resulting distortion, giving rise to segmentations which are distortion-aware. Following training, a user can use our system to interactively select a point on the mesh and obtain a large, meaningful region around the selection which induces a low-distortion parameterization. Our code and project page are publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1600.Disentangled Representation Learning for Unsupervised Neural Quantization</span><br>
                <span class="as">Noh, HaechanandHyun, SangeekandJeong, WoojinandLim, HanshinandHeo, Jae-Pil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Noh_Disentangled_Representation_Learning_for_Unsupervised_Neural_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12001-12010.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度学习量化器难以像传统浅层量化器那样从剩余向量空间中受益。<br>
                    动机：为了解决这个问题，我们提出了一种新的无监督神经量化的解耦表示学习方法。<br>
                    方法：我们的方法类似于剩余向量空间的概念，通过将倒排索引的信息与向量解耦，使潜在空间更紧凑。<br>
                    效果：在大规模数据集上的实验结果证实了我们的方法比最先进的检索系统有大幅度的提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The inverted index is a widely used data structure to avoid the infeasible exhaustive search. It accelerates retrieval significantly by splitting the database into multiple disjoint sets and restricts distance computation to a small fraction of the database. Moreover, it even improves search quality by allowing quantizers to exploit the compact distribution of residual vector space. However, we firstly point out a problem that an existing deep learning-based quantizer hardly benefits from the residual vector space, unlike conventional shallow quantizers. To cope with this problem, we introduce a novel disentangled representation learning for unsupervised neural quantization. Similar to the concept of residual vector space, the proposed method enables more compact latent space by disentangling information of the inverted index from the vectors. Experimental results on large-scale datasets confirm that our method outperforms the state-of-the-art retrieval systems by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1601.Meta-Learning With a Geometry-Adaptive Preconditioner</span><br>
                <span class="as">Kang, SuhyunandHwang, DuhunandEo, MoonjungandKim, TaesupandRhee, Wonjong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Meta-Learning_With_a_Geometry-Adaptive_Preconditioner_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16080-16090.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高元学习算法的效果？<br>
                    动机：现有的元学习算法如MAML在优化结构上存在局限，需要改进。<br>
                    方法：提出一种名为GAP的新算法，通过元学习得到一个依赖于任务特定参数的预处理器，并满足黎曼度量条件，以改善内部循环优化。<br>
                    效果：实验结果显示，GAP在多种少次学习任务上优于最先进的MAML和PGD-MAML。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model-agnostic meta-learning (MAML) is one of the most successful meta-learning algorithms. It has a bi-level optimization structure where the outer-loop process learns a shared initialization and the inner-loop process optimizes task-specific weights. Although MAML relies on the standard gradient descent in the inner-loop, recent studies have shown that controlling the inner-loop's gradient descent with a meta-learned preconditioner can be beneficial. Existing preconditioners, however, cannot simultaneously adapt in a task-specific and path-dependent way. Additionally, they do not satisfy the Riemannian metric condition, which can enable the steepest descent learning with preconditioned gradient. In this study, we propose Geometry-Adaptive Preconditioned gradient descent (GAP) that can overcome the limitations in MAML; GAP can efficiently meta-learn a preconditioner that is dependent on task-specific parameters, and its preconditioner can be shown to be a Riemannian metric. Thanks to the two properties, the geometry-adaptive preconditioner is effective for improving the inner-loop optimization. Experiment results show that GAP outperforms the state-of-the-art MAML family and preconditioned gradient descent-MAML (PGD-MAML) family in a variety of few-shot learning tasks. Code is available at: https://github.com/Suhyun777/CVPR23-GAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1602.Global Vision Transformer Pruning With Hessian-Aware Saliency</span><br>
                <span class="as">Yang, HuanruiandYin, HongxuandShen, MayingandMolchanov, PavloandLi, HaiandKautz, Jan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Global_Vision_Transformer_Pruning_With_Hessian-Aware_Saliency_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18547-18557.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低Transformer模型的计算成本？<br>
                    动机：Transformer模型在许多任务上取得了最先进的结果，但其设计的架构在推理过程中会产生巨大的计算成本。<br>
                    方法：通过首次对全局结构剪枝的尝试，重新分配参数，挑战了ViT模型中所有堆叠块具有统一维度的常见设计哲学。<br>
                    效果：通过对DeiT-Base模型进行迭代剪枝，产生了一种新的架构族NViT（新颖的ViT），其参数重分布更有效地利用了参数。在ImageNet-1K上，NViT-Base实现了2.6倍FLOPs减少、5.1倍参数减少和1.9倍运行速度提升，且几乎无损。较小的NViT变体在相同的吞吐量下实现了超过1%的精度增益，以及比SWIN-Small模型少3.3倍的参数减少，这些结果大大超过了现有技术。进一步的分析显示了ViT模型的高可剪枝性，ViT块内的敏感性差异，以及堆叠ViT块之间的独特参数分布趋势。这些见解为更有效的ViT提供了一种简单而有效的参数重分布规则，以提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a 2.6x FLOPs reduction, 5.1x parameter reduction, and 1.9x run-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless 3.3x parameter reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1603.Efficient On-Device Training via Gradient Filtering</span><br>
                <span class="as">Yang, YuedongandLi, GuihongandMarculescu, Radu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Efficient_On-Device_Training_via_Gradient_Filtering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3811-3820.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在设备上进行有效的模型训练，特别是在边缘AI中。<br>
                    动机：当前在设备上的训练存在大量运算和内存消耗的问题，这对联邦学习、持续学习等应用产生了影响。<br>
                    方法：提出一种新的梯度过滤方法，通过在梯度图中创建具有较少唯一元素的特定结构，显著减少了训练过程中的反向传播的计算复杂性和内存消耗。<br>
                    效果：在多个CNN模型（如MobileNet、DeepLabV3、UPerNet）和设备（如Raspberry Pi和Jetson Nano）上进行的图像分类和语义分割的大量实验表明，该方法有效且适用广泛。例如，与最先进的技术相比，我们在ImageNet分类上实现了高达19倍的速度提升和77.1%的内存节省，仅损失了0.1%的准确率。此外，该方法易于实施和部署，与NVIDIA Jetson Nano上的MKLDNN和CUDNN的高度优化基线相比，观察到了20倍以上的速度提升和90%的能源节省。因此，该方法为设备上的训练开辟了新的研究方向，具有巨大的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19x speedup and 77.1% memory savings on ImageNet classification with only 0.1% accuracy loss. Finally, our method is easy to implement and deploy; over 20x speedup and 90% energy savings have been observed compared to highly optimized baselines in MKLDNN and CUDNN on NVIDIA Jetson Nano. Consequently, our approach opens up a new direction of research with a huge potential for on-device training.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1604.Learning To Exploit the Sequence-Specific Prior Knowledge for Image Processing Pipelines Optimization</span><br>
                <span class="as">Qin, HainaandHan, LongfeiandXiong, WeihuaandWang, JuanandMa, WentaoandLi, BingandHu, Weiming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Learning_To_Exploit_the_Sequence-Specific_Prior_Knowledge_for_Image_Processing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22314-22323.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化图像信号处理（ISP）中的超参数以提高图像质量。<br>
                    动机：目前的ISP超参数优化方法忽视了ISP模块之间的顺序关系和参数间的相似性，导致优化效果不佳。<br>
                    方法：提出一种序贯的ISP超参数预测框架，利用ISP模块间的顺序关系和参数间的相似性来指导模型的序列过程。<br>
                    效果：在物体检测、图像分割和图像质量任务上验证了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The hardware image signal processing (ISP) pipeline is the intermediate layer between the imaging sensor and the downstream application, processing the sensor signal into an RGB image. The ISP is less programmable and consists of a series of processing modules. Each processing module handles a subtask and contains a set of tunable hyperparameters. A large number of hyperparameters form a complex mapping with the ISP output. The industry typically relies on manual and time-consuming hyperparameter tuning by image experts, biased towards human perception. Recently, several automatic ISP hyperparameter optimization methods using downstream evaluation metrics come into sight. However, existing methods for ISP tuning treat the high-dimensional parameter space as a global space for optimization and prediction all at once without inducing the structure knowledge of ISP. To this end, we propose a sequential ISP hyperparameter prediction framework that utilizes the sequential relationship within ISP modules and the similarity among parameters to guide the model sequence process. We validate the proposed method on object detection, image segmentation, and image quality tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1605.Complexity-Guided Slimmable Decoder for Efficient Deep Video Compression</span><br>
                <span class="as">Hu, ZhihaoandXu, Dong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Complexity-Guided_Slimmable_Decoder_for_Efficient_Deep_Video_Compression_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14358-14367.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度视频压缩的效率？<br>
                    动机：目前的深度视频编码方法在压缩效率上还有待提高。<br>
                    方法：提出了复杂度引导的可变卷积层解码器（cgSlimDecoder）和跳过自适应熵编码（SaEC）。其中，cgSlimDecoder通过引入新的通道宽度选择模块自动决定每个可变卷积层的最优通道宽度，并通过优化复杂度-率失真相关目标函数来直接学习新引入的通道宽度选择模块和其他模块的参数。SaEC则通过跳过已由超先验网络准确预测的元素的熵编码过程，进一步加速了运动和残差解码器的熵解码过程。<br>
                    效果：实验证明，这两种新方法可以广泛应用于三种常用的深度视频编解码器（DVC、FVC和DCVC），并在保持性能稳定的同时显著提高了编码效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we propose the complexity-guided slimmable decoder (cgSlimDecoder) in combination with skip-adaptive entropy coding (SaEC) for efficient deep video compression. Specifically, given the target complexity constraints, in our cgSlimDecoder, we introduce a set of new channel width selection modules to automatically decide the optimal channel width of each slimmable convolution layer. By optimizing the complexity-rate-distortion related objective function to directly learn the parameters of the newly introduced channel width selection modules and other modules in the decoder, our cgSlimDecoder can automatically allocate the optimal numbers of parameters for different types of modules (e.g., motion/residual decoder and the motion compensation network) and simultaneously support multiple complexity levels by using a single learnt decoder instead of multiple decoders. In addition, our proposed SaEC can further accelerate the entropy decoding procedure in both motion and residual decoders by simply skipping the entropy coding process for the elements in the encoded feature maps that are already well-predicted by the hyperprior network. As demonstrated in our comprehensive experiments, our newly proposed methods cgSlimDecoder and SaEC are general and can be readily incorporated into three widely used deep video codecs (i.e., DVC, FVC and DCVC) to significantly improve their coding efficiency with negligible performance drop.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1606.Q-DETR: An Efficient Low-Bit Quantized Detection Transformer</span><br>
                <span class="as">Xu, ShengandLi, YanjingandLin, MingbaoandGao, PengandGuo, GuodongandL\&quot;u, JinhuandZhang, Baochang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3842-3851.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何减少低比特量化DETR（Q-DETR）的计算和内存需求，同时保持其性能。<br>
                    动机：检测转换器（DETR）在资源受限的设备上的应用需要大量的计算和内存资源，而量化是一种解决方案，但现有的量化方法会导致性能大幅下降。<br>
                    方法：通过分布校正蒸馏（DRD）解决这个问题，该方法将查询信息失真最小化，并引入了一种新的前景感知查询匹配方案来有效地传递教师信息。<br>
                    效果：实验结果表明，这种方法比现有技术表现更好，例如，理论上4比特Q-DETR可以加速具有ResNet-50主干的DETR 6.6倍，并在COCO数据集上实现39.4%的AP，与实值版本只有2.6%的性能差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent detection transformer (DETR) has advanced object detection, but its application on resource-constrained devices requires massive computation and memory resources. Quantization stands out as a solution by representing the network in low-bit parameters and operations. However, there is a significant performance drop when performing low-bit quantized DETR (Q-DETR) with existing quantization methods. We find that the bottlenecks of Q-DETR come from the query information distortion through our empirical analyses. This paper addresses this problem based on a distribution rectification distillation (DRD). We formulate our DRD as a bi-level optimization problem, which can be derived by generalizing the information bottleneck (IB) principle to the learning of Q-DETR. At the inner level, we conduct a distribution alignment for the queries to maximize the self-information entropy. At the upper level, we introduce a new foreground-aware query matching scheme to effectively transfer the teacher information to distillation-desired features to minimize the conditional information entropy. Extensive experimental results show that our method performs much better than prior arts. For example, the 4-bit Q-DETR can theoretically accelerate DETR with ResNet-50 backbone by 6.6x and achieve 39.4% AP, with only 2.6% performance gaps than its real-valued counterpart on the COCO dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1607.ERM-KTP: Knowledge-Level Machine Unlearning via Knowledge Transfer</span><br>
                <span class="as">Lin, ShenandZhang, XiaoyuandChen, ChenyangandChen, XiaofengandSusilo, Willy</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_ERM-KTP_Knowledge-Level_Machine_Unlearning_via_Knowledge_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20147-20155.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的机器学习模型在删除学习数据时效率低下，且存在严重的安全漏洞。<br>
                    动机：为了解决这些问题，我们尝试从知识的角度定义机器的“反学习”，并提出了一种基于知识的反学习方法，即ERM-KTP。<br>
                    方法：我们提出了一种降低知识纠缠的“实体减少掩码”（ERM）结构，并在接收到删除请求时，通过我们的知识转移和禁止（KTP）方法，将非目标数据点的知识从原始模型转移到反学习模型，同时禁止目标数据点的知识。<br>
                    效果：实验证明，我们的ERM-KTP反学习方法具有高效、高保真度和可扩展性，并且由于其ERM结构和精心设计的掩码，该方法具有良好的解释性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Machine unlearning can fortify the privacy and security of machine learning applications. Unfortunately, the exact unlearning approaches are inefficient, and the approximate unlearning approaches are unsuitable for complicated CNNs. Moreover, the approximate approaches have serious security flaws because even unlearning completely different data points can produce the same contribution estimation as unlearning the target data points. To address the above problems, we try to define machine unlearning from the knowledge perspective, and we propose a knowledge-level machine unlearning method, namely ERM-KTP. Specifically, we propose an entanglement-reduced mask (ERM) structure to reduce the knowledge entanglement among classes during the training phase. When receiving the unlearning requests, we transfer the knowledge of the non-target data points from the original model to the unlearned model and meanwhile prohibit the knowledge of the target data points via our proposed knowledge transfer and prohibition (KTP) method. Finally, we will get the unlearned model as the result and delete the original model to accomplish the unlearning process. Especially, our proposed ERM-KTP is an interpretable unlearning method because the ERM structure and the crafted masks in KTP can explicitly explain the operation and the effect of unlearning data points. Extensive experiments demonstrate the effectiveness, efficiency, high fidelity, and scalability of the ERM-KTP unlearning method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1608.DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training</span><br>
                <span class="as">Chen, YihaoandQi, XianbiaoandWang, JiananandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DisCo-CLIP_A_Distributed_Contrastive_Loss_for_Memory_Efficient_CLIP_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22648-22657.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决训练对比学习模型时，对比损失的内存消耗过大的问题。<br>
                    动机：当前的对比学习模型在训练过程中，由于需要重复计算所有GPU间的梯度，导致内存消耗巨大。<br>
                    方法：提出DisCo-CLIP方法，将对比损失及其梯度计算分解为两部分，一部分在当前GPU上计算内部梯度，另一部分通过all_reduce从其他GPU收集外部梯度，避免了重复计算。<br>
                    效果：DisCo-CLIP方法可以将对比损失的GPU内存消耗从O(B^2)降低到O(B^2/N)，其中B和N分别是批量大小和用于训练的GPU数量。这种方法在不牺牲任何计算精度的情况下，实现了分布式的对比损失计算，对于大批量CLIP训练尤其有效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose DisCo-CLIP, a distributed memory-efficient CLIP training approach, to reduce the memory consumption of contrastive loss when training contrastive learning models. Our approach decomposes the contrastive loss and its gradient computation into two parts, one to calculate the intra-GPU gradients and the other to compute the inter-GPU gradients. According to our decomposition, only the intra-GPU gradients are computed on the current GPU, while the inter-GPU gradients are collected via all_reduce from other GPUs instead of being repeatedly computed on every GPU. In this way, we can reduce the GPU memory consumption of contrastive loss computation from O(B^2) to O(B^2 / N), where B and N are the batch size and the number of GPUs used for training. Such a distributed solution is mathematically equivalent to the original non-distributed contrastive loss computation, without sacrificing any computation accuracy. It is particularly efficient for large-batch CLIP training. For instance, DisCo-CLIP can enable contrastive training of a ViT-B/32 model with a batch size of 32K or 196K using 8 or 64 A100 40GB GPUs, compared with the original CLIP solution which requires 128 A100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1609.NIRVANA: Neural Implicit Representations of Videos With Adaptive Networks and Autoregressive Patch-Wise Modeling</span><br>
                <span class="as">Maiya, ShishiraR.andGirish, SharathandEhrlich, MaxandWang, HanyuandLee, KwotSinandPoirson, PatrickandWu, PengxiangandWang, ChenandShrivastava, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Maiya_NIRVANA_Neural_Implicit_Representations_of_Videos_With_Adaptive_Networks_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14378-14387.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用视频中的时间和空间冗余进行高质量视频压缩。<br>
                    动机：现有的视频编码方法没有充分利用视频的时空冗余，且结构固定，不适用于长时间或高分辨率的视频。<br>
                    方法：提出NIRVANA方法，将视频视为一组帧，并为每组帧分别设计网络进行预测。该方法在时间和空间维度上共享计算，减少了视频编码时间。视频表示是自回归的，当前组的网络由前一组模型的权重初始化。为了提高效率，我们在训练过程中对网络参数进行量化，无需后期剪枝或量化。<br>
                    效果：在UVG数据集上，NIRVANA将编码质量从37.36提高到37.70（以PSNR衡量），并将编码速度提高12倍，同时保持相同的压缩率。与之前的视频INR工作相比，我们的方法更适应大分辨率和长时间的视频，具有很高的灵活性和扩展性。此外，通过适应具有不同帧间运动的视频，NIRVANA实现了可变比特率压缩，解码速度快6倍，并能很好地适应更多的GPU，适用于各种部署场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit Neural Representations (INR) have recently shown to be powerful tool for high-quality video compression. However, existing works are limiting as they do not explicitly exploit the temporal redundancy in videos, leading to a long encoding time. Additionally, these methods have fixed architectures which do not scale to longer videos or higher resolutions. To address these issues, we propose NIRVANA, which treats videos as groups of frames and fits separate networks to each group performing patch-wise prediction. %This design shares computation within each group, in the spatial and temporal dimensions, resulting in reduced encoding time of the video. The video representation is modeled autoregressively, with networks fit on a current group initialized using weights from the previous group's model. To further enhance efficiency, we perform quantization of the network parameters during training, requiring no post-hoc pruning or quantization. When compared with previous works on the benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70 (in terms of PSNR) and the encoding speed by 12x, while maintaining the same compression rate. In contrast to prior video INR works which struggle with larger resolution and longer videos, we show that our algorithm is highly flexible and scales naturally due to its patch-wise and autoregressive designs. Moreover, our method achieves variable bitrate compression by adapting to videos with varying inter-frame motion. NIRVANA achieves 6x decoding speed and scales well with more GPUs, making it practical for various deployment scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1610.Preserving Linear Separability in Continual Learning by Backward Feature Projection</span><br>
                <span class="as">Gu, QiaoandShim, DongsubandShkurti, Florian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Preserving_Linear_Separability_in_Continual_Learning_by_Backward_Feature_Projection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24286-24295.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何克服灾难性遗忘，实现模型在新任务学习中的稳定性和可塑性的平衡。<br>
                    动机：现有的特征蒸馏方法在减少遗忘的同时，忽视了新特征的可塑性，导致模型在新任务学习中的表现不佳。<br>
                    方法：提出一种名为“向后特征投影”（BFP）的方法，允许新特征在旧特征的可学习线性变换下进行变化，同时保持旧类别的线性可分性。<br>
                    效果：实验证明，BFP可以显著提高模型在新任务学习中的表现，并帮助模型学习到更好的表示空间，其中线性可分性得到了良好的保持，线性探测也取得了高分类准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Catastrophic forgetting has been a major challenge in continual learning, where the model needs to learn new tasks with limited or no access to data from previously seen tasks. To tackle this challenge, methods based on knowledge distillation in feature space have been proposed and shown to reduce forgetting. However, most feature distillation methods directly constrain the new features to match the old ones, overlooking the need for plasticity. To achieve a better stability-plasticity trade-off, we propose Backward Feature Projection (BFP), a method for continual learning that allows the new features to change up to a learnable linear transformation of the old features. BFP preserves the linear separability of the old classes while allowing the emergence of new feature directions to accommodate new classes. BFP can be integrated with existing experience replay methods and boost performance by a significant margin. We also demonstrate that BFP helps learn a better representation space, in which linear separability is well preserved during continual learning and linear probing achieves high classification accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1611.Differentiable Architecture Search With Random Features</span><br>
                <span class="as">Zhang, XuanyangandLi, YonggangandZhang, XiangyuandWang, YongtaoandSun, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Differentiable_Architecture_Search_With_Random_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16060-16069.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决Differentiable architecture search (DARTS)的性能崩溃问题。<br>
                    动机：DARTS在NAS技术中具有高效的搜索效率和效果，但其存在性能崩溃的问题。<br>
                    方法：从两个方面着手解决这个问题。首先，研究了DARTS中超网络的表达能力，并提出了只训练BatchNorm的新DARTS范式。其次，理论上发现随机特征会稀释超网络优化中的跳跃连接辅助作用，使搜索算法更公平地选择操作，从而解决了性能崩溃问题。<br>
                    效果：实验结果表明，RF-DARTS在CIFAR-10上获得了94.36%的测试准确率，并在ImageNet上实现了最新的24.0%的top-1测试误差。此外，RF-DARTS在三个数据集（CIFAR-10、CIFAR-100和SVHN）和四个搜索空间（S1-S4）上表现稳健。RF-PCDARTS在ImageNet上取得了更好的结果，即23.9%的top-1和7.1%的top-5测试误差，超越了直接在ImageNet上搜索的代表性方法如单路径、训练自由和部分通道范式。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Differentiable architecture search (DARTS) has significantly promoted the development of NAS techniques because of its high search efficiency and effectiveness but suffers from performance collapse. In this paper, we make efforts to alleviate the performance collapse problem for DARTS from two aspects. First, we investigate the expressive power of the supernet in DARTS and then derive a new setup of DARTS paradigm with only training BatchNorm. Second, we theoretically find that random features dilute the auxiliary connection role of skip-connection in supernet optimization and enable search algorithm focus on fairer operation selection, thereby solving the performance collapse problem. We instantiate DARTS and PC-DARTS with random features to build an improved version for each named RF-DARTS and RF-PCDARTS respectively. Experimental results show that RF-DARTS obtains 94.36% test accuracy on CIFAR-10 (which is the nearest optimal result in NAS-Bench-201), and achieves the newest state-of-the-art top-1 test error of 24.0% on ImageNet when transferring from CIFAR-10. Moreover, RF-DARTS performs robustly across three datasets (CIFAR-10, CIFAR-100, and SVHN) and four search spaces (S1-S4). Besides, RF-PCDARTS achieves even better results on ImageNet, that is, 23.9% top-1 and 7.1% top-5 test error, surpassing representative methods like single-path, training-free, and partial-channel paradigms directly searched on ImageNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1612.Unified Pose Sequence Modeling</span><br>
                <span class="as">Foo, LinGengandLi, TianjiaoandRahmani, HosseinandKe, QiuhongandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_Unified_Pose_Sequence_Modeling_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13019-13030.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何统一基于姿态数据的不同人类行为理解任务，如动作识别、3D姿态估计和3D早期动作预测。<br>
                    动机：不同的基于姿态的任务需要不同的输出数据格式，这限制了现有方法为每个任务利用特定网络架构的能力。<br>
                    方法：提出了一种新的统一姿态序列（UPS）模型，通过将文本基的动作标签和坐标基的人体姿态视为语言序列，统一了上述任务的异构输出格式。然后，通过优化一个单一的自回归变压器，可以得到一个可以处理所有上述任务的统一输出序列。<br>
                    效果：在四个不同的任务上进行了广泛的实验，结果表明，UPS模型在四个流行的行为理解基准测试中表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a Unified Pose Sequence Modeling approach to unify heterogeneous human behavior understanding tasks based on pose data, e.g., action recognition, 3D pose estimation and 3D early action prediction. A major obstacle is that different pose-based tasks require different output data formats. Specifically, the action recognition and prediction tasks require class predictions as outputs, while 3D pose estimation requires a human pose output, which limits existing methods to leverage task-specific network architectures for each task. Hence, in this paper, we propose a novel Unified Pose Sequence (UPS) model to unify heterogeneous output formats for the aforementioned tasks by considering text-based action labels and coordinate-based human poses as language sequences. Then, by optimizing a single auto-regressive transformer, we can obtain a unified output sequence that can handle all the aforementioned tasks. Moreover, to avoid the interference brought by the heterogeneity between different tasks, a dynamic routing mechanism is also proposed to empower our UPS with the ability to learn which subsets of parameters should be shared among different tasks. To evaluate the efficacy of the proposed UPS, extensive experiments are conducted on four different tasks with four popular behavior understanding benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1613.Compression-Aware Video Super-Resolution</span><br>
                <span class="as">Wang, YingweiandIsobe, TakashiandJia, XuandTao, XinandLu, HuchuanandTai, Yu-Wing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compression-Aware_Video_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2012-2021.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高移动设备上存储或在互联网上传输的压缩视频的清晰度？<br>
                    动机：现有的大多数视频超分辨率（VSR）方法都假设理想的输入，导致实验设置与实际应用之间存在较大的性能差距。<br>
                    方法：提出一种新颖实用的压缩感知视频超分辨率模型，该模型可以根据估计的压缩级别调整其视频增强过程。设计了一个压缩编码器来模拟输入帧的压缩级别，然后在插入压缩感知模块后，基于隐式计算的表示对基础VSR模型进行条件处理。此外，还提出了通过充分利用在压缩视频流的信息融合过程中自然嵌入的元数据来进一步加强VSR模型的方法。<br>
                    效果：通过在压缩VSR基准上的大量实验，证明了所提出方法的有效性和效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Videos stored on mobile devices or delivered on the Internet are usually in compressed format and are of various unknown compression parameters, but most video super-resolution (VSR) methods often assume ideal inputs resulting in large performance gap between experimental settings and real-world applications. In spite of a few pioneering works being proposed recently to super-resolve the compressed videos, they are not specially designed to deal with videos of various levels of compression. In this paper, we propose a novel and practical compression-aware video super-resolution model, which could adapt its video enhancement process to the estimated compression level. A compression encoder is designed to model compression levels of input frames, and a base VSR model is then conditioned on the implicitly computed representation by inserting compression-aware modules. In addition, we propose to further strengthen the VSR model by taking full advantage of meta data that is embedded naturally in compressed video streams in the procedure of information fusion. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of the proposed method on compressed VSR benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1614.Regularization of Polynomial Networks for Image Recognition</span><br>
                <span class="as">Chrysos, GrigoriosG.andWang, BohanandDeng, JiankangandCevher, Volkan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chrysos_Regularization_of_Polynomial_Networks_for_Image_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16123-16132.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度神经网络（DNNs）的解释性，同时保持其高性能？<br>
                    动机：尽管深度神经网络在各种任务上表现出色，但其“黑箱”特性使得理论分析困难。与此同时，多项式网络（PNs）作为具有良好性能和改进解释性的替代方法，但尚未达到强大的DNN基线的性能。<br>
                    方法：引入一类新的PNs，能够在六个基准测试中达到ResNet的性能。通过强有力的正则化策略，使PNs能够匹配DNN的性能。此外，还提出了D-PolyNets，其扩展程度高于先前提出的PNs，同时保持相似的性能。<br>
                    效果：新的模型有助于理解元素激活函数的作用，且源代码已在GitHub上公开。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep Neural Networks (DNNs) have obtained impressive performance across tasks, however they still remain as black boxes, e.g., hard to theoretically analyze. At the same time, Polynomial Networks (PNs) have emerged as an alternative method with a promising performance and improved interpretability but have yet to reach the performance of the powerful DNN baselines. In this work, we aim to close this performance gap. We introduce a class of PNs, which are able to reach the performance of ResNet across a range of six benchmarks. We demonstrate that strong regularization is critical and conduct an extensive study of the exact regularization schemes required to match performance. To further motivate the regularization schemes, we introduce D-PolyNets that achieve a higher-degree of expansion than previously proposed polynomial networks. D-PolyNets are more parameter-efficient while achieving a similar performance as other polynomial networks. We expect that our new models can lead to an understanding of the role of elementwise activation functions (which are no longer required for training PNs). The source code is available at https://github.com/grigorisg9gr/regularized_polynomials.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1615.EfficientViT: Memory Efficient Vision Transformer With Cascaded Group Attention</span><br>
                <span class="as">Liu, XinyuandPeng, HouwenandZheng, NingxinandYang, YuqingandHu, HanandYuan, Yixuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14420-14430.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器在处理速度和效率上的问题，以适应实时应用的需求。<br>
                    动机：虽然视觉转换器具有很高的模型能力，但其显著的性能提升伴随着巨大的计算成本，使其不适合实时应用。<br>
                    方法：本文提出了一种名为EfficientViT的高效视觉转换器系列。通过设计新的构建模块，使用单内存受限的MHSA在有效的FFN层之间进行三明治布局，提高了内存效率并增强了通道通信。同时，通过级联组注意力模块，将不同的注意力头与完整的特征进行不同的分割，不仅节省了计算成本，还提高了注意力的多样性。<br>
                    效果：实验结果表明，EfficientViT在速度和准确性之间取得了良好的平衡，优于现有的高效模型。例如，EfficientViT-M5在Nvidia V100 GPU和Intel Xeon CPU上的吞吐量分别比MobileNetV3-Large高40.4%和45.2%，同时准确率高出1.9%。与最近的高效模型MobileViT-XXS相比，EfficientViT-M2在GPU/CPU上的运行速度快5.8x/3.7x，转换为ONNX格式时快7.4x，且准确率高出1.8%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision transformers named EfficientViT. We find that the speed of existing transformer models is commonly bounded by memory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we design a new building block with a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN layers, which improves memory efficiency while enhancing channel communication. Moreover, we discover that the attention maps share high similarities across heads, leading to computational redundancy. To address this, we present a cascaded group attention module feeding attention heads with different splits of the full feature, which not only saves computation cost but also improves attention diversity. Comprehensive experiments demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX format. Code and models will be available soon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1616.Simulated Annealing in Early Layers Leads to Better Generalization</span><br>
                <span class="as">Sarfi, AmirM.andKarimpour, ZahraandChaudhary, MuawizandKhalid, NasirM.andRavanelli, MircoandMudur, SudhirandBelilovsky, Eugene</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sarfi_Simulated_Annealing_in_Early_Layers_Leads_to_Better_Generalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20205-20214.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高深度学习模型的泛化能力？<br>
                    动机：现有的迭代学习方法通过延长训练时间来提高模型的泛化能力，但效果有限。<br>
                    方法：提出一种利用模拟退火算法在网络早期层进行学习的方法（SEAL），代替后期层的重新初始化。<br>
                    效果：在Tiny-ImageNet数据集上进行的实验表明，该方法在目标任务上的表现优于现有的最优方法LLF，并且在迁移学习和少样本学习任务上也取得了更好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, a number of iterative learning methods have been introduced to improve generalization. These typically rely on training for longer periods of time in exchange for improved generalization. LLF (later-layer-forgetting) is a state-of-the-art method in this category. It strengthens learning in early layers by periodically re-initializing the last few layers of the network. Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers. Essentially, later layers go through the normal gradient descent process, while the early layers go through short stints of gradient ascent followed by gradient descent. Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin. We further show that, compared to normal training, LLF features, although improving on the target task, degrade the transfer learning performance across all datasets we explored. In comparison, our method outperforms LLF across the same target datasets by a large margin. We also show that the prediction depth of our method is significantly lower than that of LLF and normal training, indicating on average better prediction performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1617.Integral Neural Networks</span><br>
                <span class="as">Solodskikh, KirillandKurbanov, AzimandAydarkhanov, RuslanandZhelavskaya, IrinaandParfenov, YuryandSong, DehuaandLefkimmiatis, Stamatios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16113-16122.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用连续层表示和积分操作来训练深度神经网络，以实现网络结构的剪枝和性能的保持。<br>
                    动机：传统的离散神经网络在结构剪枝时会损失大量性能，而本文提出的积分神经网络（INNs）可以在不进行微调的情况下实现高比例的结构剪枝，同时保持相近的性能。<br>
                    方法：采用连续层表示和积分操作来定义INNs的权重函数，将输入层的离散变换替换为连续积分操作。在推理阶段，可以通过数值积分求积将连续层转换为传统的张量表示。通过这种方法，可以将网络任意大小的离散化，并对积分核进行各种离散化间隔。<br>
                    效果：实验结果表明，所提出的INNs在多个任务上与常规离散网络具有相同的性能。与传统的离散剪枝方法相比，在没有微调的情况下，INNs可以在高比例（高达30%）的结构剪枝下保持相近的性能（对于ResNet18在Imagenet上的损失约为2%），而传统剪枝方法在这种情况下的损失为65%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a new family of deep neural networks. Instead of the conventional representation of network layers as N-dimensional weight tensors, we use continuous layer representation along the filter and channel dimensions. We call such networks Integral Neural Networks (INNs). In particular, the weights of INNs are represented as continuous functions defined on N-dimensional hypercubes, and the discrete transformations of inputs to the layers are replaced by continuous integration operations, accordingly. During the inference stage, our continuous layers can be converted into the traditional tensor representation via numerical integral quadratures. Such kind of representation allows the discretization of a network to an arbitrary size with various discretization intervals for the integral kernels. This approach can be applied to prune the model directly on the edge device while featuring only a small performance loss at high rates of structural pruning without any fine-tuning. To evaluate the practical benefits of our proposed approach, we have conducted experiments using various neural network architectures for multiple tasks. Our reported results show that the proposed INNs achieve the same performance with their conventional discrete counterparts, while being able to preserve approximately the same performance (2 % accuracy loss for ResNet18 on Imagenet) at a high rate (up to 30%) of structural pruning without fine-tuning, compared to 65 % accuracy loss of the conventional pruning methods under the same conditions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1618.Accelerating Dataset Distillation via Model Augmentation</span><br>
                <span class="as">Zhang, LeiandZhang, JieandLei, BowenandMukherjee, SubhabrataandPan, XiangandZhao, BoandDing, CaiwenandLi, YaoandXu, Dongkuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11950-11959.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20x speedup and comparable performance on par with state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1619.Solving Relaxations of MAP-MRF Problems: Combinatorial In-Face Frank-Wolfe Directions</span><br>
                <span class="as">Kolmogorov, Vladimir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kolmogorov_Solving_Relaxations_of_MAP-MRF_Problems_Combinatorial_In-Face_Frank-Wolfe_Directions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11980-11989.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决MAP-MRF推理问题的LP松弛，特别是Swoboda和Kolmogorov（2019）以及Kolmogorov和Pock（2021）最近提出的方法。<br>
                    动机：该方法使用一种变体的Frank-Wolfe（FW）方法来最小化一个平滑凸函数在一个组合多面体上，作为关键的计算子程序。<br>
                    方法：我们提出了一个基于面对面Frank-Wolfe方向的有效实现，这是在Freund等人（2017）的不同背景下引入的。更一般地，我们为组合子问题定义了一种抽象数据结构，使其能够使用面对面FW方向，并描述了其针对树形MAP-MRF推理子问题的特化。<br>
                    效果：实验结果表明，由此产生的方法在某些类别的问题上是目前最先进的LP求解器。我们的代码可以在pub.ist.ac.at/vnk/papers/IN-FACE-FW.html获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We consider the problem of solving LP relaxations of MAP-MRF inference problems, and in particular the method proposed recently in (Swoboda, Kolmogorov 2019; Kolmogorov, Pock 2021). As a key computational subroutine, it uses a variant of the Frank-Wolfe (FW) method to minimize a smooth convex function over a combinatorial polytope. We propose an efficient implementation of this subproutine based on in-face Frank-Wolfe directions, introduced in (Freund et al. 2017) in a different context. More generally, we define an abstract data structure for a combinatorial subproblem that enables in-face FW directions, and describe its specialization for tree-structured MAP-MRF inference subproblems. Experimental results indicate that the resulting method is the current state-of-art LP solver for some classes of problems. Our code is available at pub.ist.ac.at/ vnk/papers/IN-FACE-FW.html.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1620.Adapting Shortcut With Normalizing Flow: An Efficient Tuning Framework for Visual Recognition</span><br>
                <span class="as">Wang, YaomingandShi, BowenandZhang, XiaopengandLi, JinandLiu, YuchenandDai, WenruiandLi, ChenglinandXiong, HongkaiandTian, Qi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adapting_Shortcut_With_Normalizing_Flow_An_Efficient_Tuning_Framework_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15965-15974.png><br>
            
            <span class="tt"><span class="t0">研究问题：预训练后微调在视觉识别任务中已被证明有效，但所有参数的微调可能计算成本高，特别是对于大规模模型。<br>
                    动机：为了减轻计算和存储需求，近期的研究探索了参数高效微调（PEFT），其专注于调整最少数量的参数以实现有效适应。然而，现有方法未能分析额外参数对模型的影响，导致微调过程不清晰且次优。<br>
                    方法：本文引入了一种新颖而有效的PEFT范式，名为SNF（通过归一化流进行的快捷适应），它利用归一化流来调整快捷层。我们强调没有Lipschitz约束的层在适应下游数据集时可能导致误差传播。由于修改这些层中的过度参数化的残差连接是昂贵的，我们专注于调整便宜但关键的快捷方式。此外，用少量参数进行PEFT的信息学习可能是具有挑战性的，信息损失可能导致标签信息退化。为解决这个问题，我们提出了一种信息保留的归一化流。<br>
                    效果：实验结果表明SNF的有效性。具体来说，仅使用0.036M的参数，SNF就超过了以前的方法，在FGVC和VTAB-1k基准测试中使用ViT/B-16作为主干网络。代码可在https://github.com/Wang-Yaoming/SNF获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pretraining followed by fine-tuning has proven to be effective in visual recognition tasks. However, fine-tuning all parameters can be computationally expensive, particularly for large-scale models. To mitigate the computational and storage demands, recent research has explored Parameter-Efficient Fine-Tuning (PEFT), which focuses on tuning a minimal number of parameters for efficient adaptation. Existing methods, however, fail to analyze the impact of the additional parameters on the model, resulting in an unclear and suboptimal tuning process. In this paper, we introduce a novel and effective PEFT paradigm, named SNF (Shortcut adaptation via Normalization Flow), which utilizes normalizing flows to adjust the shortcut layers. We highlight that layers without Lipschitz constraints can lead to error propagation when adapting to downstream datasets. Since modifying the over-parameterized residual connections in these layers is expensive, we focus on adjusting the cheap yet crucial shortcuts. Moreover, learning new information with few parameters in PEFT can be challenging, and information loss can result in label information degradation. To address this issue, we propose an information-preserving normalizing flow. Experimental results demonstrate the effectiveness of SNF. Specifically, with only 0.036M parameters, SNF surpasses previous approaches on both the FGVC and VTAB-1k benchmarks using ViT/B-16 as the backbone. The code is available at https://github.com/Wang-Yaoming/SNF</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1621.Endpoints Weight Fusion for Class Incremental Semantic Segmentation</span><br>
                <span class="as">Xiao, Jia-WenandZhang, Chang-BinandFeng, JiekangandLiu, XialeiandvandeWeijer, JoostandCheng, Ming-Ming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_Endpoints_Weight_Fusion_for_Class_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7204-7213.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决类别增量语义分割（CISS）中的灾难性遗忘问题，以提高模型的判别能力。<br>
                    动机：现有的方法主要通过正则化（如知识蒸馏）来保持当前模型中之前的知识，但仅限制新旧模型表示的一致性往往对模型的提升有限。<br>
                    方法：提出一种名为端点权重融合（EWF）的方法，将包含旧知识的模型与保留新知识的模型动态融合，以增强在不断变化的分布中对旧类别的记忆。同时，利用蒸馏优化过程，使参数空间内的参数融合距离更近。<br>
                    效果：在两个广泛使用的数据集上进行实验，取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Class incremental semantic segmentation (CISS) focuses on alleviating catastrophic forgetting to improve discrimination. Previous work mainly exploit regularization (e.g., knowledge distillation) to maintain previous knowledge in the current model. However, distillation alone often yields limited gain to the model since only the representations of old and new models are restricted to be consistent. In this paper, we propose a simple yet effective method to obtain a model with strong memory of old knowledge, named Endpoints Weight Fusion (EWF). In our method, the model containing old knowledge is fused with the model retaining new knowledge in a dynamic fusion manner, strengthening the memory of old classes in ever-changing distributions. In addition, we analyze the relation between our fusion strategy and a popular moving average technique EMA, which reveals why our method is more suitable for class-incremental learning. To facilitate parameter fusion with closer distance in the parameter space, we use distillation to enhance the optimization process. Furthermore, we conduct experiments on two widely used datasets, achieving the state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1622.Efficient Robust Principal Component Analysis via Block Krylov Iteration and CUR Decomposition</span><br>
                <span class="as">Fang, ShunandXu, ZhengqinandWu, ShiqianandXie, Shoulie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_Efficient_Robust_Principal_Component_Analysis_via_Block_Krylov_Iteration_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1348-1357.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行主成分分析（RPCA）以解决大规模矩阵的计算问题。<br>
                    动机：现有的RPCA算法在处理大规模矩阵时，由于需要执行奇异值分解（SVD），因此需要大量的计算资源。<br>
                    方法：本文提出了一种基于块克莱洛迭代和CUR分解的高效RPCA（eRPCA）算法。具体来说，使用克莱洛迭代方法来近似特征值分解，其复杂度为O(ndrq + n(rq)^2)，其中q是一个小参数，r是目标秩。然后，根据估计的秩，采用CUR分解来替换SVD更新低秩矩阵组件，每次迭代的复杂度从O(rnd)降低到O(r^2n)。<br>
                    效果：实验结果表明，所提出的eRPCA在各种低层视觉应用中比最先进的方法更有效、更高效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Robust principal component analysis (RPCA) is widely studied in computer vision. Recently an adaptive rank estimate based RPCA has achieved top performance in low-level vision tasks without the prior rank, but both the rank estimate and RPCA optimization algorithm involve singular value decomposition, which requires extremely huge computational resource for large-scale matrices. To address these issues, an efficient RPCA (eRPCA) algorithm is proposed based on block Krylov iteration and CUR decomposition in this paper. Specifically, the Krylov iteration method is employed to approximate the eigenvalue decomposition in the rank estimation, which requires O(ndrq + n(rq)^2) for an (nxd) input matrix, in which q is a parameter with a small value, r is the target rank. Based on the estimated rank, CUR decomposition is adopted to replace SVD in updating low-rank matrix component, whose complexity reduces from O(rnd) to O(r^2n) per iteration. Experimental results verify the efficiency and effectiveness of the proposed eRPCA over the state-of-the-art methods in various low-level vision applications.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1623.Toward Accurate Post-Training Quantization for Image Super Resolution</span><br>
                <span class="as">Tu, ZhijunandHu, JieandChen, HantingandWang, Yunhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Toward_Accurate_Post-Training_Quantization_for_Image_Super_Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5856-5865.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量未标记的校准图像进行图像超分辨率的后训练量化（PTQ）。<br>
                    动机：现有的SR模型在部署到移动设备上时，需要进行模型量化，但现有方法需要完整的数据集和高昂的计算开销。<br>
                    方法：通过分析激活的非对称边界，引入基于密度的双重剪切来切断异常值。同时，提出一种像素感知的校准方法，以适应不同样本的高度动态范围。<br>
                    效果：实验证明，该方法在各种模型和数据集上显著优于现有的PTQ算法。例如，使用100个未标记的图像将EDSRx4量化为4位时，Urban100基准测试提高了2.091 dB。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Model quantization is a crucial step for deploying super resolution (SR) networks on mobile devices. However, existing works focus on quantization-aware training, which requires complete dataset and expensive computational overhead. In this paper, we study post-training quantization(PTQ) for image super resolution using only a few unlabeled calibration images. As the SR model aims to maintain the texture and color information of input images, the distribution of activations are long-tailed, asymmetric and highly dynamic compared with classification models. To this end, we introduce the density-based dual clipping to cut off the outliers based on analyzing the asymmetric bounds of activations. Moreover, we present a novel pixel aware calibration method with the supervision of the full-precision model to accommodate the highly dynamic range of different samples. Extensive experiments demonstrate that the proposed method significantly outperforms existing PTQ algorithms on various models and datasets. For instance, we get a 2.091 dB increase on Urban100 benchmark when quantizing EDSRx4 to 4-bit with 100 unlabeled images. Our code is available at both https://github.com/huawei-noah/Efficient-Computing/tree/master/Quantization/PTQ4SR and https://gitee.com/mindspore/models/tree/master/research/cv/PTQ4SR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1624.Real-Time Neural Light Field on Mobile Devices</span><br>
                <span class="as">Cao, JunliandWang, HuanandChemerys, PavloandShakhrai, VladislavandHu, JuandFu, YunandMakoviichuk, DenysandTulyakov, SergeyandRen, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Real-Time_Neural_Light_Field_on_Mobile_Devices_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8328-8337.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高神经渲染（NeRF）模型在资源受限设备（如移动设备）上的运行速度和效率。<br>
                    动机：目前的NeRF模型虽然在新颖视图合成方面取得了显著成果，但由于体积渲染过程，其推理速度非常慢，限制了其在资源受限设备上的应用。<br>
                    方法：提出一种高效的神经网络模型，该模型在移动设备上实时运行，用于神经渲染。与现有的工作不同，我们引入了一种新颖的网络架构，该架构在移动设备上运行效率高、延迟低且体积小。<br>
                    效果：我们的模型在移动设备上实现了高分辨率生成，同时保持了对合成和真实世界场景的实时推理，例如，在iPhone 13上渲染一张1008x756的真实3D场景图像只需18.04ms。此外，我们的模型实现了与NeRF相当的图像质量，并优于MobileNeRF（在真实向前看数据集上的PSNR为26.15 vs. 25.91）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. We follow the setting of NeLF to train our network. Unlike existing works, we introduce a novel network architecture that runs efficiently on mobile devices with low latency and small size, i.e., saving 15x   24x storage compared with MobileNeRF. Our model achieves high-resolution generation while maintaining real-time inference for both synthetic and real-world scenes on mobile devices, e.g., 18.04ms (iPhone 13) for rendering one 1008x756 image of real 3D scenes. Additionally, we achieve similar image quality as NeRF and better quality than MobileNeRF (PSNR 26.15 vs. 25.91 on the real-world forward-facing dataset).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1625.Deep Dive Into Gradients: Better Optimization for 3D Object Detection With Gradient-Corrected IoU Supervision</span><br>
                <span class="as">Ming, QiandMiao, LingjuanandMa, ZheandZhao, LinandZhou, ZhiqiangandHuang, XuhuiandChen, YuanpeiandGuo, Yufei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ming_Deep_Dive_Into_Gradients_Better_Optimization_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5136-5145.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D物体检测中，使用交并比（IoU）作为评估指标时，存在梯度异常和收敛速度慢的问题。<br>
                    动机：为了解决这些问题，提出了一种改进的IoU损失函数——梯度校正IoU（GCIoU）损失。<br>
                    方法：设计了一种梯度校正策略，使3D IoU损失具有合理的梯度，确保模型在训练初期快速收敛，并在后期实现精细的边界框调整。同时，引入了梯度缩放策略，以适应不同尺度物体的优化步长。<br>
                    效果：通过在KITTI数据集上的实验，证明了该方法的优越性，实现了更稳定的效果提升和更快的模型收敛。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Intersection-over-Union (IoU) is the most popular metric to evaluate regression performance in 3D object detection. Recently, there are also some methods applying IoU to the optimization of 3D bounding box regression. However, we demonstrate through experiments and mathematical proof that the 3D IoU loss suffers from abnormal gradient w.r.t. angular error and object scale, which further leads to slow convergence and suboptimal regression process, respectively. In this paper, we propose a Gradient-Corrected IoU (GCIoU) loss to achieve fast and accurate 3D bounding box regression. Specifically, a gradient correction strategy is designed to endow 3D IoU loss with a reasonable gradient. It ensures that the model converges quickly in the early stage of training, and helps to achieve fine-grained refinement of bounding boxes in the later stage. To solve suboptimal regression of 3D IoU loss for objects at different scales, we introduce a gradient rescaling strategy to adaptively optimize the step size. Finally, we integrate GCIoU Loss into multiple models to achieve stable performance gains and faster model convergence. Experiments on KITTI dataset demonstrate superiority of the proposed method. The code is available at https://github.com/ming71/GCIoU-loss.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1626.MobileOne: An Improved One Millisecond Mobile Backbone</span><br>
                <span class="as">Vasu, PavanKumarAnasosaluandGabriel, JamesandZhu, JeffandTuzel, OncelandRanjan, Anurag</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7907-7917.png><br>
            
            <span class="tt"><span class="t0">研究问题：优化神经网络的度量标准如FLOPs或参数数量，可能无法准确反映其在移动设备上的延迟。<br>
                    动机：针对此问题，我们通过在移动设备上部署几种对移动友好的网络进行广泛分析，并设计了一种高效的MobileOne网络。<br>
                    方法：我们对现有的高效神经网络进行了架构和优化瓶颈的分析，并提出相应的解决方案。然后，我们设计了一种新的、高效的神经网络Backbone MobileOne，并在iPhone12上实现了低于1ms的推理时间。<br>
                    效果：实验结果表明，MobileOne在效率架构中取得了最先进的性能，同时在移动设备上的运行速度比现有技术快很多倍。此外，我们的模型在图像分类、目标检测和语义分割等多种任务上都表现出良好的泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Efficient neural network backbones for mobile devices are often optimized for metrics such as FLOPs or parameter count. However, these metrics may not correlate well with latency of the network when deployed on a mobile device. Therefore, we perform extensive analysis of different metrics by deploying several mobile-friendly networks on a mobile device. We identify and analyze architectural and optimization bottlenecks in recent efficient neural networks and provide ways to mitigate these bottlenecks. To this end, we design an efficient backbone MobileOne, with variants achieving an inference time under 1 ms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that MobileOne achieves state-of-the-art performance within the efficient architectures while being many times faster on mobile. Our best model obtains similar performance on ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3% better top-1 accuracy on ImageNet than EfficientNet at similar latency. Furthermore, we show that our model generalizes to multiple tasks -- image classification, object detection, and semantic segmentation with significant improvements in latency and accuracy as compared to existing efficient architectures when deployed on a mobile device.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1627.AccelIR: Task-Aware Image Compression for Accelerating Neural Restoration</span><br>
                <span class="as">Ye, JuncheolandYeo, HyunhoandPark, JinwooandHan, Dongsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_AccelIR_Task-Aware_Image_Compression_for_Accelerating_Neural_Restoration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18216-18226.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化图像压缩以提高图像恢复（IR）质量并减少计算开销。<br>
                    动机：尽管深度神经网络在图像恢复方面表现出色，但需要大量的计算资源。现有的方法主要通过设计新的神经网络或参数剪枝来解决这个问题，但大多数方法没有考虑到压缩对图像恢复质量的影响。<br>
                    方法：提出了一种名为AccelIR的框架，该框架通过考虑整个图像恢复任务的端到端管道来优化图像压缩。AccelIR根据压缩对图像恢复质量的影响，优化了图像块内的压缩级别。然后，它在压缩后的图像上运行轻量级的图像恢复网络，有效地减少了图像恢复的计算开销，同时保持了相同的图像质量和大小。<br>
                    效果：通过使用七种不同的图像恢复网络进行广泛的评估，发现AccelIR可以平均将超分辨率、去噪和去模糊的计算开销分别降低49%、29%和32%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, deep neural networks have been successfully applied for image restoration (IR) (e.g., super-resolution, de-noising, de-blurring). Despite their promising performance, running IR networks requires heavy computation. A large body of work has been devoted to addressing this issue by designing novel neural networks or pruning their parameters. However, the common limitation is that while images are saved in a compressed format before being enhanced by IR, prior work does not consider the impact of compression on the IR quality. In this paper, we present AccelIR, a framework that optimizes image compression considering the end-to-end pipeline of IR tasks. AccelIR encodes an image through IR-aware compression that optimizes compression levels across image blocks within an image according to the impact on the IR quality. Then, it runs a lightweight IR network on the compressed image, effectively reducing IR computation, while maintaining the same IR quality and image size. Our extensive evaluation using seven IR networks shows that AccelIR can reduce the computing overhead of super-resolution, de-nosing, and de-blurring by 49%, 29%, and 32% on average, respectively</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1628.One-Shot Model for Mixed-Precision Quantization</span><br>
                <span class="as">Koryakovskiy, IvanandYakovleva, AlexandraandBuchnev, ValentinandIsaev, TemurandOdinokikh, Gleb</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Koryakovskiy_One-Shot_Model_for_Mixed-Precision_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7939-7949.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地压缩神经网络模型，同时保持其性能？<br>
                    动机：现代硬件支持混合精度模式下的量化，可以大大提高压缩率，但需要寻找最优位宽，这是一项具有挑战性的任务。<br>
                    方法：本文提出了一种基于梯度优化的张量位宽查找方法，包括理论推导和一种新的一触即发方法，可以在O(1)时间内找到一组多样化的帕累托前沿架构。<br>
                    效果：在两个分类和超分辨率模型上验证了该方法，预测模型性能与实际模型性能之间的相关分数超过0.93。帕累托前沿架构选择简单直观，只需进行20到40次超级网络评估，这是目前我们所知的最佳结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural network quantization is a popular approach for model compression. Modern hardware supports quantization in mixed-precision mode, which allows for greater compression rates but adds the challenging task of searching for the optimal bit width. The majority of existing searchers find a single mixed-precision architecture. To select an architecture that is suitable in terms of performance and resource consumption, one has to restart searching multiple times. We focus on a specific class of methods that find tensor bit width using gradient-based optimization. First, we theoretically derive several methods that were empirically proposed earlier. Second, we present a novel One-Shot method that finds a diverse set of Pareto-front architectures in O(1) time. For large models, the proposed method is 5 times more efficient than existing methods. We verify the method on two classification and super-resolution models and show above 0.93 correlation score between the predicted and actual model performance. The Pareto-front architecture selection is straightforward and takes only 20 to 40 supernet evaluations, which is the new state-of-the-art result to the best of our knowledge.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1629.Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the Decision Boundary</span><br>
                <span class="as">Chen, MinandGao, WeizhuoandLiu, GaoyangandPeng, KaiandWang, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7766-7775.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地让机器学习模型忘记一部分训练数据及其谱系，以满足“被遗忘权”和清除有毒数据的实际需求。<br>
                    动机：现有的深度学习网络（DNNs）的机器学习方法通过清洗模型参数来消除遗忘数据的影响，但由于参数空间的维度过大，这种方法的成本过高。<br>
                    方法：本文将注意力从参数空间转移到DNN模型的决策空间，提出了边界学习（Boundary Unlearning），这是一种快速而有效的从已训练的DNN模型中完全忘记一个类别的方法。其核心思想是通过改变原始DNN模型的决策边界，模仿从零开始重新训练的模型的决策行为。<br>
                    效果：在CIFAR-10和Vggface2数据集上广泛评估了边界学习的效果，结果显示，边界学习可以在图像分类和面部识别任务上有效地忘记遗忘的类别，与从零开始重新训练相比，预计速度提高了17倍和19倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The practical needs of the "right to be forgotten" and poisoned data removal call for efficient machine unlearning techniques, which enable machine learning models to unlearn, or to forget a fraction of training data and its lineage. Recent studies on machine unlearning for deep neural networks (DNNs) attempt to destroy the influence of the forgetting data by scrubbing the model parameters. However, it is prohibitively expensive due to the large dimension of the parameter space. In this paper, we refocus our attention from the parameter space to the decision space of the DNN model, and propose Boundary Unlearning, a rapid yet effective way to unlearn an entire class from a trained DNN model. The key idea is to shift the decision boundary of the original DNN model to imitate the decision behavior of the model retrained from scratch. We develop two novel boundary shift methods, namely Boundary Shrink and Boundary Expanding, both of which can rapidly achieve the utility and privacy guarantees. We extensively evaluate Boundary Unlearning on CIFAR-10 and Vggface2 datasets, and the results show that Boundary Unlearning can effectively forget the forgetting class on image classification and face recognition tasks, with an expected speed-up of 17x and 19x, respectively, compared with retraining from the scratch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1630.Latency Matters: Real-Time Action Forecasting Transformer</span><br>
                <span class="as">Girase, HarshayuandAgarwal, NakulandChoi, ChihoandMangalam, Karttikeya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18759-18769.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现低延迟、高性能的实时动作预测？<br>
                    动机：现有的方法在实时动作预测中，往往存在计算量大、延迟高的问题。<br>
                    方法：提出了一种名为RAFTformer的实时动作预测转换器，该模型采用两阶段全转换器架构，包括一个处理高分辨率短片段的视频转换器主干和一个通过时间聚合多个短片段信息来覆盖长期视野的头部转换器编码器。同时，还提出了一种自监督的洗牌因果掩蔽方案以提高训练过程中的模型泛化能力。<br>
                    效果：实验结果表明，RAFTformer的推理延迟比现有工作小9倍，且在相同的预测准确度下，其网络设计简单，训练计算量和参数分别减少了94%和90%，在离线设置中以Top-5 recall (T5R)为指标，性能超过了现有最先进的基线约4.9个点；在实时设置中，其在EPIC-Kitchens-100数据集上的性能超过现有工作高达4.4个T5R点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present RAFTformer, a real-time action forecasting transformer for latency aware real-world action forecasting applications. RAFTformer is a two-stage fully transformer based architecture which consists of a video transformer backbone that operates on high resolution, short range clips and a head transformer encoder that temporally aggregates information from multiple short range clips to span a long-term horizon. Additionally, we propose a self-supervised shuffled causal masking scheme to improve model generalization during training. Finally, we also propose a real-time evaluation setting that directly couples model inference latency to overall forecasting performance and brings forth an hitherto overlooked trade-off between latency and action forecasting performance. Our parsimonious network design facilitates RAFTformer inference latency to be 9x smaller than prior works at the same forecasting accuracy. Owing to its two-staged design, RAFTformer uses 94% less training compute and 90% lesser training parameters to outperform prior state-of-the-art baselines by 4.9 points on EGTEA Gaze+ and by 1.4 points on EPIC-Kitchens-100 dataset, as measured by Top-5 recall (T5R) in the offline setting. In the real-time setting, RAFTformer outperforms prior works by an even greater margin of upto 4.4 T5R points on the EPIC-Kitchens-100 dataset. Project Webpage: https://karttikeya.github.io/publication/RAFTformer/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1631.CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning</span><br>
                <span class="as">Smith, JamesSealeandKarlinsky, LeonidandGutta, VyshnaviandCascante-Bonilla, PaolaandKim, DonghyunandArbelle, AssafandPanda, RameswarandFeris, RogerioandKira, Zsolt</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11909-11919.png><br>
            
            <span class="tt"><span class="t0">研究问题：计算机视觉模型在从持续变化的训练数据中学习新概念时，会遭遇灾难性遗忘现象。<br>
                    动机：传统的解决连续学习问题的方法需要大量回顾之前看过的数据，这增加了内存成本并可能违反数据隐私。<br>
                    方法：提出一种新的基于注意力的端到端键-查询机制，通过学习一组提示组件，然后与输入条件权重组装以生成输入条件的提示。<br>
                    效果：实验表明，这种方法在现有的基准测试上比当前最先进的DualPrompt方法平均最终准确率高出4.5%，并且在包含类别增量和领域增量任务转移的连续学习基准测试上，准确率高出4.4%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are assembled with input-conditioned weights to produce input-conditioned prompts, resulting in a novel attention-based end-to-end key-query scheme. Our experiments show that we outperform the current SOTA method DualPrompt on established benchmarks by as much as 4.5% in average final accuracy. We also outperform the state of art by as much as 4.4% accuracy on a continual learning benchmark which contains both class-incremental and domain-incremental task shifts, corresponding to many practical settings. Our code is available at https://github.com/GT-RIPL/CODA-Prompt</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1632.EfficientSCI: Densely Connected Network With Space-Time Factorization for Large-Scale Video Snapshot Compressive Imaging</span><br>
                <span class="as">Wang, LishunandCao, MiaoandYuan, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_EfficientSCI_Densely_Connected_Network_With_Space-Time_Factorization_for_Large-Scale_Video_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18477-18486.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video snapshot compressive imaging (SCI) uses a two-dimensional detector to capture consecutive video frames during a single exposure time. Following this, an efficient reconstruction algorithm needs to be designed to reconstruct the desired video frames. Although recent deep learning-based state-of-the-art (SOTA) reconstruction algorithms have achieved good results in most tasks, they still face the following challenges due to excessive model complexity and GPU memory limitations: 1) these models need high computational cost, and 2) they are usually unable to reconstruct large-scale video frames at high compression ratios. To address these issues, we develop an efficient network for video SCI by using dense connections and space-time factorization mechanism within a single residual block, dubbed EfficientSCI. The EfficientSCI network can well establish spatial-temporal correlation by using convolution in the spatial domain and Transformer in the temporal domain, respectively. We are the first time to show that an UHD color video with high compression ratio can be reconstructed from a snapshot 2D measurement using a single end-to-end deep learning model with PSNR above 32 dB. Extensive results on both simulation and real data show that our method significantly outperforms all previous SOTA algorithms with better real-time performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1633.Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures</span><br>
                <span class="as">Iofinova, EugeniaandPeste, AlexandraandAlistarh, Dan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Iofinova_Bias_in_Pruned_Vision_Models_In-Depth_Analysis_and_Countermeasures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24364-24373.png><br>
            
            <span class="tt"><span class="t0">研究问题：神经网络剪枝可能会在压缩模型的输出中引发或加剧偏见，但这一现象与神经网络剪枝之间的关系尚未得到充分理解。<br>
                    动机：尽管存在证据表明这种现象的存在，但是神经网络剪枝和引发的偏见之间的关系尚不清楚。<br>
                    方法：在本研究中，我们对计算机视觉中的卷积神经网络进行了系统的研究，以了解和描述这种现象。我们首先展示了实际上可以获得高度稀疏的模型，例如保留少于10%的权重，这些模型在准确性上不会降低，也不会显著增加偏见。同时，我们还发现，在更高的稀疏度下，剪枝后的模型在其输出中表现出更高的不确定性，以及增加的相关性，这与增加的偏见直接相关。<br>
                    效果：我们提出了易于使用的准则，仅基于未压缩的模型就可以确定是否会因剪枝而增加偏见，并识别出哪些样本最容易出现压缩后的有偏预测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which, based only on the uncompressed model, establish whether bias will increase with pruning, and identify the samples most susceptible to biased predictions post-compression.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1634.Boundary-Aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval</span><br>
                <span class="as">Pan, TanandXu, FurongandYang, XudongandHe, SifengandJiang, ChenandGuo, QingpeiandQian, FengandZhang, XiaoboandCheng, YuanandYang, LeiandChu, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Boundary-Aware_Backward-Compatible_Representation_via_Adversarial_Learning_in_Image_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15201-15210.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高新旧模型之间的兼容性，同时保持检索性能不受影响。<br>
                    动机：传统的模型更新方式需要重新计算数据库中所有图像的嵌入，过程耗时长。<br>
                    方法：提出一种具有弹性边界约束的对抗性后向兼容训练（AdvBCT）方法，通过对抗学习最小化新模型和旧模型嵌入分布的差异，同时在训练过程中加入弹性边界约束以提高兼容性和区分度。<br>
                    效果：在GLDv2、ROxford和RParis数据集上的实验表明，该方法在兼容性和区分度上都优于其他后向兼容训练方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image retrieval plays an important role in the Internet world. Usually, the core parts of mainstream visual retrieval systems include an online service of the embedding model and a large-scale vector database. For traditional model upgrades, the old model will not be replaced by the new one until the embeddings of all the images in the database are re-computed by the new model, which takes days or weeks for a large amount of data. Recently, backward-compatible training (BCT) enables the new model to be immediately deployed online by making the new embeddings directly comparable to the old ones. For BCT, improving the compatibility of two models with less negative impact on retrieval performance is the key challenge. In this paper, we introduce AdvBCT, an Adversarial Backward-Compatible Training method with an elastic boundary constraint that takes both compatibility and discrimination into consideration. We first employ adversarial learning to minimize the distribution disparity between embeddings of the new model and the old model. Meanwhile, we add an elastic boundary constraint during training to improve compatibility and discrimination efficiently. Extensive experiments on GLDv2, Revisited Oxford (ROxford), and Revisited Paris (RParis) demonstrate that our method outperforms other BCT methods on both compatibility and discrimination. The implementation of AdvBCT will be publicly available at https://github.com/Ashespt/AdvBCT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1635.Sliced Optimal Partial Transport</span><br>
                <span class="as">Bai, YikunandSchmitzer, BernhardandThorpe, MatthewandKolouri, Soheil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Sliced_Optimal_Partial_Transport_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13681-13690.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决最优传输（OT）在机器学习、数据科学和计算机视觉中应用受限的问题，特别是当源和目标测量的总质量相等时。<br>
                    动机：最优传输（OT）的核心假设是源和目标测量的总质量相等，这限制了其在一些情况下的应用。为了解决这个问题，作者提出了最优部分传输（OPT）。然而，OPT的计算依赖于解决高维线性规划问题，这可能会变得计算上不可行。<br>
                    方法：本文提出了一种有效的算法，用于计算一维两个非负测量之间的最优部分传输问题。然后，作者借鉴切片最优传输距离的思想，利用切片定义切片最优部分传输距离。<br>
                    效果：通过各种数值实验，作者证明了切片最优部分传输方法在计算效率和准确性方面的优势。特别是在有噪声的点云注册中，作者展示了他们提出的切片最优部分传输方法的应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1636.NVTC: Nonlinear Vector Transform Coding</span><br>
                <span class="as">Feng, RunsenandGuo, ZongyuandLi, WeipingandChen, Zhibo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_NVTC_Nonlinear_Vector_Transform_Coding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6101-6110.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高神经网络图像压缩的性能。<br>
                    动机：尽管现代神经网络显著提高了标量量化的压缩性能，但与矢量量化相比，仍存在无法逾越的差距。<br>
                    方法：提出一种新的神经网络图像压缩框架——非线性矢量变换编码（NVTC），通过多阶段量化策略和非线性矢量变换解决矢量量化的复杂性问题，并在潜在空间应用熵约束的矢量量化自适应确定量化边界以优化联合率失真。<br>
                    效果：实验表明，NVTC在率失真性能、解码速度和模型大小方面均优于以往的NTC方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In theory, vector quantization (VQ) is always better than scalar quantization (SQ) in terms of rate-distortion (R-D) performance. Recent state-of-the-art methods for neural image compression are mainly based on nonlinear transform coding (NTC) with uniform scalar quantization, overlooking the benefits of VQ due to its exponentially increased complexity. In this paper, we first investigate on some toy sources, demonstrating that even if modern neural networks considerably enhance the compression performance of SQ with nonlinear transform, there is still an insurmountable chasm between SQ and VQ. Therefore, revolving around VQ, we propose a novel framework for neural image compression named Nonlinear Vector Transform Coding (NVTC). NVTC solves the critical complexity issue of VQ through (1) a multi-stage quantization strategy and (2) nonlinear vector transforms. In addition, we apply entropy-constrained VQ in latent space to adaptively determine the quantization boundaries for joint rate-distortion optimization, which improves the performance both theoretically and experimentally. Compared to previous NTC approaches, NVTC demonstrates superior rate-distortion performance, faster decoding speed, and smaller model size. Our code is available at https://github.com/USTC-IMCL/NVTC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1637.On the Effectiveness of Partial Variance Reduction in Federated Learning With Heterogeneous Data</span><br>
                <span class="as">Li, BoandSchmidt, MikkelN.andAlstr{\o</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_On_the_Effectiveness_of_Partial_Variance_Reduction_in_Federated_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3964-3973.png><br>
            
            <span class="tt"><span class="t0">研究问题：联邦学习中客户端数据异构性是一个关键挑战。<br>
                    动机：尽管已有的方法在凸问题或简单的非凸问题上实现了快速收敛，但在深度神经网络等过参数化模型上的性能却不尽如人意。<br>
                    方法：我们重新审视了深度神经网络中广泛使用的FedAvg算法，发现虽然特征提取层被FedAvg有效地学习，但客户端之间最终分类层的大量差异阻碍了性能。因此，我们提出只在最后几层进行模型漂移的修正。<br>
                    效果：实验结果表明，这种方法在类似的或更低的通信成本下，显著优于现有的基准测试。我们还提供了该算法的收敛速度证明。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Data heterogeneity across clients is a key challenge in federated learning. Prior works address this by either aligning client and server models or using control variates to correct client model drift. Although these methods achieve fast convergence in convex or simple non-convex problems, the performance in over-parameterized models such as deep neural networks is lacking. In this paper, we first revisit the widely used FedAvg algorithm in a deep neural network to understand how data heterogeneity influences the gradient updates across the neural network layers. We observe that while the feature extraction layers are learned efficiently by FedAvg, the substantial diversity of the final classification layers across clients impedes the performance. Motivated by this, we propose to correct model drift by variance reduction only on the final layers. We demonstrate that this significantly outperforms existing benchmarks at a similar or lower communication cost. We furthermore provide proof for the convergence rate of our algorithm.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1638.LVQAC: Lattice Vector Quantization Coupled With Spatially Adaptive Companding for Efficient Learned Image Compression</span><br>
                <span class="as">Zhang, XiandWu, Xiaolin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_LVQAC_Lattice_Vector_Quantization_Coupled_With_Spatially_Adaptive_Companding_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10239-10248.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化端到端图像压缩神经网络，提高其率失真性能。<br>
                    动机：大多数现有的端到端优化方法采用均匀标量量化器而非信息论上最优的矢量量化器，这限制了其性能。<br>
                    方法：提出一种新颖的格矢量量化与空间自适应压扩映射（LVQAC）方案。LVQ能更好地利用特征间的依赖关系，且计算复杂度与均匀标量量化相当。同时，为了提高LVQ对源统计的适应性，将其与空间自适应压扩映射相结合。<br>
                    效果：实验表明，对于任何端到端CNN图像压缩模型，用LVQAC替换均匀量化器可以在不明显增加模型复杂度的情况下获得更好的率失真性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, numerous end-to-end optimized image compression neural networks have been developed and proved themselves as leaders in rate-distortion performance. The main strength of these learnt compression methods is in powerful nonlinear analysis and synthesis transforms that can be facilitated by deep neural networks. However, out of operational expediency, most of these end-to-end methods adopt uniform scalar quantizers rather than vector quantizers, which are information-theoretically optimal. In this paper, we present a novel Lattice Vector Quantization scheme coupled with a spatially Adaptive Companding (LVQAC) mapping. LVQ can better exploit the inter-feature dependencies than scalar uniform quantization while being computationally almost as simple as the latter. Moreover, to improve the adaptability of LVQ to source statistics, we couple a spatially adaptive companding (AC) mapping with LVQ. The resulting LVQAC design can be easily embedded into any end-to-end optimized image compression system. Extensive experiments demonstrate that for any end-to-end CNN image compression models, replacing uniform quantizer by LVQAC achieves better rate-distortion performance without significantly increasing the model complexity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1639.Genie: Show Me the Data for Quantization</span><br>
                <span class="as">Jeon, YongkweonandLee, ChungmanandKim, Ho-young</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeon_Genie_Show_Me_the_Data_for_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12064-12073.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练模型的参数，在数据不可用的情况下，进行轻量级深度神经网络的零样本量化。<br>
                    动机：由于成本和隐私等问题，数据不可用的情况时有发生。零样本量化是一种有前景的方法，可以在这种情况下开发轻量级的深度神经网络。<br>
                    方法：通过利用预训练模型中批量归一化层的学习参数（u和sigma），生成合成数据，然后从预训练模型（教师）提炼知识到量化模型（学生），使量化模型可以用合成数据集进行优化。同时，提出了一种后训练量化方案，可以在几个小时内产生高质量的量化网络。<br>
                    效果：提出的GENIE框架能生成适合量化的数据，使用这些数据，我们可以在没有真实数据集的情况下生成鲁棒的量化模型，其性能与少样本量化相当。结合后训练量化算法，我们可以弥合零样本和少样本量化之间的差距，显著提高量化性能，从而获得一种独特的最先进的零样本量化方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters (u and sigma) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called GENIE that generates data suited for quantization. With the data synthesized by GENIE, we can produce robust quantized models without real datasets, which is comparable to few-shot quantization. We also propose a post-training quantization algorithm to enhance the performance of quantized models. By combining them, we can bridge the gap between zero-shot and few-shot quantization while significantly improving the quantization performance compared to that of existing approaches. In other words, we can obtain a unique state-of-the-art zero-shot quantization approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1640.Multi-Agent Automated Machine Learning</span><br>
                <span class="as">Wang, ZhaozhiandSu, KefanandZhang, JianandJia, HuizhuandYe, QixiangandXie, XiaodongandLu, Zongqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Agent_Automated_Machine_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11960-11969.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地处理自动化机器学习（AutoML）中各模块的联合优化。<br>
                    动机：现有的自动化机器学习系统在优化过程中，各模块间的协作并不理想。<br>
                    方法：提出多智能体自动化机器学习（MA2ML），将每个学习模块（如数据增强、神经结构搜索或超参数优化）视为一个智能体，以最终性能为奖励，形成一个多智能体强化学习问题。通过明确分配每个智能体的边际贡献来提高模块间的协作，并结合离线学习以提高搜索效率。<br>
                    效果：实验证明，MA2ML在满足计算成本限制的情况下，实现了ImageNet上的最先进的Top-1准确率，例如，当浮点运算次数少于6亿/8亿时，准确率分别为79.7%/80.5%。大量的消融研究表明，MA2ML的信用分配和离线学习确实带来了效益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML). MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. MA2ML explicitly assigns credit to each agent according to its marginal contribution to enhance cooperation among modules, and incorporates off-policy learning to improve search efficiency. Theoretically, MA2ML guarantees monotonic improvement of joint optimization. Extensive experiments show that MA2ML yields the state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost, e.g., 79.7%/80.5% with FLOPs fewer than 600M/800M. Extensive ablation studies verify the benefits of credit assignment and off-policy learning of MA2ML.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1641.StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition</span><br>
                <span class="as">Shen, YanqingandZhou, SanpingandFu, JingwenandWang, RuotongandChen, ShitaoandZheng, Nanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_StructVPR_Distill_Structural_Knowledge_With_Weighting_Samples_for_Visual_Place_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11217-11226.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉地点识别（VPR）问题，即如何从RGB图像中提取稳定的全局特征并利用空间结构信息提高性能。<br>
                    动机：由于现有的训练框架限制，大多数基于深度学习的VPR方法无法充分提取稳定的全局特征，需要依赖耗时的重排步骤来利用空间结构信息以提高性能。<br>
                    方法：本文提出了一种新的VPR训练架构StructVPR，通过将分割图像作为更具决定性的结构知识输入CNN网络，并应用知识蒸馏技术避免在线分割和测试中的seg-branch推理，从而增强RGB全局特征中的知识结构，提高特征稳定性。<br>
                    效果：实验结果表明，StructVPR在几个基准测试上取得了令人印象深刻的性能，仅使用全局检索就能超越许多两阶段方法。在添加额外的重排后，StructVPR实现了最先进的性能，同时保持了较低的计算成本。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual place recognition (VPR) is usually considered as a specific image retrieval problem. Limited by existing training frameworks, most deep learning-based works cannot extract sufficiently stable global features from RGB images and rely on a time-consuming re-ranking step to exploit spatial structural information for better performance. In this paper, we propose StructVPR, a novel training architecture for VPR, to enhance structural knowledge in RGB global features and thus improve feature stability in a constantly changing environment. Specifically, StructVPR uses segmentation images as a more definitive source of structural knowledge input into a CNN network and applies knowledge distillation to avoid online segmentation and inference of seg-branch in testing. Considering that not all samples contain high-quality and helpful knowledge, and some even hurt the performance of distillation, we partition samples and weigh each sample's distillation loss to enhance the expected knowledge precisely. Finally, StructVPR achieves impressive performance on several benchmarks using only global retrieval and even outperforms many two-stage approaches by a large margin. After adding additional re-ranking, ours achieves state-of-the-art performance while maintaining a low computational cost.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1642.Elastic Aggregation for Federated Optimization</span><br>
                <span class="as">Chen, DengshengandHu, JieandTan, VinceJunkaiandWei, XiaomingandWu, Enhua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Elastic_Aggregation_for_Federated_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12187-12197.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在数据异构（非独立同分布）的情况下，通过联邦学习进行隐私保护的神经网络模型训练。<br>
                    动机：现有的联邦学习优化器FedAvg在数据异构时会出现客户端漂移，导致训练不稳定且收敛缓慢。<br>
                    方法：提出一种新的聚合方法——弹性聚合，该方法根据参数敏感性自适应地插值客户端模型，通过计算每个参数变化时总体预测函数输出的变化来测量参数敏感性。<br>
                    效果：实验结果和分析结果表明，弹性聚合在凸和非凸设置中都能实现有效训练，对客户端异构性完全无感知，对大量客户端、部分参与和不平衡数据具有鲁棒性，同时与其他联邦学习优化器配合良好，能显著提高性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning enables the privacy-preserving training of neural network models using real-world data across distributed clients. FedAvg has become the preferred optimizer for federated learning because of its simplicity and effectiveness. FedAvg uses naive aggregation to update the server model, interpolating client models based on the number of instances used in their training. However, naive aggregation suffers from client-drift when the data is heterogenous (non-IID), leading to unstable and slow convergence. In this work, we propose a novel aggregation approach, elastic aggregation, to overcome these issues. Elastic aggregation interpolates client models adaptively according to parameter sensitivity, which is measured by computing how much the overall prediction function output changes when each parameter is changed. This measurement is performed in an unsupervised and online manner. Elastic aggregation reduces the magnitudes of updates to the more sensitive parameters so as to prevent the server model from drifting to any one client distribution, and conversely boosts updates to the less sensitive parameters to better explore different client distributions. Empirical results on real and synthetic data as well as analytical results show that elastic aggregation leads to efficient training in both convex and non-convex settings, while being fully agnostic to client heterogeneity and robust to large numbers of clients, partial participation, and imbalanced data. Finally, elastic aggregation works well with other federated optimizers and achieves significant improvements across the board.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1643.CABM: Content-Aware Bit Mapping for Single Image Super-Resolution Network With Large Input</span><br>
                <span class="as">Tian, SenmaoandLu, MingandLiu, JiamingandGuo, YandongandChen, YurongandZhang, Shunli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_CABM_Content-Aware_Bit_Mapping_for_Single_Image_Super-Resolution_Network_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1756-1765.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地减少高分辨率图像超分辨率重建的计算和内存成本。<br>
                    动机：现有的方法将大输入分割成局部补丁，然后将SR补丁合并到输出中，这种方法需要为每个补丁分配一个子网，且存在过拟合和欠拟合的问题。<br>
                    方法：提出一种名为内容感知位映射（CABM）的新方法，该方法在训练过程中学习每层的比特选择器，并在训练后分析输入补丁的边缘信息与每层比特的关系，设计了一种边缘到比特的查找表策略，通过所有层的查找表确定SR网络的比特配置。<br>
                    效果：实验结果表明，该方法能够找到更好的比特配置，形成更高效的混合精度网络。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the development of high-definition display devices, the practical scenario of Super-Resolution (SR) usually needs to super-resolve large input like 2K to higher resolution (4K/8K). To reduce the computational and memory cost, current methods first split the large input into local patches and then merge the SR patches into the output. These methods adaptively allocate a subnet for each patch. Quantization is a very important technique for network acceleration and has been used to design the subnets. Current methods train an MLP bit selector to determine the propoer bit for each layer. However, they uniformly sample subnets for training, making simple subnets overfitted and complicated subnets underfitted. Therefore, the trained bit selector fails to determine the optimal bit. Apart from this, the introduced bit selector brings additional cost to each layer of the SR network. In this paper, we propose a novel method named Content-Aware Bit Mapping (CABM), which can remove the bit selector without any performance loss. CABM also learns a bit selector for each layer during training. After training, we analyze the relation between the edge information of an input patch and the bit of each layer. We observe that the edge information can be an effective metric for the selected bit. Therefore, we design a strategy to build an Edge-to-Bit lookup table that maps the edge score of a patch to the bit of each layer during inference. The bit configuration of SR network can be determined by the lookup tables of all layers. Our strategy can find better bit configuration, resulting in more efficient mixed precision networks. We conduct detailed experiments to demonstrate the generalization ability of our method. The code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1644.Generalizing Dataset Distillation via Deep Generative Prior</span><br>
                <span class="as">Cazenavette, GeorgeandWang, TongzhouandTorralba, AntonioandEfros, AlexeiA.andZhu, Jun-Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cazenavette_Generalizing_Dataset_Distillation_via_Deep_Generative_Prior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3739-3748.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的数据集蒸馏方法无法适应新的架构，并且无法扩展到高分辨率的数据集。<br>
                    动机：通过预训练的深度生成模型学习先验知识，用于合成蒸馏数据。<br>
                    方法：提出一种新的优化算法，将大量图像蒸馏成生成模型潜在空间中的少数几个中间特征向量。<br>
                    效果：该方法显著提高了所有设置中跨架构泛化的能力，并改进了现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite a recent upsurge of progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1645.Event-Based Shape From Polarization</span><br>
                <span class="as">Muglikar, ManasiandBauersfeld, LeonardandMoeys, DiederikPaulandScaramuzza, Davide</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Muglikar_Event-Based_Shape_From_Polarization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1547-1556.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于极化的形状恢复（SfP）方法存在速度-分辨率权衡问题，即牺牲测量的极化角度数量或由于帧率限制需要较长的采集时间，从而影响准确性或延迟。<br>
                    动机：本文利用事件相机解决这一权衡问题。<br>
                    方法：提出一种由线性极化器在高速旋转前的事件相机组成的设置。该方法使用旋转引起的连续事件流重建多个极化器角度的相对强度。<br>
                    效果：实验表明，该方法优于使用帧的物理基础基准，在合成和真实世界数据集上将平均绝对误差降低了25%。在现实世界中，我们发现挑战性条件（即生成的事件较少）会损害物理基础解决方案的性能。为克服这一问题，我们提出了一种学习基础的方法，即使在低事件速率下也能估计表面法线，从而在实际世界数据集上将物理基础方法提高了52%。所提出的系统实现了与50 fps相当的采集速度（>商用极化传感器的帧率两倍），同时保留了1MP的空间分辨率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>State-of-the-art solutions for Shape-from-Polarization (SfP) suffer from a speed-resolution tradeoff: they either sacrifice the number of polarization angles measured or necessitate lengthy acquisition times due to framerate constraints, thus compromising either accuracy or latency. We tackle this tradeoff using event cameras. Event cameras operate at microseconds resolution with negligible motion blur, and output a continuous stream of events that precisely measures how light changes over time asynchronously. We propose a setup that consists of a linear polarizer rotating at high speeds in front of an event camera. Our method uses the continuous event stream caused by the rotation to reconstruct relative intensities at multiple polarizer angles. Experiments demonstrate that our method outperforms physics-based baselines using frames, reducing the MAE by 25% in synthetic and real-world datasets. In the real world, we observe, however, that the challenging conditions (i.e., when few events are generated) harm the performance of physics-based solutions. To overcome this, we propose a learning-based approach that learns to estimate surface normals even at low event-rates, improving the physics-based approach by 52% on the real world dataset. The proposed system achieves an acquisition speed equivalent to 50 fps (>twice the framerate of the commercial polarization sensor) while retaining the spatial resolution of 1MP. Our evaluation is based on the first large-scale dataset for event-based SfP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1646.Making Vision Transformers Efficient From a Token Sparsification View</span><br>
                <span class="as">Chang, ShuningandWang, PichaoandLin, MingandWang, FanandZhang, DavidJunhaoandJin, RongandShou, MikeZheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_Making_Vision_Transformers_Efficient_From_a_Token_Sparsification_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6195-6205.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉转换器（ViTs）的二次计算复杂性对标记数量的限制，以及现有方法在效率、准确性和通用性上的问题。<br>
                    动机：现有的修剪冗余标记的方法通常会导致准确性大幅下降，且难以应用于局部视觉转换器，也无法作为下游任务的主干网络。<br>
                    方法：本文提出了一种新的语义标记视觉转换器（STViT），用于有效全局和局部视觉转换器，也可以修订为下游任务的主干网络。语义标记代表聚类中心，通过空间中的图像标记进行初始化和注意力恢复，可以自适应地表示全局或局部语义信息。由于聚类特性，少数几个语义标记可以达到与大量图像标记相同的效果。<br>
                    效果：实验结果表明，该方法在图像分类中取得了巨大成功，并在视频识别中进行了扩展。此外，设计了一个STViT-R(recovery)网络来恢复基于STViT的详细空间信息，使其能够执行下游任务，这是以前的标记稀疏化方法无法做到的。实验证明，该方法在物体检测和实例分割等下游任务中可以获得与原始网络相当的结果，主干网络的FLOPs减少了30%以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecovery) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1647.Post-Processing Temporal Action Detection</span><br>
                <span class="as">Nag, SauradipandZhu, XiatianandSong, Yi-ZheandXiang, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nag_Post-Processing_Temporal_Action_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18837-18845.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的时序动作检测（TAD）方法在将输入的可变长度视频转换为固定长度片段表示序列之前，通常需要进行预处理步骤，这会降低视频的推理分辨率，影响原始时间分辨率下的检测性能。<br>
                    动机：这种预处理步骤引入的时间量化误差会严重影响TAD的性能，但被现有方法大大忽视。<br>
                    方法：本文提出了一种无需模型重新设计和重新训练的新的模型无关后处理方法。具体来说，我们使用高斯分布对动作实例的开始和结束点进行建模，以实现子片段级别的时间边界推断。我们还引入了一种高效的泰勒展开近似法，称为高斯近似后处理（GAP）。<br>
                    效果：大量实验证明，我们的GAP可以在具有挑战性的ActivityNet和THUMOS基准上持续提高各种预训练的现成的TAD模型的性能（平均mAP分别提高了0.2%到0.7%和0.2%到0.5%）。这种性能提升已经相当显著，与新模型设计获得的提升相当。此外，GAP可以与模型训练相结合，进一步提高性能。重要的是，GAP可以实现更低的时间分辨率，以实现更高效的推理，有利于低资源应用。代码可在https://github.com/sauradip/GAP获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing Temporal Action Detection (TAD) methods typically take a pre-processing step in converting an input varying-length video into a fixed-length snippet representation sequence, before temporal boundary estimation and action classification. This pre-processing step would temporally downsample the video, reducing the inference resolution and hampering the detection performance in the original temporal resolution. In essence, this is due to a temporal quantization error introduced during the resolution downsampling and recovery. This could negatively impact the TAD performance, but is largely ignored by existing methods. To address this problem, in this work we introduce a novel model-agnostic post-processing method without model redesign and retraining. Specifically, we model the start and end points of action instances with a Gaussian distribution for enabling temporal boundary inference at a sub-snippet level. We further introduce an efficient Taylor-expansion based approximation, dubbed as Gaussian Approximated Post-processing (GAP). Extensive experiments demonstrate that our GAP can consistently improve a wide variety of pre-trained off-the-shelf TAD models on the challenging ActivityNet (+0.2% 0.7% in average mAP) and THUMOS (+0.2% 0.5% in average mAP) benchmarks. Such performance gains are already significant and highly comparable to those achieved by novel model designs. Also, GAP can be integrated with model training for further performance gain. Importantly, GAP enables lower temporal resolutions for more efficient inference, facilitating low-resource applications. The code is available in https://github.com/sauradip/GAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1648.MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID</span><br>
                <span class="as">Gu, JianyangandWang, KaiandLuo, HaoandChen, ChenandJiang, WeiandFang, YuqiangandZhang, ShanghangandYou, YangandZhao, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_MSINet_Twins_Contrastive_Search_of_Multi-Scale_Interaction_for_Object_ReID_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19243-19253.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决物体重识别（ReID）中任务特定架构对检索性能提升的问题。<br>
                    动机：尽管现有的物体重识别方法通过优化目标和搜索空间进行改进，但他们忽视了图像分类和ReID之间训练方案的差异。<br>
                    方法：本文提出了一种新颖的双胞胎对比机制（TCM），以提供更适合ReID架构搜索的监督。同时设计了一个多尺度交互（MSI）搜索空间来寻找多尺度特征之间的合理交互操作。此外，引入了空间对齐模块（SAM）以增强面对不同来源图像的注意力一致性。<br>
                    效果：在提出的NAS方案下，自动搜索出了一个特定的架构，命名为MSINet。大量实验证明，该方法在同域和跨域场景上都超过了最先进的ReID方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural Architecture Search (NAS) has been increasingly appealing to the society of object Re-Identification (ReID), for that task-specific architectures significantly improve the retrieval performance. Previous works explore new optimizing targets and search spaces for NAS ReID, yet they neglect the difference of training schemes between image classification and ReID. In this work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more appropriate supervision for ReID architecture search. TCM reduces the category overlaps between the training and validation data, and assists NAS in simulating real-world ReID training schemes. We then design a Multi-Scale Interaction (MSI) search space to search for rational interaction operations between multi-scale features. In addition, we introduce a Spatial Alignment Module (SAM) to further enhance the attention consistency confronted with images from different sources. Under the proposed NAS scheme, a specific architecture is automatically searched, named as MSINet. Extensive experiments demonstrate that our method surpasses state-of-the-art ReID methods on both in-domain and cross-domain scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1649.Memory-Friendly Scalable Super-Resolution via Rewinding Lottery Ticket Hypothesis</span><br>
                <span class="as">Lin, JinandLuo, XiaotongandHong, MingandQu, YanyunandXie, YuanandWu, Zongze</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Memory-Friendly_Scalable_Super-Resolution_via_Rewinding_Lottery_Ticket_Hypothesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14398-14407.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的动态可伸缩SR模型内存使用过大的问题，以及如何通过优化模型结构来提高其性能。<br>
                    动机：目前的动态可伸缩SR方法由于需要保存固定大小的多尺度模型，因此内存使用过大。受Lottery Tickets Hypothesis（LTH）在图像分类上成功的启发，我们探索了未结构化的可伸缩SR深度模型的存在，即找到极度稀疏的渐进收缩子网络，命名为winning tickets。<br>
                    方法：本文提出了一种内存友好的可伸缩SR框架（MSSR）。该框架只包含一个可伸缩模型，就可以覆盖不同大小的多个SR模型，而无需重新加载不同大小的SR模型。具体来说，MSSR由前向和后向阶段组成，前者用于模型压缩，后者用于模型扩展。在前向阶段，我们利用LTH和回卷权重逐步缩小SR模型，并形成嵌套集合的剪枝掩码。此外，我们还进行了随机自我蒸馏（SSD）以提升子网络的性能。在后向阶段，较小的SR模型可以通过根据在前向阶段获得的剪枝掩码恢复和微调剪枝参数进行扩展。<br>
                    效果：大量实验表明了MSSR的有效性。最小的子网络可以达到94%的稀疏性，并且优于比较的轻量级SR方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scalable deep Super-Resolution (SR) models are increasingly in demand, whose memory can be customized and tuned to the computational recourse of the platform. The existing dynamic scalable SR methods are not memory-friendly enough because multi-scale models have to be saved with a fixed size for each model. Inspired by the success of Lottery Tickets Hypothesis (LTH) on image classification, we explore the existence of unstructured scalable SR deep models, that is, we find gradual shrinkage sub-networks of extreme sparsity named winning tickets. In this paper, we propose a Memory-friendly Scalable SR framework (MSSR). The advantage is that only a single scalable model covers multiple SR models with different sizes, instead of reloading SR models of different sizes. Concretely, MSSR consists of the forward and backward stages, the former for model compression and the latter for model expansion. In the forward stage, we take advantage of LTH with rewinding weights to progressively shrink the SR model and the pruning-out masks that form nested sets. Moreover, stochastic self-distillation (SSD) is conducted to boost the performance of sub-networks. By stochastically selecting multiple depths, the current model inputs the selected features into the corresponding parts in the larger model and improves the performance of the current model based on the feedback results of the larger model. In the backward stage, the smaller SR model could be expanded by recovering and fine-tuning the pruned parameters according to the pruning-out masks obtained in the forward. Extensive experiments show the effectiveness of MMSR. The smallest-scale sub-network could achieve the sparsity of 94% and outperforms the compared lightweight SR methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1650.YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors</span><br>
                <span class="as">Wang, Chien-YaoandBochkovskiy, AlexeyandLiao, Hong-YuanMark</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7464-7475.png><br>
            
            <span class="tt"><span class="t0">研究问题：实时物体检测是计算机视觉中最重要的研究主题之一。<br>
                    动机：随着关于架构优化和训练优化的新方法不断开发，我们发现在处理最新的最先进技术时出现了两个研究主题。<br>
                    方法：我们提出了一种可训练的“免费赠品”导向解决方案，将灵活高效的训练工具与提出的架构和复合缩放方法相结合。<br>
                    效果：YOLOv7在5FPS到120FPS的速度范围内超越了所有已知的物体检测器，并在GPU V100上以30FPS或更高的速度具有最高的精度56.8% AP，成为所有已知的实时物体检测器中准确度最高的。源代码已在https://github.com/WongKinYiu/yolov7上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-time object detection is one of the most important research topics in computer vision. As new approaches regarding architecture optimization and training optimization are continually being developed, we have found two research topics that have spawned when dealing with these latest state-of-the-art methods. To address the topics, we propose a trainable bag-of-freebies oriented solution. We combine the flexible and efficient training tools with the proposed architecture and the compound scaling method. YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 120 FPS and has the highest accuracy 56.8% AP among all known realtime object detectors with 30 FPS or higher on GPU V100. Source code is released in https://github.com/ WongKinYiu/yolov7.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1651.InstantAvatar: Learning Avatars From Monocular Video in 60 Seconds</span><br>
                <span class="as">Jiang, TianjianandChen, XuandSong, JieandHilliges, Otmar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16922-16932.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单目视频快速重建人类化身。<br>
                    动机：目前的单目神经网络化身重建方法效率低下，需要数小时的训练时间。<br>
                    方法：提出一种精心设计和构建的系统，利用新兴的神经场加速结构和动态场景的空空间跳跃策略，实现高效率的化身重建。<br>
                    效果：新方法比现有方法快130倍，可以在几分钟内训练完成，且重建质量和新姿态合成结果相当或更好。在相同的时间预算下，新方法的性能明显优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we take one step further towards real-world applicability of monocular neural avatar reconstruction by contributing InstantAvatar, a system that can reconstruct human avatars from a monocular video within seconds, and these avatars can be animated and rendered at an interactive rate. To achieve this efficiency we propose a carefully designed and engineered system, that leverages emerging acceleration structures for neural fields, in combination with an efficient empty-space skipping strategy for dynamic scenes. We also contribute an efficient implementation that we will make available for research purposes. Compared to existing methods, InstantAvatar converges 130x faster and can be trained in minutes instead of hours. It achieves comparable or even better reconstruction quality and novel pose synthesis results. When given the same time budget, our method significantly outperforms SoTA methods. InstantAvatar can yield acceptable visual quality in as little as 10 seconds training time. For code and more demo results, please refer to https://ait.ethz.ch/InstantAvatar.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1652.Learned Two-Plane Perspective Prior Based Image Resampling for Efficient Object Detection</span><br>
                <span class="as">Ghosh, AnuragandReddy, N.DineshandMertz, ChristophandNarasimhan, SrinivasaG.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ghosh_Learned_Two-Plane_Perspective_Prior_Based_Image_Resampling_for_Efficient_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13364-13373.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高自主导航和城市规模感知的实时高效感知？<br>
                    动机：为了解决实时检测性能的问题，现有的流感知方法利用了自适应采样技术。<br>
                    方法：本文提出了一种可学习的几何引导先验方法，该方法将3D场景的粗略几何信息（地面平面和上方平面）纳入图像重采样中，以实现高效的物体检测。<br>
                    效果：在自主导航方面，使用相同的探测器和尺度，该方法比最先进的方法提高了小物体的检测率和实时性能。对于固定交通摄像头，该方法可以在其他方法无法实现的图像尺度上检测小物体。在同一尺度下，该方法比朴素降采样和最先进的方法分别提高了195%和63%的小物体检测率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-time efficient perception is critical for autonomous navigation and city scale sensing. Orthogonal to architectural improvements, streaming perception approaches have exploited adaptive sampling improving real-time detection performance. In this work, we propose a learnable geometry-guided prior that incorporates rough geometry of the 3D scene (a ground plane and a plane above) to resample images for efficient object detection. This significantly improves small and far-away object detection performance while also being more efficient both in terms of latency and memory. For autonomous navigation, using the same detector and scale, our approach improves detection rate by +4.1 AP_S or +39% and in real-time performance by +5.3 sAP_S or +63% for small objects over state-of-the-art (SOTA). For fixed traffic cameras, our approach detects small objects at image scales other methods cannot. At the same scale, our approach improves detection of small objects by 195% (+12.5 AP_S) over naive-downsampling and 63% (+4.2 AP_S) over SOTA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1653.Train-Once-for-All Personalization</span><br>
                <span class="as">Chen, Hong-YouandLi, YandongandCui, YinandZhang, MingdaandChao, Wei-LunandZhang, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Train-Once-for-All_Personalization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11818-11827.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一种"个性化友好"的模型，仅根据任务描述就能适应不同用户的需求。<br>
                    动机：现有的方法需要先训练一个通用模型，然后进行分类选择，但这种方法可能不是最优的，因为模型的权重在没有个性化的情况下被冻结。<br>
                    方法：提出一种名为TAPER的框架，该框架只需训练一次，之后可以根据用户的任务描述定制不同的模型。TAPER学习一组“基础”模型和一个混合预测器，这样给定任务描述后，基础模型的权重（而不是预测结果！）可以实时组合成一个单一的“个性化”模型。<br>
                    效果：通过在多个识别任务上的大量实验，TAPER始终优于基线方法，实现了更高的个性化精度。此外，TAPER可以合成一个更小的模型，达到与大型通用模型相当的性能，使其对资源有限的终端设备更具“部署友好性”。有趣的是，即使没有用户的任务描述，TAPER仍然可以根据其过去的预测专门化到已部署的上下文中，使其更具“个性化友好性”。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of how to train a "personalization-friendly" model such that given only the task descriptions, the model can be adapted to different end-users' needs, e.g., for accurately classifying different subsets of objects. One baseline approach is to train a "generic" model for classifying a wide range of objects, followed by class selection. In our experiments, we however found it suboptimal, perhaps because the model's weights are kept frozen without being personalized. To address this drawback, we propose Train-once-for-All PERsonalization (TAPER), a framework that is trained just once and can later customize a model for different end-users given their task descriptions. TAPER learns a set of "basis" models and a mixer predictor, such that given the task description, the weights (not the predictions!) of the basis models can be on the fly combined into a single "personalized" model. Via extensive experiments on multiple recognition tasks, we show that TAPER consistently outperforms the baseline methods in achieving a higher personalized accuracy. Moreover, we show that TAPER can synthesize a much smaller model to achieve comparable performance to a huge generic model, making it "deployment-friendly" to resource-limited end devices. Interestingly, even without end-users' task descriptions, TAPER can still be specialized to the deployed context based on its past predictions, making it even more "personalization-friendly".</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1654.DepGraph: Towards Any Structural Pruning</span><br>
                <span class="as">Fang, GongfanandMa, XinyinandSong, MingliandMi, MichaelBiandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16091-16101.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现神经网络的结构剪枝，以移除结构相关的参数并加速模型？<br>
                    动机：现有的结构剪枝方法依赖于手动设计的参数分组方案，因此无法适用于新的网络架构。<br>
                    方法：提出了一种通用的全自动方法——依赖图（DepGraph），通过显式地建模层之间的依赖关系，对耦合的参数进行综合分组以便剪枝。<br>
                    效果：在多种网络架构和任务上进行了广泛评估，包括ResNe(X)t、DenseNet、MobileNet、视觉变换器等图像相关网络，GAT、DGCNN等图形相关网络，以及LSTM等语言相关网络。实验结果表明，即使在简单的归一化准则下，该方法也能持续产生令人满意的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and  fully automatic  method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning. In this work, we extensively evaluate our method on several architectures and tasks, including ResNe(X)t, DenseNet, MobileNet and Vision transformer for images, GAT for graph, DGCNN for 3D point cloud, alongside LSTM for language, and demonstrate that, even with a simple norm-based criterion, the proposed method consistently yields gratifying performances.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1655.Network Expansion for Practical Training Acceleration</span><br>
                <span class="as">Ding, NingandTang, YehuiandHan, KaiandXu, ChaoandWang, Yunhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Network_Expansion_for_Practical_Training_Acceleration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20269-20279.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何加速深度神经网络的训练过程。<br>
                    动机：随着深度学习网络和训练数据集的急剧增长，以及在视觉任务中基于变压器模型的普及，对GPU平台的压力越来越大，这些重型模型消耗大量的时间和计算资源。<br>
                    方法：提出一种通用的网络扩展方法来减少模型训练过程的实际时间成本。具体来说，利用密集模型的宽度和深度级别的稀疏性来加速深度神经网络的训练。首先，从原始密集模型中选择一个稀疏子网络作为训练的起点，然后该稀疏架构将在训练过程中逐渐扩展，最终成长为一个密集模型。<br>
                    效果：广泛的实验表明，我们的加速方法可以在普通的GPU设备上显著加快现代视觉模型的训练过程，性能下降可以忽略不计（例如，在ImageNet-1k上，ResNet-101快1.42倍，DeiT-base快1.34倍）。代码可在华为云和码云上获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, the sizes of deep neural networks and training datasets both increase drastically to pursue better performance in a practical sense. With the prevalence of transformer-based models in vision tasks, even more pressure is laid on the GPU platforms to train these heavy models, which consumes a large amount of time and computing resources as well. Therefore, it's crucial to accelerate the training process of deep neural networks. In this paper, we propose a general network expansion method to reduce the practical time cost of the model training process. Specifically, we utilize both width- and depth-level sparsity of dense models to accelerate the training of deep neural networks. Firstly, we pick a sparse sub-network from the original dense model by reducing the number of parameters as the starting point of training. Then the sparse architecture will gradually expand during the training procedure and finally grow into a dense one. We design different expanding strategies to grow CNNs and ViTs respectively, due to the great heterogeneity in between the two architectures. Our method can be easily integrated into popular deep learning frameworks, which saves considerable training time and hardware resources. Extensive experiments show that our acceleration method can significantly speed up the training process of modern vision models on general GPU devices with negligible performance drop (e.g. 1.42x faster for ResNet-101 and 1.34x faster for DeiT-base on ImageNet-1k). The code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/TrainingAcceleration/NetworkExpansion and https://gitee.com/mindspore/hub/blob/master/mshub_res/assets/noah-cvlab/gpu/1.8/networkexpansion_v1.0_imagenet2012.md.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1656.Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation</span><br>
                <span class="as">Huang, BoandChen, MingyangandWang, YiandLu, JundaandCheng, MinhaoandWang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Boosting_Accuracy_and_Robustness_of_Student_Models_via_Adaptive_Adversarial_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24668-24677.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高学生模型在边缘设备上的预测准确性和对抗鲁棒性。<br>
                    动机：现有的增强方案如对抗训练在压缩网络上表现有限，而教师-学生架构中的学生模型在边缘设备上更容易受到对抗攻击。<br>
                    方法：提出一种自适应对抗蒸馏（AdaAD）方法，让教师模型参与知识优化过程并与学生模型互动，以自适应地搜索内部结果。<br>
                    效果：与现有方法相比，AdaAD能显著提高学生模型在大多数场景下的预测准确性和对抗鲁棒性，特别是使用AdaAD训练的ResNet-18模型在RobustBench上取得了最高的鲁棒准确率（54.23%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Distilled student models in teacher-student architectures are widely considered for computational-effective deployment in real-time applications and edge devices. However, there is a higher risk of student models to encounter adversarial attacks at the edge. Popular enhancing schemes such as adversarial training have limited performance on compressed networks. Thus, recent studies concern about adversarial distillation (AD) that aims to inherit not only prediction accuracy but also adversarial robustness of a robust teacher model under the paradigm of robust optimization. In the min-max framework of AD, existing AD methods generally use fixed supervision information from the teacher model to guide the inner optimization for knowledge distillation which often leads to an overcorrection towards model smoothness. In this paper, we propose an adaptive adversarial distillation (AdaAD) that involves the teacher model in the knowledge optimization process in a way interacting with the student model to adaptively search for the inner results. Comparing with state-of-the-art methods, the proposed AdaAD can significantly boost both the prediction accuracy and adversarial robustness of student models in most scenarios. In particular, the ResNet-18 model trained by AdaAD achieves top-rank performance (54.23% robust accuracy) on RobustBench under AutoAttack.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1657.RGB No More: Minimally-Decoded JPEG Vision Transformers</span><br>
                <span class="as">Park, JeongsooandJohnson, Justin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_RGB_No_More_Minimally-Decoded_JPEG_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22334-22346.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接从JPEG编码的特征中训练视觉变换器（ViT），以减少解码开销并加速数据加载。<br>
                    动机：现有的计算机视觉神经网络通常使用RGB图像进行推断，但这些图像在保存到磁盘之前通常会被编码为JPEG格式，这给RGB网络带来了不可避免的解码开销。<br>
                    方法：本文直接从JPEG编码的特征中训练视觉变换器（ViT），避免了大部分解码开销，加快了数据加载速度。同时，我们还对这种编码特征进行了直接的数据增强处理。<br>
                    效果：通过这两种改进——使用ViT和数据增强——我们的ViT-Ti模型在训练速度上提高了39.2%，在推理速度上提高了17.9%，并且没有损失准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most neural networks for computer vision are designed to infer using RGB images. However, these RGB images are commonly encoded in JPEG before saving to disk; decoding them imposes an unavoidable overhead for RGB networks. Instead, our work focuses on training Vision Transformers (ViT) directly from the encoded features of JPEG. This way, we can avoid most of the decoding overhead, accelerating data load. Existing works have studied this aspect but they focus on CNNs. Due to how these encoded features are structured, CNNs require heavy modification to their architecture to accept such data. Here, we show that this is not the case for ViTs. In addition, we tackle data augmentation directly on these encoded features, which to our knowledge, has not been explored in-depth for training in this setting. With these two improvements -- ViT and data augmentation -- we show that our ViT-Ti model achieves up to 39.2% faster training and 17.9% faster inference with no accuracy loss compared to the RGB counterpart.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1658.CaPriDe Learning: Confidential and Private Decentralized Learning Based on Encryption-Friendly Distillation Loss</span><br>
                <span class="as">Tastan, NurbekandNandakumar, Karthik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tastan_CaPriDe_Learning_Confidential_and_Private_Decentralized_Learning_Based_on_Encryption-Friendly_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8084-8092.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练准确的深度神经网络，同时保护数据的隐私和机密性。<br>
                    动机：由于隐私问题和严格的数据法规，实体之间往往无法共享大量数据进行学习。<br>
                    方法：提出了一种名为“Confidential and Private Decentralized”学习的框架，利用全同态加密技术在不泄露数据的情况下进行协作学习。<br>
                    效果：实验表明，该方法可以在没有中央协调的情况下提高本地模型的准确性，同时保证数据的保密性和隐私性，但存在对模型架构的限制、有限的可扩展性和加密领域推理的计算复杂性等主要限制。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Large volumes of data required to train accurate deep neural networks (DNNs) are seldom available with any single entity. Often, privacy concerns and stringent data regulations prevent entities from sharing data with each other or with a third-party learning service provider. While cross-silo federated learning (FL) allows collaborative learning of large DNNs without sharing the data itself, most existing cross-silo FL algorithms have an unacceptable utility-privacy trade-off. In this work, we propose a framework called Confidential and Private Decentralized (CaPriDe) learning, which optimally leverages the power of fully homomorphic encryption (FHE) to enable collaborative learning without compromising on the confidentiality and privacy of data. In CaPriDe learning, participating entities release their private data in an encrypted form allowing other participants to perform inference in the encrypted domain. The crux of CaPriDe learning is mutual knowledge distillation between multiple local models through a novel distillation loss, which is an approximation of the Kullback-Leibler (KL) divergence between the local predictions and encrypted inferences of other participants on the same data that can be computed in the encrypted domain. Extensive experiments on three datasets show that CaPriDe learning can improve the accuracy of local models without any central coordination, provide strong guarantees of data confidentiality and privacy, and has the ability to handle statistical heterogeneity. Constraints on the model architecture (arising from the need to be FHE-friendly), limited scalability, and computational complexity of encrypted domain inference are the main limitations of the proposed approach. The code can be found at https://github.com/tnurbek/capride-learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1659.Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers</span><br>
                <span class="as">Wei, CongandDuke, BrendanandJiang, RuoweiandAarabi, ParhamandTaylor, GrahamW.andShkurti, Florian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Sparsifiner_Learning_Sparse_Instance-Dependent_Attention_for_Efficient_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22680-22689.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低视觉转换器（ViT）的计算成本，同时保持其性能。<br>
                    动机：虽然视觉转换器在性能上优于卷积神经网络，但其高计算成本是一个问题。现有的方法通过限制固定数量的空间邻近令牌来加速视觉转换器的多头自我注意力操作，但这种结构的注意力模式忽视了从全注意力掩码中学习到的语义连接。<br>
                    方法：提出一种学习实例依赖注意力模式的方法，通过设计一个轻量级的连通性预测器模块来估计每对令牌的连通性分数。如果特征在空间或语义上被认为是相关的，那么两个令牌就会有高的连通性分数。由于每个令牌只关注少数其他令牌，因此二进制连通性掩码通常是非常稀疏的，从而提供了通过稀疏计算减少网络FLOPs的机会。<br>
                    效果：配备学习的非结构化注意力模式，稀疏注意力视觉转换器（Sparsifiner）在ImageNet上的FLOPs和top-1准确率之间产生了优越的Pareto前沿，与令牌稀疏性相比，减少了48%-69%的FLOPs，而准确率下降不超过0.4%。我们还表明，结合注意力和令牌稀疏性可以将视觉转换器的FLOPs减少超过60%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision Transformers (ViT) have shown competitive advantages in terms of performance compared to convolutional neural networks (CNNs), though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose an approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module that estimates the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often very sparse by nature and therefore provide the opportunity to reduce network FLOPs via sparse computations. Equipped with the learned unstructured attention pattern, sparse attention ViT (Sparsifiner) produces a superior Pareto frontier between FLOPs and top-1 accuracy on ImageNet compared to token sparsity. Our method reduces 48%   69% FLOPs of MHSA while the accuracy drop is within 0.4%. We also show that combining attention and token sparsity reduces ViT FLOPs by over 60%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1660.Structured Sparsity Learning for Efficient Video Super-Resolution</span><br>
                <span class="as">Xia, BinandHe, JingwenandZhang, YulunandWang, YitongandTian, YapengandYang, WenmingandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_Structured_Sparsity_Learning_for_Efficient_Video_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22638-22647.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频超分辨率（VSR）模型的高计算成本阻碍了其在资源有限的设备上的部署。<br>
                    动机：现有的VSR模型中存在大量冗余的过滤器，降低了推理效率。<br>
                    方法：开发了一种名为结构稀疏学习（SSL）的结构剪枝方案，针对VSR模型中的几个关键组件设计了剪枝方案，包括残差块、循环网络和上采样网络。<br>
                    效果：实验表明，SSL在数量和质量上都显著优于最近的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The high computational costs of video super-resolution (VSR) models hinder their deployment on resource-limited devices, e.g., smartphones and drones. Existing VSR models contain considerable redundant filters, which drag down the inference efficiency. To prune these unimportant filters, we develop a structured pruning scheme called Structured Sparsity Learning (SSL) according to the properties of VSR. In SSL, we design pruning schemes for several key components in VSR models, including residual blocks, recurrent networks, and upsampling networks. Specifically, we develop a Residual Sparsity Connection (RSC) scheme for residual blocks of recurrent networks to liberate pruning restrictions and preserve the restoration information. For upsampling networks, we design a pixel-shuffle pruning scheme to guarantee the accuracy of feature channel-space conversion. In addition, we observe that pruning error would be amplified as the hidden states propagate along with recurrent networks. To alleviate the issue, we design Temporal Finetuning (TF). Extensive experiments show that SSL can significantly outperform recent methods quantitatively and qualitatively. The code is available at https://github.com/Zj-BinXia/SSL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1661.MMVC: Learned Multi-Mode Video Compression With Block-Based Prediction Mode Selection and Density-Adaptive Entropy Coding</span><br>
                <span class="as">Liu, BowenandChen, YuandMachineni, RakeshChowdaryandLiu, ShiyuandKim, Hun-Seok</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MMVC_Learned_Multi-Mode_Video_Compression_With_Block-Based_Prediction_Mode_Selection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18487-18496.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于学习的视频压缩方法在适应不同的运动模式和熵模型方面存在限制。<br>
                    动机：提出一种多模态视频压缩（MMVC）方法，通过选择最优的模式进行特征域预测，以适应不同的运动模式。<br>
                    方法：该方法包括基于ConvLSTM的特征域预测、光流条件的特征域预测和特征传播等多模态，并将特征空间划分为块进行时空预测。对于熵编码，考虑了密集和稀疏的后量化残差块，并对稀疏残差应用可选的游程编码以提高压缩率。<br>
                    效果：通过一些流行的基准数据集验证了该方法，与最先进的视频压缩方案和标准编解码器相比，该方法在PSNR和MS-SSIM指标上获得了更好的或相当的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based video compression has been extensively studied over the past years, but it still has limitations in adapting to various motion patterns and entropy models. In this paper, we propose multi-mode video compression (MMVC), a block wise mode ensemble deep video compression framework that selects the optimal mode for feature domain prediction adapting to different motion patterns. Proposed multi-modes include ConvLSTM-based feature domain prediction, optical flow conditioned feature domain prediction, and feature propagation to address a wide range of cases from static scenes without apparent motions to dynamic scenes with a moving camera. We partition the feature space into blocks for temporal prediction in spatial block-based representations. For entropy coding, we consider both dense and sparse post-quantization residual blocks, and apply optional run-length coding to sparse residuals to improve the compression rate. In this sense, our method uses a dual-mode entropy coding scheme guided by a binary density map, which offers significant rate reduction surpassing the extra cost of transmitting the binary selection map. We validate our scheme with some of the most popular benchmarking datasets. Compared with state-of-the-art video compression schemes and standard codecs, our method yields better or competitive results measured with PSNR and MS-SSIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1662.DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network</span><br>
                <span class="as">Shen, XuanandWang, YaohuaandLin, MingandHuang, YilunandTang, HaoandSun, XiuyuandWang, Yanzhi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DeepMAD_Mathematical_Architecture_Design_for_Deep_Convolutional_Neural_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6163-6173.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计高性能的CNN模型，以在各种视觉任务上取得与ViT模型相媲美的性能。<br>
                    动机：尽管ViT模型在各种视觉任务上取得了显著的进步，但设计这样的高性能CNN模型仍然具有挑战性，需要对网络设计有深入的了解。<br>
                    方法：提出了一种新的框架——深度CNN的数学架构设计（DeepMAD），该框架将CNN网络建模为一个信息处理系统，其表达性和有效性可以通过其结构参数进行解析。然后提出一个约束的数学规划（MP）问题来优化这些结构参数。这个MP问题可以在CPU上使用小型内存的现成的MP求解器轻松解决。<br>
                    效果：DeepMAD在多个大规模的计算机视觉基准数据集上进行了验证，证明了其优越性。特别是在ImageNet-1k上，仅使用传统的卷积层，DeepMAD在Tiny级别上的top-1准确率比ConvNeXt和Swin高出0.7%和1.5%，在Small级别上高出0.8%和0.9%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The rapid advances in Vision Transformer (ViT) refresh the state-of-the-art performances in various vision tasks, overshadowing the conventional CNN-based models. This ignites a few recent striking-back research in the CNN world showing that pure CNN models can achieve as good performance as ViT models when carefully tuned. While encouraging, designing such high-performance CNN models is challenging, requiring non-trivial prior knowledge of network design. To this end, a novel framework termed Mathematical Architecture Design for Deep CNN (DeepMAD) is proposed to design high-performance CNN models in a principled way. In DeepMAD, a CNN network is modeled as an information processing system whose expressiveness and effectiveness can be analytically formulated by their structural parameters. Then a constrained mathematical programming (MP) problem is proposed to optimize these structural parameters. The MP problem can be easily solved by off-the-shelf MP solvers on CPUs with a small memory footprint. In addition, DeepMAD is a pure mathematical framework: no GPU or training data is required during network design. The superiority of DeepMAD is validated on multiple large-scale computer vision benchmark datasets. Notably on ImageNet-1k, only using conventional convolutional layers, DeepMAD achieves 0.7% and 1.5% higher top-1 accuracy than ConvNeXt and Swin on Tiny level, and 0.8% and 0.9% higher on Small level.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1663.Batch Model Consolidation: A Multi-Task Model Consolidation Framework</span><br>
                <span class="as">Fostiropoulos, IordanisandZhu, JiayeandItti, Laurent</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fostiropoulos_Batch_Model_Consolidation_A_Multi-Task_Model_Consolidation_Framework_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3664-3676.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在连续学习中，使模型在处理一系列任务时，不会对之前学习的任务的性能产生显著的下降。<br>
                    动机：现有的连续学习方法在处理长时间序列和跨领域难度大的任务时效果不佳，且许多方法由于内存成本高、训练时间长或与单一设备紧密耦合而难以应用。<br>
                    方法：提出批量模型整合（BMC）方法，通过并行训练多个专家模型来支持更真实的连续学习环境。每个专家通过稳定性损失保持与基础模型的权重相似性，并从部分任务数据构建缓冲区。在整合阶段，使用聚合所有缓冲区的内存数据的批量整合损失来整合“批量”专家模型的学习知识。<br>
                    效果：在标准化基准数据集Split-CIFAR-100、Tiny-ImageNet和包含71个图像分类任务的Stream数据集上进行评估，该方法比次优的连续学习方法提高了70%，并且是唯一一个能在完成71个任务后仍保持性能的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation (BMC) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a regularization phase, BMC trains multiple expert models in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a base model through a stability loss, and constructs a buffer from a fraction of the task's data. During the consolidation phase, we combine the learned knowledge on 'batches' of expert models using a batched consolidation loss in memory data that aggregates all buffers. We thoroughly evaluate each component of our method in an ablation study and demonstrate the effectiveness on standardized benchmark datasets Split-CIFAR-100, Tiny-ImageNet, and the Stream dataset composed of 71 image classification tasks from diverse domains and difficulties. Our method outperforms the next best CL approach by 70% and is the only approach that can maintain performance at the end of 71 tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1664.FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning</span><br>
                <span class="as">Xiong, YuanhaoandWang, RuochenandCheng, MinhaoandYu, FelixandHsieh, Cho-Jui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16323-16332.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在隐私和通信限制下实现协作训练。<br>
                    动机：现有的迭代模型平均的联邦学习算法需要大量的通信轮次来获得性能良好的模型，因为不同客户端之间的数据分配非常不平衡和非独立同分布。<br>
                    方法：我们提出了FedDM，通过在每个客户端上构建合成数据集，局部匹配原始数据的损失景观，从多个局部替代函数建立全局训练目标，使服务器能够更全面地了解损失景观。<br>
                    效果：我们在三个图像分类数据集上进行了广泛的实验，结果显示，与其它联邦学习方法相比，FedDM在效率和模型性能方面表现更好。此外，我们还证明FedDM可以适应保护差分隐私的高斯机制，并在相同的隐私预算下训练出更好的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning (FL) has recently attracted increasing attention from academia and industry, with the ultimate goal of achieving collaborative training under privacy and communication constraints. Existing iterative model averaging based FL algorithms require a large number of communication rounds to obtain a well-performed model due to extremely unbalanced and non-i.i.d data partitioning among different clients. Thus, we propose FedDM to build the global training objective from multiple local surrogate functions, which enables the server to gain a more global view of the loss landscape. In detail, we construct synthetic sets of data on each client to locally match the loss landscape from original data through distribution matching. FedDM reduces communication rounds and improves model quality by transmitting more informative and smaller synthesized data compared with unwieldy model weights. We conduct extensive experiments on three image classification datasets, and results show that our method can outperform other FL counterparts in terms of efficiency and model performance. Moreover, we demonstrate that FedDM can be adapted to preserve differential privacy with Gaussian mechanism and train a better model under the same privacy budget.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1665.Bit-Shrinking: Limiting Instantaneous Sharpness for Improving Post-Training Quantization</span><br>
                <span class="as">Lin, ChenandPeng, BoandLi, ZheyangandTan, WenmingandRen, YeandXiao, JunandPu, Shiliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Bit-Shrinking_Limiting_Instantaneous_Sharpness_for_Improving_Post-Training_Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16196-16205.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地压缩模型大小和计算成本，同时保持较好的性能？<br>
                    动机：目前的量化方法在低比特量化时，容易陷入局部最优解，导致性能下降。<br>
                    方法：通过分析不同比特宽度量化网络的损失表面，发现粗糙的表面是由于过度的量化噪声引起的。因此，提出一种平滑损失表面的Bit-shrinking策略，通过限制锐度项的大小和稳定性来优化量化网络。<br>
                    效果：实验结果表明，该方法在分类和检测任务上取得了较好的效果，并在视觉变换器模型和传统CNN网络上实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Post-training quantization (PTQ) is an effective compression method to reduce the model size and computational cost. However, quantizing a model into a low-bit one, e.g., lower than 4, is difficult and often results in nonnegligible performance degradation. To address this, we investigate the loss landscapes of quantized networks with various bit-widths. We show that the network with more ragged loss surface, is more easily trapped into bad local minima, which mostly appears in low-bit quantization. A deeper analysis indicates, the ragged surface is caused by the injection of excessive quantization noise. To this end, we detach a sharpness term from the loss which reflects the impact of quantization noise. To smooth the rugged loss surface, we propose to limit the sharpness term small and stable during optimization. Instead of directly optimizing the target bit network, the bit-width of quantized network has a self-adapted shrinking scheduler in continuous domain from high bit-width to the target by limiting the increasing sharpness term within a proper range. It can be viewed as iteratively adding small "instant" quantization noise and adjusting the network to eliminate its impact. Widely experiments including classification and detection tasks demonstrate the effectiveness of the Bit-shrinking strategy in PTQ. On the Vision Transformer models, our INT8 and INT6 models drop within 0.5% and 1.5% Top-1 accuracy, respectively. On the traditional CNN networks, our INT4 quantized models drop within 1.3% and 3.5% Top-1 accuracy on ResNet18 and MobileNetV2 without fine-tuning, which achieves the state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1666.PIVOT: Prompting for Video Continual Learning</span><br>
                <span class="as">Villa, Andr\&#x27;esandAlc\&#x27;azar, JuanLe\&#x27;onandAlfarra, MotasemandAlhamoud, KumailandHurtado, JulioandHeilbron, FabianCabaandSoto, AlvaroandGhanem, Bernard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24214-24223.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现代机器学习管道在数据可用性、存储配额、隐私法规和昂贵的注释过程等方面的限制，特别是在动态注释集上训练和更新大型模型的问题。<br>
                    动机：持续学习直接解决了这个问题，其最终目标是设计出一种方法，让深度神经网络有效地学习新（未见过的）类别的相关模式，同时不显著改变其对之前学过的模式的性能。<br>
                    方法：本文提出了一种新的方法PIVOT，该方法利用预训练模型中的大量知识，从而减少可训练参数的数量和相关的遗忘。与以往的方法不同，我们的方法首次有效地使用了提示机制进行持续学习，而无需进行任何领域内预训练。<br>
                    效果：实验表明，PIVOT在20个任务的活动网络设置中将最先进的方法提高了27%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern machine learning pipelines are limited due to data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints make it difficult or impossible to train and update large-scale models on such dynamic annotated sets. Continual learning directly approaches this problem, with the ultimate goal of devising methods where a deep neural network effectively learns relevant patterns for new (unseen) classes, without significantly altering its performance on previously learned ones. In this paper, we address the problem of continual learning for video data. We introduce PIVOT, a novel method that leverages extensive knowledge in pre-trained models from the image domain, thereby reducing the number of trainable parameters and the associated forgetting. Unlike previous methods, ours is the first approach that effectively uses prompting mechanisms for continual learning without any in-domain pre-training. Our experiments show that PIVOT improves state-of-the-art methods by a significant 27% on the 20-task ActivityNet setup.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1667.Focused and Collaborative Feedback Integration for Interactive Image Segmentation</span><br>
                <span class="as">Wei, QiaoqiaoandZhang, HuiandYong, Jun-Hai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Focused_and_Collaborative_Feedback_Integration_for_Interactive_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18643-18652.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用用户反馈进行交互式图像分割。<br>
                    动机：现有的方法忽视了反馈的重要性，或者只是简单地将其与原始输入连接起来，导致反馈未被充分利用，需要更多的标注。<br>
                    方法：提出了一种名为“聚焦和协作反馈集成”（FCFI）的方法，通过关注新点击点周围的局部区域并基于高级特征的相似性来校正反馈，然后交替和协作地更新反馈和深层特征以将反馈集成到特征中。<br>
                    效果：在四个基准测试（GrabCut、Berkeley、SBD和DAVIS）上验证了FCFI的有效性和效率，实验结果表明，FCFI在计算开销低于以往方法的情况下实现了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Interactive image segmentation aims at obtaining a segmentation mask for an image using simple user annotations. During each round of interaction, the segmentation result from the previous round serves as feedback to guide the user's annotation and provides dense prior information for the segmentation model, effectively acting as a bridge between interactions. Existing methods overlook the importance of feedback or simply concatenate it with the original input, leading to underutilization of feedback and an increase in the number of required annotations. To address this, we propose an approach called Focused and Collaborative Feedback Integration (FCFI) to fully exploit the feedback for click-based interactive image segmentation. FCFI first focuses on a local area around the new click and corrects the feedback based on the similarities of high-level features. It then alternately and collaboratively updates the feedback and deep features to integrate the feedback into the features. The efficacy and efficiency of FCFI were validated on four benchmarks, namely GrabCut, Berkeley, SBD, and DAVIS. Experimental results show that FCFI achieved new state-of-the-art performance with less computational overhead than previous methods. The source code is available at https://github.com/veizgyauzgyauz/FCFI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1668.Dynamic Neural Network for Multi-Task Learning Searching Across Diverse Network Topologies</span><br>
                <span class="as">Choi, WonhyeokandIm, Sunghoon</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_Dynamic_Neural_Network_for_Multi-Task_Learning_Searching_Across_Diverse_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3779-3788.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的多任务学习框架，用于寻找优化多种任务的结构，这些任务具有不同的图拓扑结构，并在任务之间共享特征。<br>
                    动机：现有的多任务学习框架在处理具有不同图拓扑结构的任务时，往往需要为每个任务单独设计网络，这既耗时又耗资源。<br>
                    方法：我们设计了一个受限的基于有向无环图的中心网络，该网络具有读取输入/输出层的层，以构建拓扑结构多样的任务自适应结构，同时限制搜索空间和时间。我们使用三阶段训练过程来寻找一个单一的优化网络，该网络可以作为多个任务自适应子网络。为了使网络紧凑和离散化，我们提出了一种基于流的缩减算法和一种压缩损失函数，用于训练过程。<br>
                    效果：我们在各种公共多任务学习数据集上评估我们的优化网络，结果显示我们的方法实现了最先进的性能。一项广泛的消融实验验证了我们框架中的子模块和方案的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we present a new MTL framework that searches for structures optimized for multiple tasks with diverse graph topologies and shares features among tasks. We design a restricted DAG-based central network with read-in/read-out layers to build topologically diverse task-adaptive structures while limiting search space and time. We search for a single optimized network that serves as multiple task adaptive sub-networks using our three-stage training process. To make the network compact and discretized, we propose a flow-based reduction algorithm and a squeeze loss used in the training process. We evaluate our optimized network on various public MTL datasets and show ours achieves state-of-the-art performance. An extensive ablation study experimentally validates the effectiveness of the sub-module and schemes in our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1669.Re-GAN: Data-Efficient GANs Training via Architectural Reconfiguration</span><br>
                <span class="as">Saxena, DivyaandCao, JiannongandXu, JiahaoandKulshrestha, Tarun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saxena_Re-GAN_Data-Efficient_GANs_Training_via_Architectural_Reconfiguration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16230-16240.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练高保真图像的生成对抗网络（GANs）通常需要大量的训练图像，而寻找有效的子网络结构可以提高模型的训练效率。<br>
                    动机：最近的研究发现，稠密的GANs模型中存在稀疏的子网络或"彩票票"，当单独训练时，可以在有限的数据下获得更好的结果。然而，找到这些"彩票票"需要进行昂贵的训练-剪枝-再训练过程。<br>
                    方法：本文提出了Re-GAN，一种动态重构GANs架构的数据高效GANs训练方法，以在训练过程中探索不同的子网络结构。该方法通过反复剪枝不重要的连接来正则化GANs网络，并在需要时重新生长它们，以降低过早剪枝重要连接的风险。<br>
                    效果：实验结果表明，Re-GAN是一种通用的训练方法，可以在不同规模、领域和分辨率的数据集（如CIFAR-10、Tiny-ImageNet和多个少样本生成数据集）以及不同的GANs架构（如SNGAN、ProGAN、StyleGAN2和AutoGAN）上实现稳定。此外，当与最近的增强方法结合使用时，Re-GAN还可以提高性能。同时，通过在GANs训练过程中删除不重要的连接，Re-GAN所需的浮点运算次数更少，训练时间也更短，同时保持了相当甚至更高的样本质量。与最先进的StyleGAN2相比，我们的方法无需任何额外的微调步骤就能取得更好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training Generative Adversarial Networks (GANs) on high-fidelity images usually requires a vast number of training images. Recent research on GAN tickets reveals that dense GANs models contain sparse sub-networks or "lottery tickets" that, when trained separately, yield better results under limited data. However, finding GANs tickets requires an expensive process of train-prune-retrain. In this paper, we propose Re-GAN, a data-efficient GANs training that dynamically reconfigures GANs architecture during training to explore different sub-network structures in training time. Our method repeatedly prunes unimportant connections to regularize GANs network and regrows them to reduce the risk of prematurely pruning important connections. Re-GAN stabilizes the GANs models with less data and offers an alternative to the existing GANs tickets and progressive growing methods. We demonstrate that Re-GAN is a generic training methodology which achieves stability on datasets of varying sizes, domains, and resolutions (CIFAR-10, Tiny-ImageNet, and multiple few-shot generation datasets) as well as different GANs architectures (SNGAN, ProGAN, StyleGAN2 and AutoGAN). Re-GAN also improves performance when combined with the recent augmentation approaches. Moreover, Re-GAN requires fewer floating-point operations (FLOPs) and less training time by removing the unimportant connections during GANs training while maintaining comparable or even generating higher-quality samples. When compared to state-of-the-art StyleGAN2, our method outperforms without requiring any additional fine-tuning step. Code can be found at this link: https://github.com/IntellicentAI-Lab/Re-GAN</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1670.Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers</span><br>
                <span class="as">Wei, SiyuanandYe, TianzhuandZhang, ShenandTang, YaoandLiang, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Joint_Token_Pruning_and_Squeezing_Towards_More_Aggressive_Compression_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2092-2101.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉转换器（ViTs）在计算机视觉任务中表现出色，但其高昂的计算成本限制了实际应用。<br>
                    动机：先前的剪枝冗余令牌的方法虽然在性能和计算成本之间取得了良好的平衡，但由剪枝策略引起的错误可能导致重大信息损失。<br>
                    方法：我们提出了一种新的联合令牌剪枝和压缩模块（TPS），用于更高效地压缩视觉转换器。首先，TPS采用剪枝来获取保留和被剪枝的子集。其次，TPS通过单向最近邻匹配和相似性导向融合步骤将剪枝令牌的信息压缩到部分保留令牌中。<br>
                    效果：与最先进的方法相比，我们的方法在所有令牌剪枝强度下都表现更好。特别是在将DeiT-tiny&small的计算预算缩小到35%时，它在ImageNet分类上比基线提高了1%-6%的准确率。该方法可以加速DeiT-small的处理速度超过DeiT-tiny，同时其准确率超过DeiT-tiny 4.78%。对各种转换器的实验证明了我们方法的有效性，而分析实验证明了我们对令牌剪枝策略的错误具有更高的鲁棒性。代码可在https://github.com/megvii-research/TPS-CVPR2023获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although vision transformers (ViTs) have shown promising results in various computer vision tasks recently, their high computational cost limits their practical applications. Previous approaches that prune redundant tokens have demonstrated a good trade-off between performance and computation costs. Nevertheless, errors caused by pruning strategies can lead to significant information loss. Our quantitative experiments reveal that the impact of pruned tokens on performance should be noticeable. To address this issue, we propose a novel joint Token Pruning & Squeezing module (TPS) for compressing vision transformers with higher efficiency. Firstly, TPS adopts pruning to get the reserved and pruned subsets. Secondly, TPS squeezes the information of pruned tokens into partial reserved tokens via the unidirectional nearest-neighbor matching and similarity-oriented fusing steps. Compared to state-of-the-art methods, our approach outperforms them under all token pruning intensities. Especially while shrinking DeiT-tiny&small computational budgets to 35%, it improves the accuracy by 1%-6% compared with baselines on ImageNet classification. The proposed method can accelerate the throughput of DeiT-small beyond DeiT-tiny, while its accuracy surpasses DeiT-tiny by 4.78%. Experiments on various transformers demonstrate the effectiveness of our method, while analysis experiments prove our higher robustness to the errors of the token pruning policy. Code is available at https://github.com/megvii-research/TPS-CVPR2023.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1671.Solving Oscillation Problem in Post-Training Quantization Through a Theoretical Perspective</span><br>
                <span class="as">Ma, YuexiaoandLi, HuixiaandZheng, XiawuandXiao, XuefengandWang, RuiandWen, ShileiandPan, XinandChao, FeiandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Solving_Oscillation_Problem_in_Post-Training_Quantization_Through_a_Theoretical_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7950-7959.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决预训练量化（PTQ）中常被忽视的振荡问题。<br>
                    动机：振荡问题在PTQ中是一个关键问题，由于其数据隐私和低计算成本的优点，PTQ被认为是最有效的压缩方法之一。<br>
                    方法：我们首先定义了PTQ中的振荡现象，并证明该问题是由模块容量的差异引起的。然后，我们通过选择前k个差异值来解决这个问题，对应的模块将进行联合优化和量化。<br>
                    效果：实验结果表明，我们的方法成功地减少了性能下降，并且可以推广到不同的神经网络和PTQ方法上。例如，使用2/4位ResNet-50量化，我们的方法比之前最先进的方法提高了1.9%。在小型模型量化方面，例如在MobileNetV2*0.5上，我们的方法比BRECQ方法提高了6.61%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Post-training quantization (PTQ) is widely regarded as one of the most efficient compression methods practically, benefitting from its data privacy and low computation costs. We argue that an overlooked problem of oscillation is in the PTQ methods. In this paper, we take the initiative to explore and present a theoretical proof to explain why such a problem is essential in PTQ. And then, we try to solve this problem by introducing a principled and generalized framework theoretically. In particular, we first formulate the oscillation in PTQ and prove the problem is caused by the difference in module capacity. To this end, we define the module capacity (ModCap) under data-dependent and data-free scenarios, where the differentials between adjacent modules are used to measure the degree of oscillation. The problem is then solved by selecting top-k differentials, in which the corresponding modules are jointly optimized and quantized. Extensive experiments demonstrate that our method successfully reduces the performance drop and is generalized to different neural networks and PTQ methods. For example, with 2/4 bit ResNet-50 quantization, our method surpasses the previous state-of-the-art method by 1.9%. It becomes more significant on small model quantization, e.g. surpasses BRECQ method by 6.61% on MobileNetV2*0.5.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1672.Masked Image Modeling With Local Multi-Scale Reconstruction</span><br>
                <span class="as">Wang, HaoqingandTang, YehuiandWang, YunheandGuo, JianyuanandDeng, Zhi-HongandHan, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Image_Modeling_With_Local_Multi-Scale_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2122-2131.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的掩蔽图像建模（MIM）模型虽然在自监督表示学习上取得了成功，但计算负担大且学习过程缓慢，限制了其在工业应用中的使用。<br>
                    动机：为了解决MIM模型的问题，我们提出了一种将重建任务应用于多个局部层（包括低层和高层）的方法，并设计了局部多尺度重建策略，以加速表示学习过程并促进对输入的多尺度语义理解。<br>
                    方法：我们将重建任务应用于编码器的多个局部层，包括低层和高层，并设计了局部多尺度重建策略，其中低层和高层分别重建细粒度和粗粒度的监督信号。<br>
                    效果：实验表明，我们的模型在分类、检测和分割任务上的表现与现有的MIM模型相当或更好，而且预训练负担显著减少。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Masked Image Modeling (MIM) achieves outstanding success in self-supervised representation learning. Unfortunately, MIM models typically have huge computational burden and slow learning process, which is an inevitable obstacle for their industrial applications. Although the lower layers play the key role in MIM, existing MIM models conduct reconstruction task only at the top layer of encoder. The lower layers are not explicitly guided and the interaction among their patches is only used for calculating new activations. Considering the reconstruction task requires non-trivial inter-patch interactions to reason target signals, we apply it to multiple local layers including lower and upper layers. Further, since the multiple layers expect to learn the information of different scales, we design local multi-scale reconstruction, where the lower and upper layers reconstruct fine-scale and coarse-scale supervision signals respectively. This design not only accelerates the representation learning process by explicitly guiding multiple layers, but also facilitates multi-scale semantical understanding to the input. Extensive experiments show that with significantly less pre-training burden, our model achieves comparable or better performance on classification, detection and segmentation tasks than existing MIM models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1673.Learning To Zoom and Unzoom</span><br>
                <span class="as">Thavamani, ChitteshandLi, MengtianandFerroni, FrancescoandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Thavamani_Learning_To_Zoom_and_Unzoom_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5086-5095.png><br>
            
            <span class="tt"><span class="t0">研究问题：移动计算、自主导航和AR/VR中的许多感知系统面临严格的计算限制，这对高分辨率输入图像尤其具有挑战性。<br>
                    动机：先前的研究表明，非均匀下采样器可以"学习缩放"显著的图像区域，减少计算量的同时保留与任务相关的图像信息。然而，对于具有空间标签的任务（如2D/3D物体检测和语义分割），这种变形可能会损害性能。<br>
                    方法：在这项工作中（LZU），我们首先"学习缩放"输入图像，计算空间特征，然后"取消缩放"以恢复任何变形。为了实现高效且可微的取消缩放，我们使用可逆的分段双线性映射来近似缩放变换。<br>
                    效果：LZU可以应用于任何具有2D空间输入和任何具有2D空间特征的模型，我们在各种任务和数据集上进行评估：Argoverse-HD上的目标检测、Cityscapes上的语义分割以及nuScenes上的单眼3D目标检测。有趣的是，即使没有高分辨率传感器数据，我们也观察到性能的提升，这意味着LZU也可以用于"学习上采样"。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Many perception systems in mobile computing, autonomous navigation, and AR/VR face strict compute constraints that are particularly challenging for high-resolution input images. Previous works propose nonuniform downsamplers that "learn to zoom" on salient image regions, reducing compute while retaining task-relevant image information. However, for tasks with spatial labels (such as 2D/3D object detection and semantic segmentation), such distortions may harm performance. In this work (LZU), we "learn to zoom" in on the input image, compute spatial features, and then "unzoom" to revert any deformations. To enable efficient and differentiable unzooming, we approximate the zooming warp with a piecewise bilinear mapping that is invertible. LZU can be applied to any task with 2D spatial input and any model with 2D spatial features, and we demonstrate this versatility by evaluating on a variety of tasks and datasets: object detection on Argoverse-HD, semantic segmentation on Cityscapes, and monocular 3D object detection on nuScenes. Interestingly, we observe boosts in performance even when high-resolution sensor data is unavailable, implying that LZU can be used to "learn to upsample" as well. Code and additional visuals are available at https://tchittesh.github.io/lzu/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1674.Task Difficulty Aware Parameter Allocation \&amp; Regularization for Lifelong Learning</span><br>
                <span class="as">Wang, WenjinandHu, YunqingandChen, QianglongandZhang, Yin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Task_Difficulty_Aware_Parameter_Allocation__Regularization_for_Lifelong_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7776-7785.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决终身学习中灾难性遗忘的问题，特别是在处理不同难度任务时，参数正则化或分配方法的不足。<br>
                    动机：现有的参数正则化或分配方法在处理终身学习中的不同任务时，存在一些问题。例如，当学习与已学任务非常不同的新任务时，参数正则化方法会面临显著的遗忘问题；而当学习简单任务时，参数分配方法会面临不必要的参数开销。<br>
                    方法：本文提出了一种参数分配和正则化（PAR）的方法，该方法根据任务的学习难度自适应地为每个任务选择适当的策略。我们提出了一种基于最近原型距离的发散估计方法，仅使用新任务的特征来测量任务相关性。此外，我们还提出了一种时间高效的相关感知采样基础架构搜索策略，以减少分配的参数开销。<br>
                    效果：实验结果表明，与现有技术相比，我们的方法具有可扩展性，可以显著减少模型的冗余，同时提高模型的性能。进一步的定性分析表明，PAR可以获得合理的任务相关性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Parameter regularization or allocation methods are effective in overcoming catastrophic forgetting in lifelong learning. However, they solve all tasks in a sequence uniformly and ignore the differences in the learning difficulty of different tasks. So parameter regularization methods face significant forgetting when learning a new task very different from learned tasks, and parameter allocation methods face unnecessary parameter overhead when learning simple tasks. In this paper, we propose the Parameter Allocation & Regularization (PAR), which adaptively select an appropriate strategy for each task from parameter allocation and regularization based on its learning difficulty. A task is easy for a model that has learned tasks related to it and vice versa. We propose a divergence estimation method based on the Nearest-Prototype distance to measure the task relatedness using only features of the new task. Moreover, we propose a time-efficient relatedness-aware sampling-based architecture search strategy to reduce the parameter overhead for allocation. Experimental results on multiple benchmarks demonstrate that, compared with SOTAs, our method is scalable and significantly reduces the model's redundancy while improving the model's performance. Further qualitative analysis indicates that PAR obtains reasonable task-relatedness.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1675.Polynomial Implicit Neural Representations for Large Diverse Datasets</span><br>
                <span class="as">Singh, RajhansandShukla, AnkitaandTuraga, Pavan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_Polynomial_Implicit_Neural_Representations_for_Large_Diverse_Datasets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2041-2051.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高隐式神经表示（INR）模型在信号和图像表示上的表现力，以应对更复杂任务的需求。<br>
                    动机：现有的INR架构主要依赖正弦位置编码来捕捉数据中的高频信息，但其有限的编码大小限制了模型的表示能力。为了从表示单个图像到表示大型和多样化的数据集，需要更高的表示能力。<br>
                    方法：提出一种新方法，通过使用多项式函数来表示图像，并消除了位置编码的需求。通过在每个ReLU层之后的特征和仿射变换后的坐标位置之间进行逐元素乘法，实现逐步提高多项式表示的阶数。<br>
                    效果：在ImageNet等大型数据集上对所提出的方法进行了定性和定量评估。结果表明，提出的Poly-INR模型在无需任何卷积、归一化或自注意力层的情况下，其表现与最先进的生成模型相当，且训练参数数量大大减少。这种方法为INR模型在复杂领域的生成建模任务中的更广泛应用铺平了道路。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representations (INR) have gained significant popularity for signal and image representation for many end-tasks, such as superresolution, 3D modeling, and more. Most INR architectures rely on sinusoidal positional encoding, which accounts for high-frequency information in data. However, the finite encoding size restricts the model's representational power. Higher representational power is needed to go from representing a single given image to representing large and diverse datasets. Our approach addresses this gap by representing an image with a polynomial function and eliminates the need for positional encodings. Therefore, to achieve a progressively higher degree of polynomial representation, we use element-wise multiplications between features and affine-transformed coordinate locations after every ReLU layer. The proposed method is evaluated qualitatively and quantitatively on large datasets like ImageNet. The proposed Poly-INR model performs comparably to state-of-the-art generative models without any convolution, normalization, or self-attention layers, and with far fewer trainable parameters. With much fewer training parameters and higher representative power, our approach paves the way for broader adoption of INR models for generative modeling tasks in complex domains. The code is available at https://github.com/Rajhans0/Poly_INR</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1676.System-Status-Aware Adaptive Network for Online Streaming Video Understanding</span><br>
                <span class="as">Foo, LinGengandGong, JiaandFan, ZhipengandLiu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Foo_System-Status-Aware_Adaptive_Network_for_Online_Streaming_Video_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10514-10523.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数现有的深度学习网络模型并未考虑到设备状态和可用资源的实时变化，也未研究或解决计算资源变动对在线视频理解任务的影响。<br>
                    动机：本文提出了一个系统状态感知的自适应网络（SAN），该网络考虑设备的实时状态，以低延迟提供高质量的预测。<br>
                    方法：通过使用我们的代理策略，SAN在两个广泛使用的在线视频理解任务上获得了最先进的性能，同时保持了低处理延迟。此外，由于标记的训练数据可能不可用或计算成本过高，因此在各种类型的硬件配置上训练这样的代理并不容易。为此，我们提出了元自监督适应（MSA）方法，使代理策略能够在测试时适应新的硬件配置，从而允许模型轻松部署到其他未见过的平台。<br>
                    效果：实验结果表明，SAN在各种在线视频理解任务上都取得了最先进的性能，同时保持了低延迟。而MSA方法则成功地解决了在不同硬件配置上训练代理的问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent years have witnessed great progress in deep neural networks for real-time applications. However, most existing works do not explicitly consider the general case where the device's state and the available resources fluctuate over time, and none of them investigate or address the impact of varying computational resources for online video understanding tasks. This paper proposes a System-status-aware Adaptive Network (SAN) that considers the device's real-time state to provide high-quality predictions with low delay. Usage of our agent's policy improves efficiency and robustness to fluctuations of the system status. On two widely used video understanding tasks, SAN obtains state-of-the-art performance while constantly keeping processing delays low. Moreover, training such an agent on various types of hardware configurations is not easy as the labeled training data might not be available, or can be computationally prohibitive. To address this challenging problem, we propose a Meta Self-supervised Adaptation (MSA) method that adapts the agent's policy to new hardware configurations at test-time, allowing for easy deployment of the model onto other unseen hardware platforms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1677.FFCV: Accelerating Training by Removing Data Bottlenecks</span><br>
                <span class="as">Leclerc, GuillaumeandIlyas, AndrewandEngstrom, LoganandPark, SungMinandSalman, HadiandM\k{a</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12011-12020.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高机器学习模型的训练效率和资源利用率？<br>
                    动机：现有的训练方法存在数据瓶颈，导致GPU资源无法充分利用，训练效率低下。<br>
                    方法：开发FFCV库，采用高效的文件存储格式、缓存、预加载数据、异步数据传输和即时编译等技术，提高数据加载和传输的效率，尽可能将数据处理任务异步转移到CPU上，释放GPU的容量用于训练。<br>
                    效果：使用FFCV库，在ImageNet数据集上训练ResNet-18和ResNet-50模型，取得了优秀的准确率与训练时间之间的权衡。测试结果显示，使用FFCV库训练的ResNet-50模型在一半的时间内达到了与最佳基线相同的准确率。通过多个案例研究展示了FFCV的性能、易用性、可扩展性和适应资源限制的能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present FFCV, a library for easy, fast, resource-efficient training of machine learning models. FFCV speeds up model training by eliminating (often subtle) data bottlenecks from the training process. In particular, we combine techniques such as an efficient file storage format, caching, data pre-loading, asynchronous data transfer, and just-in-time compilation to (a) make data loading and transfer significantly more efficient, ensuring that GPUs can reach full utilization; and (b) offload as much data processing as possible to the CPU asynchronously, freeing GPU up capacity for training. Using FFCV, we train ResNet-18 and ResNet-50 on the ImageNet dataset with a state-of-the-art tradeoff between accuracy and training time. For example, across the range of ResNet-50 models we test, we obtain the same accuracy as the best baselines in half the time. We demonstrate FFCV's performance, ease-of-use, extensibility, and ability to adapt to resource constraints through several case studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1678.Adaptive Channel Sparsity for Federated Learning Under System Heterogeneity</span><br>
                <span class="as">Liao, DongpingandGao, XitongandZhao, YirenandXu, Cheng-Zhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Adaptive_Channel_Sparsity_for_Federated_Learning_Under_System_Heterogeneity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20432-20441.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于客户端数据的非独立同分布特性，联邦学习模型的通道神经元可能为不同的客户端专门化不同的特征。然而，现有的稀疏联邦学习方法对客户端模型规定了固定的稀疏策略，可能会阻止客户端协同训练通道神经元。<br>
                    动机：为了最小化稀疏性对联邦学习收敛的影响，我们提出了Flado方法，通过调整每个客户端中每个神经元的稀疏性来改善客户端模型更新轨迹的对齐。<br>
                    方法：Flado方法通过调整每个客户端中每个神经元的稀疏性来改善客户端模型更新轨迹的对齐。<br>
                    效果：实验结果表明，虽然其他稀疏方法对收敛影响显著，但Flado不仅可以在各种数据集上以无限的预算达到最高的任务准确率，而且在相同的通信预算下，还可以显著减少超过10倍的训练FLOPs需求，并将通信/计算权衡的帕累托前沿推向比竞争联邦学习算法更远的位置。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Owing to the non-i.i.d. nature of client data, channel neurons in federated-learned models may specialize to distinct features for different clients. Yet, existing channel-sparse federated learning (FL) algorithms prescribe fixed sparsity strategies for client models, and may thus prevent clients from training channel neurons collaboratively. To minimize the impact of sparsity on FL convergence, we propose Flado to improve the alignment of client model update trajectories by tailoring the sparsities of individual neurons in each client. Empirical results show that while other sparse methods are surprisingly impactful to convergence, Flado can not only attain the highest task accuracies with unlimited budget across a range of datasets, but also significantly reduce the amount of FLOPs required for training more than by 10x under the same communications budget, and push the Pareto frontier of communication/computation trade-off notably further than competing FL algorithms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1679.NIPQ: Noise Proxy-Based Integrated Pseudo-Quantization</span><br>
                <span class="as">Shin, JuncheolandSo, JunhyukandPark, SeinandKang, SeungyeopandYoo, SungjooandPark, Eunhyeok</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_NIPQ_Noise_Proxy-Based_Integrated_Pseudo-Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3852-3861.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过近似方法解决非可微函数的梯度流问题，以提高量化感知训练（QAT）的稳定性和精度。<br>
                    动机：尽管直通估计器（STE）在QAT中受到欢迎，但其在训练过程中的不稳定性导致低精度表示的质量下降。<br>
                    方法：提出一种新的基于噪声代理的综合伪量化（NIPQ）方法，该方法通过整合截断思想，在伪量化框架上实现了激活和权重的伪量化的统一支持，并通过梯度下降更新所有量化参数和网络参数，无需STE不稳定，大大简化了但可靠的精度分配，无需人工干预。<br>
                    效果：实验表明，NIPQ在各种视觉和语言应用中优于现有的量化算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low-precision representation. Recently, pseudo-quantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudo-quantization (NIPQ) that enables unified support of pseudo-quantization for both activation and weight with minimal error by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability, resulting in greatly-simplified but reliable precision allocation without human intervention. Our extensive experiments show that NIPQ outperforms existing quantization algorithms in various vision and language applications by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1680.Poly-PC: A Polyhedral Network for Multiple Point Cloud Tasks at Once</span><br>
                <span class="as">Xie, TaoandWang, ShiguangandWang, KeandYang, LinqiandJiang, ZhiqiangandZhang, XingchengandDai, KunandLi, RuifengandCheng, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Poly-PC_A_Polyhedral_Network_for_Multiple_Point_Cloud_Tasks_at_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1233-1243.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在点云上同时进行多项任务的困难。<br>
                    动机：现有的方法在处理点云上的多任务学习时存在诸多难题，如任务偏见导致的不同模型架构和多个数据集域之间的冲突梯度等。<br>
                    方法：本文提出了一种名为Poly-PC的框架，通过设计一种高效的残差集抽象（Res-SA）层来适应网络的宽度和深度需求，以适应各种任务的需求。同时，开发了一种基于权重纠缠的一次性NAS技术来寻找所有任务的最佳架构。此外，这种技术将多个任务的权重在每一层中纠缠在一起，提供用于有效存储部署的任务共享参数，同时提供用于学习任务相关特征的任务特定参数。最后，为了便于Poly-PC的训练，引入了一种基于任务优先级的梯度平衡算法，利用任务优先级来解决冲突的梯度，确保所有任务的高性能。<br>
                    效果：受益于所提出的技术，由Poly-PC优化的所有任务模型的总FLOPs和参数更少，性能超过了之前的方法。我们还证明，当调整到新任务时，Poly-PC可以进行增量学习和避免灾难性遗忘。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we show that it is feasible to perform multiple tasks concurrently on point cloud with a straightforward yet effective multi-task network. Our framework, Poly-PC, tackles the inherent obstacles (e.g., different model architectures caused by task bias and conflicting gradients caused by multiple dataset domains, etc.) of multi-task learning on point cloud. Specifically, we propose a residual set abstraction (Res-SA) layer for efficient and effective scaling in both width and depth of the network, hence accommodating the needs of various tasks. We develop a weight-entanglement-based one-shot NAS technique to find optimal architectures for all tasks. Moreover, such technique entangles the weights of multiple tasks in each layer to offer task-shared parameters for efficient storage deployment while providing ancillary task-specific parameters for learning task-related features. Finally, to facilitate the training of Poly-PC, we introduce a task-prioritization-based gradient balance algorithm that leverages task prioritization to reconcile conflicting gradients, ensuring high performance for all tasks. Benefiting from the suggested techniques, models optimized by Poly-PC collectively for all tasks keep fewer total FLOPs and parameters and outperform previous methods. We also demonstrate that Poly-PC allows incremental learning and evades catastrophic forgetting when tuned to a new task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1681.Efficient Verification of Neural Networks Against LVM-Based Specifications</span><br>
                <span class="as">Hanspal, HarleenandLomuscio, Alessio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hanspal_Efficient_Verification_of_Neural_Networks_Against_LVM-Based_Specifications_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3894-3903.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何确保基于神经网络的感知系统在安全关键应用中的稳健性。<br>
                    动机：神经网络的稳健性需要形式化验证，但现有的标准方法只能分析对解析定义的转换的不变性，无法处理物体姿态、场景视点、遮挡等多样化和普遍存在的变化。<br>
                    方法：提出了一种有效的方法，通过在待验证的网络中添加一个可逆编码头，以最小化重建开销来验证能够捕获这些多样化变化的潜变量模型（Latent Variable Models）所定义的规范。<br>
                    效果：对于三种不同类型的现实输入变化，进行了验证实验，并报告了结果。与现有工作不同的是，该方法相对独立于输入维度和规模，并通过减轻当前最先进技术中的效率低下和解码器表达能力依赖性，适用于广泛的深度网络和真实世界数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The deployment of perception systems based on neural networks in safety critical applications requires assurance on their robustness. Deterministic guarantees on network robustness require formal verification. Standard approaches for verifying robustness analyse invariance to analytically defined transformations, but not the diverse and ubiquitous changes involving object pose, scene viewpoint, occlusions, etc. To this end, we present an efficient approach for verifying specifications definable using Latent Variable Models that capture such diverse changes. The approach involves adding an invertible encoding head to the network to be verified, enabling the verification of latent space sets with minimal reconstruction overhead. We report verification experiments for three classes of proposed latent space specifications, each capturing different types of realistic input variations. Differently from previous work in this area, the proposed approach is relatively independent of input dimensionality and scales to a broad class of deep networks and real-world datasets by mitigating the inefficiency and decoder expressivity dependence in the present state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1682.A Unified Knowledge Distillation Framework for Deep Directed Graphical Models</span><br>
                <span class="as">Chen, YizhuoandLiang, KaizhaoandZeng, ZheandYao, ShuochaoandShao, Huajie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_A_Unified_Knowledge_Distillation_Framework_for_Deep_Directed_Graphical_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7795-7804.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的知识蒸馏方法无法泛化到具有任意层随机变量的深度有向图模型（DGM）。<br>
                    动机：为了解决这一问题，我们提出了一种针对深度DGM的统一知识蒸馏框架。<br>
                    方法：我们利用重参数化技巧隐藏中间潜在变量，从而得到一个紧凑的DGM。然后，我们开发了一种替代蒸馏损失函数，以减少通过多层随机变量的错误累积。<br>
                    效果：在四个应用中评估了我们的框架，包括无数据分层变分自动编码器（VAE）压缩、无数据变分循环神经网络（VRNN）压缩、无数据亥姆霍兹机（HM）压缩和VAE持续学习。实验结果表明，我们的蒸馏方法在无数据模型压缩任务上优于基线，并在基于KD的持续学习数据生成方面显著提高了性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Knowledge distillation (KD) is a technique that transfers the knowledge from a large teacher network to a small student network. It has been widely applied to many different tasks, such as model compression and federated learning. However, existing KD methods fail to generalize to general deep directed graphical models (DGMs) with arbitrary layers of random variables. We refer by deep DGMs to DGMs whose conditional distributions are parameterized by deep neural networks. In this work, we propose a novel unified knowledge distillation framework for deep DGMs on various applications. Specifically, we leverage the reparameterization trick to hide the intermediate latent variables, resulting in a compact DGM. Then we develop a surrogate distillation loss to reduce error accumulation through multiple layers of random variables. Moreover, we present the connections between our method and some existing knowledge distillation approaches. The proposed framework is evaluated on four applications: data-free hierarchical variational autoencoder (VAE) compression, data-free variational recurrent neural networks (VRNN) compression, data-free Helmholtz Machine (HM) compression, and VAE continual learning. The results show that our distillation method outperforms the baselines in data-free model compression tasks. We further demonstrate that our method significantly improves the performance of KD-based continual learning for data generation. Our source code is available at https://github.com/YizhuoChen99/KD4DGM-CVPR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1683.DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning</span><br>
                <span class="as">Gao, XinyuanandHe, YuhangandDong, SonglinandCheng, JieandWei, XingandGong, Yihong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24236-24245.png><br>
            
            <span class="tt"><span class="t0">研究问题：深度神经网络在类别增量学习中存在灾难性遗忘问题，即在新类别知识学习过程中，旧类别的分类准确度会大幅下降。<br>
                    动机：目前解决类别增量学习问题的方法要么存在严重的灾难性遗忘和稳定性-可塑性两难问题，要么需要过多的额外参数和计算。<br>
                    方法：提出一种新颖的框架——多样化知识转移Transformer（DKT），包含两种基于注意力机制的任务特定知识和任务通用知识转移，以减轻灾难性遗忘。同时，提出一个双工分类器来解决稳定性-可塑性两难问题，以及一种新的损失函数来在特征空间中对相同类别进行聚类，并在新旧任务之间区分特征，以强制任务特定知识更加多样化。<br>
                    效果：在CIFAR100、ImageNet100/1000数据集上进行了全面实验，结果表明该方法优于其他竞争方法，并取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks suffer from catastrophic forgetting in class incremental learning, where the classification accuracy of old classes drastically deteriorates when the networks learn the knowledge of new classes. Many works have been proposed to solve the class incremental learning problem. However, most of them either suffer from serious catastrophic forgetting and stability-plasticity dilemma or need too many extra parameters and computations. To meet the challenge, we propose a novel framework, Diverse Knowledge Transfer Transformer (DKT). which contains two novel knowledge transfers based on the attention mechanism to transfer the task-general knowledge and task-specific knowledge to the current task to alleviate catastrophic forgetting. Besides, we propose a duplex classifier to address the stability-plasticity dilemma, and a novel loss function to cluster the same categories in feature space and discriminate the features between old and new tasks to force the task specific knowledge to be more diverse. Our method needs only a few extra parameters, which are negligible, to tackle the increasing number of tasks. We conduct comprehensive experimental results on CIFAR100, ImageNet100/1000 datasets. The experiment results show that our method outperforms other competitive methods and achieves state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1684.DynamicDet: A Unified Dynamic Architecture for Object Detection</span><br>
                <span class="as">Lin, ZhihaoandWang, YongtaoandZhang, JinheandChu, Xiaojie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_DynamicDet_A_Unified_Dynamic_Architecture_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6282-6291.png><br>
            
            <span class="tt"><span class="t0">研究问题：设计一种强大的动态探测器，以解决对象检测任务中没有合适的动态架构和退出标准的问题。<br>
                    动机：动态神经网络是深度学习中的新兴研究课题，其自适应推理能力可以实现显著的准确率和计算效率。<br>
                    方法：提出了一种名为DynamicDet的对象检测动态框架。首先，根据对象检测任务的性质精心设计了一种动态架构。然后，提出了一种自适应路由器来分析多尺度信息并自动决定推理路线。还提出了一种基于检测损失的新型优化策略和一种可变速度推理策略。<br>
                    效果：在COCO基准测试上进行的大量实验表明，所提出的DynamicDet实现了新的最先进的准确率-速度权衡。例如，在相当的准确率下，我们的动态探测器Dy-YOLOv7-W6的推理速度比YOLOv7-E6快12%，比YOLOv7-D6快17%，比YOLOv7-E6E快39%。代码可在https://github.com/VDIGPKU/DynamicDet获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dynamic neural network is an emerging research topic in deep learning. With adaptive inference, dynamic models can achieve remarkable accuracy and computational efficiency. However, it is challenging to design a powerful dynamic detector, because of no suitable dynamic architecture and exiting criterion for object detection. To tackle these difficulties, we propose a dynamic framework for object detection, named DynamicDet. Firstly, we carefully design a dynamic architecture based on the nature of the object detection task. Then, we propose an adaptive router to analyze the multi-scale information and to decide the inference route automatically. We also present a novel optimization strategy with an exiting criterion based on the detection losses for our dynamic detectors. Last, we present a variable-speed inference strategy, which helps to realize a wide range of accuracy-speed trade-offs with only one dynamic detector. Extensive experiments conducted on the COCO benchmark demonstrate that the proposed DynamicDet achieves new state-of-the-art accuracy-speed trade-offs. For instance, with comparable accuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses YOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is available at https://github.com/VDIGPKU/DynamicDet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1685.MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer</span><br>
                <span class="as">Wang, ShiguangandXie, TaoandCheng, JianandZhang, XingchengandLiu, Haijun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20094-20104.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将多个视觉任务整合到一个可管理的超级网络中，并在不同数据集领域中进行集体优化？<br>
                    动机：现有的方法在处理多任务学习时，通常需要为每个任务单独设计模型，存储和计算效率低下。<br>
                    方法：提出MDL-NAS框架，将多个任务集成到一个管理性强的超级网络中，通过粗到细的搜索空间进行联合优化。在细粒度搜索空间中，提出了顺序共享策略和掩码共享策略，实现真正的细粒度参数共享。<br>
                    效果：实验证明，MDL-NAS在保持高效存储部署和计算的同时，对所有任务的性能与最先进的方法相当。同时，MDL-NAS还支持增量学习和新任务泛化时的遗忘规避。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we introduce MDL-NAS, a unified framework that integrates multiple vision tasks into a manageable supernet and optimizes these tasks collectively under diverse dataset domains. MDL-NAS is storage-efficient since multiple models with a majority of shared parameters can be deposited into a single one. Technically, MDL-NAS constructs a coarse-to-fine search space, where the coarse search space offers various optimal architectures for different tasks while the fine search space provides fine-grained parameter sharing to tackle the inherent obstacles of multi-domain learning. In the fine search space, we suggest two parameter sharing policies, i.e., sequential sharing policy and mask sharing policy. Compared with previous works, such two sharing policies allow for the partial sharing and non-sharing of parameters at each layer of the network, hence attaining real fine-grained parameter sharing. Finally, we present a joint-subnet search algorithm that finds the optimal architecture and sharing parameters for each task within total resource constraints, challenging the traditional practice that downstream vision tasks are typically equipped with backbone networks designed for image classification. Experimentally, we demonstrate that MDL-NAS families fitted with non-hierarchical or hierarchical transformers deliver competitive performance for all tasks compared with state-of-the-art methods while maintaining efficient storage deployment and computation. We also demonstrate that MDL-NAS allows incremental learning and evades catastrophic forgetting when generalizing to a new task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1686.ScaleFL: Resource-Adaptive Federated Learning With Heterogeneous Clients</span><br>
                <span class="as">Ilhan, FatihandSu, GongandLiu, Ling</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24532-24541.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决联邦学习中资源异构性的问题，即部分客户端计算能力有限，只能训练较小的本地模型。<br>
                    动机：在现实生活中，一些客户端的计算资源非常有限，无法参与深度神经网络的学习。因此，需要一种能够处理资源异构性的联邦学习方法。<br>
                    方法：本文提出了一种新的联邦学习方法——ScaleFL。该方法通过早期退出策略，自适应地缩小深度神经网络的宽度和深度，以适应不同规模的本地模型。同时，通过自我蒸馏在退出预测中进行知识转移，提高子网络之间的聚合效果。<br>
                    效果：实验结果表明，ScaleFL在全局/局部模型性能上优于现有的代表性异构联邦学习方法，并且在保持性能下降不超过2%的情况下，推理效率提高了2倍，模型大小减少了4倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning (FL) is an attractive distributed learning paradigm supporting real-time continuous learning and client privacy by default. In most FL approaches, all edge clients are assumed to have sufficient computation capabilities to participate in the learning of a deep neural network (DNN) model. However, in real-life applications, some clients may have severely limited resources and can only train a much smaller local model. This paper presents ScaleFL, a novel FL approach with two distinctive mechanisms to handle resource heterogeneity and provide an equitable FL framework for all clients. First, ScaleFL adaptively scales down the DNN model along width and depth dimensions by leveraging early exits to find the best-fit models for resource-aware local training on distributed clients. In this way, ScaleFL provides an efficient balance of preserving basic and complex features in local model splits with various sizes for joint training while enabling fast inference for model deployment. Second, ScaleFL utilizes self-distillation among exit predictions during training to improve aggregation through knowledge transfer among subnetworks. We conduct extensive experiments on benchmark CV (CIFAR-10/100, ImageNet) and NLP datasets (SST-2, AgNews). We demonstrate that ScaleFL outperforms existing representative heterogeneous FL approaches in terms of global/local model performance and provides inference efficiency, with up to 2x latency and 4x model size reduction with negligible performance drop below 2%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1687.Reliable and Interpretable Personalized Federated Learning</span><br>
                <span class="as">Qin, ZixuanandYang, LiuandWang, QilongandHan, YahongandHu, Qinghua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Reliable_and_Interpretable_Personalized_Federated_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20422-20431.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一种可靠的个性化联邦学习方法，以更好地利用群体知识。<br>
                    动机：在数据分布存在较大差异的情况下，联邦学习需要设计可靠的客户端选择策略和可解释的客户端通信框架。<br>
                    方法：提出了一种称为RIPFL的可靠个性化联邦学习方法，该方法通过贝叶斯决策规则和证据理论将个人信息与全局模型生成的社会信息有效整合。<br>
                    效果：实验结果表明，该方法比其他最先进的联邦学习算法具有更强的鲁棒性和准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning can coordinate multiple users to participate in data training while ensuring data privacy. The collaboration of multiple agents allows for a natural connection between federated learning and collective intelligence. When there are large differences in data distribution among clients, it is crucial for federated learning to design a reliable client selection strategy and an interpretable client communication framework to better utilize group knowledge. Herein, a reliable personalized federated learning approach, termed RIPFL, is proposed and fully interpreted from the perspective of social learning. RIPFL reliably selects and divides the clients involved in training such that each client can use different amounts of social information and more effectively communicate with other clients. Simultaneously, the method effectively integrates personal information with the social information generated by the global model from the perspective of Bayesian decision rules and evidence theory, enabling individuals to grow better with the help of collective wisdom. An interpretable federated learning mind is well scalable, and the experimental results indicate that the proposed method has superior robustness and accuracy than other state-of-the-art federated learning algorithms.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1688.Equivalent Transformation and Dual Stream Network Construction for Mobile Image Super-Resolution</span><br>
                <span class="as">Chao, JiahaoandZhou, ZhouandGao, HongfanandGong, JialiandYang, ZhengfengandZeng, ZhenbingandDehbi, Lydia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chao_Equivalent_Transformation_and_Dual_Stream_Network_Construction_for_Mobile_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14102-14111.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，对移动设备上的实时超分辨率网络的需求日益增长。<br>
                    动机：尽管已经提出了许多轻量级的超分辨率模型，但这些模型仍然包含增加推理延迟的耗时组件，限制了它们在移动设备上的现实应用。<br>
                    方法：本文提出了一种基于等效变换和双流网络构建（ETDS）的新型单图像超分辨率模型。等效变换方法将耗时的操作转换为移动设备上的友好操作，如卷积和ReLU。然后设计了一个双流网络来减轻由等效变换产生的冗余参数并增强特征提取能力。<br>
                    效果：通过充分利用等效变换和双流网络结构的进步，我们开发了用于移动设备的高效SR模型ETDS。实验结果表明，与先前的轻量级SR方法相比，我们的ETDS在移动设备上实现了优越的推理速度和重建质量。代码可在https://github.com/ECNUSR/ETDS获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, there has been an increasing demand for real-time super-resolution networks on mobile devices. To address this issue, many lightweight super-resolution models have been proposed. However, these models still contain time-consuming components that increase inference latency, limiting their real-world applications on mobile devices. In this paper, we propose a novel model for singleimage super-resolution based on Equivalent Transformation and Dual Stream network construction (ETDS). ET method is proposed to transform time-consuming operators into time-friendly ones such as convolution and ReLU on mobile devices. Then, a dual stream network is designed to alleviate redundant parameters yielded from ET and enhance the feature extraction ability. Taking full advantage of the advance of ET and the dual stream network structure, we develop the efficient SR model ETDS for mobile devices. The experimental results demonstrate that our ETDS achieves superior inference speed and reconstruction quality compared to prior lightweight SR methods on mobile devices. The code is available at https://github.com/ECNUSR/ETDS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1689.DyNCA: Real-Time Dynamic Texture Synthesis Using Neural Cellular Automata</span><br>
                <span class="as">Pajouheshgar, EhsanandXu, YitaoandZhang, TongandS\&quot;usstrunk, Sabine</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pajouheshgar_DyNCA_Real-Time_Dynamic_Texture_Synthesis_Using_Neural_Cellular_Automata_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20742-20751.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的动态纹理合成模型需要慢速迭代优化过程来合成固定大小的短视频，并且无法对合成过程进行后训练控制。<br>
                    动机：提出一种实时、可控的动态神经网络细胞自动机（DyNCA）框架，用于真实感视频纹理的实时合成。<br>
                    方法：基于最近引入的NCA模型构建，可以在实时内合成无限长和任意大小的真实感视频纹理。<br>
                    效果：通过定量和定性评估，证明我们的模型生成的视频比现有结果更真实。在SOTA DyTS性能上提高了2-4个数量级。此外，我们的模型提供了包括运动速度、运动方向和编辑刷工具在内的几种实时视频控制功能。我们在本地硬件上运行的在线交互式演示中展示了我们的训练模型，可在个人电脑和智能手机上访问。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-size realistic video textures in real-time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by 2 4 orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal computers and smartphones.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1690.Ultrahigh Resolution Image/Video Matting With Spatio-Temporal Sparsity</span><br>
                <span class="as">Sun, YananandTang, Chi-KeungandTai, Yu-Wing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Ultrahigh_Resolution_ImageVideo_Matting_With_Spatio-Temporal_Sparsity_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14112-14121.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对超高清（UHR）图像/视频进行高质量的抠图？<br>
                    动机：现有的抠图算法无法直接处理全分辨率的超高清图像，而基于补丁的方法可能会引入不美观的人工痕迹。<br>
                    方法：提出了一种名为SparseMat的新方法，该方法利用空间和时间稀疏性来解决通用的超高清抠图问题。在处理视频时，通过合理利用空间和时间稀疏性，可以大大减少计算冗余。<br>
                    效果：实验证明，SparseMat可以在一次处理中有效地为超高清图像和视频生成高质量的alpha通道。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Commodity ultra-high definition (UHD) displays are becoming more affordable which demand imaging in ultra high resolution (UHR). This paper proposes SparseMat, a computationally efficient approach for UHR image/video matting. Note that it is infeasible to directly process UHR images at full resolution in one shot using existing matting algorithms without running out of memory on consumer-level computational platforms, e.g., Nvidia 1080Ti with 11G memory, while patch-based approaches can introduce unsightly artifacts due to patch partitioning. Instead, our method resorts to spatial and temporal sparsity for solving general UHR matting. During processing videos, huge computation redundancy can be reduced through the rational use of spatial and temporal sparsity. In this paper, we show how to effectively estimate spatio-temporal sparsity, which serves as a gate to activate input pixels for the matting model. Under the guidance of such sparsity, our method discards patch-based inference in lieu of memory-efficient and full-resolution matte refinement. Extensive experiments demonstrate that SparseMat can effectively and efficiently generate high-quality alpha matte for UHR images and videos in one shot.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1691.Tunable Convolutions With Parametric Multi-Loss Optimization</span><br>
                <span class="as">Maggioni, MatteoandTanay, ThomasandBabiloni, FrancescaandMcDonagh, StevenandLeonardis, Ale\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Maggioni_Tunable_Convolutions_With_Parametric_Multi-Loss_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20226-20236.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何根据外部因素在推理时调整神经网络的行为，特别是在图像到图像的转换任务中平衡感知失真。<br>
                    动机：传统的卷积神经网络在训练时损失和数据的选择是固定的，但在推理时需要根据用户偏好或数据的动态特性进行调整。<br>
                    方法：提出一种参数化的可调卷积层，该层包含多个不同的核，并使用一个包含相同数量目标的参数化多损失进行优化。通过共享一组参数来动态地插值目标和内核，从而在训练时随机采样这些参数以明确优化所有可能的目标组合，并在推理时将这些参数作为模型的交互输入，从而实现对模型行为的可靠和一致的控制。<br>
                    效果：实验结果表明，这种可调卷积可以有效地替代传统卷积神经网络中的现有卷积，几乎不需要额外的计算成本，并在包括图像去噪、去模糊、超分辨率和风格转换在内的广泛应用中优于最先进的控制策略。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Behavior of neural networks is irremediably determined by the specific loss and data used during training. However it is often desirable to tune the model at inference time based on external factors such as preferences of the user or dynamic characteristics of the data. This is especially important to balance the perception-distortion trade-off of ill-posed image-to-image translation tasks. In this work, we propose to optimize a parametric tunable convolutional layer, which includes a number of different kernels, using a parametric multi-loss, which includes an equal number of objectives. Our key insight is to use a shared set of parameters to dynamically interpolate both the objectives and the kernels. During training, these parameters are sampled at random to explicitly optimize all possible combinations of objectives and consequently disentangle their effect into the corresponding kernels. During inference, these parameters become interactive inputs of the model hence enabling reliable and consistent control over the model behavior. Extensive experimental results demonstrate that our tunable convolutions effectively work as a drop-in replacement for traditional convolutions in existing neural networks at virtually no extra computational cost, outperforming state-of-the-art control strategies in a wide range of applications; including image denoising, deblurring, super-resolution, and style transfer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1692.Dense Network Expansion for Class Incremental Learning</span><br>
                <span class="as">Hu, ZhiyuanandLi, YunshengandLyu, JianchengandGao, DashanandVasconcelos, Nuno</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Dense_Network_Expansion_for_Class_Incremental_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11858-11867.png><br>
            
            <span class="tt"><span class="t0">研究问题：考虑类别增量学习（CIL）的问题。<br>
                    动机：现有的方法使用基于网络扩展（NE）的动态架构，每增加一个任务就添加一个任务专家，虽然有效但导致模型快速增大。<br>
                    方法：提出密集网络扩展（DNE）的新方法以实现准确性和模型复杂性之间的更好平衡。通过在任务专家网络的中间层之间引入密集连接，使旧任务和新任务的知识转移通过特征共享和重用实现。<br>
                    效果：实验结果表明，DNE方法严格保持旧类别的特征空间，同时网络和特征规模的增长速度远低于以往的方法。在准确性方面，DNE方法比先前的最先进方法高出4%，并且模型规模相似甚至更小。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The problem of class incremental learning (CIL) is considered. State-of-the-art approaches use a dynamic architecture based on network expansion (NE), in which a task expert is added per task. While effective from a computational standpoint, these methods lead to models that grow quickly with the number of tasks. A new NE method, dense network expansion (DNE), is proposed to achieve a better trade-off between accuracy and model complexity. This is accomplished by the introduction of dense connections between the intermediate layers of the task expert networks, that enable the transfer of knowledge from old to new tasks via feature sharing and reusing. This sharing is implemented with a cross-task attention mechanism, based on a new task attention block (TAB), that fuses information across tasks. Unlike traditional attention mechanisms, TAB operates at the level of the feature mixing and is decoupled with spatial attentions. This is shown more effective than a joint spatial-and-task attention for CIL. The proposed DNE approach can strictly maintain the feature space of old classes while growing the network and feature scale at a much slower rate than previous methods. In result, it outperforms the previous SOTA methods by a margin of 4% in terms of accuracy, with similar or even smaller model scale.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1693.Rethinking Gradient Projection Continual Learning: Stability / Plasticity Feature Space Decoupling</span><br>
                <span class="as">Zhao, ZhenandZhang, ZhizhongandTan, XinandLiu, JunandQu, YanyunandXie, YuanandMa, Lizhuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Rethinking_Gradient_Projection_Continual_Learning_Stability__Plasticity_Feature_Space_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3718-3727.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在持续学习中，使模型在不断学习新类别的同时，不忘记已学知识。<br>
                    动机：现有的方法需要梯度与整个特征空间完全正交，导致在新任务到来时，特征空间无限扩大，可行的梯度方向变窄，影响了模型的可塑性。<br>
                    方法：提出空间解耦（SD）算法，将特征空间解耦为互补的两个子空间：稳定性空间I和可塑性空间R。I通过历史和当前特征空间的交集建立，包含更多任务共享的基础；R是I的正交补空间，主要包含更多特定任务的基础。通过对R和I施加区分性约束，实现稳定性和可塑性之间的更好平衡。<br>
                    效果：实验表明，SD对梯度投影基线的应用具有模型无关性，并在公开数据集上取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space I, and the plasticity space R. I is established by conducting space intersection between the historic and current feature space, and thus I contains more task-shared bases. R is constructed by seeking the orthogonal complementary subspace of I, and thus R mainly contains more task-specific bases. By putting the distinguishing constraints on R and I, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1694.DisWOT: Student Architecture Search for Distillation WithOut Training</span><br>
                <span class="as">Dong, PeijieandLi, LujunandWei, Zimian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11898-11908.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地训练轻量级的学生模型，以提高其性能？<br>
                    动机：现有的知识蒸馏（KD）策略在教师-学生对架构差异大的情况下，限制了蒸馏的收益。<br>
                    方法：我们提出了一种无需训练的框架，通过遗传算法寻找给定教师的最佳学生架构。我们首先实证发现，基础训练下的最优模型不一定是蒸馏中的赢家。其次，我们发现随机初始化的教师-学生网络的特征语义和样本关系的相似性与最终的蒸馏性能有良好相关性。因此，我们通过条件于语义激活图的相似性矩阵来选择最佳学生，从而显著提高了模型在蒸馏阶段的性能，并至少加速了180倍的训练。<br>
                    效果：我们在CIFAR、ImageNet和NAS-Bench-201上进行实验，证明我们的技术在不同的搜索空间上都取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation WithOut Training (DisWOT) significantly improves the performance of the model in the distillation stage with at least 180x training acceleration. Additionally, we extend similarity metrics in DisWOT as new distillers and KD-based zero-proxies. Our experiments on CIFAR, ImageNet and NAS-Bench-201 demonstrate that our technique achieves state-of-the-art results on different search spaces. Our project and code are available at https://lilujunai.github.io/DisWOT-CVPR2023/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1695.Independent Component Alignment for Multi-Task Learning</span><br>
                <span class="as">Senushkin, DmitryandPatakin, NikolayandKuznetsov, ArsenyandKonushin, Anton</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Senushkin_Independent_Component_Alignment_for_Multi-Task_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20083-20093.png><br>
            
            <span class="tt"><span class="t0">研究问题：多任务学习（MTL）优化中存在冲突和主导梯度的问题。<br>
                    动机：提出一种新的MTL优化方法，通过消除训练过程中的不稳定性来提高性能。<br>
                    方法：提出了一种基于线性梯度系统条件数的稳定性标准，并据此提出了新的MTL优化方法Aligned-MTL。<br>
                    效果：实验证明，该方法在语义和实例分割、深度估计、表面法线估计和强化学习等多个MTL基准测试中，都能持续提升性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In a multi-task learning (MTL) setting, a single model is trained to tackle a diverse set of tasks jointly. Despite rapid progress in the field, MTL remains challenging due to optimization issues such as conflicting and dominating gradients. In this work, we propose using a condition number of a linear system of gradients as a stability criterion of an MTL optimization. We theoretically demonstrate that a condition number reflects the aforementioned optimization issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach based on the proposed criterion, that eliminates instability in the training process by aligning the orthogonal components of the linear system of gradients. While many recent MTL approaches guarantee convergence to a minimum, task trade-offs cannot be specified in advance. In contrast, Aligned-MTL provably converges to an optimal point with pre-defined task-specific weights, which provides more control over the optimization result. Through experiments, we show that the proposed approach consistently improves performance on a diverse set of MTL benchmarks, including semantic and instance segmentation, depth estimation, surface normal estimation, and reinforcement learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1696.Improved Distribution Matching for Dataset Condensation</span><br>
                <span class="as">Zhao, GanlongandLi, GuanbinandQin, YipengandYu, Yizhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7856-7865.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决传统数据集压缩方法在优化过程中计算量大、效率低的问题。<br>
                    动机：现有的数据集压缩方法主要通过在模型优化过程中进行梯度或参数匹配来进行数据压缩，这种方法即使在小数据集和模型上也非常消耗计算资源。<br>
                    方法：本文提出了一种基于分布匹配的新型数据集压缩方法，该方法更加高效且有前景。我们识别了朴素分布匹配的两个重要缺点（即特征数量不平衡和距离计算的未验证嵌入），并通过三种新技术（即分区和扩展增强、有效且丰富的模型采样以及类别感知的分布正则化）来解决这些问题。<br>
                    效果：我们的简单而有效的方法以更少的计算资源优于大多数先前的优化导向方法，从而将数据压缩扩展到更大的数据集和模型。大量实验证明了我们的方法的有效性。代码可在https://github.com/uitrbn/IDM获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previous optimization-oriented methods with much fewer computational resources, thereby scaling data condensation to larger datasets and models. Extensive experiments demonstrate the effectiveness of our method. Codes are available at https://github.com/uitrbn/IDM</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1697.Slimmable Dataset Condensation</span><br>
                <span class="as">Liu, SonghuaandYe, JingwenandYu, RunpengandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3759-3768.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的数据集蒸馏方法在预算改变时，需要重新访问原始数据集进行合成过程，这既繁琐又可能无法实现。<br>
                    动机：为了解决这一问题，本文提出了可调整的数据集蒸馏方法，通过仅使用之前的压缩结果来提取更小的合成数据集。<br>
                    方法：我们首先研究了现有数据集蒸馏算法在这种连续压缩设置下的局限性，并确定了两个关键因素：（1）神经网络在不同压缩时间下的不一致性（2）合成数据的欠定解空间。因此，我们提出了一种新的可调整的数据集蒸馏训练目标，以明确考虑这两个因素。此外，我们的合成数据集采用重要性感知参数化。理论推导表明，通过丢弃次要成分可以在不训练的情况下实现上限误差。或者，如果允许训练，这种策略可以作为快速收敛的强大初始化。<br>
                    效果：广泛的比较和消融实验证明，所提出的方法在多个基准测试上优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dataset distillation, also known as dataset condensation, aims to compress a large dataset into a compact synthetic one. Existing methods perform dataset condensation by assuming a fixed storage or transmission budget. When the budget changes, however, they have to repeat the synthesizing process with access to original datasets, which is highly cumbersome if not infeasible at all. In this paper, we explore the problem of slimmable dataset condensation, to extract a smaller synthetic dataset given only previous condensation results. We first study the limitations of existing dataset condensation algorithms on such a successive compression setting and identify two key factors: (1) the inconsistency of neural networks over different compression times and (2) the underdetermined solution space for synthetic data. Accordingly, we propose a novel training objective for slimmable dataset condensation to explicitly account for both factors. Moreover, synthetic datasets in our method adopt an significance-aware parameterization. Theoretical derivation indicates that an upper-bounded error can be achieved by discarding the minor components without training. Alternatively, if training is allowed, this strategy can serve as a strong initialization that enables a fast convergence. Extensive comparisons and ablations demonstrate the superiority of the proposed solution over existing methods on multiple benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1698.Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint</span><br>
                <span class="as">Yu, ShikangandChen, JiachenandHan, HuandJiang, Shuqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Data-Free_Knowledge_Distillation_via_Feature_Exchange_and_Activation_Region_Constraint_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24266-24275.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管基于合成数据生成的数据自由知识蒸馏（DFKD）取得了巨大进展，但在多样化和高效数据合成方面仍存在限制。<br>
                    动机：简单结合基于生成网络的数据合成和数据增强并不能解决这些问题。因此，本文提出了一种基于通道特征交换（CFE）和多尺度空间激活区域一致性（mSARC）约束的新型数据自由知识蒸馏方法（SpaceshipNet）。<br>
                    方法：具体来说，CFE使我们的生成网络更好地从特征空间中采样，并有效地合成多样化的图像以学习学生网络。然而，仅使用CFE可能会严重放大合成图像中的不需要的噪声，这可能导致蒸馏学习无法改进甚至产生负面影响。因此，我们提出mSARC以确保学生网络不仅可以模仿教师网络的输出，还可以模仿其空间激活区域，以减轻不同合成图像中不需要的噪声对蒸馏学习的影响。<br>
                    效果：在CIFAR-10、CIFAR-100、Tiny-ImageNet、Imagenette和ImageNet100上进行的大量实验表明，我们的方法可以与不同的主干网络一起工作，并且优于最先进的DFKD方法。代码将在https://github.com/skgyu/SpaceshipNet上提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the tremendous progress on data-free knowledge distillation (DFKD) based on synthetic data generation, there are still limitations in diverse and efficient data synthesis. It is naive to expect that a simple combination of generative network-based data synthesis and data augmentation will solve these issues. Therefore, this paper proposes a novel data-free knowledge distillation method (SpaceshipNet) based on channel-wise feature exchange (CFE) and multi-scale spatial activation region consistency (mSARC) constraint. Specifically, CFE allows our generative network to better sample from the feature space and efficiently synthesize diverse images for learning the student network. However, using CFE alone can severely amplify the unwanted noises in the synthesized images, which may result in failure to improve distillation learning and even have negative effects. Therefore, we propose mSARC to assure the student network can imitate not only the logit output but also the spatial activation region of the teacher network in order to alleviate the influence of unwanted noises in diverse synthetic images on distillation learning. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, Imagenette, and ImageNet100 show that our method can work well with different backbone networks, and outperform the state-of-the-art DFKD methods. Code will be available at: https://github.com/skgyu/SpaceshipNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1699.FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation</span><br>
                <span class="as">He, JunjieandLi, PengyuandGeng, YifengandXie, Xuansong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_FastInst_A_Simple_Query-Based_Model_for_Real-Time_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23663-23672.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决目前实例分割中，查询模型在高效实时基准测试上的优势尚未得到充分证明的问题。<br>
                    动机：尽管查询模型无需最大抑制（NMS）并且是端到端的，但它们在高准确度实时基准测试上的优越性尚未得到充分证明。<br>
                    方法：本文提出了一种名为FastInst的简单有效的查询框架，用于实时实例分割。FastInst遵循最近引入的Mask2Former的元架构，其关键设计包括实例激活引导的查询、双路径更新策略和真实掩码引导的学习，使得我们可以使用更轻量的像素解码器和更少的Transformer解码器层，同时实现更好的性能。<br>
                    效果：实验表明，FastInst在速度和准确性上都优于大多数最先进的实时对应模型，包括强大的全卷积基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent attention in instance segmentation has focused on query-based models. Despite being non-maximum suppression (NMS)-free and end-to-end, the superiority of these models on high-accuracy real-time benchmarks has not been well demonstrated. In this paper, we show the strong potential of query-based models on efficient instance segmentation algorithm designs. We present FastInst, a simple, effective query-based framework for real-time instance segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells and whistles. Specifically, FastInst follows the meta-architecture of recently introduced Mask2Former. Its key designs include instance activation-guided queries, dual-path update strategy, and ground truth mask-guided learning, which enable us to use lighter pixel decoders, fewer Transformer decoder layers, while achieving better performance. The experiments show that FastInst outperforms most state-of-the-art real-time counterparts, including strong fully convolutional baselines, in both speed and accuracy. Code can be found at https://github.com/junjiehe96/FastInst.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1700.Transformer-Based Learned Optimization</span><br>
                <span class="as">G\&quot;artner, ErikandMetz, LukeandAndriluka, MykhayloandFreeman, C.DanielandSminchisescu, Cristian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gartner_Transformer-Based_Learned_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11970-11979.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的学习优化方法，通过神经网络表示优化器的更新步骤。<br>
                    动机：现有的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a new approach to learned optimization where we represent the computation of an optimizer's update step using a neural network. The parameters of the optimizer are then learned by training on a set of optimization tasks with the objective to perform minimization efficiently. Our innovation is a new neural network architecture, Optimus, for the learned optimizer inspired by the classic BFGS algorithm. As in BFGS, we estimate a preconditioning matrix as a sum of rank-one updates but use a Transformer-based neural network to predict these updates jointly with the step length and direction. In contrast to several recent learned optimization-based approaches, our formulation allows for conditioning across the dimensions of the parameter space of the target problem while remaining applicable to optimization tasks of variable dimensionality without retraining. We demonstrate the advantages of our approach on a benchmark composed of objective functions traditionally used for the evaluation of optimization algorithms, as well as on the real world-task of physics-based visual reconstruction of articulated 3d human motion.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1701.Dealing With Cross-Task Class Discrimination in Online Continual Learning</span><br>
                <span class="as">Guo, YiduoandLiu, BingandZhao, Dongyan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Dealing_With_Cross-Task_Class_Discrimination_in_Online_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11878-11887.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文探讨了现有持续学习研究中忽视的问题，即跨任务类别判别（CTCD），即如何在无法（或有限）访问旧任务数据的情况下，在新任务的类别和旧任务之间建立决策边界。<br>
                    动机：现有的再播放方法虽然部分解决了CTCD问题，但其在在线持续学习过程中存在动态训练偏见问题，这降低了重播数据解决CTCD问题的有效性。<br>
                    方法：提出了一种新的优化目标和基于梯度的自适应方法，以动态处理在线持续学习过程中的问题。<br>
                    效果：实验结果表明，新方法在在线持续学习中取得了更好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing continual learning (CL) research regards catastrophic forgetting (CF) as almost the only challenge. This paper argues for another challenge in class-incremental learning (CIL), which we call cross-task class discrimination (CTCD), i.e., how to establish decision boundaries between the classes of the new task and old tasks with no (or limited) access to the old task data. CTCD is implicitly and partially dealt with by replay-based methods. A replay method saves a small amount of data (replay data) from previous tasks. When a batch of current task data arrives, the system jointly trains the new data and some sampled replay data. The replay data enables the system to partially learn the decision boundaries between the new classes and the old classes as the amount of the saved data is small. However, this paper argues that the replay approach also has a dynamic training bias issue which reduces the effectiveness of the replay data in solving the CTCD problem. A novel optimization objective with a gradient-based adaptive method is proposed to dynamically deal with the problem in the online CL process. Experimental results show that the new method achieves much better results in online CL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1702.A-La-Carte Prompt Tuning (APT): Combining Distinct Data via Composable Prompting</span><br>
                <span class="as">Bowman, BenjaminandAchille, AlessandroandZancato, LucaandTrager, MatthewandPerera, PramudithaandPaolini, GiovanniandSoatto, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bowman_A-La-Carte_Prompt_Tuning_APT_Combining_Distinct_Data_via_Composable_Prompting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14984-14993.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用不同的数据源训练独立的提示，并在推理时任意组合它们？<br>
                    动机：为了解决在特定数据源上训练的模型无法适应其他数据源的问题。<br>
                    方法：提出了一种基于变压器的方案A-la-carte Prompt Tuning（APT），可以在不同的设备、时间和分布或领域上独立地训练各个提示。每个提示只包含其在训练期间接触的数据子集的信息。在推理时，可以根据任意选择的数据源组装模型，这被称为a-la-carte学习。<br>
                    效果：实验证明，a-la-carte构建的模型在各自数据源上的准确率达到了5%，并且在训练和推理时间上具有可比的成本。在持续学习基准测试Split CIFAR-100和CORe50上，实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce A-la-carte Prompt Tuning (APT), a transformer-based scheme to tune prompts on distinct data so that they can be arbitrarily composed at inference time. The individual prompts can be trained in isolation, possibly on different devices, at different times, and on different distributions or domains. Furthermore each prompt only contains information about the subset of data it was exposed to during training. During inference, models can be assembled based on arbitrary selections of data sources, which we call a-la-carte learning. A-la-carte learning enables constructing bespoke models specific to each user's individual access rights and preferences. We can add or remove information from the model by simply adding or removing the corresponding prompts without retraining from scratch. We demonstrate that a-la-carte built models achieve accuracy within 5% of models trained on the union of the respective sources, with comparable cost in terms of training and inference time. For the continual learning benchmarks Split CIFAR-100 and CORe50, we achieve state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1703.Computationally Budgeted Continual Learning: What Does Matter?</span><br>
                <span class="as">Prabhu, AmeyaandAlKaderHammoud, HasanAbedandDokania, PuneetK.andTorr, PhilipH.S.andLim, Ser-NamandGhanem, BernardandBibi, Adel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3698-3707.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决持续学习（CL）在实际应用中的问题，即如何在有限的计算和时间预算下进行有效的模型训练。<br>
                    动机：目前的持续学习方法主要关注于保留先前看到的数据，而对训练的计算预算没有任何限制。然而，对于实际应用来说，系统主要受限于计算和时间预算，而不是存储。<br>
                    方法：本文通过大规模的基准测试，分析了传统持续学习方法在计算受限环境下的性能。作者评估了各种持续学习的采样策略、蒸馏损失和部分微调等方法。<br>
                    效果：实验结果表明，在计算受限的环境下，传统的持续学习方法无法超越简单的基线方法。这一结论在不同的数据流时间步长和不同的计算预算下都是一致的，表明大多数现有的持续学习方法对于实际的预算部署来说过于昂贵。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1704.Decentralized Learning With Multi-Headed Distillation</span><br>
                <span class="as">Zhmoginov, AndreyandSandler, MarkandMiller, NolanandKristiansen, GusandVladymyrov, Max</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhmoginov_Decentralized_Learning_With_Multi-Headed_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8053-8063.png><br>
            
            <span class="tt"><span class="t0">研究问题：分散式学习中，多个拥有私有非独立同分布数据的代理如何在不共享数据、权重或权重更新的情况下相互学习。<br>
                    动机：解决机器学习中的分散式学习问题，允许多个代理在不共享数据的情况下从彼此那里学习。<br>
                    方法：提出一种新的基于蒸馏的分散式学习方法，该方法利用未标记的公共数据集和每个客户端的多个辅助头，大大提高了异构数据处理的训练效率。<br>
                    效果：这种方法使各个模型能够在保留和提高其私有任务性能的同时，也显著提高了其在全局聚合数据分布上的性能。研究表明，与孤立学习相比，我们的代理可以显著提高其性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Decentralized learning with private data is a central problem in machine learning. We propose a novel distillation-based decentralized learning technique that allows multiple agents with private non-iid data to learn from each other, without having to share their data, weights or weight updates. Our approach is communication efficient, utilizes an unlabeled public dataset and uses multiple auxiliary heads for each client, greatly improving training efficiency in the case of heterogeneous data. This approach allows individual models to preserve and enhance performance on their private tasks while also dramatically improving their performance on the global aggregated data distribution. We study the effects of data and model architecture heterogeneity and the impact of the underlying communication graph topology on learning efficiency and show that our agents can significantly improve their performance compared to learning in isolation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1705.Heterogeneous Continual Learning</span><br>
                <span class="as">Madaan, DivyamandYin, HongxuandByeon, WonminandKautz, JanandMolchanov, Pavlo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Madaan_Heterogeneous_Continual_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15985-15995.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决随着网络架构的快速进步，如何将现有的解决方案适应到新的架构中的持续学习（CL）问题。<br>
                    动机：大多数CL方法都集中在通过修改权重来适应新任务/类别的单一架构上，但随着架构设计的迅速发展，如何将现有解决方案适应到新的架构中的问题变得相关。<br>
                    方法：我们提出了异构持续学习（HCL），其中各种不断发展的网络架构与新的数据/任务一起不断出现。作为解决方案，我们在蒸馏技术系列的基础上进行了修改，使较弱的模型扮演教师的角色；同时，一个新的更强的架构充当学生的角色。此外，我们还考虑了对以前的数据访问有限的设置，并提出了快速深度反演（QDI）以恢复先前任务的视觉特征以支持知识转移。<br>
                    效果：我们的评估表明，与各种网络架构上的最先进方法相比，我们的方案在准确性方面有了显著的提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel framework and a solution to tackle the continual learning (CL) problem with changing network architectures. Most CL methods focus on adapting a single architecture to a new task/class by modifying its weights. However, with rapid progress in architecture design, the problem of adapting existing solutions to novel architectures becomes relevant. To address this limitation, we propose Heterogeneous Continual Learning (HCL), where a wide range of evolving network architectures emerge continually together with novel data/tasks. As a solution, we build on top of the distillation family of techniques and modify it to a new setting where a weaker model takes the role of a teacher; meanwhile, a new stronger architecture acts as a student. Furthermore, we consider a setup of limited access to previous data and propose Quick Deep Inversion (QDI) to recover prior task visual features to support knowledge transfer. QDI significantly reduces computational costs compared to previous solutions and improves overall performance. In summary, we propose a new setup for CL with a modified knowledge distillation paradigm and design a quick data inversion method to enhance distillation. Our evaluation of various benchmarks shows a significant improvement on accuracy in comparison to state-of-the-art methods over various networks architectures.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1706.Deep Graph Reprogramming</span><br>
                <span class="as">Jing, YongchengandYuan, ChongbinandJu, LiandYang, YidingandWang, XinchaoandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jing_Deep_Graph_Reprogramming_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24345-24354.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索一种针对图神经网络（GNNs）的新颖模型重用任务，称为“深度图重编程”。<br>
                    动机：为了在不修改原始节点特征或模型参数的情况下，重新编程预训练的GNN以处理各种领域中的跨级别下游任务。<br>
                    方法：提出了创新的数据重编程和模型重编程两种范式。数据重编程旨在解决输入端不同任务的异构图形特征维度的挑战，而模型重编程则缓解了固定每任务每模型行为的困境。<br>
                    效果：实验结果表明，所提出的方法在14个数据集上产生了令人满意的结果，与从头开始重新训练的结果相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we explore a novel model reusing task tailored for graph neural networks (GNNs), termed as "deep graph reprogramming". We strive to reprogram a pre-trained GNN, without amending raw node features nor model parameters, to handle a bunch of cross-level downstream tasks in various domains. To this end, we propose an innovative Data Reprogramming paradigm alongside a Model Reprogramming paradigm. The former one aims to address the challenge of diversified graph feature dimensions for various tasks on the input side, while the latter alleviates the dilemma of fixed per-task-per-model behavior on the model side. For data reprogramming, we specifically devise an elaborated Meta-FeatPadding method to deal with heterogeneous input dimensions, and also develop a transductive Edge-Slimming as well as an inductive Meta-GraPadding approach for diverse homogenous samples. Meanwhile, for model reprogramming, we propose a novel task-adaptive Reprogrammable-Aggregator, to endow the frozen model with larger expressive capacities in handling cross-domain tasks. Experiments on fourteen datasets across node/graph classification/regression, 3D object recognition, and distributed action recognition, demonstrate that the proposed methods yield gratifying results, on par with those by re-training from scratch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1707.Compacting Binary Neural Networks by Sparse Kernel Selection</span><br>
                <span class="as">Wang, YikaiandHuang, WenbingandDong, YinpengandSun, FuchunandYao, Anbang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Compacting_Binary_Neural_Networks_by_Sparse_Kernel_Selection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24374-24383.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过学习非重复的二进制核子空间来压缩典型的二值神经网络，并进一步优化性能。<br>
                    动机：成功的二值神经网络中的二进制核通常呈幂律分布，其值大多聚集在少数几个码字中，这种现象鼓励我们压缩典型的二值神经网络并通过学习二进制核子空间内的非重复核子来获得更接近的性能。<br>
                    方法：我们将二值化过程视为基于二进制码本的核分组，任务是学习从完整码本中选择较小的码字子集。然后利用Gumbel-Sinkhorn技术近似码字选择过程，并开发排列直接估计器（PSTE），该估计器不仅可以端到端优化选择过程，还可以保持所选码字的非重复占用。<br>
                    效果：实验证明，我们的方法减少了模型大小和位计算成本，并在可比预算下实现了与最先进的二值神经网络相比的准确性改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of storage and computation. This paper is motivated by a previously revealed phenomenon that the binary kernels in successful BNNs are nearly power-law distributed: their values are mostly clustered into a small number of codewords. This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning non-repetitive kernels within a binary kernel subspace. Specifically, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-to-end but also maintain the non-repetitive occupancy of selected codewords. Experiments verify that our method reduces both the model size and bit-wise computational costs, and achieves accuracy improvements compared with state-of-the-art BNNs under comparable budgets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1708.EMT-NAS:Transferring Architectural Knowledge Between Tasks From Different Datasets</span><br>
                <span class="as">Liao, PengandJin, YaochuandDu, Wenli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_EMT-NASTransferring_Architectural_Knowledge_Between_Tasks_From_Different_Datasets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3643-3653.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练多个相关任务来提高深度学习模型的性能，同时避免负迁移的问题。<br>
                    动机：多任务学习的成功主要归功于相关任务的共享表示，使模型能够更好地泛化。然而，在多个相关任务上联合训练权重参数可能会导致性能下降，即负迁移。<br>
                    方法：本文提出了一种进化多任务神经架构搜索（EMT-NAS）算法，通过在不同相关任务之间转移架构知识来加速搜索过程。与传统的多任务学习不同，EMT-NAS中的每个任务都有个性化的网络架构和自己的权重，从而有效地减轻了负迁移的影响。<br>
                    效果：通过在CIFAR-10、CIFAR-100和四个MedMNIST数据集上进行分类任务的实验，证明了EMT-NAS在找到具有竞争力的神经网络架构方面比单任务版本更快，在CIFAR和MedMNIST上分别节省了8%和40%的时间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The success of multi-task learning (MTL) can largely be attributed to the shared representation of related tasks, allowing the models to better generalise. In deep learning, this is usually achieved by sharing a common neural network architecture and jointly training the weights. However, the joint training of weighting parameters on multiple related tasks may lead to performance degradation, known as negative transfer. To address this issue, this work proposes an evolutionary multi-tasking neural architecture search (EMT-NAS) algorithm to accelerate the search process by transferring architectural knowledge across multiple related tasks. In EMT-NAS, unlike the traditional MTL, the model for each task has a personalised network architecture and its own weights, thus offering the capability of effectively alleviating negative transfer. A fitness re-evaluation method is suggested to alleviate fluctuations in performance evaluations resulting from parameter sharing and the mini-batch gradient descent training method, thereby avoiding losing promising solutions during the search process. To rigorously verify the performance of EMT-NAS, the classification tasks used in the empirical assessments are derived from different datasets, including the CIFAR-10 and CIFAR-100, and four MedMNIST datasets. Extensive comparative experiments on different numbers of tasks demonstrate that EMT-NAS takes 8% and up to 40% on CIFAR and MedMNIST, respectively, less time to find competitive neural architectures than its single-task counterparts.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1709.Hierarchical B-Frame Video Coding Using Two-Layer CANF Without Motion Coding</span><br>
                <span class="as">Alexandre, DavidandHang, Hsueh-MingandPeng, Wen-Hsiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Alexandre_Hierarchical_B-Frame_Video_Coding_Using_Two-Layer_CANF_Without_Motion_Coding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10249-10258.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新颖的B-frame编码架构，该架构无需传输任何运动信息。<br>
                    动机：传统的视频压缩系统通常包括运动编码和残差编码两个主要模块，而深度学习基础的编码方案也采用了这种通用架构。作者提出了一种新的基于两层条件增强正规化流（CANF）的视频压缩架构，其显著特点是不需要传输任何运动信息。<br>
                    方法：作者提出的视频压缩无运动编码的想法为学习视频编码提供了新的方向。基本层是一个低分辨率图像压缩器，取代了全分辨率的运动压缩器。低分辨率编码的图像与扭曲的高分辨率图像合并，生成高质量的图像，作为全分辨率增强层图像编码的条件信号。<br>
                    效果：虽然该方案的率失真性能略低于最先进的学习B-frame编码方案B-CANF，但优于其他学习B-frame编码方案。相比于B-CANF，该方案在编码和解码过程中分别节省了45%和27%的乘法累加运算（MACs）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Typical video compression systems consist of two main modules: motion coding and residual coding. This general architecture is adopted by classical coding schemes (such as international standards H.265 and H.266) and deep learning-based coding schemes. We propose a novel B-frame coding architecture based on two-layer Conditional Augmented Normalization Flows (CANF). It has the striking feature of not transmitting any motion information. Our proposed idea of video compression without motion coding offers a new direction for learned video coding. Our base layer is a low-resolution image compressor that replaces the full-resolution motion compressor. The low-resolution coded image is merged with the warped high-resolution images to generate a high-quality image as a conditioning signal for the enhancement-layer image coding in full resolution. One advantage of this architecture is significantly reduced computational complexity due to eliminating the motion information compressor. In addition, we adopt a skip-mode coding technique to reduce the transmitted latent samples. The rate-distortion performance of our scheme is slightly lower than that of the state-of-the-art learned B-frame coding scheme, B-CANF, but outperforms other learned B-frame coding schemes. However, compared to B-CANF, our scheme saves 45% of multiply-accumulate operations (MACs) for encoding and 27% of MACs for decoding. The code is available at https://nycu-clab.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1710.SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</span><br>
                <span class="as">Chen, XuanyaoandLiu, ZhijianandTang, HaotianandYi, LiandZhao, HangandHan, Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SparseViT_Revisiting_Activation_Sparsity_for_Efficient_High-Resolution_Vision_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2061-2070.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低高分辨率图像在神经网络中学习丰富视觉表示的计算复杂性，以适应对延迟敏感的应用。<br>
                    动机：虽然高分辨率图像可以提升神经网络的学习效果，但其带来的计算复杂性增加却阻碍了其在延迟敏感应用中的使用。<br>
                    方法：本文提出了SparseViT，通过激活稀疏化重新审视了基于窗口的视觉转换器（ViTs）。由于窗口注意力自然地批量处理块，因此实际的窗口激活剪枝加速成为可能。<br>
                    效果：实验结果表明，与密集模型相比，SparseViT在单目3D对象检测、2D实例分割和2D语义分割任务上分别实现了1.5倍、1.4倍和1.3倍的速度提升，同时精度损失可忽略不计。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-resolution images enable neural networks to learn richer visual representations. However, this improved performance comes at the cost of growing computational complexity, hindering their usage in latency-sensitive applications. As not all pixels are equal, skipping computations for less-important regions offers a simple and effective measure to reduce the computation. This, however, is hard to be translated into actual speedup for CNNs since it breaks the regularity of the dense convolution workload. In this paper, we introduce SparseViT that revisits activation sparsity for recent window-based vision transformers (ViTs). As window attentions are naturally batched over blocks, actual speedup with window activation pruning becomes possible: i.e.,  50% latency reduction with 60% sparsity. Different layers should be assigned with different pruning ratios due to their diverse sensitivities and computational costs. We introduce sparsity-aware adaptation and apply the evolutionary search to efficiently find the optimal layerwise sparsity configuration within the vast search space. SparseViT achieves speedups of 1.5x, 1.4x, and 1.3x compared to its dense counterpart in monocular 3D object detection, 2D instance segmentation, and 2D semantic segmentation, respectively, with negligible to no loss of accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1711.Efficient Semantic Segmentation by Altering Resolutions for Compressed Videos</span><br>
                <span class="as">Hu, YubinandHe, YuzeandLi, YanghaoandLi, JishengandHan, YuxingandWen, JiangtaoandLiu, Yong-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Efficient_Semantic_Segmentation_by_Altering_Resolutions_for_Compressed_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22627-22637.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频语义分割（VSS）是一项计算密集型任务，由于需要对高帧率的视频进行逐帧预测。<br>
                    动机：现有的VSS模型或策略没有考虑到影响计算成本的一个重要因素——输入分辨率。<br>
                    方法：本文提出了一种名为AR-Seg的可变分辨率框架，通过降低非关键帧的分辨率来有效进行VSS。设计了一个跨分辨率特征融合（CReFF）模块，并采用新颖的特征相似性训练（FST）策略进行监督。<br>
                    效果：在CamVid和Cityscapes上的大量实验表明，AR-Seg实现了最先进的性能，并且与不同的分割骨干网络兼容。在CamVid上，AR-Seg在使用PSPNet18骨干网络的情况下，节省了67%的计算成本（以GFLOPs为单位），同时保持了高精度的分割准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Video semantic segmentation (VSS) is a computationally expensive task due to the per-frame prediction for videos of high frame rates. In recent work, compact models or adaptive network strategies have been proposed for efficient VSS. However, they did not consider a crucial factor that affects the computational cost from the input side: the input resolution. In this paper, we propose an altering resolution framework called AR-Seg for compressed videos to achieve efficient VSS. AR-Seg aims to reduce the computational cost by using low resolution for non-keyframes. To prevent the performance degradation caused by downsampling, we design a Cross Resolution Feature Fusion (CReFF) module, and supervise it with a novel Feature Similarity Training (FST) strategy. Specifically, CReFF first makes use of motion vectors stored in a compressed video to warp features from high-resolution keyframes to low-resolution non-keyframes for better spatial alignment, and then selectively aggregates the warped features with local attention mechanism. Furthermore, the proposed FST supervises the aggregated features with high-resolution features through an explicit similarity loss and an implicit constraint from the shared decoding layer. Extensive experiments on CamVid and Cityscapes show that AR-Seg achieves state-of-the-art performance and is compatible with different segmentation backbones. On CamVid, AR-Seg saves 67% computational cost (measured in GFLOPs) with the PSPNet18 backbone while maintaining high segmentation accuracy. Code: https://github.com/THU-LYJ-Lab/AR-Seg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1712.FlowGrad: Controlling the Output of Generative ODEs With Gradients</span><br>
                <span class="as">Liu, XingchaoandWu, LemengandZhang, ShujianandGong, ChengyueandPing, WeiandLiu, Qiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlowGrad_Controlling_the_Output_of_Generative_ODEs_With_Gradients_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24335-24344.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何控制预训练的基于微分方程（ODE）的生成模型的生成内容。<br>
                    动机：尽管基于ODE的生成模型在各种应用上取得了显著的成果，但很少有研究关注如何控制其生成的内容。<br>
                    方法：提出一种优化ODE模型输出的方法，根据指导函数实现可控生成。通过分解反向传播和计算向量雅可比积，将梯度从输出有效地反向传播到ODE轨迹的任何中间时间步。为了进一步加速反向传播的计算，提出了一种非均匀离散化方法来近似ODE轨迹，根据轨迹的直线程度进行测量并将直线部分聚集到一个离散化步骤中。<br>
                    效果：该方法被称为FlowGrad，在文本引导的图像操作上超过了最先进的基线。此外，FlowGrad能够在冻结的基于ODE的生成模型中找到全局语义方向，用于操纵新的图像而无需额外的优化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generative modeling with ordinary differential equations (ODEs) has achieved fantastic results on a variety of applications. Yet, few works have focused on controlling the generated content of a pre-trained ODE-based generative model. In this paper, we propose to optimize the output of ODE models according to a guidance function to achieve controllable generation. We point out that, the gradients can be efficiently back-propagated from the output to any intermediate time steps on the ODE trajectory, by decomposing the back-propagation and computing vector-Jacobian products. To further accelerate the computation of the back-propagation, we propose to use a non-uniform discretization to approximate the ODE trajectory, where we measure how straight the trajectory is and gather the straight parts into one discretization step. This allows us to save  90% of the back-propagation time with ignorable error. Our framework, named FlowGrad, outperforms the state-of-the-art baselines on text-guided image manipulation. Moreover, FlowGrad enables us to find global semantic directions in frozen ODE-based generative models that can be used to manipulate new images without extra optimization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1713.SMPConv: Self-Moving Point Representations for Continuous Convolution</span><br>
                <span class="as">Kim, SanghyeonandPark, Eunbyung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_SMPConv_Self-Moving_Point_Representations_for_Continuous_Convolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10289-10299.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种无需神经网络的连续卷积构建方法，以提高计算效率和性能。<br>
                    动机：目前的连续卷积实现主要依赖于多层感知器（MLPs），但存在计算成本高、超参数调整复杂以及滤波器描述能力有限等问题。<br>
                    方法：我们提出了自我移动点表示法，其中权重参数自由移动，并使用插值方案来实现连续函数。在构造卷积核时，实验结果表明，该方法在现有框架中具有改进的性能。<br>
                    效果：由于其轻量级结构，我们是首次在大规模设置（如ImageNet）中展示连续卷积的有效性，并展示了对先前技术的改进。我们的代码可在https://github.com/sangnekim/SMPConv获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continuous convolution has recently gained prominence due to its ability to handle irregularly sampled data and model long-term dependency. Also, the promising experimental results of using large convolutional kernels have catalyzed the development of continuous convolution since they can construct large kernels very efficiently. Leveraging neural networks, more specifically multilayer perceptrons (MLPs), is by far the most prevalent approach to implementing continuous convolution. However, there are a few drawbacks, such as high computational costs, complex hyperparameter tuning, and limited descriptive power of filters. This paper suggests an alternative approach to building a continuous convolution without neural networks, resulting in more computationally efficient and improved performance. We present self-moving point representations where weight parameters freely move, and interpolation schemes are used to implement continuous functions. When applied to construct convolutional kernels, the experimental results have shown improved performance with drop-in replacement in the existing frameworks. Due to its lightweight structure, we are first to demonstrate the effectiveness of continuous convolution in a large-scale setting, e.g., ImageNet, presenting the improvements over the prior arts. Our code is available on https://github.com/sangnekim/SMPConv</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1714.HNeRV: A Hybrid Neural Representation for Videos</span><br>
                <span class="as">Chen, HaoandGwilliam, MatthewandLim, Ser-NamandShrivastava, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_HNeRV_A_Hybrid_Neural_Representation_for_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10270-10279.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频插值任务中隐式神经网络表示的局限性，如重构能力和内部泛化能力不足。<br>
                    动机：目前的隐式神经网络表示（NeRV, E-NeRV等）通过固定和与内容无关的嵌入来重构视频帧，这在很大程度上限制了视频插值的回归能力和内部泛化能力。<br>
                    方法：本文提出了一种混合神经网络视频表示（HNeRV），其中可学习和与内容自适应的嵌入作为解码器输入。此外，引入了HNeRV块，使模型参数在整个网络中均匀分布，因此靠近输出的高层可以具有更高的存储高分辨率内容和视频细节的能力。<br>
                    效果：通过与内容自适应的嵌入和重新设计的模型架构，HNeRV在视频插值任务上优于隐式方法（NeRV, E-NeRV），无论是在重构质量和收敛速度上，还是在内部泛化上。作为一种简单而高效的视频表示，HNeRV在速度、灵活性和部署方面也优于传统编解码器（H.264, H.265）和基于学习的压缩方法。最后，探索了HNeRV在视频压缩和视频修复等下游任务上的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Implicit neural representations store videos as neural networks and have performed well for vision tasks such as video compression and denoising. With frame index and/or positional index as input, implicit representations (NeRV, E-NeRV, etc.) reconstruct video frames from fixed and content-agnostic embeddings. Such embedding largely limits the regression capacity and internal generalization for video interpolation. In this paper, we propose a Hybrid Neural Representation for Videos (HNeRV), where learnable and content-adaptive embeddings act as decoder input. Besides the input embedding, we introduce a HNeRV block to make model parameters evenly distributed across the entire network, therefore higher layers (layers near the output) can have more capacity to store high-resolution content and video details. With content-adaptive embedding and re-designed model architecture, HNeRV outperforms implicit methods (NeRV, E-NeRV) in video regression task for both reconstruction quality and convergence speed, and shows better internal generalization. As a simple and efficient video representation, HNeRV also shows decoding advantages for speed, flexibility, and deployment, compared to traditional codecs (H.264, H.265) and learning-based compression methods. Finally, we explore the effectiveness of HNeRV on downstream tasks such as video compression and video inpainting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1715.Decoupling Learning and Remembering: A Bilevel Memory Framework With Knowledge Projection for Task-Incremental Learning</span><br>
                <span class="as">Sun, WenjuandLi, QingyongandZhang, JingandWang, WenandGeng, Yangli-ao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Decoupling_Learning_and_Remembering_A_Bilevel_Memory_Framework_With_Knowledge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20186-20195.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决增量学习中面临的可塑性和稳定性之间的两难困境。<br>
                    动机：人类记忆系统能够解决这个难题，因为它具有多级记忆结构，这激发了我们提出一种具有知识投影的双层记忆系统（BMKP）用于增量学习。<br>
                    方法：通过双层记忆设计，BMKP将学习和知识记忆的功能解耦：一个工作记忆负责自适应模型学习，以确保可塑性；一个长期记忆负责持久存储所学模型中融入的知识，以保证稳定性。为了解决如何从工作记忆中提取所学知识并将其整合到长期记忆中的问题，我们发现工作记忆中学习的模型实际上位于一个冗余的高维空间，而模型中融入的知识可以在所有增量学习任务共享的一组模式基下具有相当紧凑的表示。因此，我们提出了一种知识投影过程来自适应地维护共享的基，通过这个过程，工作记忆中松散组织的知识模型被投影到紧凑的表示中，以便在长期记忆中记住。<br>
                    效果：我们在CIFAR-10、CIFAR-100和Tiny-ImageNet上评估BMKP。实验结果表明，BMKP在使用较低内存的情况下实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The dilemma between plasticity and stability arises as a common challenge for incremental learning. In contrast, the human memory system is able to remedy this dilemma owing to its multi-level memory structure, which motivates us to propose a Bilevel Memory system with Knowledge Projection (BMKP) for incremental learning. BMKP decouples the functions of learning and knowledge remembering via a bilevel-memory design: a working memory responsible for adaptively model learning, to ensure plasticity; a long-term memory in charge of enduringly storing the knowledge incorporated within the learned model, to guarantee stability. However, an emerging issue is how to extract the learned knowledge from the working memory and assimilate it into the long-term memory. To approach this issue, we reveal that the model learned by the working memory are actually residing in a redundant high-dimensional space, and the knowledge incorporated in the model can have a quite compact representation under a group of pattern basis shared by all incremental learning tasks. Therefore, we propose a knowledge projection process to adapatively maintain the shared basis, with which the loosely organized model knowledge of working memory is projected into the compact representation to be remembered in the long-term memory. We evaluate BMKP on CIFAR-10, CIFAR-100, and Tiny-ImageNet. The experimental results show that BMKP achieves state-of-the-art performance with lower memory usage.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1716.RepMode: Learning to Re-Parameterize Diverse Experts for Subcellular Structure Prediction</span><br>
                <span class="as">Zhou, DonghaoandGu, ChunbinandXu, JundeandLiu, FuruiandWang, QiongandChen, GuangyongandHeng, Pheng-Ann</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_RepMode_Learning_to_Re-Parameterize_Diverse_Experts_for_Subcellular_Structure_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3312-3322.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决生物研究中荧光染色技术慢、贵且对细胞有害的问题，以及预测亚细胞结构3D荧光图像的挑战。<br>
                    动机：由于现有生物科技的限制，每个图像在亚细胞结构预测任务中只有部分标签，同时亚细胞结构的大小差异导致多尺度问题。<br>
                    方法：提出重新参数化混合专家网络（RepMode），通过动态组织任务感知先验参数来处理特定的单标签预测任务。<br>
                    效果：实验表明，RepMode在亚细胞结构预测任务上取得了最先进的整体性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In biological research, fluorescence staining is a key technique to reveal the locations and morphology of subcellular structures. However, it is slow, expensive, and harmful to cells. In this paper, we model it as a deep learning task termed subcellular structure prediction (SSP), aiming to predict the 3D fluorescent images of multiple subcellular structures from a 3D transmitted-light image. Unfortunately, due to the limitations of current biotechnology, each image is partially labeled in SSP. Besides, naturally, subcellular structures vary considerably in size, which causes the multi-scale issue of SSP. To overcome these challenges, we propose Re-parameterizing Mixture-of-Diverse-Experts (RepMode), a network that dynamically organizes its parameters with task-aware priors to handle specified single-label prediction tasks. In RepMode, the Mixture-of-Diverse-Experts (MoDE) block is designed to learn the generalized parameters for all tasks, and gating re-parameterization (GatRep) is performed to generate the specialized parameters for each task, by which RepMode can maintain a compact practical topology exactly like a plain network, and meanwhile achieves a powerful theoretical topology. Comprehensive experiments show that RepMode can achieve state-of-the-art overall performance in SSP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1717.Pruning Parameterization With Bi-Level Optimization for Efficient Semantic Segmentation on the Edge</span><br>
                <span class="as">Yang, ChangdiandZhao, PuandLi, YanyuandNiu, WeiandGuan, JiexiongandTang, HaoandQin, MinghaiandRen, BinandLin, XueandWang, Yanzhi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Pruning_Parameterization_With_Bi-Level_Optimization_for_Efficient_Semantic_Segmentation_on_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15402-15412.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在边缘设备上实现实时分割，以适应自动驾驶等应用的需求。<br>
                    动机：随着边缘设备的普及，对实时分割的需求日益增加。然而，全注意力机制的视觉转换器（ViTs）通常消耗大量的计算资源，导致在边缘设备上的实时推理困难。<br>
                    方法：提出了一种剪枝参数化方法来形成语义分割的剪枝问题，并采用双层优化方法通过隐式梯度解决这个问题。<br>
                    效果：实验结果表明，该方法可以在Samsung S21上以56.5 FPS的速度实现38.9 mIoU的ADE20K验证集分割，这是相同计算约束下实时推理的最高mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the ever-increasing popularity of edge devices, it is necessary to implement real-time segmentation on the edge for autonomous driving and many other applications. Vision Transformers (ViTs) have shown considerably stronger results for many vision tasks. However, ViTs with the full-attention mechanism usually consume a large number of computational resources, leading to difficulties for real-time inference on edge devices. In this paper, we aim to derive ViTs with fewer computations and fast inference speed to facilitate the dense prediction of semantic segmentation on edge devices. To achieve this, we propose a pruning parameterization method to formulate the pruning problem of semantic segmentation. Then we adopt a bi-level optimization method to solve this problem with the help of implicit gradients. Our experimental results demonstrate that we can achieve 38.9 mIoU on ADE20K val with a speed of 56.5 FPS on Samsung S21, which is the highest mIoU under the same computation constraint with real-time inference.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1718.Less Is More: Reducing Task and Model Complexity for 3D Point Cloud Semantic Segmentation</span><br>
                <span class="as">Li, LiandShum, HubertP.H.andBreckon, TobyP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Less_Is_More_Reducing_Task_and_Model_Complexity_for_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9361-9371.png><br>
            
            <span class="tt"><span class="t0">研究问题：近年来，尽管3D激光雷达点云数据的可用性显著增长，但标注仍然昂贵且耗时，因此需要一种半监督的语义分割方法。<br>
                    动机：现有的工作通常使用较大的分割骨干网络来提高分割精度，但这会增加计算成本。此外，许多方法使用均匀采样来减少学习所需的地面真值数据，这通常会导致次优的性能。<br>
                    方法：我们提出了一种新的管道，它使用较小的架构，通过一种新的稀疏深度可分离卷积模块大大减少了网络参数数量，同时保持了整体任务性能。为了有效地对我们的训练数据进行子采样，我们提出了一种新的时空冗余帧降采样（ST-RFD）方法，该方法利用环境中传感器运动的知识提取更多样化的训练数据帧样本。为了利用有限的标注数据样本，我们还提出了一种基于激光反射率的软伪标签方法。<br>
                    效果：在SemanticKITTI（59.5@5%）和ScribbleKITTI（58.1@5%）基准数据集上，我们的方法在使用更少的标注数据的情况下，优于当代的半监督工作，在模型参数减少了2.3倍，乘法加法操作减少了641倍的同时，也显示出在有限训练数据上的重大性能改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Whilst the availability of 3D LiDAR point cloud data has significantly grown in recent years, annotation remains expensive and time-consuming, leading to a demand for semi-supervised semantic segmentation methods with application domains such as autonomous driving. Existing work very often employs relatively large segmentation backbone networks to improve segmentation accuracy, at the expense of computational costs. In addition, many use uniform sampling to reduce ground truth data requirements for learning needed, often resulting in sub-optimal performance. To address these issues, we propose a new pipeline that employs a smaller architecture, requiring fewer ground-truth annotations to achieve superior segmentation accuracy compared to contemporary approaches. This is facilitated via a novel Sparse Depthwise Separable Convolution module that significantly reduces the network parameter count while retaining overall task performance. To effectively sub-sample our training data, we propose a new Spatio-Temporal Redundant Frame Downsampling (ST-RFD) method that leverages knowledge of sensor motion within the environment to extract a more diverse subset of training data frame samples. To leverage the use of limited annotated data samples, we further propose a soft pseudo-label method informed by LiDAR reflectivity. Our method outperforms contemporary semi-supervised work in terms of mIoU, using less labeled data, on the SemanticKITTI (59.5@5%) and ScribbleKITTI (58.1@5%) benchmark datasets, based on a 2.3x reduction in model parameters and 641x fewer multiply-add operations whilst also demonstrating significant performance improvement on limited training data (i.e., Less is More).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1719.Constructing Deep Spiking Neural Networks From Artificial Neural Networks With Knowledge Distillation</span><br>
                <span class="as">Xu, QiandLi, YaxinandShen, JiangrongandLiu, JianK.andTang, HuajinandPan, Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Constructing_Deep_Spiking_Neural_Networks_From_Artificial_Neural_Networks_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7886-7895.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的脉冲神经网络（SNNs）由于网络结构和训练方法的限制，其性能受到限制的问题。<br>
                    动机：尽管基于脉冲的信号使SNNs具有了接近生物神经系统的高计算效率和能量效率，但由于其离散信号的特性，传统的SNNs无法像人工神经网络（ANNs）那样直接应用梯度下降规则进行参数调整。<br>
                    方法：本文提出了一种利用知识蒸馏（KD）构建深度SNN模型的新方法，其中ANN作为教师模型，SNN作为学生模型。通过ANN-SNN联合训练算法，学生SNN模型可以通过KD方法从教师ANN模型中学习丰富的特征信息，同时避免了在与不可微分的脉冲进行交流时从零开始训练SNN。<br>
                    效果：该方法不仅可以合理有效地构建更高效的深层脉冲结构，而且与直接训练或ANN到SNN的方法相比，使用较少的时间步骤来训练整个模型。更重要的是，它对各种类型的人工噪声和自然信号具有出色的抗噪能力。这种新方法为提高SNN的性能提供了有效的途径，有可能用于实际场景中的轻量级和高效的人脑启发式计算。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spiking neural networks (SNNs) are well known as the brain-inspired models with high computing efficiency, due to a key component that they utilize spikes as information units, close to the biological neural systems. Although spiking based models are energy efficient by taking advantage of discrete spike signals, their performance is limited by current network structures and their training methods. As discrete signals, typical SNNs cannot apply the gradient descent rules directly into parameters adjustment as artificial neural networks (ANNs). Aiming at this limitation, here we propose a novel method of constructing deep SNN models with knowledge distillation (KD) that uses ANN as teacher model and SNN as student model. Through ANN-SNN joint training algorithm, the student SNN model can learn rich feature information from the teacher ANN model through the KD method, yet it avoids training SNN from scratch when communicating with non-differentiable spikes. Our method can not only build a more efficient deep spiking structure feasibly and reasonably, but use few time steps to train whole model compared to direct training or ANN to SNN methods. More importantly, it has a superb ability of noise immunity for various types of artificial noises and natural signals. The proposed novel method provides efficient ways to improve the performance of SNN through constructing deeper structures in a high-throughput fashion, with potential usage for light and efficient brain-inspired computing of practical scenarios.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1720.The Differentiable Lens: Compound Lens Search Over Glass Surfaces and Materials for Object Detection</span><br>
                <span class="as">C\^ot\&#x27;e, GeoffroiandMannan, FahimandThibault, SimonandLalonde, Jean-Fran\c{c</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cote_The_Differentiable_Lens_Compound_Lens_Search_Over_Glass_Surfaces_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20803-20812.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数相机镜头系统是独立设计的，与下游的计算机视觉方法分开。<br>
                    动机：近年来，联合优化方法在图像获取和处理管道的其他组件——特别是下游神经网络——中设计镜头，已经取得了改善成像质量或在视觉任务上表现更好的效果。然而，这些现有方法只优化了镜头参数的一部分，无法优化玻璃材料，因为玻璃材料的类别性质。<br>
                    方法：我们开发了一个可微分的球面镜头模拟模型，准确地捕捉到几何像差。我们提出了一种优化策略，以解决镜头设计的挑战——由于非凸损失函数景观和许多制造约束而加剧的问题——这些问题在联合优化任务中更加严重。具体来说，我们在端到端设计环境中引入量化连续的玻璃变量，以便于优化和选择玻璃材料，并结合精心设计的约束条件来支持可制造性。<br>
                    效果：在汽车目标检测中，我们报告说，即使简化设计为两元素或三元素镜头，也比现有的设计有更高的检测性能，尽管这显著降低了图像质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Most camera lens systems are designed in isolation, separately from downstream computer vision methods. Recently, joint optimization approaches that design lenses alongside other components of the image acquisition and processing pipeline--notably, downstream neural networks--have achieved improved imaging quality or better performance on vision tasks. However, these existing methods optimize only a subset of lens parameters and cannot optimize glass materials given their categorical nature. In this work, we develop a differentiable spherical lens simulation model that accurately captures geometrical aberrations. We propose an optimization strategy to address the challenges of lens design--notorious for non-convex loss function landscapes and many manufacturing constraints--that are exacerbated in joint optimization tasks. Specifically, we introduce quantized continuous glass variables to facilitate the optimization and selection of glass materials in an end-to-end design context, and couple this with carefully designed constraints to support manufacturability. In automotive object detection, we report improved detection performance over existing designs even when simplifying designs to two- or three-element lenses, despite significantly degrading the image quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1721.Abstract Visual Reasoning: An Algebraic Approach for Solving Raven&#x27;s Progressive Matrices</span><br>
                <span class="as">Xu, JingyiandVaidya, TusharandWu, YufeiandChandra, SaketandLai, ZhangshengandChong, KaiFongErnest</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Abstract_Visual_Reasoning_An_Algebraic_Approach_for_Solving_Ravens_Progressive_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6715-6724.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在介绍一种适合抽象推理的新推理框架——代数机器推理。<br>
                    动机：代数机器推理将新颖的问题解决过程简化为常规的代数计算，可以有效降低复杂性。<br>
                    方法：通过求解Raven's Progressive Matrices (RPMs)作为代数计算问题，结合各种已知的代数子程序，如计算理想Grobner基、检查理想包含等。<br>
                    效果：在I-RAVEN数据集上的实验中，该模型的整体准确率达到93.2%，显著优于当前最先进的77.0%准确率，甚至超过了人类的84.4%准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce algebraic machine reasoning, a new reasoning framework that is well-suited for abstract reasoning. Effectively, algebraic machine reasoning reduces the difficult process of novel problem-solving to routine algebraic computation. The fundamental algebraic objects of interest are the ideals of some suitably initialized polynomial ring. We shall explain how solving Raven's Progressive Matrices (RPMs) can be realized as computational problems in algebra, which combine various well-known algebraic subroutines that include: Computing the Grobner basis of an ideal, checking for ideal containment, etc. Crucially, the additional algebraic structure satisfied by ideals allows for more operations on ideals beyond set-theoretic operations. Our algebraic machine reasoning framework is not only able to select the correct answer from a given answer set, but also able to generate the correct answer with only the question matrix given. Experiments on the I-RAVEN dataset yield an overall 93.2% accuracy, which significantly outperforms the current state-of-the-art accuracy of 77.0% and exceeds human performance at 84.4% accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1722.ABCD: Arbitrary Bitwise Coefficient for De-Quantization</span><br>
                <span class="as">Han, WooKyoungandLee, ByeonghunandPark, SangHyunandJin, KyongHwan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_ABCD_Arbitrary_Bitwise_Coefficient_for_De-Quantization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5876-5885.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从任意量化的输入中恢复去量化的图像。<br>
                    动机：现有的位深度扩展方法在处理低比特深度图像时，如压缩编解码器产生的8位以下图像，会出现带状和模糊的人工痕迹，效果并不理想。<br>
                    方法：提出一种隐式神经网络函数，通过引入位查询来从任意量化的输入中恢复去量化的图像，并开发了一个相位估计器来利用最近像素的信息。<br>
                    效果：在自然和动画图像上，该方法的性能超过了先前的位深度扩展方法。同时，在YouTube UGC数据集上进行去带状化演示，也取得了良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern displays and contents support more than 8bits image and video. However, bit-starving situations such as compression codecs make low bit-depth (LBD) images (<8bits), occurring banding and blurry artifacts. Previous bit depth expansion (BDE) methods still produce unsatisfactory high bit-depth (HBD) images. To this end, we propose an implicit neural function with a bit query to recover de-quantized images from arbitrarily quantized inputs. We develop a phasor estimator to exploit the information of the nearest pixels. Our method shows superior performance against prior BDE methods on natural and animation images. We also demonstrate our model on YouTube UGC datasets for de-banding. Our source code is available at https://github.com/WooKyoungHan/ABCD</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1723.CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval</span><br>
                <span class="as">Pei, RenjingandLiu, JianzhuangandLi, WeimianandShao, BinandXu, SongcenandDai, PengandLu, JuweiandYan, Youliang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18983-18992.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将预训练的视觉语言模型的知识有效地转移到小模型中，同时保持准确性。<br>
                    动机：预训练的视觉语言模型通常推理时间长，而知识蒸馏是一种有效的技术，可以将大模型的能力转移到小模型中，同时保持准确性。<br>
                    方法：提出一种新的知识蒸馏方法，名为CLIPPING，通过在微调阶段将大量已针对视频-语言任务进行微调的大型教师模型的知识有效转移到小型学生模型中。特别是，提出了一种新的层对齐方法，以学生为基准进行中间层的蒸馏，使学生的层成为教师的基础，从而让学生充分吸收教师的知识。<br>
                    效果：CLIPPING在三个视频-语言检索基准上实现了其教师88.1%-95.3%的性能，其视觉编码器的大小仅为原来的19.5倍。CLIPPING在MSR-VTT数据集上也显著优于最先进的小型基线（ALL-in-one-B），获得了相对7.4%的性能提升，参数减少了29%，运算减少了86.9%。此外，CLIPPING与许多大型预训练模型相当甚至更优。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pre-training a vison-language model and then fine-tuning it on downstream tasks have become a popular paradigm. However, pre-trained vison-language models with the Transformer architecture usually take long inference time. Knowledge distillation has been an efficient technique to transfer the capability of a large model to a small one while maintaining the accuracy, which has achieved remarkable success in natural language processing. However, it faces many problems when applying KD to the multi-modality applications. In this paper, we propose a novel knowledge distillation method, named CLIPPING, where the plentiful knowledge of a large teacher model that has been fine-tuned for video-language tasks with the powerful pre-trained CLIP can be effectively transferred to a small student only at the fine-tuning stage. Especially, a new layer-wise alignment with the student as the base is proposed for knowledge distillation of the intermediate layers in CLIPPING, which enables the student's layers to be the bases of the teacher, and thus allows the student to fully absorb the knowledge of the teacher. CLIPPING with MobileViT-v2 as the vison encoder without any vison-language pre-training achieves 88.1%-95.3% of the performance of its teacher on three video-language retrieval benchmarks, with its vison encoder being 19.5x smaller. CLIPPING also significantly outperforms a state-of-the-art small baseline (ALL-in-one-B) on the MSR-VTT dataset, obtaining relatively 7.4% performance gain, with 29% fewer parameters and 86.9% fewer flops. Moreover, CLIPPING is comparable or even superior to many large pre-training models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1724.Learning Federated Visual Prompt in Null Space for MRI Reconstruction</span><br>
                <span class="as">Feng, Chun-MeiandLi, BangjunandXu, XinxingandLiu, YongandFu, HuazhuandZuo, Wangmeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Learning_Federated_Visual_Prompt_in_Null_Space_for_MRI_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8064-8073.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用联邦磁共振成像（MRI）重建技术，在不聚合本地数据的情况下实现多医院分布式协作，保护患者隐私。<br>
                    动机：由于不同的MRI协议、不足的本地训练数据和有限的通信带宽导致的数据异质性，不可避免地影响了全局模型的收敛和更新。<br>
                    方法：本文提出了一种新的算法FedPR，用于学习MRI重建中全局提示为零空间的联邦视觉提示。FedPR是一种新的联邦范式，采用强大的预训练模型，同时仅学习和通信具有少量可学习参数的提示，从而显著降低通信成本并在有限的本地数据上实现竞争性能。此外，为了解决由数据异质性引起的灾难性遗忘问题，FedPR还更新了高效的联邦视觉提示，将局部提示投影到全局提示的近似零空间中，从而抑制梯度对服务器性能的干扰。<br>
                    效果：在联邦MRI上的大量实验表明，当给定有限数量的本地数据时，FedPR的性能明显优于最先进的FL算法，通信成本降低了6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Magnetic Resonance Imaging (MRI) reconstruction enables multiple hospitals to collaborate distributedly without aggregating local data, thereby protecting patient privacy. However, the data heterogeneity caused by different MRI protocols, insufficient local training data, and limited communication bandwidth inevitably impair global model convergence and updating. In this paper, we propose a new algorithm, FedPR, to learn federated visual prompts in the null space of global prompt for MRI reconstruction. FedPR is a new federated paradigm that adopts a powerful pre-trained model while only learning and communicating the prompts with few learnable parameters, thereby significantly reducing communication costs and achieving competitive performance on limited local data. Moreover, to deal with catastrophic forgetting caused by data heterogeneity, FedPR also updates efficient federated visual prompts that project the local prompts into an approximate null space of the global prompt, thereby suppressing the interference of gradients on the server performance. Extensive experiments on federated MRI show that FedPR significantly outperforms state-of-the-art FL algorithms with < 6% of communication costs when given the limited amount of local data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1725.Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning</span><br>
                <span class="as">Tu, Cheng-HaoandMai, ZhedaandChao, Wei-Lun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7725-7735.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用预训练模型的中间特征进行准确的下游任务预测。<br>
                    动机：预训练模型的中间特征对于下游任务预测具有重要信息，但如何有效利用这些特征仍是一个挑战。<br>
                    方法：提出视觉查询调优（VQT）方法，通过在每一层引入可学习的“查询”令牌，利用Transformer的内部工作机制对各层的丰富中间特征进行“总结”，然后用于训练下游任务的预测头。<br>
                    效果：实验表明，VQT在许多情况下优于其他参数高效微调方法，并在内存限制下实现了更高的准确率。同时，VQT与这些方法兼容，可以进一步提高迁移学习的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Intermediate features of a pre-trained model have been shown informative for making accurate predictions on downstream tasks, even if the model backbone is frozen. The key challenge is how to utilize them, given the gigantic amount. We propose visual query tuning (VQT), a simple yet effective approach to aggregate intermediate features of Vision Transformers. Through introducing a handful of learnable "query" tokens to each layer, VQT leverages the inner workings of Transformers to "summarize" rich intermediate features of each layer, which can then be used to train the prediction heads of downstream tasks. As VQT keeps the intermediate features intact and only learns to combine them, it enjoys memory efficiency in training, compared to many other parameter-efficient fine-tuning approaches that learn to adapt features and need back-propagation through the entire backbone. This also suggests the complementary role between VQT and those approaches in transfer learning. Empirically, VQT consistently surpasses the state-of-the-art approach that utilizes intermediate features for transfer learning and outperforms full fine-tuning in many cases. Compared to parameter-efficient approaches that adapt features, VQT achieves much higher accuracy under memory constraints. Most importantly, VQT is compatible with these approaches to attain higher accuracy, making it a simple add-on to further boost transfer learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1726.Efficient Scale-Invariant Generator With Column-Row Entangled Pixel Synthesis</span><br>
                <span class="as">Nguyen, ThuanHoangandVanLe, ThanhandTran, Anh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_Efficient_Scale-Invariant_Generator_With_Column-Row_Entangled_Pixel_Synthesis_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22408-22417.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地合成任意尺度的图像，特别是在超过2K分辨率的情况下。<br>
                    动机：现有的基于GAN的解决方案过度依赖卷积和分层架构，导致输出分辨率扩展时出现不一致性及“纹理粘贴”问题。而INR基生成器虽然设计上具有尺度等变性，但其巨大的内存占用和慢速推理阻碍了其在大规模或实时系统中的应用。<br>
                    方法：提出一种名为Column-Row Entangled Pixel Synthesis（CREPS）的新生成模型，该模型无需使用任何空间卷积或粗到细的设计，既能高效运行又具有尺度等变性。为了节省内存并使系统可扩展，我们采用了一种新的双行表示法，将逐层特征图分解为独立的“厚”列和行编码。<br>
                    效果：在FFHQ、LSUN-Church和MetFaces等标准数据集上的实验证明，CREPS能够合成尺度一致且无混叠的图像，最高可达4K分辨率，同时具备适当的训练和推理速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the "texture sticking" issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose Column-Row Entangled Pixel Synthesisthes (CREPS), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate "thick" column and row encodings. Experiments on standard datasets, including FFHQ, LSUN-Church, and MetFaces, confirm CREPS' ability to synthesize scale-consistent and alias-free images up to 4K resolution with proper training and inference speed.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1727.Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm</span><br>
                <span class="as">Xie, YichenandLu, HanandYan, JunchiandYang, XiaokangandTomizuka, MasayoshiandZhan, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Active_Finetuning_Exploiting_Annotation_Budget_in_the_Pretraining-Finetuning_Paradigm_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23715-23724.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用标注预算进行预训练-微调范式中的样本选择和优化。<br>
                    动机：尽管预训练-微调在计算机视觉任务中被广泛使用，但很少有研究关注如何优化微调阶段的标注预算。<br>
                    方法：提出了一种名为ActiveFT的新颖方法，通过优化连续空间的参数模型来选择与整个未标记池分布相似且具有足够多样性的数据子集。<br>
                    效果：实验证明，这种方法在选择的子集和整个数据池的分布之间的Earth Mover距离也减小了。在图像分类和语义分割任务上，ActiveFT的性能和效率均优于基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given the large-scale data and the high annotation cost, pretraining-finetuning becomes a popular paradigm in multiple computer vision tasks. Previous research has covered both the unsupervised pretraining and supervised finetuning in this paradigm, while little attention is paid to exploiting the annotation budget for finetuning. To fill in this gap, we formally define this new active finetuning task focusing on the selection of samples for annotation in the pretraining-finetuning paradigm. We propose a novel method called ActiveFT for active finetuning task to select a subset of data distributing similarly with the entire unlabeled pool and maintaining enough diversity by optimizing a parametric model in the continuous space. We prove that the Earth Mover's distance between the distributions of the selected subset and the entire data pool is also reduced in this process. Extensive experiments show the leading performance and high efficiency of ActiveFT superior to baselines on both image classification and semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1728.MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering</span><br>
                <span class="as">Jiang, JingjingandZheng, Nanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MixPHM_Redundancy-Aware_Parameter-Efficient_Tuning_for_Low-Resource_Visual_Question_Answering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24203-24213.png><br>
            
            <span class="tt"><span class="t0">研究问题：预训练视觉-语言模型（VLMs）在低资源设置下进行特定任务的全参数微调时，计算成本高、存储效率低且容易过拟合。<br>
                    动机：尽管现有的参数高效微调方法大大减少了可调参数的数量，但在低资源设置下的VQA任务中，与全微调相比仍存在显著的性能差距。<br>
                    方法：本文提出了MixPHM，一种对冗余敏感的参数高效微调方法，该方法在低资源VQA任务上优于全微调。MixPHM是一个轻量级模块，由多个PHM专家以混合专家的方式实现。为了减少参数冗余，我们在低秩子空间中重新参数化专家权重，并共享MixPHM内部和跨部分的权重。此外，基于对表示冗余性的定量分析，我们提出了冗余正则化，这有助于MixPHM减少与任务无关的冗余，同时促进与任务相关的相关性。<br>
                    效果：在VQA v2、GQA和OK-VQA等不同低资源设置下进行的实验表明，我们的MixPHM超越了最先进的参数高效方法，并且是唯一能持续超越全微调的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, finetuning pretrained vision-language models (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in VQA. However, as VLMs scale, it becomes computationally expensive, storage inefficient, and prone to overfitting when tuning full model parameters for a specific task in low-resource settings. Although current parameter-efficient tuning methods dramatically reduce the number of tunable parameters, there still exists a significant performance gap with full finetuning. In this paper, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM-experts in a mixture-of-experts manner. To reduce parameter redundancy, we reparameterize expert weights in a low-rank subspace and share part of the weights inside and across MixPHM. Moreover, based on our quantitative analysis of representation redundancy, we propose Redundancy Regularization, which facilitates MixPHM to reduce task-irrelevant redundancy while promoting task-relevant correlation. Experiments conducted on VQA v2, GQA, and OK-VQA with different low-resource settings show that our MixPHM outperforms state-of-the-art parameter-efficient methods and is the only one consistently surpassing full finetuning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1729.A Dynamic Multi-Scale Voxel Flow Network for Video Prediction</span><br>
                <span class="as">Hu, XiaotaoandHuang, ZheweiandHuang, AilinandXu, JunandZhou, Shuchang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6121-6131.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视频预测的性能，同时降低计算成本和模型大小。<br>
                    动机：现有的视频预测方法大多需要额外的输入（如语义/深度图），并且模型大、计算成本高。<br>
                    方法：提出一种动态多尺度体素流网络（DMVFN），仅使用RGB图像进行训练和预测，通过可微分的路由模块感知视频帧的运动尺度，选择适应的子网络进行输入。<br>
                    效果：实验证明，DMVFN比现有的方法快一个数量级，并在生成的图像质量上超过了最先进的迭代方法OPT。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The performance of video prediction has been greatly boosted by advanced deep neural networks. However, most of the current methods suffer from large model sizes and require extra inputs, e.g., semantic/depth maps, for promising performance. For efficiency consideration, in this paper, we propose a Dynamic Multi-scale Voxel Flow Network (DMVFN) to achieve better video prediction performance at lower computational costs with only RGB images, than previous methods. The core of our DMVFN is a differentiable routing module that can effectively perceive the motion scales of video frames. Once trained, our DMVFN selects adaptive sub-networks for different inputs at the inference stage. Experiments on several benchmarks demonstrate that our DMVFN is an order of magnitude faster than Deep Voxel Flow and surpasses the state-of-the-art iterative-based OPT on generated image quality. Our code and demo are available at https://huxiaotaostasy.github.io/DMVFN/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1730.Stitchable Neural Networks</span><br>
                <span class="as">Pan, ZizhengandCai, JianfeiandZhuang, Bohan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Stitchable_Neural_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16102-16112.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地组装预训练模型族，以实现运行时的动态精度-效率权衡。<br>
                    动机：预训练模型族的规模空前庞大，包含各种规模和性能的预训练模型，因此需要一种有效的方法来组装这些模型。<br>
                    方法：提出可拼接神经网络（SN-Net）框架，该框架通过将预训练神经网络（称为锚点）拆分并在不同的块/层之间进行拼接，然后使用简单的拼接层将这些锚点映射到另一个锚点的激活上，从而在有限的训练轮次内有效地在不同规模的锚点之间进行插值。<br>
                    效果：实验结果表明，SN-Net在ImageNet分类任务上的表现与许多单独训练的网络相当甚至更好，同时支持多样化的部署场景。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The public model zoo containing enormous powerful pretrained model families (e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which significantly contributes to the success of deep learning. As each model family consists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it naturally arises a fundamental question of how to efficiently assemble these readily available models in a family for dynamic accuracy-efficiency trade-offs at runtime. To this end, we present Stitchable Neural Networks (SN-Net), a novel scalable and efficient framework for model deployment. It cheaply produces numerous networks with different complexity and performance trade-offs given a family of pretrained neural networks, which we call anchors. Specifically, SN-Net splits the anchors across the blocks/layers and then stitches them together with simple stitching layers to map the activations from one anchor to another. With only a few epochs of training, SN-Net effectively interpolates between the performance of anchors with varying scales. At runtime, SN-Net can instantly adapt to dynamic resource constraints by switching the stitching positions. Extensive experiments on ImageNet classification demonstrate that SN-Net can obtain on-par or even better performance than many individually trained networks while supporting diverse deployment scenarios. For example, by stitching Swin Transformers, we challenge hundreds of models in Timm model zoo with a single network. We believe this new elastic model framework can serve as a strong baseline for further research in wider communities.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1731.Federated Learning With Data-Agnostic Distribution Fusion</span><br>
                <span class="as">Duan, Jian-huiandLi, WenzhongandZou, DerunandLi, RuichenandLu, Sanglu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Duan_Federated_Learning_With_Data-Agnostic_Distribution_Fusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8074-8083.png><br>
            
            <span class="tt"><span class="t0">研究问题：联邦学习中，由于数据样本在各客户端间不独立同分布（non-IID），导致全局模型的收敛速度慢且性能下降。<br>
                    动机：为了解决联邦学习中non-IID数据的模型聚合问题，需要在保护隐私政策的前提下，推断出未知的全局分布。<br>
                    方法：本文提出了一种名为FedFusion的数据无关分布融合模型聚合方法，该方法基于变分自编码器（VAE）学习分布融合组件的最优参数，以优化具有非IID本地数据集的联邦学习。<br>
                    效果：通过在各种联邦学习场景和真实世界数据集上的大量实验，发现FedFusion相比现有技术有显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning has emerged as a promising distributed machine learning paradigm to preserve data privacy. One of the fundamental challenges of federated learning is that data samples across clients are usually not independent and identically distributed (non-IID), leading to slow convergence and severe performance drop of the aggregated global model. To facilitate model aggregation on non-IID data, it is desirable to infer the unknown global distributions without violating privacy protection policy. In this paper, we propose a novel data-agnostic distribution fusion based model aggregation method called FedFusion to optimize federated learning with non-IID local datasets, based on which the heterogeneous clients' data distributions can be represented by a global distribution of several virtual fusion components with different parameters and weights. We develop a Variational AutoEncoder (VAE) method to learn the optimal parameters of the distribution fusion components based on limited statistical information extracted from the local models, and apply the derived distribution fusion model to optimize federated model aggregation with non-IID data. Extensive experiments based on various federated learning scenarios with real-world datasets show that FedFusion achieves significant performance improvement compared to the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1732.PIDNet: A Real-Time Semantic Segmentation Network Inspired by PID Controllers</span><br>
                <span class="as">Xu, JiacongandXiong, ZixiangandBhattacharyya, ShankarP.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_PIDNet_A_Real-Time_Semantic_Segmentation_Network_Inspired_by_PID_Controllers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19529-19539.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的两分支网络在实时语义分割任务中虽然有效，但高分辨率细节和研究问题：现有的两分支网络在实时语义分割任务中虽然有效，但高分辨率细节和低频率上下文的直接融合会导致细节特征被周围环境信息所淹没，限制了分割精度的提升。<br>
                    动机：为了解决两分支网络在直接融合高分辨率细节和低频率上下文时出现的特征淹没问题，作者将卷积神经网络（CNN）与比例-积分-微分（PID）控制器进行关联，发现两分支网络相当于一个比例-积分（PI）控制器，同样存在过冲问题。<br>
                    方法：为此，作者提出了一种新的三分支网络架构——PIDNet，它包含三个分支分别解析细节、上下文和边界信息，并采用边界注意力引导细节和上下文分支的融合。<br>
                    效果：实验证明，PIDNet系列模型在推理速度和准确性之间取得了最佳平衡，其准确性在所有具有相似推理速度的现有模型上均超过了Cityscapes和CamVid数据集。具体来说，PIDNet-S在Cityscapes上实现了78.6 mIOU，推理速度为93.2 FPS；在CamVid上实现了80.1 mIOU，速度为153.7 FPS。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Two-branch network architecture has shown its efficiency and effectiveness in real-time semantic segmentation tasks. However, direct fusion of high-resolution details and low-frequency context has the drawback of detailed features being easily overwhelmed by surrounding contextual information. This overshoot phenomenon limits the improvement of the segmentation accuracy of existing two-branch models. In this paper, we make a connection between Convolutional Neural Networks (CNN) and Proportional-Integral-Derivative (PID) controllers and reveal that a two-branch network is equivalent to a Proportional-Integral (PI) controller, which inherently suffers from similar overshoot issues. To alleviate this problem, we propose a novel three-branch network architecture: PIDNet, which contains three branches to parse detailed, context and boundary information, respectively, and employs boundary attention to guide the fusion of detailed and context branches. Our family of PIDNets achieve the best trade-off between inference speed and accuracy and their accuracy surpasses all the existing models with similar inference speed on the Cityscapes and CamVid datasets. Specifically, PIDNet-S achieves 78.6 mIOU with inference speed of 93.2 FPS on Cityscapes and 80.1 mIOU with speed of 153.7 FPS on CamVid.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1733.How To Prevent the Poor Performance Clients for Personalized Federated Learning?</span><br>
                <span class="as">Qu, ZheandLi, XingyuandHan, XiaoandDuan, RuiandShen, ChengchaoandChen, Lixing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_How_To_Prevent_the_Poor_Performance_Clients_for_Personalized_Federated_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12167-12176.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在异构分布式本地数据中，为每个客户端提供定制化模型解决方案。<br>
                    动机：尽管许多最新研究已应用各种算法来提高个性化联邦学习中的个性化程度，但他们主要关注从平均或顶级角度提高性能，而忽视了部分表现不佳的客户端。<br>
                    方法：提出了一种名为“局部个性化，普遍通用化”（PLGU）的新型联邦学习策略。通过设计一个分层锐度感知最小化（LWSAM）算法，在保持个性化的同时，对细粒度的普遍信息进行泛化并调整其有偏的性能。<br>
                    效果：实验结果表明，所提出的基于PLGU的策略在两种联邦学习方案上都实现了具有竞争力的泛化界限，且所有基于PLGU的算法都达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Personalized federated learning (pFL) collaboratively trains personalized models, which provides a customized model solution for individual clients in the presence of heterogeneous distributed local data. Although many recent studies have applied various algorithms to enhance personalization in pFL, they mainly focus on improving the performance from averaging or top perspective. However, part of the clients may fall into poor performance and are not clearly discussed. Therefore, how to prevent these poor clients should be considered critically. Intuitively, these poor clients may come from biased universal information shared with others. To address this issue, we propose a novel pFL strategy, called Personalize Locally, Generalize Universally (PLGU). PLGU generalizes the fine-grained universal information and moderates its biased performance by designing a Layer-Wised Sharpness Aware Minimization (LWSAM) algorithm while keeping the personalization local. Specifically, we embed our proposed PLGU strategy into two pFL schemes concluded in this paper: with/without a global model, and present the training procedures in detail. Through in-depth study, we show that the proposed PLGU strategy achieves competitive generalization bounds on both considered pFL schemes. Our extensive experimental results show that all the proposed PLGU based-algorithms achieve state-of-the-art performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1734.CP3: Channel Pruning Plug-In for Point-Based Networks</span><br>
                <span class="as">Huang, YaominandLiu, NingandChe, ZhengpingandXu, ZhiyuanandShen, ChaominandPeng, YaxinandZhang, GuixuandLiu, XinmeiandFeng, FeifeiandTang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_CP3_Channel_Pruning_Plug-In_for_Point-Based_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5302-5312.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地减少三维点基神经网络的计算成本和内存占用，同时保持相当的准确性。<br>
                    动机：尽管2D图像卷积神经网络（CNNs）的通道剪枝方法取得了巨大成功，但现有的工作很少将其扩展到3D点基神经网络（PNNs）。<br>
                    方法：提出了一种针对点基网络的通道剪枝插件CP^3，该插件精心设计以利用点云和PNN的特性，使2D通道剪枝方法适用于PNN。具体来说，它提出了一个坐标增强的通道重要性度量，以反映维度信息与单个通道特征之间的相关性，并在PNN的采样过程中回收被丢弃的点，重新考虑其可能独有的信息，以提高通道剪枝的鲁棒性。<br>
                    效果：在各种PNN架构上的实验表明，CP^3不断改进了最先进的2D CNN剪枝方法在不同点云任务上的性能。例如，我们的压缩PointNeXt-S在ScanObjectNN上实现了88.52%的准确率，剪枝率为57.8%，比基线剪枝方法提高了1.94%的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Channel pruning has been widely studied as a prevailing method that effectively reduces both computational cost and memory footprint of the original network while keeping a comparable accuracy performance. Though great success has been achieved in channel pruning for 2D image-based convolutional networks (CNNs), existing works seldom extend the channel pruning methods to 3D point-based neural networks (PNNs). Directly implementing the 2D CNN channel pruning methods to PNNs undermine the performance of PNNs because of the different representations of 2D images and 3D point clouds as well as the network architecture disparity. In this paper, we proposed CP^3, which is a Channel Pruning Plug-in for Point-based network. CP^3 is elaborately designed to leverage the characteristics of point clouds and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically, it presents a coordinate-enhanced channel importance metric to reflect the correlation between dimensional information and individual channel features, and it recycles the discarded points in PNN's sampling process and reconsiders their potentially-exclusive information to enhance the robustness of channel pruning. Experiments on various PNN architectures show that CP^3 constantly improves state-of-the-art 2D CNN pruning approaches on different point cloud tasks. For instance, our compressed PointNeXt-S on ScanObjectNN achieves an accuracy of 88.52% with a pruning rate of 57.8%, outperforming the baseline pruning methods with an accuracy gain of 1.94%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1735.MobileVOS: Real-Time Video Object Segmentation Contrastive Learning Meets Knowledge Distillation</span><br>
                <span class="as">Miles, RoyandYucel, MehmetKerimandManganelli, BrunoandSa\`a-Garriga, Albert</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Miles_MobileVOS_Real-Time_Video_Object_Segmentation_Contrastive_Learning_Meets_Knowledge_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10480-10490.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决资源受限设备上的半监督视频对象分割问题。<br>
                    动机：在资源有限的设备上，如手机，进行高效的视频对象分割。<br>
                    方法：通过知识蒸馏任务，构建了一个理论框架，将对比性表示学习和知识蒸馏相结合，同时从预训练的教师模型中进行学习。<br>
                    效果：在DAVIS和YouTube基准测试中，该方法在运行速度提高5倍，参数减少32倍的情况下，仍能达到与最先进的技术相竞争的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper tackles the problem of semi-supervised video object segmentation on resource-constrained devices, such as mobile phones. We formulate this problem as a distillation task, whereby we demonstrate that small space-time-memory networks with finite memory can achieve competitive results with state of the art, but at a fraction of the computational cost (32 milliseconds per frame on a Samsung Galaxy S22). Specifically, we provide a theoretically grounded framework that unifies knowledge distillation with supervised contrastive representation learning. These models are able to jointly benefit from both pixel-wise contrastive learning and distillation from a pre-trained teacher. We validate this loss by achieving competitive J&F to state of the art on both the standard DAVIS and YouTube benchmarks, despite running up to x5 faster, and with x32 fewer parameters.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1736.Unsupervised Continual Semantic Adaptation Through Neural Rendering</span><br>
                <span class="as">Liu, ZhizhengandMilano, FrancescoandFrey, JonasandSiegwart, RolandandBlum, HermannandCadena, Cesar</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Unsupervised_Continual_Semantic_Adaptation_Through_Neural_Rendering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3031-3040.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何适应新场景进行语义分割任务，同时保持之前场景的性能。<br>
                    动机：由于训练和部署数据之间的不匹配，模型在新场景上的适应性通常至关重要。<br>
                    方法：提出为每个场景训练一个Semantic-NeRF网络，通过融合分割模型的预测结果，并使用一致的渲染语义标签作为伪标签来调整模型。<br>
                    效果：在ScanNet上评估该方法，其表现优于基于体素的基线方法和最新的无监督领域适应方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>An increasing amount of applications rely on data-driven models that are deployed for perception tasks across a sequence of scenes. Due to the mismatch between training and deployment data, adapting the model on the new scenes is often crucial to obtain good performance. In this work, we study continual multi-scene adaptation for the task of semantic segmentation, assuming that no ground-truth labels are available during deployment and that performance on the previous scenes should be maintained. We propose training a Semantic-NeRF network for each scene by fusing the predictions of a segmentation model and then using the view-consistent rendered semantic labels as pseudo-labels to adapt the model. Through joint training with the segmentation model, the Semantic-NeRF model effectively enables 2D-3D knowledge transfer. Furthermore, due to its compact size, it can be stored in a long-term memory and subsequently used to render data from arbitrary viewpoints to reduce forgetting. We evaluate our approach on ScanNet, where we outperform both a voxel-based baseline and a state-of-the-art unsupervised domain adaptation method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1737.GradMA: A Gradient-Memory-Based Accelerated Federated Learning With Alleviated Catastrophic Forgetting</span><br>
                <span class="as">Luo, KangyangandLi, XiangandLan, YunshiandGao, Ming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_GradMA_A_Gradient-Memory-Based_Accelerated_Federated_Learning_With_Alleviated_Catastrophic_Forgetting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3708-3717.png><br>
            
            <span class="tt"><span class="t0">研究问题：联邦学习中的数据异质性和部分参与导致的灾难性遗忘对性能产生负面影响。<br>
                    动机：提出一种新的联邦学习方法（即GradMA），借鉴持续学习的思想，同时修正服务器端和工作端更新方向，充分利用服务器的丰富计算和内存资源。<br>
                    方法：设计了一种记忆减少策略，使GradMA能够适应大规模的工人。在平滑非凸设置下从理论上分析了GradMA的收敛性，并证明其收敛速度比采样活跃工人数量增加的速度提高了线性倍率。<br>
                    效果：在各种图像分类任务上进行的广泛实验表明，与最先进的SOTA基线相比，GradMA在准确性和通信效率方面取得了显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Learning (FL) has emerged as a de facto machine learning area and received rapid increasing research interests from the community. However, catastrophic forgetting caused by data heterogeneity and partial participation poses distinctive challenges for FL, which are detrimental to the performance. To tackle the problems, we propose a new FL approach (namely GradMA), which takes inspiration from continual learning to simultaneously correct the server-side and worker-side update directions as well as take full advantage of server's rich computing and memory resources. Furthermore, we elaborate a memory reduction strategy to enable GradMA to accommodate FL with a large scale of workers. We then analyze convergence of GradMA theoretically under the smooth non-convex setting and show that its convergence rate achieves a linear speed up w.r.t the increasing number of sampled active workers. At last, our extensive experiments on various image classification tasks show that GradMA achieves significant performance gains in accuracy and communication efficiency compared to SOTA baselines. We provide our code here: https://github.com/lkyddd/GradMA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1738.POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery</span><br>
                <span class="as">Zheng, CeandLiu, XianpengandQi, Guo-JunandChen, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_POTTER_Pooling_Attention_Transformer_for_Efficient_Human_Mesh_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1611-1620.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低在单目图像中进行人体网格恢复（HMR）的Transformer架构的内存和计算开销。<br>
                    动机：虽然Transformer架构在单目图像的人体网格恢复任务上取得了最先进的性能，但其高昂的内存和计算成本限制了其在实际应用中的使用。<br>
                    方法：提出了一种名为POoling aTtention TransformER（POTTER）的纯Transformer架构，通过引入高效的池化注意力模块来显著降低内存和计算成本，同时设计了一个集成高分辨率（HR）流的新Transformer架构，利用其高分辨率的局部和全局特征来恢复更准确的人体网格。<br>
                    效果：实验结果表明，POTTER在Human3.6M和3DPW数据集上的PA-MPJPE和所有三个指标上都优于最先进的METRO方法，而且所需的参数和乘积累加运算分别只有METRO的7%和14%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Transformer architectures have achieved SOTA performance on the human mesh recovery (HMR) from monocular images. However, the performance gain has come at the cost of substantial memory and computational overhead. A lightweight and efficient model to reconstruct accurate human mesh is needed for real-world applications. In this paper, we propose a pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images. Observing that the conventional attention module is memory and computationally expensive, we propose an efficient pooling attention module, which significantly reduces the memory and computational cost without sacrificing performance. Furthermore, we design a new transformer architecture by integrating a High-Resolution (HR) stream for the HMR task. The high-resolution local and global features from the HR stream can be utilized for recovering more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations on the Human3.6M (PA-MPJPE) and 3DPW (all three metrics) datasets. Code will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1739.DynaFed: Tackling Client Data Heterogeneity With Global Dynamics</span><br>
                <span class="as">Pi, RenjieandZhang, WeizhongandXie, YueqiandGao, JiahuiandWang, XiaoyuandKim, SunghunandChen, Qifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pi_DynaFed_Tackling_Client_Data_Heterogeneity_With_Global_Dynamics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12177-12186.png><br>
            
            <span class="tt"><span class="t0">研究问题：联邦学习（FL）在面对异构客户端数据时面临挑战，本地训练非iid分布的数据会导致局部最优解偏离，使得客户端模型彼此偏离，并降低全局模型的性能。<br>
                    动机：为了解决这一问题，本文提出了一种在不妨碍数据隐私的情况下收集和利用服务器上的全局知识的方法。<br>
                    方法：首先在服务器上保留一段全球模型快照的轨迹，然后合成一个小的伪数据集，使在该数据集上训练的模型能够模仿保留的全球模型轨迹的动态。之后，使用合成的数据帮助聚合偏离的客户端到全局模型中。该方法被称为DynaFed。<br>
                    效果：实验结果表明，DynaFed在广泛的基准测试中都表现出了良好的效果。同时，也提供了对该方法底层机制的深入理解和见解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The Federated Learning (FL) paradigm is known to face challenges under heterogeneous client data. Local training on non-iid distributed data results in deflected local optimum, which causes the client models drift further away from each other and degrades the aggregated global model's performance. A natural solution is to gather all client data onto the server, such that the server has a global view of the entire data distribution. Unfortunately, this reduces to regular training, which compromises clients' privacy and conflicts with the purpose of FL. In this paper, we put forth an idea to collect and leverage global knowledge on the server without hindering data privacy. We unearth such knowledge from the dynamics of the global model's trajectory. Specifically, we first reserve a short trajectory of global model snapshots on the server. Then, we synthesize a small pseudo dataset such that the model trained on it mimics the dynamics of the reserved global model trajectory. Afterward, the synthesized data is used to help aggregate the deflected clients into the global model. We name our method DynaFed, which enjoys the following advantages: 1) we do not rely on any external on-server dataset, which requires no additional cost for data collection; 2) the pseudo data can be synthesized in early communication rounds, which enables DynaFed to take effect early for boosting the convergence and stabilizing training; 3) the pseudo data only needs to be synthesized once and can be directly utilized on the server to help aggregation in subsequent rounds. Experiments across extensive benchmarks are conducted to showcase the effectiveness of DynaFed. We also provide insights and understanding of the underlying mechanism of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1740.DistilPose: Tokenized Pose Regression With Heatmap Distillation</span><br>
                <span class="as">Ye, SuhangandZhang, YingyiandHu, JieandCao, LiujuanandZhang, ShengchuanandShen, LeiandWang, JunandDing, ShouhongandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2163-2172.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在人体姿态估计中同时利用基于热图的方法和基于回归的方法，以提高性能并保持效率。<br>
                    动机：基于热图的方法在性能上优于基于回归的方法，但速度较慢；而基于回归的方法在速度上占优，但性能较差。如何结合两者的优势是一个挑战。<br>
                    方法：提出一种名为DistilPose的新型人体姿态估计框架，通过令牌蒸馏编码器（TDE）和模拟热图，将知识从基于热图的教师模型转移到基于回归的学生模型。<br>
                    效果：实验表明，提出的DistilPose可以显著提高基于回归的模型的性能，同时保持效率。在MSCOCO验证数据集上，DistilPose-S获得了71.6%的mAP，参数量为5.36M，GFLOPs为2.38，FPS为40.2，比其教师模型节省了12.95x、7.16x的计算成本，快了4.9倍，性能仅下降0.9点。此外，DistilPose-L在MSCOCO验证数据集上获得了74.4%的mAP，成为主流基于回归的模型中的新领先者。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In the field of human pose estimation, regression-based methods have been dominated in terms of speed, while heatmap-based methods are far ahead in terms of performance. How to take advantage of both schemes remains a challenging problem. In this paper, we propose a novel human pose estimation framework termed DistilPose, which bridges the gaps between heatmap-based and regression-based methods. Specifically, DistilPose maximizes the transfer of knowledge from the teacher model (heatmap-based) to the student model (regression-based) through Token-distilling Encoder (TDE) and Simulated Heatmaps. TDE aligns the feature spaces of heatmap-based and regression-based models by introducing tokenization, while Simulated Heatmaps transfer explicit guidance (distribution and confidence) from teacher heatmaps into student models. Extensive experiments show that the proposed DistilPose can significantly improve the performance of the regression-based models while maintaining efficiency. Specifically, on the MSCOCO validation dataset, DistilPose-S obtains 71.6% mAP with 5.36M parameter, 2.38 GFLOPs and 40.2 FPS, which saves 12.95x, 7.16x computational cost and is 4.9x faster than its teacher model with only 0.9 points performance drop. Furthermore, DistilPose-L obtains 74.4% mAP on MSCOCO validation dataset, achieving a new state-of-the-art among predominant regression-based models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1741.CUF: Continuous Upsampling Filters</span><br>
                <span class="as">Vasconcelos, CristinaN.andOztireli, CengizandMatthews, MarkandHashemi, MiladandSwersky, KevinandTagliasacchi, Andrea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vasconcelos_CUF_Continuous_Upsampling_Filters_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9999-10008.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将神经场应用于2D图像处理中的一个重要操作——上采样。<br>
                    动机：尽管神经场已被广泛用于3D信号表示，但在经典的2D图像处理中的应用相对有限。<br>
                    方法：我们将上采样核参数化为神经场，这种参数化方式使得我们的架构比竞争的任意尺度超分辨率架构减少了40倍的参数数量。<br>
                    效果：在对大小为256x256的图像进行上采样时，我们的架构比竞争的任意尺度超分辨率架构效率高2-10倍，并且在实例化为单尺度模型时，比亚像素卷积更高效。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Neural fields have rapidly been adopted for representing 3D signals, but their application to more classical 2D image-processing has been relatively limited. In this paper, we consider one of the most important operations in image processing: upsampling. In deep learning, learnable upsampling layers have extensively been used for single image super-resolution. We propose to parameterize upsampling kernels as neural fields. This parameterization leads to a compact architecture that obtains a 40-fold reduction in the number of parameters when compared with competing arbitrary-scale super-resolution architectures. When upsampling images of size 256x256 we show that our architecture is 2x-10x more efficient than competing arbitrary-scale super-resolution architectures, and more efficient than sub-pixel convolutions when instantiated to a single-scale model. In the general setting, these gains grow polynomially with the square of the target scale. We validate our method on standard benchmarks showing such efficiency gains can be achieved without sacrifices in super-resolution performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1742.HOTNAS: Hierarchical Optimal Transport for Neural Architecture Search</span><br>
                <span class="as">Yang, JiechaoandLiu, YongandXu, Hongteng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_HOTNAS_Hierarchical_Optimal_Transport_for_Neural_Architecture_Search_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11990-12000.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在多个相对较小的细胞中搜索网络架构，同时衡量细胞微观结构和不同基于细胞的网络之间的宏观结构差异。<br>
                    动机：目前的NAS方法需要在整个网络中进行直接搜索，成本较高。为了降低搜索成本，越来越多的方法是搜索多个相对较小的细胞。然而，如何衡量不同网络的相似性和差异性是一个主要挑战。<br>
                    方法：提出了一种称为HOTNN的分层最优传输度量方法，用于测量不同网络的相似性。HOTNN通过考虑每个节点的相似性和每个细胞内节点对之间的信息流成本差异来计算不同网络中的细胞级相似性。通过网络级相似性和各自网络中每个细胞的全局位置变化来计算网络级相似性。然后在一个名为HOTNAS的贝叶斯优化框架中探索HOTNN，并证明其在多种任务中的有效性。<br>
                    效果：实验表明，HOTNAS可以在多个模块化的基于细胞的搜索空间中发现性能更好的网络架构。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Instead of searching the entire network directly, current NAS approaches increasingly search for multiple relatively small cells to reduce search costs. A major challenge is to jointly measure the similarity of cell micro-architectures and the difference in macro-architectures between different cell-based networks. Recently, optimal transport (OT) has been successfully applied to NAS as it can capture the operational and structural similarity across various networks. However, existing OT-based NAS methods either ignore the cell similarity or focus solely on searching for a single cell architecture. To address these issues, we propose a hierarchical optimal transport metric called HOTNN for measuring the similarity of different networks. In HOTNN, the cell-level similarity computes the OT distance between cells in various networks by considering the similarity of each node and the differences in the information flow costs between node pairs within each cell in terms of operational and structural information. The network-level similarity calculates OT distance between networks by considering both the cell-level similarity and the variation in the global position of each cell within their respective networks. We then explore HOTNN in a Bayesian optimization framework called HOTNAS, and demonstrate its efficacy in diverse tasks. Extensive experiments demonstrate that HOTNAS can discover network architectures with better performance in multiple modular cell-based search spaces.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1743.Practical Network Acceleration With Tiny Sets</span><br>
                <span class="as">Wang, Guo-HuaandWu, Jianxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Practical_Network_Acceleration_With_Tiny_Sets_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20331-20340.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量训练样本加速神经网络。<br>
                    动机：由于数据隐私问题，使用少量训练样本来加速网络在实践中变得至关重要。<br>
                    方法：提出了一种全新的网络压缩方式——丢弃块，并定义了一个新的概念“可恢复性”来衡量压缩后的网络的恢复难度。<br>
                    效果：实验结果表明，该方法在减少网络延迟方面优于之前的方法，并且在ImageNet-1k上平均比之前的方法高出7%。此外，该方法还具有良好的泛化能力，可以在无数据或领域外数据设置下良好运行。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to data privacy issues, accelerating networks with tiny training sets has become a critical need in practice. Previous methods mainly adopt filter-level pruning to accelerate networks with scarce training samples. In this paper, we reveal that dropping blocks is a fundamentally superior approach in this scenario. It enjoys a higher acceleration ratio and results in a better latency-accuracy performance under the few-shot setting. To choose which blocks to drop, we propose a new concept namely recoverability to measure the difficulty of recovering the compressed network. Our recoverability is efficient and effective for choosing which blocks to drop. Finally, we propose an algorithm named PRACTISE to accelerate networks using only tiny sets of training images. PRACTISE outperforms previous methods by a significant margin. For 22% latency reduction, PRACTISE surpasses previous methods by on average 7% on ImageNet-1k. It also enjoys high generalization ability, working well under data-free or out-of-domain data settings, too. Our code is at https://github.com/DoctorKey/Practise.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1744.AstroNet: When Astrocyte Meets Artificial Neural Network</span><br>
                <span class="as">Han, MengqiaoandPan, LiyuanandLiu, Xiabi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_AstroNet_When_Astrocyte_Meets_Artificial_Neural_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20258-20268.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化网络结构，提高其效率而不牺牲性能？<br>
                    动机：通过研究星形胶质细胞这种新的神经元连接调控机制，提出一种可以自适应优化神经元连接的AstroNet模型。<br>
                    方法：基于构建的星形胶质细胞-神经元模型，利用星形胶质细胞的双向通信特性，设计了一种具有时间调控机制和全局连接机制的AstroNet模型。该模型使用神经网络执行任务，同时用星形胶质细胞网络不断优化神经网络的连接，即自适应地为神经网络的神经元单元分配权重。<br>
                    效果：在分类任务上的实验表明，我们的AstroNet模型可以在优化网络结构的同时实现最先进的准确率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Network structure learning aims to optimize network architectures and make them more efficient without compromising performance. In this paper, we first study the astrocytes, a new mechanism to regulate connections in the classic M-P neuron. Then, with the astrocytes, we propose an AstroNet that can adaptively optimize neuron connections and therefore achieves structure learning to achieve higher accuracy and efficiency. AstroNet is based on our built Astrocyte-Neuron model, with a temporal regulation mechanism and a global connection mechanism, which is inspired by the bidirectional communication property of astrocytes. With the model, the proposed AstroNet uses a neural network (NN) for performing tasks, and an astrocyte network (AN) to continuously optimize the connections of NN, i.e., assigning weight to the neuron units in the NN adaptively. Experiments on the classification task demonstrate that our AstroNet can efficiently optimize the network structure while achieving state-of-the-art (SOTA) accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1745.Parameter Efficient Local Implicit Image Function Network for Face Segmentation</span><br>
                <span class="as">Sarkar, MausoomandNikitha, SRandHemani, MayurandJain, RishabhandKrishnamurthy, Balaji</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sarkar_Parameter_Efficient_Local_Implicit_Image_Function_Network_for_Face_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20970-20980.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种轻量级的人脸解析方法，利用人脸的结构一致性进行像素级别的标签标注。<br>
                    动机：现有的人脸解析模型参数量大，不适用于低计算或低带宽的设备。<br>
                    方法：提出了一种局部隐式函数网络（FP-LIIF）的轻量级人脸解析方法，其结构包括一个卷积编码器和一个像素多层感知器解码器，参数数量仅为最先进的模型的1/26，且无需预训练。<br>
                    效果：在CelebAMask-HQ和LaPa等多个数据集上，该方法在不改变输入分辨率的情况下生成不同分辨率的分割结果，性能与最先进的模型相当或更好，同时具有较高的帧率和较小的模型大小，适用于低计算或低带宽的设备。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face parsing is defined as the per-pixel labeling of images containing human faces. The labels are defined to identify key facial regions like eyes, lips, nose, hair, etc. In this work, we make use of the structural consistency of the human face to propose a lightweight face-parsing method using a Local Implicit Function network, FP-LIIF. We propose a simple architecture having a convolutional encoder and a pixel MLP decoder that uses 1/26th number of parameters compared to the state-of-the-art models and yet matches or outperforms state-of-the-art models on multiple datasets, like CelebAMask-HQ and LaPa. We do not use any pretraining, and compared to other works, our network can also generate segmentation at different resolutions without any changes in the input resolution. This work enables the use of facial segmentation on low-compute or low-bandwidth devices because of its higher FPS and smaller model size.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1746.Modality-Invariant Visual Odometry for Embodied Vision</span><br>
                <span class="as">Memmel, MariusandBachmann, RomanandZamir, Amir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Memmel_Modality-Invariant_Visual_Odometry_for_Embodied_Vision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21549-21559.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在真实、嘈杂的环境中有效地定位代理？<br>
                    动机：在现实环境中，视觉里程计（VO）是替代不可靠的GPS和罗盘传感器的有效方法，但现有的深度学习VO模型在传感器失效或改变时会崩溃。<br>
                    方法：我们提出了一种基于Transformer的模态不变视觉里程计方法，该方法可以处理导航代理的多样化或变化的传感器套件。<br>
                    效果：我们的模型在训练数据仅为以前一小部分的情况下，表现优于以往的方法，并希望这种方法能为从灵活和学习的VO模型中获益的更广泛的现实应用打开大门。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Effectively localizing an agent in a realistic, noisy setting is crucial for many embodied vision tasks. Visual Odometry (VO) is a practical substitute for unreliable GPS and compass sensors, especially in indoor environments. While SLAM-based methods show a solid performance without large data requirements, they are less flexible and robust w.r.t. to noise and changes in the sensor suite compared to learning-based approaches. Recent deep VO models, however, limit themselves to a fixed set of input modalities, e.g., RGB and depth, while training on millions of samples. When sensors fail, sensor suites change, or modalities are intentionally looped out due to available resources, e.g., power consumption, the models fail catastrophically. Furthermore, training these models from scratch is even more expensive without simulator access or suitable existing models that can be fine-tuned. While such scenarios get mostly ignored in simulation, they commonly hinder a model's reusability in real-world applications. We propose a Transformer-based modality-invariant VO approach that can deal with diverse or changing sensor suites of navigation agents. Our model outperforms previous methods while training on only a fraction of the data. We hope this method opens the door to a broader range of real-world applications that can benefit from flexible and learned VO models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1747.Towards a Smaller Student: Capacity Dynamic Distillation for Efficient Image Retrieval</span><br>
                <span class="as">Xie, YiandZhang, HuaidongandXu, XuemiaoandZhu, JianqingandHe, Shengfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Towards_a_Smaller_Student_Capacity_Dynamic_Distillation_for_Efficient_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16006-16015.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过联合训练大规模文本语料库和知识图谱，利用外部知识增强语言表示。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出ERNIE模型，该模型能同时充分利用词汇、句法和知识信息。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous Knowledge Distillation based efficient image retrieval methods employ a lightweight network as the student model for fast inference. However, the lightweight student model lacks adequate representation capacity for effective knowledge imitation during the most critical early training period, causing final performance degeneration. To tackle this issue, we propose a Capacity Dynamic Distillation framework, which constructs a student model with editable representation capacity. Specifically, the employed student model is initially a heavy model to fruitfully learn distilled knowledge in the early training epochs, and the student model is gradually compressed during the training. To dynamically adjust the model capacity, our dynamic framework inserts a learnable convolutional layer within each residual block in the student model as the channel importance indicator. The indicator is optimized simultaneously by the image retrieval loss and the compression loss, and a retrieval-guided gradient resetting mechanism is proposed to release the gradient conflict. Extensive experiments show that our method has superior inference speed and accuracy, e.g., on the VeRi-776 dataset, given the ResNet101 as a teacher, our method saves 67.13% model parameters and 65.67% FLOPs without sacrificing accuracy.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1748.Federated Incremental Semantic Segmentation</span><br>
                <span class="as">Dong, JiahuaandZhang, DuzhenandCong, YangandCong, WeiandDing, HenghuiandDai, Dengxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3934-3943.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在联邦学习中解决语义分割模型对旧类别的遗忘问题。<br>
                    动机：现有的联邦学习模型在处理新类别时，会对旧类别产生严重的遗忘现象，且无法应对新客户端收集的新类别加入全局训练的情况。<br>
                    方法：提出一种遗忘平衡学习（FBL）模型，通过自适应类别平衡伪标签生成的伪标签指导，开发遗忘平衡语义补偿损失和遗忘平衡关系一致性损失来修正具有背景偏移的本地客户端内部异构遗忘旧类别的问题。同时，提出任务转换监视器来解决客户端间的异构遗忘问题。<br>
                    效果：实验结果表明，FBL模型在处理新旧类别问题上有显著改进，优于对比方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated learning-based semantic segmentation (FSS) has drawn widespread attention via decentralized training on local clients. However, most FSS models assume categories are fxed in advance, thus heavily undergoing forgetting on old categories in practical applications where local clients receive new categories incrementally while have no memory storage to access old classes. Moreover, new clients collecting novel classes may join in the global training of FSS, which further exacerbates catastrophic forgetting. To surmount the above challenges, we propose a Forgetting-Balanced Learning (FBL) model to address heterogeneous forgetting on old classes from both intra-client and inter-client aspects. Specifically, under the guidance of pseudo labels generated via adaptive class-balanced pseudo labeling, we develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to rectify intra-client heterogeneous forgetting of old categories with background shift. It performs balanced gradient propagation and relation consistency distillation within local clients. Moreover, to tackle heterogeneous forgetting from inter-client aspect, we propose a task transition monitor. It can identify new classes under privacy protection and store the latest old global model for relation distillation. Qualitative experiments reveal large improvement of our model against comparison methods. The code is available at https://github.com/JiahuaDong/FISS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1749.Avatars Grow Legs: Generating Smooth Human Motion From Sparse Tracking Inputs With Diffusion Model</span><br>
                <span class="as">Du, YumingandKips, RobinandPumarola, AlbertandStarke, SebastianandThabet, AliandSanakoyeu, Artsiom</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Avatars_Grow_Legs_Generating_Smooth_Human_Motion_From_Sparse_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/481-490.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确控制3D全身虚拟形象，特别是在只有稀疏的上半身跟踪信号的情况下。<br>
                    动机：随着AR/VR应用的普及，对3D全身虚拟形象的真实和准确控制需求日益增加，但现有的跟踪信号通常只能追踪到用户的头部和手腕，下半身需要通过上半身关节的有限信息来合成。<br>
                    方法：提出了一种名为AGRoL的新型条件扩散模型，该模型基于简单的多层感知器（MLP）架构和一种新的运动数据条件方案，能够根据稀疏的上半身跟踪信号预测准确的全身运动，特别是具有挑战性的下半身运动。<br>
                    效果：在AMASS运动捕捉数据集上进行训练和评估后，AGRoL在生成的运动准确性和平滑性方面优于现有方法，且由于其简洁的设计，可以实时运行，非常适合在线身体跟踪应用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the recent surge in popularity of AR/VR applications, realistic and accurate control of 3D full-body avatars has become a highly demanded feature. A particular challenge is that only a sparse tracking signal is available from standalone HMDs (Head Mounted Devices), often limited to tracking the user's head and wrists. While this signal is resourceful for reconstructing the upper body motion, the lower body is not tracked and must be synthesized from the limited information provided by the upper body joints. In this paper, we present AGRoL, a novel conditional diffusion model specifically designed to track full bodies given sparse upper-body tracking signals. Our model is based on a simple multi-layer perceptron (MLP) architecture and a novel conditioning scheme for motion data. It can predict accurate and smooth full-body motion, particularly the challenging lower body movement. Unlike common diffusion architectures, our compact architecture can run in real-time, making it suitable for online body-tracking applications. We train and evaluate our model on AMASS motion capture dataset, and demonstrate that our approach outperforms state-of-the-art methods in generated motion accuracy and smoothness. We further justify our design choices through extensive experiments and ablation studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1750.NAR-Former: Neural Architecture Representation Learning Towards Holistic Attributes Prediction</span><br>
                <span class="as">Yi, YunandZhang, HaokuiandHu, WenzeandWang, NannanandWang, Xiaoyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_NAR-Former_Neural_Architecture_Representation_Learning_Towards_Holistic_Attributes_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7715-7724.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何模型化和学习神经网络自身的表示，以估计不同神经网络架构的属性，如准确性和延迟时间。<br>
                    动机：随着深度学习模型在实际应用中的广泛深入使用，对模型化和学习神经网络自身表示的需求日益增加。<br>
                    方法：本文提出了一种神经架构表示模型，该模型可以全面估计这些属性。具体来说，首先提出了一种简单而有效的标记器，将神经网络的操作和拓扑信息编码为单个序列。然后设计了一个多阶段融合转换器，从转换后的序列中构建紧凑的向量表示。为了有效训练模型，进一步提出了信息流一致性增强和相应的架构一致性损失，与以前的随机增强策略相比，这种策略可以在较少的增强样本下带来更多的好处。<br>
                    效果：实验结果在NAS-Bench-101、NAS-Bench-201、DARTS搜索空间和NNLQP上表明，提出的框架可以预测细胞架构和整个深度神经网络的准确性和延迟属性，并取得了良好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the wide and deep adoption of deep learning models in real applications, there is an increasing need to model and learn the representations of the neural networks themselves. These models can be used to estimate attributes of different neural network architectures such as the accuracy and latency, without running the actual training or inference tasks. In this paper, we propose a neural architecture representation model that can be used to estimate these attributes holistically. Specifically, we first propose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fusion transformer to build a compact vector representation from the converted sequence. For efficient model training, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with previous random augmentation strategies. Experiment results on NAS-Bench-101, NAS-Bench-201, DARTS search space and NNLQP show that our proposed framework can be used to predict the aforementioned latency and accuracy attributes of both cell architectures and whole deep neural networks, and achieves promising performance. Code is available at https://github.com/yuny220/NAR-Former.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1751.Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using RGB and Poses</span><br>
                <span class="as">Brachmann, EricandCavallari, TommasoandPrisacariu, VictorAdrian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5044-5053.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于学习的视觉重定位方法虽然具有较高的准确性，但需要数小时或数天的培训时间，使得其在大多数应用中并不实用。<br>
                    动机：为了解决训练时间长的问题，使基于学习的重定位方法在实际中得到应用。<br>
                    方法：将重定位网络分为场景无关的特征主干和场景特定的预测头，并使用MLP预测头在每次训练迭代中同时优化数千个视点，实现快速收敛。<br>
                    效果：该方法比最先进的场景坐标回归快300倍，同时保持了相同的精度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning-based visual relocalizers exhibit leading pose accuracy, but require hours or days of training. Since training needs to happen on each new scene again, long training times make learning-based relocalization impractical for most applications, despite its promise of high accuracy. In this paper we show how such a system can actually achieve the same accuracy in less than 5 minutes. We start from the obvious: a relocalization network can be split in a scene-agnostic feature backbone, and a scene-specific prediction head. Less obvious: using an MLP prediction head allows us to optimize across thousands of view points simultaneously in each single training iteration. This leads to stable and extremely fast convergence. Furthermore, we substitute effective but slow end-to-end training using a robust pose solver with a curriculum over a reprojection loss. Our approach does not require privileged knowledge, such a depth maps or a 3D model, for speedy training. Overall, our approach is up to 300x faster in mapping than state-of-the-art scene coordinate regression, while keeping accuracy on par. Code is available: https://nianticlabs.github.io/ace</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1752.Switchable Representation Learning Framework With Self-Compatibility</span><br>
                <span class="as">Wu, ShengsenandBai, YanandLou, YihangandLinghu, XiongkunandHe, JianzhongandDuan, Ling-Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Switchable_Representation_Learning_Framework_With_Self-Compatibility_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15943-15953.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在具有不同计算和存储资源的多个平台上部署视觉搜索系统，并实现模型之间的特征对齐。<br>
                    动机：现有的统一模型在最小约束平台上的精度有限，需要开发适应资源限制的不同容量的模型，并要求这些模型提取的特征在度量空间中对齐。<br>
                    方法：提出一种可切换表示学习框架（SFSC），通过一次训练过程生成一系列具有不同容量的兼容子模型。通过估计不确定性动态调整子模型的优先级，并对冲突方向的梯度进行投影以避免相互干扰。<br>
                    效果：SFSC在评估数据集上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-world visual search systems involve deployments on multiple platforms with different computing and storage resources. Deploying a unified model that suits the minimal-constrain platforms leads to limited accuracy. It is expected to deploy models with different capacities adapting to the resource constraints, which requires features extracted by these models to be aligned in the metric space. The method to achieve feature alignments is called "compatible learning". Existing research mainly focuses on the one-to-one compatible paradigm, which is limited in learning compatibility among multiple models. We propose a Switchable representation learning Framework with Self-Compatibility (SFSC). SFSC generates a series of compatible sub-models with different capacities through one training process. The optimization of sub-models faces gradients conflict, and we mitigate this problem from the perspective of the magnitude and direction. We adjust the priorities of sub-models dynamically through uncertainty estimation to co-optimize sub-models properly. Besides, the gradients with conflicting directions are projected to avoid mutual interference. SFSC achieves state-of-the-art performance on the evaluated datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1753.Partial Network Cloning</span><br>
                <span class="as">Ye, JingwenandLiu, SonghuaandWang, Xinchao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Partial_Network_Cloning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/20137-20146.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了一种新的部分知识转移任务，即部分网络克隆（PNC）。<br>
                    动机：与需要在整个知识转移过程中更新目标网络的所有或至少部分参数的现有方法不同，PNC从源网络进行部分参数“克隆”，然后将克隆的模块注入目标网络，而不修改其参数。<br>
                    方法：我们引入了一种创新的学习方案，可以同时确定要从源网络克隆的组件以及在目标网络中要插入的位置，以确保最优性能。<br>
                    效果：实验结果表明，与基于参数调整的方法相比，我们的方法在准确率和局部性方面分别提高了5%和50%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study a novel task that enables partial knowledge transfer from pre-trained models, which we term as Partial Network Cloning (PNC). Unlike prior methods that update all or at least part of the parameters in the target network throughout the knowledge transfer process, PNC conducts partial parametric "cloning" from a source network and then injects the cloned module to the target, without modifying its parameters. Thanks to the transferred module, the target network is expected to gain additional functionality, such as inference on new classes; whenever needed, the cloned module can be readily removed from the target, with its original parameters and competence kept intact. Specifically, we introduce an innovative learning scheme that allows us to identify simultaneously the component to be cloned from the source and the position to be inserted within the target network, so as to ensure the optimal performance. Experimental results on several datasets demonstrate that, our method yields a significant improvement of 5% in accuracy and 50% in locality when compared with parameter-tuning based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1754.Principles of Forgetting in Domain-Incremental Semantic Segmentation in Adverse Weather Conditions</span><br>
                <span class="as">Kalb, TobiasandBeyerer, J\&quot;urgen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kalb_Principles_of_Forgetting_in_Domain-Incremental_Semantic_Segmentation_in_Adverse_Weather_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19508-19518.png><br>
            
            <span class="tt"><span class="t0">研究问题：在自动驾驶车辆的场景感知中，深度神经网络在训练领域上取得了优秀的结果，但在现实世界条件下，操作领域及其底层数据分布会发生变化，如何减少模型性能的下降？<br>
                    动机：恶劣的天气条件会显著降低模型性能，当这种数据在训练期间不可用时。此外，当模型逐步适应新领域时，会出现灾难性遗忘，导致之前观察到的领域的性能大幅下降。<br>
                    方法：通过实验和表示分析，研究语义分割模型在恶劣天气条件下进行领域增量学习时的表示变化。<br>
                    效果：实验和表示分析表明，灾难性遗忘主要是由于领域增量学习中低层次特征的变化引起的。使用预训练和图像增强在源领域学习更通用的特征可以有效地重用后续任务的特征，从而大大减少灾难性遗忘。这些发现强调了促进通用特征的方法对于有效的持续学习算法的重要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep neural networks for scene perception in automated vehicles achieve excellent results for the domains they were trained on. However, in real-world conditions, the domain of operation and its underlying data distribution are subject to change. Adverse weather conditions, in particular, can significantly decrease model performance when such data are not available during training. Additionally, when a model is incrementally adapted to a new domain, it suffers from catastrophic forgetting, causing a significant drop in performance on previously observed domains. Despite recent progress in reducing catastrophic forgetting, its causes and effects remain obscure. Therefore, we study how the representations of semantic segmentation models are affected during domain-incremental learning in adverse weather conditions. Our experiments and representational analyses indicate that catastrophic forgetting is primarily caused by changes to low-level features in domain-incremental learning and that learning more general features on the source domain using pre-training and image augmentations leads to efficient feature reuse in subsequent tasks, which drastically reduces catastrophic forgetting. These findings highlight the importance of methods that facilitate generalized features for effective continual learning algorithms.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>