<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic9</h2>
      <b>Topic words : &ensp;</b>segmentation, &ensp;semantic, &ensp;supervised, &ensp;object, &ensp;detection, &ensp;labels, &ensp;instance, &ensp;level</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1984.HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation</span><br>
                <span class="as">Ding, JianandXue, NanandXia, Gui-SongandSchiele, BerntandDai, Dengxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_HGFormer_Hierarchical_Grouping_Transformer_for_Domain_Generalized_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15413-15423.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的语义分割模型在独立同分布条件下取得了巨大成功，但在真实世界应用中，测试数据可能来自与训练数据不同的领域。因此，提高模型对领域差异的鲁棒性非常重要。<br>
                    动机：本研究探讨了在领域泛化设置下的语义分割，即模型仅在源领域进行训练，并在未见过的目标领域进行测试。<br>
                    方法：我们提出了一种新的分层分组变换器（HGFormer），通过显式地将像素分组形成部分级别的掩码和整体级别的掩码，来提高模型的鲁棒性。<br>
                    效果：实验表明，HGFormer比像素级分类方法和平面分组变换器产生了更鲁棒的语义分割结果，并且显著优于先前的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current semantic segmentation models have achieved great success under the independent and identically distributed (i.i.d.) condition. However, in real-world applications, test data might come from a different domain than training data. Therefore, it is important to improve model robustness against domain differences. This work studies semantic segmentation under the domain generalization setting, where a model is trained only on the source domain and tested on the unseen target domain. Existing works show that Vision Transformers are more robust than CNNs and show that this is related to the visual grouping property of self-attention. In this work, we propose a novel hierarchical grouping transformer (HGFormer) to explicitly group pixels to form part-level masks and then whole-level masks. The masks at different scales aim to segment out both parts and a whole of classes. HGFormer combines mask classification results at both scales for class label prediction. We assemble multiple interesting cross-domain settings by using seven public semantic segmentation datasets. Experiments show that HGFormer yields more robust semantic segmentation results than per-pixel classification methods and flat-grouping transformers, and outperforms previous methods significantly. Code will be available at https://github.com/dingjiansw101/HGFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1985.Distilling Vision-Language Pre-Training To Collaborate With Weakly-Supervised Temporal Action Localization</span><br>
                <span class="as">Ju, ChenandZheng, KunhaoandLiu, JinxiangandZhao, PeisenandZhang, YaandChang, JianlongandTian, QiandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ju_Distilling_Vision-Language_Pre-Training_To_Collaborate_With_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14751-14762.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过弱监督进行时间动作定位，并解决现有方法中优化目标不同导致的不完整问题。<br>
                    动机：大部分方法采用现成的分类预训练生成视频特征进行动作定位，但分类和定位的优化目标不同，导致定位结果存在严重的不完整问题。<br>
                    方法：从视觉语言预训练（VLP）中提炼自由动作知识，构建一个新的蒸馏协作框架，该框架由两个分别执行CBP和VLP的分支组成，并通过双分支交替训练策略进行优化。<br>
                    效果：在THUMOS14和ActivityNet1.2上进行的大量实验和消融研究表明，该方法显著优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-supervised temporal action localization (WTAL) learns to detect and classify action instances with only category labels. Most methods widely adopt the off-the-shelf Classification-Based Pre-training (CBP) to generate video features for action localization. However, the different optimization objectives between classification and localization, make temporally localized results suffer from the serious incomplete issue. To tackle this issue without additional annotations, this paper considers to distill free action knowledge from Vision-Language Pre-training (VLP), as we surprisingly observe that the localization results of vanilla VLP have an over-complete issue, which is just complementary to the CBP results. To fuse such complementarity, we propose a novel distillation-collaboration framework with two branches acting as CBP and VLP respectively. The framework is optimized through a dual-branch alternate training strategy. Specifically, during the B step, we distill the confident background pseudo-labels from the CBP branch; while during the F step, the confident foreground pseudo-labels are distilled from the VLP branch. As a result, the dual-branch complementarity is effectively fused to promote one strong alliance. Extensive experiments and ablation studies on THUMOS14 and ActivityNet1.2 reveal that our method significantly outperforms state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1986.Exploring Structured Semantic Prior for Multi Label Recognition With Incomplete Labels</span><br>
                <span class="as">Ding, ZixuanandWang, AoandChen, HuiandZhang, QiangandLiu, PengzhangandBao, YongjunandYan, WeipengandHan, Jungong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Exploring_Structured_Semantic_Prior_for_Multi_Label_Recognition_With_Incomplete_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3398-3407.png><br>
            
            <span class="tt"><span class="t0">研究问题：多标签识别（MLR）在标签不完整的情况下具有挑战性。<br>
                    动机：尽管现有的视觉语言模型CLIP在图像到标签的对应关系上表现出色，但它们通常忽视了标签到标签对应关系的宝贵先验知识。<br>
                    方法：通过语义先验提示器获取标签到标签对应关系的结构化语义先验，并提出了一种新的语义对应提示网络（SCPNet）。<br>
                    效果：在多个广泛使用的基准数据集上的全面实验和分析表明，该方法在所有数据集上都显著优于现有方法，充分展示了该方法的有效性和优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-label recognition (MLR) with incomplete labels is very challenging. Recent works strive to explore the image-to-label correspondence in the vision-language model, i.e., CLIP, to compensate for insufficient annotations. In spite of promising performance, they generally overlook the valuable prior about the label-to-label correspondence. In this paper, we advocate remedying the deficiency of label supervision for the MLR with incomplete labels by deriving a structured semantic prior about the label-to-label correspondence via a semantic prior prompter. We then present a novel Semantic Correspondence Prompt Network (SCPNet), which can thoroughly explore the structured semantic prior. A Prior-Enhanced Self-Supervised Learning method is further introduced to enhance the use of the prior. Comprehensive experiments and analyses on several widely used benchmark datasets show that our method significantly outperforms existing methods on all datasets, well demonstrating the effectiveness and the superiority of our method. Our code will be available at https://github.com/jameslahm/SCPNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1987.Instance-Specific and Model-Adaptive Supervision for Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Zhao, ZhenandLong, SifanandPi, JiminandWang, JingdongandZhou, Luping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Instance-Specific_and_Model-Adaptive_Supervision_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23705-23714.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督语义分割方法在处理未标注数据时，没有考虑到不同实例间的差异和训练难度，本研究旨在解决这个问题。<br>
                    动机：通过区分未标注实例，可以促进针对特定实例的监督，使模型能够动态适应其演变。<br>
                    方法：提出了一种名为iMAS的实例特定和模型自适应的监督方法。该方法根据模型性能使用类加权对称交并比来评估每个未标注实例的定量难度，并根据评估的难度对未标注数据的进行逐步学习。<br>
                    效果：实验结果表明，iMAS在各种半监督分区协议下的分割基准测试中，都能显著提高性能，超越了当前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, semi-supervised semantic segmentation has achieved promising performance with a small fraction of labeled data. However, most existing studies treat all unlabeled data equally and barely consider the differences and training difficulties among unlabeled instances. Differentiating unlabeled instances can promote instance-specific supervision to adapt to the model's evolution dynamically. In this paper, we emphasize the cruciality of instance differences and propose an instance-specific and model-adaptive supervision for semi-supervised semantic segmentation, named iMAS. Relying on the model's performance, iMAS employs a class-weighted symmetric intersection-over-union to evaluate quantitative hardness of each unlabeled instance and supervises the training on unlabeled data in a model-adaptive manner. Specifically, iMAS learns from unlabeled instances progressively by weighing their corresponding consistency losses based on the evaluated hardness. Besides, iMAS dynamically adjusts the augmentation for each instance such that the distortion degree of augmented instances is adapted to the model's generalization capability across the training course. Not integrating additional losses and training procedures, iMAS can obtain remarkable performance gains against current state-of-the-art approaches on segmentation benchmarks under different semi-supervised partition protocols.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1988.Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target Detection With Single Point Supervision</span><br>
                <span class="as">Ying, XinyiandLiu, LiandWang, YingqianandLi, RuojingandChen, NuoandLin, ZaipingandSheng, WeidongandZhou, Shilin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ying_Mapping_Degeneration_Meets_Label_Evolution_Learning_Infrared_Small_Target_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15528-15538.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练卷积神经网络（CNN）以全监督方式检测红外小目标，但需要大量的像素级标注，成本高昂。<br>
                    动机：为了解决这个问题，本文首次尝试使用点级监督进行红外小目标检测。<br>
                    方法：在点标签监督的训练阶段，我们发现CNNs首先学习分割目标附近的像素群集，然后逐渐收敛到预测地面真值点标签。受此“映射退化”现象的启发，我们提出了一种标签演化框架，名为单点监督下的标签演化（LESPS），通过利用CNN的中间预测来逐步扩展点标签。<br>
                    效果：实验结果表明，配备LESPS的CNNs可以从相应的点标签中恢复目标掩码，并在像素级交并比（IoU）和对象级检测概率（Pd）方面分别实现超过70%和95%的全监督性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training a convolutional neural network (CNN) to detect infrared small targets in a fully supervised manner has gained remarkable research interests in recent years, but is highly labor expensive since a large number of per-pixel annotations are required. To handle this problem, in this paper, we make the first attempt to achieve infrared small target detection with point-level supervision. Interestingly, during the training phase supervised by point labels, we discover that CNNs first learn to segment a cluster of pixels near the targets, and then gradually converge to predict groundtruth point labels. Motivated by this "mapping degeneration" phenomenon, we propose a label evolution framework named label evolution with single point supervision (LESPS) to progressively expand the point label by leveraging the intermediate predictions of CNNs. In this way, the network predictions can finally approximate the updated pseudo labels, and a pixel-level target mask can be obtained to train CNNs in an end-to-end manner. We conduct extensive experiments with insightful visualizations to validate the effectiveness of our method. Experimental results show that CNNs equipped with LESPS can well recover the target masks from corresponding point labels, and can achieve over 70% and 95% of their fully supervised performance in terms of pixel-level intersection over union (IoU) and object-level probability of detection (Pd), respectively. Code is available at https://github.com/XinyiYing/LESPS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1989.Delving Into Shape-Aware Zero-Shot Semantic Segmentation</span><br>
                <span class="as">Liu, XinyuandTian, BeiwenandWang, ZhenandWang, RuiandSheng, KehuaandZhang, BoandZhao, HaoandZhou, Guyue</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Delving_Into_Shape-Aware_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2999-3009.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将视觉语言预训练的成功应用于语义分割。<br>
                    动机：现有的视觉语言模型虽然能准确理解语义，但在精细的形状描绘和密集预测任务上表现不佳。<br>
                    方法：借鉴图像分割文献中的古典谱方法，提出利用自监督像素级特征构建的拉普拉斯矩阵的特征向量来提升形状感知能力。<br>
                    效果：该方法在Pascal和COCO数据集上实现了新的最先进的零样本语义分割性能，且具有显著优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Thanks to the impressive progress of large-scale vision-language pretraining, recent recognition models can classify arbitrary objects in a zero-shot and open-set manner, with a surprisingly high accuracy. However, translating this success to semantic segmentation is not trivial, because this dense prediction task requires not only accurate semantic understanding but also fine shape delineation and existing vision-language models are trained with image-level language descriptions. To bridge this gap, we pursue shape-aware zero-shot semantic segmentation in this study. Inspired by classical spectral methods in the image segmentation literature, we propose to leverage the eigen vectors of Laplacian matrices constructed with self-supervised pixel-wise features to promote shape-awareness. Despite that this simple and effective technique does not make use of the masks of seen classes at all, we demonstrate that it out-performs a state-of-the-art shape-aware formulation that aligns ground truth and predicted edges during training. We also delve into the performance gains achieved on different datasets using different backbones and draw several interesting and conclusive observations: the benefits of promoting shape-awareness highly relates to mask compactness and language embedding locality. Finally, our method sets new state-of-the-art performance for zero-shot semantic segmentation on both Pascal and COCO, with significant margins. Code and models will be accessed at https://github.com/Liuxinyv/SAZS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1990.Annealing-Based Label-Transfer Learning for Open World Object Detection</span><br>
                <span class="as">Ma, YuqingandLi, HainanandZhang, ZhangeandGuo, JinyangandZhang, ShanghangandGong, RuihaoandLiu, Xianglong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Annealing-Based_Label-Transfer_Learning_for_Open_World_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11454-11463.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高开放世界目标检测（OWOD）的性能，特别是在未知目标的识别上。<br>
                    动机：现有的OWOD方法需要手动选择未知目标，缺乏适当的先验知识，导致不确定性。<br>
                    方法：提出一种基于退火的标签转移框架，将未知特征通过卷积操作传播到已知目标，并通过标签转移学习范式和锯齿退火调度策略来重建已知和未知类别的决策边界，以提升已知和未知的识别能力。<br>
                    效果：在常用基准测试中，该方法实现了优越的检测性能（未知目标的mAP提高了200%，同时已知目标的检测性能更高），并且是首个无需手动选择未知目标的OWOD方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open world object detection (OWOD) has attracted extensive attention due to its practicability in the real world. Previous OWOD works manually designed unknown-discover strategies to select unknown proposals from the background, suffering from uncertainties without appropriate priors. In this paper, we claim the learning of object detection could be seen as an object-level feature-entanglement process, where unknown traits are propagated to the known proposals through convolutional operations and could be distilled to benefit unknown recognition without manual selection. Therefore, we propose a simple yet effective Annealing-based Label-Transfer framework, which sufficiently explores the known proposals to alleviate the uncertainties. Specifically, a Label-Transfer Learning paradigm is introduced to decouple the known and unknown features, while a Sawtooth Annealing Scheduling strategy is further employed to rebuild the decision boundaries of the known and unknown classes, thus promoting both known and unknown recognition. Moreover, previous OWOD works neglected the trade-off of known and unknown performance, and we thus introduce a metric called Equilibrium Index to comprehensively evaluate the effectiveness of the OWOD models. To the best of our knowledge, this is the first OWOD work without manual unknown selection. Extensive experiments conducted on the common-used benchmark validate that our model achieves superior detection performance (200% unknown mAP improvement with the even higher known detection performance) compared to other state-of-the-art methods. Our code is available at https://github.com/DIG-Beihang/ALLOW.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1991.DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting</span><br>
                <span class="as">Tyagi, AayushKumarandMohapatra, ChiragandDas, PrasenjitandMakharia, GovindandMehra, LalitaandAP, PrathoshandMausam</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tyagi_DeGPR_Deep_Guided_Posterior_Regularization_for_Multi-Class_Cell_Detection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23913-23923.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地检测和计数医学图像中的细胞，特别是在数据有限、对象重叠、多种细胞类型、严重的类别不平衡以及细胞大小/形状差异微小等情况下。<br>
                    动机：手动计数既繁琐又可能导致病理学家之间的观察者差异。现有的深度学习基础的对象检测和计数方法可能无法直接应用于医学图像中的细胞检测和计数。<br>
                    方法：提出引导后正则化DeGPR，通过指导对象检测器利用细胞之间的判别特征来辅助其进行检测和计数。这些特征可以由病理学家提供，也可以直接从视觉数据中推断出来。<br>
                    效果：在两个公开可用的数据集（CoNSeP和MoNuSAC）以及我们贡献的新数据集MuCeD上进行验证。MuCeD包含55张人类十二指肠活检图像，用于预测腹腔疾病。我们在三个数据集上与三种对象检测基线进行了大量实验，结果显示DeGPR是模型无关的，并持续提高基线，获得了高达9%（绝对）的mAP增益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-class cell detection and counting is an essential task for many pathological diagnoses. Manual counting is tedious and often leads to inter-observer variations among pathologists. While there exist multiple, general-purpose, deep learning-based object detection and counting methods, they may not readily transfer to detecting and counting cells in medical images, due to the limited data, presence of tiny overlapping objects, multiple cell types, severe class-imbalance, minute differences in size/shape of cells, etc. In response, we propose guided posterior regularization DeGPR, which assists an object detector by guiding it to exploit discriminative features among cells. The features may be pathologist-provided or inferred directly from visual data. We validate our model on two publicly available datasets (CoNSeP and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists of 55 biopsy images of the human duodenum for predicting celiac disease. We perform extensive experimentation with three object detection baselines on three datasets to show that DeGPR is model-agnostic, and consistently improves baselines obtaining up to 9% (absolute) mAP gains.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1992.itKD: Interchange Transfer-Based Knowledge Distillation for 3D Object Detection</span><br>
                <span class="as">Cho, HyeonandChoi, JunyongandBaek, GeonwooandHwang, Wonjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_itKD_Interchange_Transfer-Based_Knowledge_Distillation_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13540-13549.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前，大多数基于点云的3D物体检测器的研究仅关注网络架构的开发以提高准确性，而没有考虑计算效率。<br>
                    动机：本文提出了一种自动编码器风格的框架，通过基于交换传输的知识蒸馏进行通道压缩和解压缩，以解决上述问题。<br>
                    方法：我们首先使用共享的自动编码器独立地传递教师和学生网络的特征来学习教师网络的地图视图特征；然后，我们使用一种压缩表示损失来绑定来自学生和教师网络的通道压缩知识作为某种正则化。解压缩后的特征以相反的方向转移以减小交换重建的差距。最后，我们提出头部注意力损失来匹配由多头自我注意机制提取的3D物体检测信息。<br>
                    效果：通过大量实验，我们验证了该方法可以训练出与3D点云检测任务紧密对齐的轻量级模型，并在著名的公开数据集（如Waymo和nuScenes）上展示了其优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Point-cloud based 3D object detectors recently have achieved remarkable progress. However, most studies are limited to the development of network architectures for improving only their accuracy without consideration of the computational efficiency. In this paper, we first propose an autoencoder-style framework comprising channel-wise compression and decompression via interchange transfer-based knowledge distillation. To learn the map-view feature of a teacher network, the features from teacher and student networks are independently passed through the shared autoencoder; here, we use a compressed representation loss that binds the channel-wised compression knowledge from both student and teacher networks as a kind of regularization. The decompressed features are transferred in opposite directions to reduce the gap in the interchange reconstructions. Lastly, we present an head attention loss to match the 3D object detection information drawn by the multi-head self-attention mechanism. Through extensive experiments, we verify that our method can train the lightweight model that is well-aligned with the 3D point cloud detection task and we demonstrate its superiority using the well-known public datasets; e.g., Waymo and nuScenes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1993.2PCNet: Two-Phase Consistency Training for Day-to-Night Unsupervised Domain Adaptive Object Detection</span><br>
                <span class="as">Kennerley, MikhailandWang, Jian-GangandVeeravalli, BharadwajandTan, RobbyT.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kennerley_2PCNet_Two-Phase_Consistency_Training_for_Day-to-Night_Unsupervised_Domain_Adaptive_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11484-11493.png><br>
            
            <span class="tt"><span class="t0">研究问题：夜间目标检测由于缺乏夜间图像标注，是一个具有挑战性的问题。<br>
                    动机：尽管存在多种领域适应方法，但在高精度结果上仍存在问题，特别是在小尺度和低光照物体上的错误传播。<br>
                    方法：本文提出了一种两阶段一致性无监督领域适应网络2PCNet，通过在第一阶段使用教师的高置信边界框预测并将其附加到学生的区域建议中，然后在第二阶段让教师重新评估，生成高、低置信度伪标签的组合。同时，为了解决低光照区域和其他夜间相关属性引起的错误，我们提出了一个名为NightAug的夜间特定增强管道。<br>
                    效果：实验结果表明，该方法在公开数据集上的表现优于最先进的方法20%，并且比直接在目标数据上训练的有监督模型表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object detection at night is a challenging problem due to the absence of night image annotations. Despite several domain adaptation methods, achieving high-precision results remains an issue. False-positive error propagation is still observed in methods using the well-established student-teacher framework, particularly for small-scale and low-light objects. This paper proposes a two-phase consistency unsupervised domain adaptation network, 2PCNet, to address these issues. The network employs high-confidence bounding-box predictions from the teacher in the first phase and appends them to the student's region proposals for the teacher to re-evaluate in the second phase, resulting in a combination of high and low confidence pseudo-labels. The night images and pseudo-labels are scaled-down before being used as input to the student, providing stronger small-scale pseudo-labels. To address errors that arise from low-light regions and other night-related attributes in images, we propose a night-specific augmentation pipeline called NightAug. This pipeline involves applying random augmentations, such as glare, blur, and noise, to daytime images. Experiments on publicly available datasets demonstrate that our method achieves superior results to state-of-the-art methods by 20%, and to supervised models trained directly on the target data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1994.Generating Features With Increased Crop-Related Diversity for Few-Shot Object Detection</span><br>
                <span class="as">Xu, JingyiandLe, HieuandSamaras, Dimitris</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Generating_Features_With_Increased_Crop-Related_Diversity_for_Few-Shot_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19713-19722.png><br>
            
            <span class="tt"><span class="t0">研究问题：两阶段物体检测器在图像中生成物体提议并对其进行分类以检测物体，但这些提议并不完美地包含物体，而是以许多可能的方式与物体重叠，表现出提议难度水平的极大变化性。<br>
                    动机：训练一个对抗这种裁剪相关变化的鲁棒分类器需要大量的训练数据，但在少数镜头设置中这是不可用的。为了解决这个问题，我们提出了一种新的基于变分自动编码器（VAE）的数据生成模型，该模型能够生成具有增加的裁剪相关多样性的数据。<br>
                    方法：我们的主要思想是转换潜在空间，使得具有不同范数的潜在代码代表不同的裁剪相关变化。这使得我们可以通过简单地改变潜在范数来生成具有增加的裁剪相关难度水平的特征。具体来说，每个潜在代码都被重新缩放，使其范数与输入裁剪相对于地面真值框的IoU分数线性相关。在这里，IoU分数是一个代表裁剪难度水平的代理。我们在基本类别上训练这个VAE模型，然后使用训练好的模型为新类别生成特征。<br>
                    效果：我们的实验结果表明，我们生成的特征在PASCAL VOC和MS COCO数据集上始终改进了最先进的少数镜头物体检测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Two-stage object detectors generate object proposals and classify them to detect objects in images. These proposals often do not perfectly contain the objects but overlap with them in many possible ways, exhibiting great variability in the difficulty levels of the proposals. Training a robust classifier against this crop-related variability requires abundant training data, which is not available in few-shot settings. To mitigate this issue, we propose a novel variational autoencoder (VAE) based data generation model, which is capable of generating data with increased crop-related diversity. The main idea is to transform the latent space such the latent codes with different norms represent different crop-related variations. This allows us to generate features with increased crop-related diversity in difficulty levels by simply varying the latent norm. In particular, each latent code is rescaled such that its norm linearly correlates with the IoU score of the input crop w.r.t. the ground-truth box. Here the IoU score is a proxy that represents the difficulty level of the crop. We train this VAE model on base classes conditioned on the semantic code of each class and then use the trained model to generate features for novel classes. Our experimental results show that our generated features consistently improve state-of-the-art few-shot object detection methods on PASCAL VOC and MS COCO datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1995.The Devil Is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation</span><br>
                <span class="as">Kim, BeomyoungandJeong, JoonhyunandHan, DongyoonandHwang, SungJu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_The_Devil_Is_in_the_Points_Weakly_Semi-Supervised_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11360-11370.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在预算有限的情况下，通过弱监督学习实现高性能的实例分割。<br>
                    动机：现有的半监督学习方法主要面临的挑战是误报和漏报实例提议之间的权衡，而弱监督学习可以有效利用经济实惠的点标签作为强大的弱监督源来解决这个挑战。<br>
                    方法：提出了一种名为弱半监督实例分割（WSSIS）的新型学习方案，该方案考虑了一个由少量全标记图像和大量点标记图像组成的数据集设置。同时，为了处理全标记数据非常有限这一困难情况，还提出了一种MaskRefineNet来精炼粗糙掩码中的噪声。<br>
                    效果：在COCO和BDD100K数据集上进行了广泛的实验，所提出的方法即使在只有50%的全标定COCO数据（38.8% vs. 39.7%）的情况下，也能取得与全监督模型相当的结果。当只使用5%的全标定COCO数据时，该方法的性能明显优于最先进的半监督学习方法（33.7% vs. 24.9%）。代码可在https://github.com/clovaai/PointWSSIS获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we introduce a novel learning scheme named weakly semi-supervised instance segmentation (WSSIS) with point labels for budget-efficient and high-performance instance segmentation. Namely, we consider a dataset setting consisting of a few fully-labeled images and a lot of point-labeled images. Motivated by the main challenge of semi-supervised approaches mainly derives from the trade-off between false-negative and false-positive instance proposals, we propose a method for WSSIS that can effectively leverage the budget-friendly point labels as a powerful weak supervision source to resolve the challenge. Furthermore, to deal with the hard case where the amount of fully-labeled data is extremely limited, we propose a MaskRefineNet that refines noise in rough masks. We conduct extensive experiments on COCO and BDD100K datasets, and the proposed method achieves promising results comparable to those of the fully-supervised model, even with 50% of the fully labeled COCO data (38.8% vs. 39.7%). Moreover, when using as little as 5% of fully labeled COCO data, our method shows significantly superior performance over the state-of-the-art semi-supervised learning method (33.7% vs. 24.9%). The code is available at https://github.com/clovaai/PointWSSIS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1996.DynaMask: Dynamic Mask Selection for Instance Segmentation</span><br>
                <span class="as">Li, RuihuangandHe, ChenhangandLi, ShuaiandZhang, YabinandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynaMask_Dynamic_Mask_Selection_for_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11279-11288.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对不同物体实例进行分割，同时解决低分辨率掩膜丢失丰富细节和高分辨率掩膜导致二次计算开销的问题。<br>
                    动机：目前的代表性实例分割方法大多使用固定分辨率的掩膜（如28x 28网格）来分割不同的物体实例，但这种方法在保留细节和控制计算开销上存在困难。<br>
                    方法：本文提出了一种动态选择适合不同物体建议的掩膜的方法。首先，开发了一种具有自适应特征聚合的双级特征金字塔网络（FPN），以逐渐增加掩膜网格的分辨率，确保高质量地分割对象。其次，为了缓解使用大掩膜导致的计算和内存成本的增加，设计了一种计算开销极小的掩膜切换模块（MSM），为每个实例选择最合适的掩膜分辨率，实现了高效率的同时保持了高精度的分割。<br>
                    效果：该方法被称为DynaMask，无需复杂的操作，就能在其他最先进的方法中实现一致且显著的性能提升，同时只需要适度的计算开销。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The representative instance segmentation methods mostly segment different object instances with a mask of the fixed resolution, e.g., 28x 28 grid. However, a low-resolution mask loses rich details, while a high-resolution mask incurs quadratic computation overhead. It is a challenging task to predict the optimal binary mask for each instance. In this paper, we propose to dynamically select suitable masks for different object proposals. First, a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation is developed to gradually increase the mask grid resolution, ensuring high-quality segmentation of objects. Specifically, an efficient region-level top-down path (r-FPN) is introduced to incorporate complementary contextual and detailed information from different stages of image-level FPN (i-FPN). Then, to alleviate the increase of computation and memory costs caused by using large masks, we develop a Mask Switch Module (MSM) with negligible computational cost to select the most suitable mask resolution for each instance, achieving high efficiency while maintaining high segmentation accuracy. Without bells and whistles, the proposed method, namely DynaMask, brings consistent and noticeable performance improvements over other state-of-the-arts at a moderate computation overhead. The source code: https://github.com/lslrh/DynaMask.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1997.SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency</span><br>
                <span class="as">Liu, YangandZhang, YaoandWang, YixinandZhang, YangandTian, JiangandShi, ZhongchaoandFan, JianpingandHe, Zhiqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15539-15547.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高基于Transformer的目标检测器的收敛速度和性能。<br>
                    动机：目前的DETR方法通过使用中心概念的空间先验来加速Transformer检测器的收敛，但这种方法可能会降低查询的显著性并混淆检测器。<br>
                    方法：提出了一种名为SAP-DETR的新方法，将目标检测视为从显著点到实例对象的转换。在SAP-DETR中，为每个对象查询显式初始化一个特定于查询的参考点，逐步将其聚合成实例对象，然后预测边界框的每一侧到这些点的距离。<br>
                    效果：实验表明，SAP-DETR以显著的速度收敛，并在性能上具有竞争力。在标准训练方案下，SAP-DETR稳定地提升了最先进的方法1.0 AP。在ResNet-DC-101的基础上，SAP-DETR实现了46.9 AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, the dominant DETR-based approaches apply central-concept spatial prior to accelerating Transformer detector convergency. These methods gradually refine the reference points to the center of target objects and imbue object queries with the updated central reference information for spatially conditional attention. However, centralizing reference points may severely deteriorate queries' saliency and confuse detectors due to the indiscriminative spatial prior. To bridge the gap between the reference points of salient queries and Transformer detectors, we propose SAlient Point-based DETR (SAP-DETR) by treating object detection as a transformation from salient points to instance objects. In SAP-DETR, we explicitly initialize a query-specific reference point for each object query, gradually aggregate them into an instance object, and then predict the distance from each side of the bounding box to these points. By rapidly attending to query-specific reference region and other conditional extreme regions from the image features, SAP-DETR can effectively bridge the gap between the salient point and the query-based Transformer detector with a significant convergency speed. Our extensive experiments have demonstrated that SAP-DETR achieves 1.4 times convergency speed with competitive performance. Under the standard training scheme, SAP-DETR stably promotes the SOTA approaches by  1.0 AP. Based on ResNet-DC-101, SAP-DETR achieves 46.9 AP. The code will be released at https://github.com/liuyang-ict/SAP-DETR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1998.DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection</span><br>
                <span class="as">Zhang, XuanandLi, ShiyuandLi, XiandHuang, PingandShan, JiulongandChen, Ting</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DeSTSeg_Segmentation_Guided_Denoising_Student-Teacher_for_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3914-3923.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练教师网络、去噪学生编码器-解码器和分割网络，提高计算机视觉中的异常检测效果。<br>
                    动机：现有的基于学生-教师框架的异常检测方法仅对正常数据施加经验性约束并融合多级信息，效果有限。<br>
                    方法：提出一种改进的模型DeSTSeg，将预训练教师网络、去噪学生编码器-解码器和分割网络整合到一个框架中。首先引入去噪过程强化对异常数据的约束；其次通过合成异常掩模为分割网络提供丰富的监督，自适应地融合多级学生-教师特征。<br>
                    效果：在工业检测基准数据集上进行实验，该方法在图像级别AUC、像素级别平均精度和实例级别平均精度上分别达到98.6%、75.8%和76.4%，取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual anomaly detection, an important problem in computer vision, is usually formulated as a one-class classification and segmentation task. The student-teacher (S-T) framework has proved to be effective in solving this challenge. However, previous works based on S-T only empirically applied constraints on normal data and fused multi-level information. In this study, we propose an improved model called DeSTSeg, which integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into one framework. First, to strengthen the constraints on anomalous data, we introduce a denoising procedure that allows the student network to learn more robust representations. From synthetically corrupted normal images, we train the student network to match the teacher network feature of the same images without corruption. Second, to fuse the multi-level S-T features adaptively, we train a segmentation network with rich supervision from synthetic anomaly masks, achieving a substantial performance improvement. Experiments on the industrial inspection benchmark dataset demonstrate that our method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8% on pixel-level average precision, and 76.4% on instance-level average precision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">1999.PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification</span><br>
                <span class="as">Nauta, MeikeandSchl\&quot;otterer, J\&quot;organdvanKeulen, MauriceandSeifert, Christin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2744-2753.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于原型的图像解释模型无法很好地符合人类的视觉感知，同一原型可能对应现实世界中的不同概念，使得解释不够直观。<br>
                    动机：为了解决这个问题，我们提出了PIP-Net（基于补丁的直观原型网络），这是一种可解释的图像分类模型，它以自监督的方式学习与人类视觉更相关的原型部分。<br>
                    方法：PIP-Net可以被解释为一个稀疏评分表，图像中原型部分的存在为某一类添加证据。对于超出分布范围的数据，模型可以拒绝做出决策。<br>
                    效果：我们的原型与真实物体部分相关联，这表明PIP-Net缩小了潜在空间和像素空间之间的“语义差距”。因此，我们的PIP-Net及其可解释的原型使用户能够以一种直观、忠实且具有语义意义的方式理解决策过程。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Interpretable methods based on prototypical patches recognize various components in an image in order to explain their reasoning to humans. However, existing prototype-based methods can learn prototypes that are not in line with human visual perception, i.e., the same prototype can refer to different concepts in the real world, making interpretation not intuitive. Driven by the principle of explainability-by-design, we introduce PIP-Net (Patch-based Intuitive Prototypes Network): an interpretable image classification model that learns prototypical parts in a self-supervised fashion which correlate better with human vision. PIP-Net can be interpreted as a sparse scoring sheet where the presence of a prototypical part in an image adds evidence for a class. The model can also abstain from a decision for out-of-distribution data by saying "I haven't seen this before". We only use image-level labels and do not rely on any part annotations. PIP-Net is globally interpretable since the set of learned prototypes shows the entire reasoning of the model. A smaller local explanation locates the relevant prototypes in one image. We show that our prototypes correlate with ground-truth object parts, indicating that PIP-Net closes the "semantic gap" between latent space and pixel space. Hence, our PIP-Net with interpretable prototypes enables users to interpret the decision making process in an intuitive, faithful and semantically meaningful way. Code is available at https://github.com/M-Nauta/PIPNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2000.PROB: Probabilistic Objectness for Open World Object Detection</span><br>
                <span class="as">Zohar, OrrandWang, Kuan-ChiehandYeung, Serena</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zohar_PROB_Probabilistic_Objectness_for_Open_World_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11444-11453.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在开放世界中进行物体检测，特别是在未知物体的检测上。<br>
                    动机：传统的物体检测方法无法有效处理开放世界中的未知物体，因为未知物体和背景的区分缺乏监督信息。<br>
                    方法：提出了一种新的概率框架进行物体性估计，通过交替进行概率分布估计和已知物体在嵌入特征空间的对象性似然最大化，从而估计不同提案的物体性概率。<br>
                    效果：在开放世界物体检测基准测试中，该方法在未知物体检测和已知物体检测上都超过了所有现有的开放世界物体检测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open World Object Detection (OWOD) is a new and challenging computer vision task that bridges the gap between classic object detection (OD) benchmarks and object detection in the real world. In addition to detecting and classifying seen/labeled objects, OWOD algorithms are expected to detect novel/unknown objects - which can be classified and incrementally learned. In standard OD, object proposals not overlapping with a labeled object are automatically classified as background. Therefore, simply applying OD methods to OWOD fails as unknown objects would be predicted as background. The challenge of detecting unknown objects stems from the lack of supervision in distinguishing unknown objects and background object proposals. Previous OWOD methods have attempted to overcome this issue by generating supervision using pseudo-labeling - however, unknown object detection has remained low. Probabilistic/generative models may provide a solution for this challenge. Herein, we introduce a novel probabilistic framework for objectness estimation, where we alternate between probability distribution estimation and objectness likelihood maximization of known objects in the embedded feature space - ultimately allowing us to estimate the objectness probability of different proposals. The resulting Probabilistic Objectness transformer-based open-world detector, PROB, integrates our framework into traditional object detection models, adapting them for the open-world setting. Comprehensive experiments on OWOD benchmarks show that PROB outperforms all existing OWOD methods in both unknown object detection ( 2x unknown recall) and known object detection (  mAP). Our code is available at https://github.com/orrzohar/PROB.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2001.AUNet: Learning Relations Between Action Units for Face Forgery Detection</span><br>
                <span class="as">Bai, WeimingandLiu, YufanandZhang, ZhipengandLi, BingandHu, Weiming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_AUNet_Learning_Relations_Between_Action_Units_for_Face_Forgery_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24709-24719.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于人脸篡改技术引发的严重安全问题，人脸伪造检测变得越来越重要。<br>
                    动机：尽管现有的深度学习方法在训练和测试来自同一领域的人脸伪造品时取得了良好的效果，但当试图将检测器推广到训练期间未见过的伪造方法时，问题仍然具有挑战性。<br>
                    方法：我们提出了一个动作单元关系学习框架来提高伪造检测的通用性。具体来说，它包括动作单元关系转换器（ART）和篡改的动作单元预测（TAP）。ART通过与特定分支互补并共同工作的动作单元无关分支来构建不同动作单元之间的关系，以利用伪造线索。在篡改的动作单元预测中，我们在图像级别篡改与动作单元相关的区域，并在特征级别开发具有挑战性的伪样本。然后，模型被训练以预测篡改的动作单元区域，并生成特定位置的监督信息。<br>
                    效果：实验结果表明，我们的方法在数据集内和跨数据集评估中都能达到最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Face forgery detection becomes increasingly crucial due to the serious security issues caused by face manipulation techniques. Recent studies in deepfake detection have yielded promising results when the training and testing face forgeries are from the same domain. However, the problem remains challenging when one tries to generalize the detector to forgeries created by unseen methods during training. Observing that face manipulation may alter the relation between different facial action units (AU), we propose the Action Units Relation Learning framework to improve the generality of forgery detection. In specific, it consists of the Action Units Relation Transformer (ART) and the Tampered AU Prediction (TAP). The ART constructs the relation between different AUs with AU-agnostic Branch and AU-specific Branch, which complement each other and work together to exploit forgery clues. In the Tampered AU Prediction, we tamper AU-related regions at the image level and develop challenging pseudo samples at the feature level. The model is then trained to predict the tampered AU regions with the generated location-specific supervision. Experimental results demonstrate that our method can achieve state-of-the-art performance in both the in-dataset and cross-dataset evaluations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2002.PolyFormer: Referring Image Segmentation As Sequential Polygon Generation</span><br>
                <span class="as">Liu, JiangandDing, HuiandCai, ZhaoweiandZhang, YutingandSatzoda, RaviKumarandMahadevan, VijayandManmatha, R.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18653-18663.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像分割问题，通过将问题转化为序列多边形生成，并利用新的序列到序列框架Polygon Transformer进行预测。<br>
                    动机：直接预测像素级的分割掩模在准确性和效率上存在问题，因此提出将图像分割问题转化为序列多边形生成的新方法。<br>
                    方法：提出了一种新的序列到序列框架Polygon Transformer，它接受一系列图像补丁和文本查询令牌作为输入，并自动回归输出一系列多边形顶点。为了更准确的几何定位，还提出了一种基于回归的解码器，可以直接预测精确的浮点坐标，无需任何坐标量化误差。<br>
                    效果：实验结果表明，PolyFormer在挑战性的RefCOCO+和RefCOCOg数据集上比现有技术有明显优势，例如分别提高了5.40%和4.52%。在没有微调的情况下评估参考视频分割任务时，也显示出强大的泛化能力，例如在Ref-DAVIS17数据集上达到了具有竞争力的61.5% J&F。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, instead of directly predicting the pixel-level segmentation masks, the problem of referring image segmentation is formulated as sequential polygon generation, and the predicted polygons can be later converted into segmentation masks. This is enabled by a new sequence-to-sequence framework, Polygon Transformer (PolyFormer), which takes a sequence of image patches and text query tokens as input, and outputs a sequence of polygon vertices autoregressively. For more accurate geometric localization, we propose a regression-based decoder, which predicts the precise floating-point coordinates directly, without any coordinate quantization error. In the experiments, PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52% absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It also shows strong generalization ability when evaluated on the referring video segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&F on the Ref-DAVIS17 dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2003.Interactive Segmentation As Gaussion Process Classification</span><br>
                <span class="as">Zhou, MinghaoandWang, HongandZhao, QianandLi, YuexiangandHuang, YawenandMeng, DeyuandZheng, Yefeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Interactive_Segmentation_As_Gaussion_Process_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19488-19497.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决基于深度学习的点击式交互分割（IS）任务中，现有方法无法充分明确利用和传播点击信息的问题。<br>
                    动机：尽管现有的深度学习方法在点击式交互分割任务上取得了不错的效果，但它们并未完全明确地利用并传播点击信息，导致分割结果不尽人意，甚至在点击点上也是如此。<br>
                    方法：本文将点击式交互分割任务表述为每个图像上的高斯过程（GP）像素级二元分类模型。为了解决这个问题，我们使用变分推断对难以处理的高斯过程后验进行近似，并以数据驱动的方式解算该模型，然后将近似的高斯过程后验解耦为双空间形式，以实现线性复杂度的高效采样。然后，我们构建了一个名为GPCIS的高斯过程分类框架，该框架集成了深度核学习机制，以提高灵活性。<br>
                    效果：通过在几个基准测试集上进行全面实验，并与代表性方法进行定量和定性比较，证明了GPCIS的优点以及其良好的通用性和高效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Click-based interactive segmentation (IS) aims to extract the target objects under user interaction. For this task, most of the current deep learning (DL)-based methods mainly follow the general pipelines of semantic segmentation. Albeit achieving promising performance, they do not fully and explicitly utilize and propagate the click information, inevitably leading to unsatisfactory segmentation results, even at clicked points. Against this issue, in this paper, we propose to formulate the IS task as a Gaussian process (GP)-based pixel-wise binary classification model on each image. To solve this model, we utilize amortized variational inference to approximate the intractable GP posterior in a data-driven manner and then decouple the approximated GP posterior into double space forms for efficient sampling with linear complexity. Then, we correspondingly construct a GP classification framework, named GPCIS, which is integrated with the deep kernel learning mechanism for more flexibility. The main specificities of the proposed GPCIS lie in: 1) Under the explicit guidance of the derived GP posterior, the information contained in clicks can be finely propagated to the entire image and then boost the segmentation; 2) The accuracy of predictions at clicks has good theoretical support. These merits of GPCIS as well as its good generality and high efficiency are substantiated by comprehensive experiments on several benchmarks, as compared with representative methods both quantitatively and qualitatively. Codes will be released at https://github.com/zmhhmz/GPCIS_CVPR2023.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2004.Efficient Mask Correction for Click-Based Interactive Image Segmentation</span><br>
                <span class="as">Du, FeiandYuan, JianlongandWang, ZhibinandWang, Fan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Efficient_Mask_Correction_for_Click-Based_Interactive_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22773-22782.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过点击操作有效地进行交互式图像分割。<br>
                    动机：现有的点击式交互图像分割方法在每次点击后都需要运行整个分割网络，效率低下。<br>
                    方法：提出一种有效的方法，使用轻量级的掩模修正网络来修正掩模。同时，引入了点击引导的自注意力模块和点击引导的相关模块，以有效利用点击信息提高性能。<br>
                    效果：新方法在性能和效率上都优于现有方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The goal of click-based interactive image segmentation is to extract target masks with the input of positive/negative clicks. Every time a new click is placed, existing methods run the whole segmentation network to obtain a corrected mask, which is inefficient since several clicks may be needed to reach satisfactory accuracy. To this end, we propose an efficient method to correct the mask with a lightweight mask correction network. The whole network remains a low computational cost from the second click, even if we have a large backbone. However, a simple correction network with limited capacity is not likely to achieve comparable performance with a classic segmentation network. Thus, we propose a click-guided self-attention module and a click-guided correlation module to effectively exploits the click information to boost performance. First, several templates are selected based on the semantic similarity with click features. Then the self-attention module propagates the template information to other pixels, while the correlation module directly uses the templates to obtain target outlines. With the efficient architecture and two click-guided modules, our method shows preferable performance and efficiency compared to existing methods. The code will be released at https://github.com/feiaxyt/EMC-Click.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2005.Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection</span><br>
                <span class="as">Liu, ChangandZhang, WeimingandLin, XiangruandZhang, WeiandTan, XiaoandHan, JunyuandLi, XiaomaoandDing, ErruiandWang, Jingdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Ambiguity-Resistant_Semi-Supervised_Learning_for_Dense_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15579-15588.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基础半监督目标检测技术中，单阶段检测器相比于双阶段检测器提升有限。<br>
                    动机：实验发现这主要源于两种类型的模糊性：选择模糊性和分配模糊性。<br>
                    方法：为解决这些问题，提出了一种抗模糊半监督学习（ARSL）方法。具体来说，为了减轻选择模糊性，提出了联合置信度估计（JCE）来共同量化伪标签的分类和定位质量；对于分配模糊性，引入了任务分离分配（TSA），基于像素级预测而不是不可靠的伪框进行标签分配。<br>
                    效果：综合实验证明，ARSL有效地缓解了模糊性，并在MS COCO和PASCAL VOC上实现了最先进的半监督目标检测性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With basic Semi-Supervised Object Detection (SSOD) techniques, one-stage detectors generally obtain limited promotions compared with two-stage clusters. We experimentally find that the root lies in two kinds of ambiguities: (1) Selection ambiguity that selected pseudo labels are less accurate, since classification scores cannot properly represent the localization quality. (2) Assignment ambiguity that samples are matched with improper labels in pseudo-label assignment, as the strategy is misguided by missed objects and inaccurate pseudo boxes. To tackle these problems, we propose a Ambiguity-Resistant Semi-supervised Learning (ARSL) for one-stage detectors. Specifically, to alleviate the selection ambiguity, Joint-Confidence Estimation (JCE) is proposed to jointly quantifies the classification and localization quality of pseudo labels. As for the assignment ambiguity, Task-Separation Assignment (TSA) is introduced to assign labels based on pixel-level predictions rather than unreliable pseudo boxes. It employs a 'divide-and-conquer' strategy and separately exploits positives for the classification and localization task, which is more robust to the assignment ambiguity. Comprehensive experiments demonstrate that ARSL effectively mitigates the ambiguities and achieves state-of-the-art SSOD performance on MS COCO and PASCAL VOC. Codes can be found at https://github.com/PaddlePaddle/PaddleDetection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2006.SFD2: Semantic-Guided Feature Detection and Description</span><br>
                <span class="as">Xue, FeiandBudvytis, IgnasandCipolla, Roberto</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_SFD2_Semantic-Guided_Feature_Detection_and_Description_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5206-5216.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉定位的效率和准确性，特别是在大规模环境中的具有挑战性的条件下。<br>
                    动机：现有的方法主要依赖于提取大量常常是冗余的局部可靠特征，这在效率和准确性上存在限制，尤其是在大规模环境下的具有挑战性的条件下。<br>
                    方法：我们提出通过将高层语义隐式嵌入检测和描述过程中来提取全局可靠的特征。具体来说，我们的语义感知检测器能够从可靠区域（如建筑物、车道）中检测关键点，并隐式地抑制可靠区域（如天空、汽车），而不是依赖于显式的语义标签。<br>
                    效果：实验结果表明，我们的模型在长期大规模的视觉定位Aachen Day-Night和RobotCar-Seasons数据集上优于先前的局部特征，并在使用2k和4k关键点时分别比先进的匹配器快约2倍和3倍，同时保持了有竞争力的准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual localization is a fundamental task for various applications including autonomous driving and robotics. Prior methods focus on extracting large amounts of often redundant locally reliable features, resulting in limited efficiency and accuracy, especially in large-scale environments under challenging conditions. Instead, we propose to extract globally reliable features by implicitly embedding high-level semantics into both the detection and description processes. Specifically, our semantic-aware detector is able to detect keypoints from reliable regions (e.g. building, traffic lane) and suppress reliable areas (e.g. sky, car) implicitly instead of relying on explicit semantic labels. This boosts the accuracy of keypoint matching by reducing the number of features sensitive to appearance changes and avoiding the need of additional segmentation networks at test time. Moreover, our descriptors are augmented with semantics and have stronger discriminative ability, providing more inliers at test time. Particularly, experiments on long-term large-scale visual localization Aachen Day-Night and RobotCar-Seasons datasets demonstrate that our model outperforms previous local features and gives competitive accuracy to advanced matchers but is about 2 and 3 times faster when using 2k and 4k keypoints, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2007.Semi-Supervised Stereo-Based 3D Object Detection via Cross-View Consensus</span><br>
                <span class="as">Wu, WenhaoandWong, HauSanandWu, Si</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Stereo-Based_3D_Object_Detection_via_Cross-View_Consensus_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17471-17481.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用有限的标注数据和大量的未标注数据，实现基于立体视觉的三维物体检测。<br>
                    动机：虽然基于立体视觉的三维物体检测在低成本部署上具有巨大潜力，但其出色的性能需要高质量的手动标注，这在现实中很难实现。<br>
                    方法：提出一种通过从时间聚合教师模型生成伪标注来实现半监督学习的方案，该教师模型会从学生模型中累积知识。同时引入了跨视图一致性约束策略和交叉视图一致策略来提高深度估计的稳定性和准确性，减少伪标注噪声。<br>
                    效果：在KITTI 3D数据集上的大量实验表明，该方法能够有效地利用大量的未标注立体图像，显著提高检测效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Stereo-based 3D object detection, which aims at detecting 3D objects with stereo cameras, shows great potential in low-cost deployment compared to LiDAR-based methods and excellent performance compared to monocular-based algorithms. However, the impressive performance of stereo-based 3D object detection is at the huge cost of high-quality manual annotations, which are hardly attainable for any given scene. Semi-supervised learning, in which limited annotated data and numerous unannotated data are required to achieve a satisfactory model, is a promising method to address the problem of data deficiency. In this work, we propose to achieve semi-supervised learning for stereo-based 3D object detection through pseudo annotation generation from a temporal-aggregated teacher model, which temporally accumulates knowledge from a student model. To facilitate a more stable and accurate depth estimation, we introduce Temporal-Aggregation-Guided (TAG) disparity consistency, a cross-view disparity consistency constraint between the teacher model and the student model for robust and improved depth estimation. To mitigate noise in pseudo annotation generation, we propose a cross-view agreement strategy, in which pseudo annotations should attain high degree of agreements between 3D and 2D views, as well as between binocular views. We perform extensive experiments on the KITTI 3D dataset to demonstrate our proposed method's capability in leveraging a huge amount of unannotated stereo images to attain significantly improved detection results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2008.SCPNet: Semantic Scene Completion on Point Cloud</span><br>
                <span class="as">Xia, ZhaoyangandLiu, YouquanandLi, XinandZhu, XingeandMa, YuexinandLi, YikangandHou, YuenanandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_SCPNet_Semantic_Scene_Completion_on_Point_Cloud_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17642-17651.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练深度模型进行语义场景补全面临挑战，如输入稀疏和不完整、大量不同尺度的对象以及移动对象的固有标签噪声。<br>
                    动机：为了解决上述问题，我们提出了三个解决方案：1）重新设计补全网络；2）从多帧模型中提炼丰富的知识；3）补全标签矫正。<br>
                    方法：我们设计了一个新颖的补全网络，由多个多路径块组成，用于聚合多尺度特征，并且没有损失降采样操作。我们还设计了一种名为密集到稀疏的知识蒸馏（DSKD）的新型知识蒸馏目标，将密集的、基于关系的场景语义知识从多帧教师模型转移到单帧学生模型，显著提高了单帧模型的表示学习能力。此外，我们还提出了一种简单而有效的标签矫正策略，使用现成的全景分割标签来消除补全标签中的动态对象痕迹，大大提高了深度模型的性能，特别是对于移动对象。<br>
                    效果：我们在两个公共语义场景补全基准测试集SemanticKITTI和SemanticPOSS上进行了广泛的实验。我们的SCPNet在SemanticKITTI语义场景补全挑战中排名第一，并比竞争性S3CNet高出7.2 mIoU。SCPNet还在SemanticPOSS数据集上优于以前的补全算法。此外，我们的方法在SemanticKITTI语义分割任务上也取得了有竞争力的结果，表明在场景补全中学到的知识对分割任务有益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training deep models for semantic scene completion is challenging due to the sparse and incomplete input, a large quantity of objects of diverse scales as well as the inherent label noise for moving objects. To address the above-mentioned problems, we propose the following three solutions: 1) Redesigning the completion network. We design a novel completion network, which consists of several Multi-Path Blocks (MPBs) to aggregate multi-scale features and is free from the lossy downsampling operations. 2) Distilling rich knowledge from the multi-frame model. We design a novel knowledge distillation objective, dubbed Dense-to-Sparse Knowledge Distillation (DSKD). It transfers the dense, relation-based semantic knowledge from the multi-frame teacher to the single-frame student, significantly improving the representation learning of the single-frame model. 3) Completion label rectification. We propose a simple yet effective label rectification strategy, which uses off-the-shelf panoptic segmentation labels to remove the traces of dynamic objects in completion labels, greatly improving the performance of deep models especially for those moving objects. Extensive experiments are conducted in two public semantic scene completion benchmarks, i.e., SemanticKITTI and SemanticPOSS. Our SCPNet ranks 1st on SemanticKITTI semantic scene completion challenge and surpasses the competitive S3CNet by 7.2 mIoU. SCPNet also outperforms previous completion algorithms on the SemanticPOSS dataset. Besides, our method also achieves competitive results on SemanticKITTI semantic segmentation tasks, showing that knowledge learned in the scene completion is beneficial to the segmentation task.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2009.Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection</span><br>
                <span class="as">Song, XiaolinandChen, BinghuiandLi, PengyuandHe, Jun-YanandWang, BiaoandGeng, YifengandXie, XuansongandZhang, Honggang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Optimal_Proposal_Learning_for_Deployable_End-to-End_Pedestrian_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3250-3260.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个端到端的行人检测模型，以消除非最大抑制（NMS）后处理。<br>
                    动机：尽管已经探索了一些方法，但大多数方法仍然存在训练时间长、部署复杂等问题，无法在实际工业应用中部署。<br>
                    方法：提出了一种优化提案学习（OPL）框架，用于可部署的端到端行人检测。具体来说，我们使用基于CNN的轻量级探测器，并引入了两个新的模块，包括一个粗到精（C2F）的学习策略，用于通过减少训练/测试中样本分配/输出的模糊性，为真值（GT）实例提出精确的正提案；以及一个完整的提案网络（CPN），用于产生额外的信息补偿，以进一步召回困难的行人样本。<br>
                    效果：在CrowdHuman、TJU-Ped和Caltech等数据集上进行了广泛的实验，结果表明，我们提出的OPL方法显著优于竞争方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>End-to-end pedestrian detection focuses on training a pedestrian detection model via discarding the Non-Maximum Suppression (NMS) post-processing. Though a few methods have been explored, most of them still suffer from longer training time and more complex deployment, which cannot be deployed in the actual industrial applications. In this paper, we intend to bridge this gap and propose an Optimal Proposal Learning (OPL) framework for deployable end-to-end pedestrian detection. Specifically, we achieve this goal by using CNN-based light detector and introducing two novel modules, including a Coarse-to-Fine (C2F) learning strategy for proposing precise positive proposals for the Ground-Truth (GT) instances by reducing the ambiguity of sample assignment/output in training/testing respectively, and a Completed Proposal Network (CPN) for producing extra information compensation to further recall the hard pedestrian samples. Extensive experiments are conducted on CrowdHuman, TJU-Ped and Caltech, and the results show that our proposed OPL method significantly outperforms the competing methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2010.Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly Supervised Video Anomaly Detection</span><br>
                <span class="as">Zhang, ChenandLi, GuorongandQi, YuankaiandWang, ShuhuiandQing, LaiyunandHuang, QingmingandYang, Ming-Hsuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploiting_Completeness_and_Uncertainty_of_Pseudo_Labels_for_Weakly_Supervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16271-16280.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督视频异常检测旨在仅使用视频级别标签识别视频中的异常事件。<br>
                    动机：最近，两阶段自我训练方法通过自我生成伪标签并用这些标签自我精炼异常分数取得了显著改进。由于伪标签起着关键作用，我们提出了一种利用完整性和不确定性属性进行有效自我训练的增强框架。<br>
                    方法：我们首先设计了一个多头部分类模块（每个头都作为一个分类器），并带有一个多样性损失，以最大化预测的伪标签在各个头部之间的分布差异。这鼓励生成的伪标签覆盖尽可能多的异常事件。然后，我们设计了一种迭代的不确定性伪标签细化策略，不仅改善初始的伪标签，也改善第二阶段所需分类器获得的更新伪标签。<br>
                    效果：大量的实验结果表明，所提出的方法在UCF-Crime、TAD和XD-Violence基准数据集上的表现优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly supervised video anomaly detection aims to identify abnormal events in videos using only video-level labels. Recently, two-stage self-training methods have achieved significant improvements by self-generating pseudo labels and self-refining anomaly scores with these labels. As the pseudo labels play a crucial role, we propose an enhancement framework by exploiting completeness and uncertainty properties for effective self-training. Specifically, we first design a multi-head classification module (each head serves as a classifier) with a diversity loss to maximize the distribution differences of predicted pseudo labels across heads. This encourages the generated pseudo labels to cover as many abnormal events as possible. We then devise an iterative uncertainty pseudo label refinement strategy, which improves not only the initial pseudo labels but also the updated ones obtained by the desired classifier in the second stage. Extensive experimental results demonstrate the proposed method performs favorably against state-of-the-art approaches on the UCF-Crime, TAD, and XD-Violence benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2011.Full or Weak Annotations? An Adaptive Strategy for Budget-Constrained Annotation Campaigns</span><br>
                <span class="as">Tejero, JavierGamazoandZinkernagel, MartinS.andWolf, SebastianandSznitman, RaphaelandM\&#x27;arquez-Neila, Pablo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tejero_Full_or_Weak_Annotations_An_Adaptive_Strategy_for_Budget-Constrained_Annotation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11381-11391.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何为新的机器学习任务分配标注预算，特别是在图像分割应用中。<br>
                    动机：手动标注相关图像内容既昂贵又耗时，而且需要专业知识。尽管弱监督学习和迁移学习的发展使分割模型可以从各种类型的标注中受益，但对于任何新的领域应用，数据集构建者仍然需要定义一个策略来分配完全的分割和其他弱标注。<br>
                    方法：我们提出了一种新的方法来确定分割数据集的标注策略，即在给定固定预算的情况下，估计应收集多少比例的分割和分类标注。为此，我们的方法通过模拟最终分割模型的预期改进来顺序确定预算分数的分割和分类标注的比例。<br>
                    效果：我们的实验表明，对于许多不同的标注预算和数据集，我们的方法产生的标注性能非常接近最优。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Annotating new datasets for machine learning tasks is tedious, time-consuming, and costly. For segmentation applications, the burden is particularly high as manual delineations of relevant image content are often extremely expensive or can only be done by experts with domain-specific knowledge. Thanks to developments in transfer learning and training with weak supervision, segmentation models can now also greatly benefit from annotations of different kinds. However, for any new domain application looking to use weak supervision, the dataset builder still needs to define a strategy to distribute full segmentation and other weak annotations. Doing so is challenging, however, as it is a priori unknown how to distribute an annotation budget for a given new dataset. To this end, we propose a novel approach to determine annotation strategies for segmentation datasets, whereby estimating what proportion of segmentation and classification annotations should be collected given a fixed budget. To do so, our method sequentially determines proportions of segmentation and classification annotations to collect for budget-fractions by modeling the expected improvement of the final segmentation model. We show in our experiments that our approach yields annotations that perform very close to the optimal for a number of different annotation budgets and datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2012.Leveraging Hidden Positives for Unsupervised Semantic Segmentation</span><br>
                <span class="as">Seong, HyunSeokandMoon, WonJunandLee, SuBeenandHeo, Jae-Pil</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seong_Leveraging_Hidden_Positives_for_Unsupervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19540-19549.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高无监督语义分割的性能，并保证任务特定训练指导和局部语义一致性。<br>
                    动机：虽然使用视觉转换器（ViT）主干的最新工作表现出色，但仍缺乏对任务特定训练指导和局部语义一致性的考虑。<br>
                    方法：通过挖掘隐藏的正例进行对比学习，学习丰富的语义关系，确保局部区域的语义一致性。具体包括发现两种类型的全局隐藏正例，基于预训练的主干和训练中的分割头定义的特征相似性，分别为每个锚点发现任务无关和任务特定的隐藏正例。逐渐增加后者的贡献，使模型捕获任务特定的语义特征。此外，引入梯度传播策略，学习相邻补丁之间的语义一致性。<br>
                    效果：在COCO-stuff、Cityscapes和Potsdam-3数据集上，该方法实现了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Dramatic demand for manpower to label pixel-level annotations triggered the advent of unsupervised semantic segmentation. Although the recent work employing the vision transformer (ViT) backbone shows exceptional performance, there is still a lack of consideration for task-specific training guidance and local semantic consistency. To tackle these issues, we leverage contrastive learning by excavating hidden positives to learn rich semantic relationships and ensure semantic consistency in local regions. Specifically, we first discover two types of global hidden positives, task-agnostic and task-specific ones for each anchor based on the feature similarities defined by a fixed pre-trained backbone and a segmentation head-in-training, respectively. A gradual increase in the contribution of the latter induces the model to capture task-specific semantic features. In addition, we introduce a gradient propagation strategy to learn semantic consistency between adjacent patches, under the inherent premise that nearby patches are highly likely to possess the same semantics. Specifically, we add the loss propagating to local hidden positives, semantically similar nearby patches, in proportion to the predefined similarity scores. With these training schemes, our proposed method achieves new state-of-the-art (SOTA) results in COCO-stuff, Cityscapes, and Potsdam-3 datasets. Our code is available at: https://github.com/hynnsk/HP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2013.ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation</span><br>
                <span class="as">Boulch, AlexandreandSautier, CorentinandMichele, Bj\&quot;ornandPuy, GillesandMarlet, Renaud</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boulch_ALSO_Automotive_Lidar_Self-Supervision_by_Occupancy_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13455-13465.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种新的自我监督方法，用于预训练处理点云的深度感知模型。<br>
                    动机：目前的预训练方法大多需要大量标注数据，而本文提出的新方法可以在无标注的情况下学习有用的表示。<br>
                    方法：通过在重建采样3D点的表面的预训练任务上训练模型，并将底层潜在向量作为感知头输入，来预训练深度感知模型。<br>
                    效果：实验结果表明，该方法在各种自动驾驶数据集上，对于语义分割和对象检测任务，都能有效地学习有用的表示，并且优于现有的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a new self-supervised method for pre-training the backbone of deep perception models operating on point clouds. The core idea is to train the model on a pretext task which is the reconstruction of the surface on which the 3D points are sampled, and to use the underlying latent vectors as input to the perception head. The intuition is that if the network is able to reconstruct the scene surface, given only sparse input points, then it probably also captures some fragments of semantic information, that can be used to boost an actual perception task. This principle has a very simple formulation, which makes it both easy to implement and widely applicable to a large range of 3D sensors and deep networks performing semantic segmentation or object detection. In fact, it supports a single-stream pipeline, as opposed to most contrastive learning approaches, allowing training on limited resources. We conducted extensive experiments on various autonomous driving datasets, involving very different kinds of lidars, for both semantic segmentation and object detection. The results show the effectiveness of our method to learn useful representations without any annotation, compared to existing approaches. The code is available at github.com/valeoai/ALSO</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2014.Object Detection With Self-Supervised Scene Adaptation</span><br>
                <span class="as">Zhang, ZekunandHoai, Minh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Object_Detection_With_Self-Supervised_Scene_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21589-21599.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高在固定相机视角下的场景中已训练目标检测器的性能。<br>
                    动机：在固定相机视角的场景中，现有的目标检测器性能有待提高。<br>
                    方法：提出一种自我监督适应的方法，通过交叉教学的方式，使用检测器自身生成的伪真标签和物体跟踪器进行适应训练。同时，利用背景等变特性进行无伪影对象混合作为数据增强手段，并利用准确的背景提取作为额外的输入模态。<br>
                    效果：实验结果表明，该方法可以提高原始检测器的平均精度，大幅超越先前最先进的自我监督领域适应目标检测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper proposes a novel method to improve the performance of a trained object detector on scenes with fixed camera perspectives based on self-supervised adaptation. Given a specific scene, the trained detector is adapted using pseudo-ground truth labels generated by the detector itself and an object tracker in a cross-teaching manner. When the camera perspective is fixed, our method can utilize the background equivariance by proposing artifact-free object mixup as a means of data augmentation, and utilize accurate background extraction as an additional input modality. We also introduce a large-scale and diverse dataset for the development and evaluation of scene-adaptive object detection. Experiments on this dataset show that our method can improve the average precision of the original detector, outperforming the previous state-of-the-art self-supervised domain adaptive object detection methods by a large margin. Our dataset and code are published at https://github.com/cvlab-stonybrook/scenes100.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2015.DeepLSD: Line Segment Detection and Refinement With Deep Image Gradients</span><br>
                <span class="as">Pautrat, R\&#x27;emiandBarath, DanielandLarsson, ViktorandOswald, MartinR.andPollefeys, Marc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pautrat_DeepLSD_Line_Segment_Detection_and_Refinement_With_Deep_Image_Gradients_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17327-17336.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高线段检测器的准确性和鲁棒性，使其能在没有真实地面线条的情况下进行训练？<br>
                    动机：传统的基于图像梯度的线段检测器快速准确，但在噪声图像和挑战性条件下缺乏鲁棒性。而学习型的线段检测器虽然更稳定，但准确性较低且偏向于线框线。<br>
                    方法：提出结合传统方法和学习型方法的新线段检测器DeepLSD，通过深度网络生成线吸引力场，然后转换为替代图像梯度大小和角度，输入到任何现有的手工制作的线段检测器中。此外，还提出了一种新的优化工具，根据吸引力场和消失点来细化线段。<br>
                    效果：在低级别的线检测指标以及多个具有挑战性的数据集上的下游任务上展示了该方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Line segments are ubiquitous in our human-made world and are increasingly used in vision tasks. They are complementary to feature points thanks to their spatial extent and the structural information they provide. Traditional line detectors based on the image gradient are extremely fast and accurate, but lack robustness in noisy images and challenging conditions. Their learned counterparts are more repeatable and can handle challenging images, but at the cost of a lower accuracy and a bias towards wireframe lines. We propose to combine traditional and learned approaches to get the best of both worlds: an accurate and robust line detector that can be trained in the wild without ground truth lines. Our new line segment detector, DeepLSD, processes images with a deep network to generate a line attraction field, before converting it to a surrogate image gradient magnitude and angle, which is then fed to any existing handcrafted line detector. Additionally, we propose a new optimization tool to refine line segments based on the attraction field and vanishing points. This refinement improves the accuracy of current deep detectors by a large margin. We demonstrate the performance of our method on low-level line detection metrics, as well as on several downstream tasks using multiple challenging datasets. The source code and models are available at https://github.com/cvg/DeepLSD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2016.Learning Common Rationale To Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems</span><br>
                <span class="as">Shu, YangyangandvandenHengel, AntonandLiu, Lingqiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shu_Learning_Common_Rationale_To_Improve_Self-Supervised_Representation_for_Fine-Grained_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11392-11401.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自监督学习方法在细粒度视觉识别（FGVR）任务中表现不佳，因为其优化目标并不适用于捕捉FGVR中的微妙差异。<br>
                    动机：为了解决这个问题，我们提出了一种学习额外筛选机制的方法，通过识别跨实例和类别的常见线索，即公共理性，来改善FGVR的表现。<br>
                    方法：我们利用SSL目标产生的GradCAM，无需使用预训练的对象部分或显著性检测器，就可以学习到公共理性检测器。具体来说，我们在GradCAM上添加一个具有有限拟合能力的分支，使其能够捕获公共理性并丢弃较少见的判别模式。<br>
                    效果：在四个视觉任务上的大量实验结果表明，该方法可以在不同的评估设置中带来显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning (SSL) strategies have demonstrated remarkable performance in various recognition tasks. However, both our preliminary investigation and recent studies suggest that they may be less effective in learning representations for fine-grained visual recognition (FGVR) since many features helpful for optimizing SSL objectives are not suitable for characterizing the subtle differences in FGVR. To overcome this issue, we propose learning an additional screening mechanism to identify discriminative clues commonly seen across instances and classes, dubbed as common rationales in this paper. Intuitively, common rationales tend to correspond to the discriminative patterns from the key parts of foreground objects. We show that a common rationale detector can be learned by simply exploiting the GradCAM induced from the SSL objective without using any pre-trained object parts or saliency detectors, making it seamlessly to be integrated with the existing SSL process. Specifically, we fit the GradCAM with a branch with limited fitting capacity, which allows the branch to capture the common rationales and discard the less common discriminative patterns. At the test stage, the branch generates a set of spatial weights to selectively aggregate features representing an instance. Extensive experimental results on four visual tasks demonstrate that the proposed method can lead to a significant improvement in different evaluation settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2017.Co-Salient Object Detection With Uncertainty-Aware Group Exchange-Masking</span><br>
                <span class="as">Wu, YangandSong, HuihuiandLiu, BoandZhang, KaihuaandLiu, Dong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Co-Salient_Object_Detection_With_Uncertainty-Aware_Group_Exchange-Masking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19639-19648.png><br>
            
            <span class="tt"><span class="t0">研究问题：传统的共同显著物体检测（CoSOD）任务是分割一组相关图像中的常见显著物体，但现有的CoSOD模型在测试图像组中存在无关图像的情况下，模型的鲁棒性存在缺陷，这阻碍了CoSOD模型在现实世界的应用。<br>
                    动机：为了解决这个问题，本文提出了一种用于学习鲁棒CoSOD模型的群体交换屏蔽（GEM）策略。<br>
                    方法：通过两个包含不同类型的显著物体的图像组作为输入，GEM首先通过提出基于学习的策略从每个组中选择一组图像，然后将这些图像进行交换。提出的特征提取模块考虑了剩余相关图像中由无关图像引起的不确定性和群体一致性。设计了一个由条件变分自动编码器生成的潜变量生成器分支，以生成基于不确定性的全局随机特征。设计了一个CoSOD转换器分支来捕获包含群体一致性信息的相关性局部特征。最后，将两个分支的输出连接起来并输入到一个基于变压器的解码器，产生鲁棒的共同显著性预测。<br>
                    效果：通过对有无关图像的共同显著性检测进行广泛的评估，证明了该方法优于各种最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The traditional definition of co-salient object detection (CoSOD) task is to segment the common salient objects in a group of relevant images. Existing CoSOD models by default adopt the group consensus assumption. This brings about model robustness defect under the condition of irrelevant images in the testing image group, which hinders the use of CoSOD models in real-world applications. To address this issue, this paper presents a group exchange-masking (GEM) strategy for robust CoSOD model learning. With two group of image containing different types of salient object as input, the GEM first selects a set of images from each group by the proposed learning based strategy, then these images are exchanged. The proposed feature extraction module considers both the uncertainty caused by the irrelevant images and group consensus in the remaining relevant images. We design a latent variable generator branch which is made of conditional variational autoencoder to generate uncertainly-based global stochastic features. A CoSOD transformer branch is devised to capture the correlation-based local features that contain the group consistency information. At last, the output of two branches are concatenated and fed into a transformer-based decoder, producing robust co-saliency prediction. Extensive evaluations on co-saliency detection with and without irrelevant images demonstrate the superiority of our method over a variety of state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2018.Extracting Class Activation Maps From Non-Discriminative Features As Well</span><br>
                <span class="as">Chen, ZhaozhengandSun, Qianru</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Extracting_Class_Activation_Maps_From_Non-Discriminative_Features_As_Well_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3135-3144.png><br>
            
            <span class="tt"><span class="t0">研究问题：从分类模型中提取类激活映射（CAM）时，通常会导致前景物体的覆盖范围不佳，即只能识别出判别区域（如“羊头”），其余部分（如“羊腿”）误认为是背景。<br>
                    动机：这个问题的关键在于分类器（用于计算CAM）的权重仅捕获了对象的判别特征。我们通过引入一种新的CAM计算方法来解决这个问题，该方法明确地捕获非判别特征，从而扩大CAM以覆盖整个物体。<br>
                    方法：我们省略分类模型的最后一层池化层，并对一个对象类的每个局部特征进行聚类，其中“局部”意味着“在空间像素位置”。我们将结果K个聚类中心称为局部原型——代表像“羊头”、“羊腿”和“羊身”这样的局部语义。对于该类别的一张新图像，我们将其未池化的特征与每个原型进行比较，得出K个相似性矩阵，然后将它们聚合成热力图（即我们的CAM）。因此，我们的CAM捕获了该类别的所有局部特征，没有歧视。<br>
                    效果：我们在具有挑战性的弱监督语义分割（WSSS）任务中评估了这种方法，并将其插入到多种最先进的WSSS方法中，如MCTformer和AMN，只需用我们的CAM替换它们的原始CAM即可。我们在标准的WSSS基准测试（PASCAL VOC和MS COCO）上进行了大量实验，结果显示我们的方法具有优越性：持续改进且计算开销小。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Extracting class activation maps (CAM) from a classification model often results in poor coverage on foreground objects, i.e., only the discriminative region (e.g., the "head" of "sheep") is recognized and the rest (e.g., the "leg" of "sheep") mistakenly as background. The crux behind is that the weight of the classifier (used to compute CAM) captures only the discriminative features of objects. We tackle this by introducing a new computation method for CAM that explicitly captures non-discriminative features as well, thereby expanding CAM to cover whole objects. Specifically, we omit the last pooling layer of the classification model, and perform clustering on all local features of an object class, where "local" means "at a spatial pixel position". We call the resultant K cluster centers local prototypes - represent local semantics like the "head", "leg", and "body" of "sheep". Given a new image of the class, we compare its unpooled features to every prototype, derive K similarity matrices, and then aggregate them into a heatmap (i.e., our CAM). Our CAM thus captures all local features of the class without discrimination. We evaluate it in the challenging tasks of weakly-supervised semantic segmentation (WSSS), and plug it in multiple state-of-the-art WSSS methods, such as MCTformer and AMN, by simply replacing their original CAM with ours. Our extensive experiments on standard WSSS benchmarks (PASCAL VOC and MS COCO) show the superiority of our method: consistent improvements with little computational overhead.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2019.Towards Professional Level Crowd Annotation of Expert Domain Data</span><br>
                <span class="as">Wang, PeiandVasconcelos, Nuno</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Towards_Professional_Level_Crowd_Annotation_of_Expert_Domain_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3166-3175.png><br>
            
            <span class="tt"><span class="t0">研究问题：专家领域的图像识别通常需要精细标注，但成本高昂，限制了数据集大小和学习系统的准确性。<br>
                    动机：为了解决这个问题，我们考虑使用众包来注释专家数据，提出了一种基于半监督学习和人类过滤的新方法。<br>
                    方法：我们提出了一种人机协作的半监督学习方法（SSL-HF），通过众包工人作为伪标签的过滤器，取代了最先进的半监督学习方法中使用的不可靠的置信度阈值。<br>
                    效果：实验表明，在几个基准测试中，SSL-HF显著优于各种替代方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image recognition on expert domains is usually fine-grained and requires expert labeling, which is costly. This limits dataset sizes and the accuracy of learning systems. To address this challenge, we consider annotating expert data with crowdsourcing. This is denoted as PrOfeSsional lEvel cRowd (POSER) annotation. A new approach, based on semi-supervised learning (SSL) and denoted as SSL with human filtering (SSL-HF) is proposed. It is a human-in-the-loop SSL method, where crowd-source workers act as filters of pseudo-labels, replacing the unreliable confidence thresholding used by state-of-the-art SSL methods. To enable annotation by non-experts, classes are specified implicitly, via positive and negative sets of examples and augmented with deliberative explanations, which highlight regions of class ambiguity. In this way, SSL-HF leverages the strong low-shot learning and confidence estimation ability of humans to create an intuitive but effective labeling experience. Experiments show that SSL-HF significantly outperforms various alternative approaches in several benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2020.Semi-Weakly Supervised Object Kinematic Motion Prediction</span><br>
                <span class="as">Liu, GengxinandSun, QianandHuang, HaibinandMa, ChongyangandGuo, YulanandYi, LiandHuang, HuiandHu, Ruizhen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Semi-Weakly_Supervised_Object_Kinematic_Motion_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21726-21735.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决3D物体运动预测问题，即识别可移动部分及其相应的运动参数。<br>
                    动机：由于3D物体在拓扑结构和几何细节上存在大量变化，以及缺乏大规模标记数据，使得基于深度学习的方法在此任务上面临挑战。<br>
                    方法：本文采用半弱监督的方式处理3D物体的运动预测问题。首先，利用现有的大规模语义分割数据集和对象部分分割方法；其次，通过图神经网络学习分层部分分割与可移动部分参数之间的映射关系，并基于几何对齐进行进一步优化。<br>
                    效果：实验结果表明，该方法在3D部分扫描的运动预测任务上取得了显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Given a 3D object, kinematic motion prediction aims to identify the mobile parts as well as the corresponding motion parameters. Due to the large variations in both topological structure and geometric details of 3D objects, this remains a challenging task and the lack of large scale labeled data also constrain the performance of deep learning based approaches. In this paper, we tackle the task of object kinematic motion prediction problem in a semi-weakly supervised manner. Our key observations are two-fold. First, although 3D dataset with fully annotated motion labels is limited, there are existing datasets and methods for object part semantic segmentation at large scale. Second, semantic part segmentation and mobile part segmentation is not always consistent but it is possible to detect the mobile parts from the underlying 3D structure. Towards this end, we propose a graph neural network to learn the map between hierarchical part-level segmentation and mobile parts parameters, which are further refined based on geometric alignment. This network can be first trained on PartNet-Mobility dataset with fully labeled mobility information and then applied on PartNet dataset with fine-grained and hierarchical part-level segmentation. The network predictions yield a large scale of 3D objects with pseudo labeled mobility information and can further be used for weakly-supervised learning with pre-existing segmentation. Our experiments show there are significant performance boosts with the augmented data for previous method designed for kinematic motion prediction on 3D partial scans.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2021.Improving Robustness of Semantic Segmentation to Motion-Blur Using Class-Centric Augmentation</span><br>
                <span class="as">AakankshaandRajagopalan, A.N.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Aakanksha_Improving_Robustness_of_Semantic_Segmentation_to_Motion-Blur_Using_Class-Centric_Augmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10470-10479.png><br>
            
            <span class="tt"><span class="t0">研究问题：语义分割在模糊图像中的性能提升。<br>
                    动机：现有的研究主要关注清晰图像的分割性能，对于模糊图像的处理较少。<br>
                    方法：提出一种基于类别的运动模糊增强（CCMBA）策略，通过使用分割图注释生成空间变化的模糊，使网络同时学习清晰图像、自我运动模糊和动态场景模糊的语义分割。<br>
                    效果：在PASCAL VOC和Cityscapes数据集上，该方法在CNN和Vision Transformer-based语义分割网络上均取得了良好的效果，并在常用的去模糊数据集GoPro和REDS上展示了对复杂真实世界模糊的改善泛化能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic segmentation involves classifying each pixel into one of a pre-defined set of object/stuff classes. Such a fine-grained detection and localization of objects in the scene is challenging by itself. The complexity increases manifold in the presence of blur. With cameras becoming increasingly light-weight and compact, blur caused by motion during capture time has become unavoidable. Most research has focused on improving segmentation performance for sharp clean images and the few works that deal with degradations, consider motion-blur as one of many generic degradations. In this work, we focus exclusively on motion-blur and attempt to achieve robustness for semantic segmentation in its presence. Based on the observation that segmentation annotations can be used to generate synthetic space-variant blur, we propose a Class-Centric Motion-Blur Augmentation (CCMBA) strategy. Our approach involves randomly selecting a subset of semantic classes present in the image and using the segmentation map annotations to blur only the corresponding regions. This enables the network to simultaneously learn semantic segmentation for clean images, images with egomotion blur, as well as images with dynamic scene blur. We demonstrate the effectiveness of our approach for both CNN and Vision Transformer-based semantic segmentation networks on PASCAL VOC and Cityscapes datasets. We also illustrate the improved generalizability of our method to complex real-world blur by evaluating on the commonly used deblurring datasets GoPro and REDS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2022.SMAE: Few-Shot Learning for HDR Deghosting With Saturation-Aware Masked Autoencoders</span><br>
                <span class="as">Yan, QingsenandZhang, SongandChen, WeiyeandTang, HaoandZhu, YuandSun, JinqiuandVanGool, LucandZhang, Yanning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_SMAE_Few-Shot_Learning_for_HDR_Deghosting_With_Saturation-Aware_Masked_Autoencoders_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5775-5784.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量数据生成高质量的高动态范围（HDR）图像。<br>
                    动机：大多数基于深度神经网络（DNNs）的方法需要大量带有真实标签的训练数据，这既繁琐又耗时。少数几篇研究尝试通过使用少量的训练数据来生成满意的HDR图像，但现代的DNN在只有几张图片的情况下很容易过拟合。<br>
                    方法：我们提出了一种新的半监督方法，称为SSHDR，通过两个阶段的培训来实现少数几篇HDR成像。首先，我们使用自我监督机制生成饱和区域的图像内容，然后通过迭代的半监督学习框架解决鬼影问题。<br>
                    效果：实验证明，SSHDR在各种数据集上的性能都优于最先进的方法，无论是定量还是定性，都能用少量的标记样本实现吸引人的HDR可视化。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating a high-quality High Dynamic Range (HDR) image from dynamic scenes has recently been extensively studied by exploiting Deep Neural Networks (DNNs). Most DNNs-based methods require a large amount of training data with ground truth, requiring tedious and time-consuming work. Few-shot HDR imaging aims to generate satisfactory images with limited data. However, it is difficult for modern DNNs to avoid overfitting when trained on only a few images. In this work, we propose a novel semi-supervised approach to realize few-shot HDR imaging via two stages of training, called SSHDR. Unlikely previous methods, directly recovering content and removing ghosts simultaneously, which is hard to achieve optimum, we first generate content of saturated regions with a self-supervised mechanism and then address ghosts via an iterative semi-supervised learning framework. Concretely, considering that saturated regions can be regarded as masking Low Dynamic Range (LDR) input regions, we design a Saturated Mask AutoEncoder (SMAE) to learn a robust feature representation and reconstruct a non-saturated HDR image. We also propose an adaptive pseudo-label selection strategy to pick high-quality HDR pseudo-labels in the second stage to avoid the effect of mislabeled samples. Experiments demonstrate that SSHDR outperforms state-of-the-art methods quantitatively and qualitatively within and across different datasets, achieving appealing HDR visualization with few labeled samples.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2023.Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor</span><br>
                <span class="as">Kweon, HyeokjunandYoon, Sung-HoonandYoon, Kuk-Jin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kweon_Weakly_Supervised_Semantic_Segmentation_via_Adversarial_Learning_of_Classifier_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11329-11339.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督语义分割中的类别激活图（CAMs）通常无法覆盖整个对象，并且会在无关区域被激活。<br>
                    动机：为了解决这个问题，我们提出了一种新的弱监督语义分割框架，通过对抗性学习分类器和图像重建器。<br>
                    方法：我们同时训练两个模型：一个生成类别激活图的分类器，将图像分解为段；另一个测量段之间的可推理性的重建器。如果一个段可以从其他段重建，那么这个段就是不精确的。<br>
                    效果：我们在广泛的消融研究中验证了该框架的优势。我们的方法在PASCAL VOC 2012和MS COCO 2014上都取得了新的最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In Weakly Supervised Semantic Segmentation (WSSS), Class Activation Maps (CAMs) usually 1) do not cover the whole object and 2) be activated on irrelevant regions. To address the issues, we propose a novel WSSS framework via adversarial learning of a classifier and an image reconstructor. When an image is perfectly decomposed into class-wise segments, information (i.e., color or texture) of a single segment could not be inferred from the other segments. Therefore, inferability between the segments can represent the preciseness of segmentation. We quantify the inferability as a reconstruction quality of one segment from the other segments. If one segment could be reconstructed from the others, then the segment would be imprecise. To bring this idea into WSSS, we simultaneously train two models: a classifier generating CAMs that decompose an image into segments and a reconstructor that measures the inferability between the segments. As in GANs, while being alternatively trained in an adversarial manner, two networks provide positive feedback to each other. We verify the superiority of the proposed framework with extensive ablation studies. Our method achieves new state-of-the-art performances on both PASCAL VOC 2012 and MS COCO 2014. The code is available at https://github.com/sangrockEG/ACR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2024.Foundation Model Drives Weakly Incremental Learning for Semantic Segmentation</span><br>
                <span class="as">Yu, ChaohuiandZhou, QiangandLi, JingliangandYuan, JianlongandWang, ZhibinandWang, Fan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Foundation_Model_Drives_Weakly_Incremental_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23685-23694.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用图像级别的标签来学习新的类别，同时避免忘记旧的类别。<br>
                    动机：现有的弱增量学习语义分割方法虽然取得了一些成果，但图像级别的标签无法提供足够的细节来定位每个分割区域，这限制了其性能。<br>
                    方法：提出了一种新颖且数据高效的弱增量学习语义分割框架（FMWIS），包括预训练基础模型生成密集伪标签的知识蒸馏、优化噪声伪掩模的教师-学生架构以及解决旧类别灾难性遗忘问题的基于记忆的复制粘贴增强等方法。<br>
                    效果：在Pascal VOC和COCO数据集上的大量实验表明，该框架的性能优越，例如，在15-5 VOC设置中，FMWIS达到了70.7%和73.3%的准确率，比最先进的方法高出3.4%和6.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern incremental learning for semantic segmentation methods usually learn new categories based on dense annotations. Although achieve promising results, pixel-by-pixel labeling is costly and time-consuming. Weakly incremental learning for semantic segmentation (WILSS) is a novel and attractive task, which aims at learning to segment new classes from cheap and widely available image-level labels. Despite the comparable results, the image-level labels can not provide details to locate each segment, which limits the performance of WILSS. This inspires us to think how to improve and effectively utilize the supervision of new classes given image-level labels while avoiding forgetting old ones. In this work, we propose a novel and data-efficient framework for WILSS, named FMWISS. Specifically, we propose pre-training based co-segmentation to distill the knowledge of complementary foundation models for generating dense pseudo labels. We further optimize the noisy pseudo masks with a teacher-student architecture, where a plug-in teacher is optimized with a proposed dense contrastive loss. Moreover, we introduce memory-based copy-paste augmentation to improve the catastrophic forgetting problem of old classes. Extensive experiments on Pascal VOC and COCO datasets demonstrate the superior performance of our framework, e.g., FMWISS achieves 70.7% and 73.3% in the 15-5 VOC setting, outperforming the state-of-the-art method by 3.4% and 6.1%, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2025.ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation</span><br>
                <span class="as">Li, KehanandWang, ZhennanandCheng, ZesenandYu, RunyiandZhao, YianandSong, GuoliandLiu, ChangandYuan, LiandChen, Jie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ACSeg_Adaptive_Conceptualization_for_Unsupervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7162-7172.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用预训练的视觉模型提取像素级语义关系，并有效地进行无监督语义分割。<br>
                    动机：预训练的视觉模型能够捕捉到像素级的语义关系，但如何将这些关系转化为语义一致的像素组或区域仍是一个挑战。<br>
                    方法：提出了一种自适应概念化方法（ACSeg），通过将概念编码为可学习的原型，并设计自适应概念生成器（ACG）来适应每张图像的信息概念。同时，考虑到不同图像的场景复杂性，提出了模块化损失函数，以优化ACG的概念数量。<br>
                    效果：实验结果表明，该方法在无监督语义分割任务上取得了优于现有技术的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, self-supervised large-scale visual pre-training models have shown great promise in representing pixel-level semantic relationships, significantly promoting the development of unsupervised dense prediction tasks, e.g., unsupervised semantic segmentation (USS). The extracted relationship among pixel-level representations typically contains rich class-aware information that semantically identical pixel embeddings in the representation space gather together to form sophisticated concepts. However, leveraging the learned models to ascertain semantically consistent pixel groups or regions in the image is non-trivial since over/ under-clustering overwhelms the conceptualization procedure under various semantic distributions of different images. In this work, we investigate the pixel-level semantic aggregation in self-supervised ViT pre-trained models as image Segmentation and propose the Adaptive Conceptualization approach for USS, termed ACSeg. Concretely, we explicitly encode concepts into learnable prototypes and design the Adaptive Concept Generator (ACG), which adaptively maps these prototypes to informative concepts for each image. Meanwhile, considering the scene complexity of different images, we propose the modularity loss to optimize ACG independent of the concept number based on estimating the intensity of pixel pairs belonging to the same concept. Finally, we turn the USS task into classifying the discovered concepts in an unsupervised manner. Extensive experiments with state-of-the-art results demonstrate the effectiveness of the proposed ACSeg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2026.Similarity Metric Learning for RGB-Infrared Group Re-Identification</span><br>
                <span class="as">Xiong, JianghaoandLai, Jianhuang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13662-13671.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决跨模态的群体重识别（G-ReID）问题，特别是RGB-红外（RGB-IR）的匹配问题。<br>
                    动机：现有的研究主要关注基于RGB的问题，而RGB-IR的跨模态匹配问题尚未得到研究。<br>
                    方法：提出了一种用于RGB-IR G-ReID的度量学习方法——最接近排列匹配（CPM）。该方法将每个群体建模为一组由MPANet提取的单人特征，然后提出最接近排列距离（CPD）来测量两组特征之间的相似性。CPD在群体成员的顺序变化下保持不变，从而解决了G-ReID中的布局变化问题。此外，还引入了无人员标签的G-ReID问题。在弱监督的情况下，设计了一种关系感知模块（RAM），该模块利用视觉上下文和群体成员之间的关系来生成每个群体中的特征顺序，以形成一种对抗模态变化的稳健的群体表示。<br>
                    效果：通过在新的大规模RGB-IR G-ReID数据集CM-Group上进行大量实验，证明了所提出模型的有效性和CM-Group的复杂性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Group re-identification (G-ReID) aims to re-identify a group of people that is observed from non-overlapping camera systems. The existing literature has mainly addressed RGB-based problems, but RGB-infrared (RGB-IR) cross-modality matching problem has not been studied yet. In this paper, we propose a metric learning method Closest Permutation Matching (CPM) for RGB-IR G-ReID. We model each group as a set of single-person features which are extracted by MPANet, then we propose the metric Closest Permutation Distance (CPD) to measure the similarity between two sets of features. CPD is invariant with order changes of group members so that it solves the layout change problem in G-ReID. Furthermore, we introduce the problem of G-ReID without person labels. In the weak-supervised case, we design the Relation-aware Module (RAM) that exploits visual context and relations among group members to produce a modality-invariant order of features in each group, with which group member features within a set can be sorted to form a robust group representation against modality change. To support the study on RGB-IR G-ReID, we construct a new large-scale RGB-IR G-ReID dataset CM-Group. The dataset contains 15,440 RGB images and 15,506 infrared images of 427 groups and 1,013 identities. Extensive experiments on the new dataset demonstrate the effectiveness of the proposed models and the complexity of CM-Group. The code and dataset are available at: https://github.com/WhollyOat/CM-Group.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2027.PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery</span><br>
                <span class="as">Zhang, ShengandKhan, SalmanandShen, ZhiqiangandNaseer, MuzammalandChen, GuangyiandKhan, FahadShahbaz</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PromptCAL_Contrastive_Affinity_Learning_via_Auxiliary_Prompts_for_Generalized_Novel_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3479-3488.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督学习模型在处理来自新语义类别的未标注数据时，由于其封闭集假设，大多无法进行有效学习。<br>
                    动机：针对这一挑战，我们提出了一种通用新颖类别发现（GNCD）方法，旨在利用部分标注已知类别的信息对来自已知和新颖类别的未标注训练数据进行分类。<br>
                    方法：我们提出了一种两阶段对比亲和性学习方法，并辅以辅助视觉提示，称为PromptCAL。该方法通过发现可靠的样本对亲和性来更好地对已知和新颖类别进行语义聚类。<br>
                    效果：实验表明，即使在标注有限的情况下，我们的PromptCAL方法也能更有效地发现新颖类别，并在整体准确性上超过了当前最先进的方法（例如，在CUB-200上提高了近11%，在ImageNet-100上提高了9%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although existing semi-supervised learning models achieve remarkable success in learning with unannotated in-distribution data, they mostly fail to learn on unlabeled data sampled from novel semantic classes due to their closed-set assumption. In this work, we target a pragmatic but under-explored Generalized Novel Category Discovery (GNCD) setting. The GNCD setting aims to categorize unlabeled training data coming from known and novel classes by leveraging the information of partially labeled known classes. We propose a two-stage Contrastive Affinity Learning method with auxiliary visual Prompts, dubbed PromptCAL, to address this challenging problem. Our approach discovers reliable pairwise sample affinities to learn better semantic clustering of both known and novel classes for the class token and visual prompts. First, we propose a discriminative prompt regularization loss to reinforce semantic discriminativeness of prompt-adapted pre-trained vision transformer for refined affinity relationships. Besides, we propose contrastive affinity learning to calibrate semantic representations based on our iterative semi-supervised affinity graph generation method for semantically-enhanced supervision. Extensive experimental evaluation demonstrates that our PromptCAL method is more effective in discovering novel classes even with limited annotations and surpasses the current state-of-the-art on generic and fine-grained benchmarks (e.g., with nearly 11% gain on CUB-200, and 9% on ImageNet-100) on overall accuracy. Our code will be released to the public.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2028.Camouflaged Instance Segmentation via Explicit De-Camouflaging</span><br>
                <span class="as">Luo, NaisongandPan, YuwenandSun, RuiandZhang, TianzhuandXiong, ZhiweiandWu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Camouflaged_Instance_Segmentation_via_Explicit_De-Camouflaging_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17918-17927.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决伪装物体实例分割的问题，即预测野生动物适应周围环境的外观的伪装物体的实例级掩膜。<br>
                    动机：以往的实例分割方法在处理这种任务时表现不佳，因为它们容易被欺骗性的伪装所干扰。<br>
                    方法：我们提出了一种新的去伪装网络（DCNet），包括一个像素级的伪装解耦模块和一个实例级的伪装抑制模块。<br>
                    效果：实验结果表明，我们的DCNet在两个基准测试中的表现优于最先进的CIS方法，例如在COD10K和NC4K数据集上的平均精度提高了5%以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Camouflaged Instance Segmentation (CIS) aims at predicting the instance-level masks of camouflaged objects, which are usually the animals in the wild adapting their appearance to match the surroundings. Previous instance segmentation methods perform poorly on this task as they are easily disturbed by the deceptive camouflage. To address these challenges, we propose a novel De-camouflaging Network (DCNet) including a pixel-level camouflage decoupling module and an instance-level camouflage suppression module. The proposed DCNet enjoys several merits. First, the pixel-level camouflage decoupling module can extract camouflage characteristics based on the Fourier transformation. Then a difference attention mechanism is proposed to eliminate the camouflage characteristics while reserving target object characteristics in the pixel feature. Second, the instance-level camouflage suppression module can aggregate rich instance information from pixels by use of instance prototypes. To mitigate the effect of background noise during segmentation, we introduce some reliable reference points to build a more robust similarity measurement. With the aid of these two modules, our DCNet can effectively model de-camouflaging and achieve accurate segmentation for camouflaged instances. Extensive experimental results on two benchmarks demonstrate that our DCNet performs favorably against state-of-the-art CIS methods, e.g., with more than 5% performance gains on COD10K and NC4K datasets in average precision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2029.Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning</span><br>
                <span class="as">Song, KaiyouandXie, JinandZhang, ShanandLuo, Zimeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Multi-Mode_Online_Knowledge_Distillation_for_Self-Supervised_Visual_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11848-11857.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高小型模型的视觉表示学习性能。<br>
                    动机：现有的自监督视觉表示学习方法通过结合知识蒸馏可以提升性能，但大多采用静态预训练教师向学生转移知识的方式。<br>
                    方法：提出一种多模式在线知识蒸馏方法（MOKD），让两个不同的模型以自监督的方式进行协作学习。具体包括自我蒸馏和交叉蒸馏两种模式，其中自我蒸馏让每个模型独立进行自监督学习，而交叉蒸馏实现不同模型之间的知识交互。在交叉蒸馏中，提出了一种跨注意力特征搜索策略来增强不同模型之间的语义特征对齐。<br>
                    效果：实验结果表明，两种异构模型可以通过MOKD受益并超越独立训练的基线。此外，MOKD也超越了现有的自监督知识蒸馏方法，无论对于学生模型还是教师模型都表现出更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning (SSL) has made remarkable progress in visual representation learning. Some studies combine SSL with knowledge distillation (SSL-KD) to boost the representation learning performance of small models. In this study, we propose a Multi-mode Online Knowledge Distillation method (MOKD) to boost self-supervised visual representation learning. Different from existing SSL-KD methods that transfer knowledge from a static pre-trained teacher to a student, in MOKD, two different models learn collaboratively in a self-supervised manner. Specifically, MOKD consists of two distillation modes: self-distillation and cross-distillation modes. Among them, self-distillation performs self-supervised learning for each model independently, while cross-distillation realizes knowledge interaction between different models. In cross-distillation, a cross-attention feature search strategy is proposed to enhance the semantic feature alignment between different models. As a result, the two models can absorb knowledge from each other to boost their representation learning performance. Extensive experimental results on different backbones and datasets demonstrate that two heterogeneous models can benefit from MOKD and outperform their independently trained baseline. In addition, MOKD also outperforms existing SSL-KD methods for both the student and teacher models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2030.ScarceNet: Animal Pose Estimation With Scarce Annotations</span><br>
                <span class="as">Li, ChenandLee, GimHee</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ScarceNet_Animal_Pose_Estimation_With_Scarce_Annotations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17174-17183.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决动物姿态估计任务中由于缺乏标注数据而未充分探索的问题。<br>
                    动机：现有方法在只有少量标记数据和未标记图像的情况下，无法有效处理动物姿态估计任务。<br>
                    方法：提出ScarceNet，一种基于伪标签的方法，为未标记的图像生成人工标签。通过使用小批量标记图像训练的模型生成的伪标签通常带有噪声，直接用于训练可能会影响性能。为此，首先采用小损失技巧选择可靠的伪标签，然后通过一致性约束和可重用样本再标记等方法充分利用未标记数据。<br>
                    效果：在具有挑战性的AP-10K数据集上评估该方法，其性能大幅超过现有的半监督学习方法。在只有很少标注可用的TigDog数据集上，该方法的性能也优于基于领域适应的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Animal pose estimation is an important but under-explored task due to the lack of labeled data. In this paper, we tackle the task of animal pose estimation with scarce annotations, where only a small set of labeled data and unlabeled images are available. At the core of the solution to this problem setting is the use of the unlabeled data to compensate for the lack of well-labeled animal pose data. To this end, we propose the ScarceNet, a pseudo label-based approach to generate artificial labels for the unlabeled images. The pseudo labels, which are generated with a model trained with the small set of labeled images, are generally noisy and can hurt the performance when directly used for training. To solve this problem, we first use a small-loss trick to select reliable pseudo labels. Although effective, the selection process is improvident since numerous high-loss samples are left unused. We further propose to identify reusable samples from the high-loss samples based on an agreement check. Pseudo labels are re-generated to provide supervision for those reusable samples. Lastly, we introduce a student-teacher framework to enforce a consistency constraint since there are still samples that are neither reliable nor reusable. By combining the reliable pseudo label selection with the reusable sample re-labeling and the consistency constraint, we can make full use of the unlabeled data. We evaluate our approach on the challenging AP-10K dataset, where our approach outperforms existing semi-supervised approaches by a large margin. We also test on the TigDog dataset, where our approach can achieve better performance than domain adaptation based approaches when only very few annotations are available. Our code is available at the project website.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2031.MIANet: Aggregating Unbiased Instance and General Information for Few-Shot Semantic Segmentation</span><br>
                <span class="as">Yang, YongandChen, QiongandFeng, YuanandHuang, Tianlin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_MIANet_Aggregating_Unbiased_Instance_and_General_Information_for_Few-Shot_Semantic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7131-7140.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的少数镜头分割方法基于元学习策略，从支持集中提取实例知识，然后将该知识应用于查询集中的目标对象分割。然而，由于从支持集中的少量样本中获取的知识无法应对类内差异的变化，因此提取的知识不足以应对这个问题。<br>
                    动机：为了解决这个问题，我们提出了一种多信息聚合网络（MIANet），它有效地利用了一般知识（即语义词嵌入）和实例信息进行精确分割。<br>
                    方法：在MIANet中，我们提出了一个通用信息模块（GIM）来从词嵌入中提取一个通用的类原型作为实例信息的补充。为此，我们设计了一个三元组损失，将通用类原型视为锚点，并在支持集中的局部特征中采样正负对。计算出的三元组损失可以将语言身份之间的语义相似性从词嵌入空间转移到视觉表示空间。<br>
                    效果：通过引入非参数分层先验模块（HPM）生成无偏的实例级信息，并通过计算支持和查询图像特征之间的像素级相似性，以减轻模型对训练类别的偏见并获取多尺度信息。最后，信息融合模块（IFM）将一般知识和实例信息结合起来，为查询图像进行预测。在PASCAL-5i和COCO-20i上的大量实验表明，MIANet具有优越的性能，并设定了新的最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing few-shot segmentation methods are based on the meta-learning strategy and extract instance knowledge from a support set and then apply the knowledge to segment target objects in a query set. However, the extracted knowledge is insufficient to cope with the variable intra-class differences since the knowledge is obtained from a few samples in the support set. To address the problem, we propose a multi-information aggregation network (MIANet) that effectively leverages the general knowledge, i.e., semantic word embeddings, and instance information for accurate segmentation. Specifically, in MIANet, a general information module (GIM) is proposed to extract a general class prototype from word embeddings as a supplement to instance information. To this end, we design a triplet loss that treats the general class prototype as an anchor and samples positive-negative pairs from local features in the support set. The calculated triplet loss can transfer semantic similarities among language identities from a word embedding space to a visual representation space. To alleviate the model biasing towards the seen training classes and to obtain multi-scale information, we then introduce a non-parametric hierarchical prior module (HPM) to generate unbiased instance-level information via calculating the pixel-level similarity between the support and query image features. Finally, an information fusion module (IFM) combines the general and instance information to make predictions for the query image. Extensive experiments on PASCAL-5i and COCO-20i show that MIANet yields superior performance and set a new state-of-the-art. Code is available at github.com/Aldrich2y/MIANet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2032.MarginMatch: Improving Semi-Supervised Learning with Pseudo-Margins</span><br>
                <span class="as">Sosea, TiberiuandCaragea, Cornelia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sosea_MarginMatch_Improving_Semi-Supervised_Learning_with_Pseudo-Margins_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15773-15782.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用未标记数据进行半监督学习，提高模型在低数据量情况下的表现。<br>
                    动机：现有的半监督学习方法主要依赖模型对未标记数据的预测置信度，但这种方法可能会因为模型的不稳定预测而导致效果不佳。<br>
                    方法：提出一种新的半监督学习方法MarginMatch，该方法结合了一致性正则化和伪标签技术，通过分析模型在训练过程中对伪标签样本的行为，确保模型预测的稳定性。<br>
                    效果：实验结果表明，MarginMatch在四个视觉基准测试中以及两个大规模数据集上都有显著的提升，尤其在数据量较少的情况下，如在每个类别只有25个样本的CIFAR-100上，错误率降低了3.25%；在每个类别只有4个样本的STL-10上，错误率降低了4.19%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce MarginMatch, a new SSL approach combining consistency regularization and pseudo-labeling, with its main novelty arising from the use of unlabeled data training dynamics to measure pseudo-label quality. Instead of using only the model's confidence on an unlabeled example at an arbitrary iteration to decide if the example should be masked or not, MarginMatch also analyzes the behavior of the model on the pseudo-labeled examples as the training progresses, ensuring low fluctuations in the model's predictions from one iteration to another. MarginMatch brings substantial improvements on four vision benchmarks in low data regimes and on two large-scale datasets, emphasizing the importance of enforcing high-quality pseudo-labels. Notably, we obtain an improvement in error rate over the state-of-the-art of 3.25% on CIFAR-100 with only 25 examples per class and of 4.19% on STL-10 using as few as 4 examples per class.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2033.ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector</span><br>
                <span class="as">Zhu, YichenandZhou, QiqiandLiu, NingandXu, ZhiyuanandOu, ZhicaiandMou, XiaofengandTang, Jian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19723-19733.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管通用目标检测取得了显著的成功，但小目标检测（SOD）的性能和效率仍然不尽人意。<br>
                    动机：现有的方法在推理速度和SOD性能之间难以取得平衡，因此本文提出了一种新的尺度感知知识蒸馏（ScaleKD）方法，将复杂教师模型的知识转移到紧凑的学生模型中。<br>
                    方法：设计了两个新的模块来提高SOD中知识转移的质量：1) 一个尺度解耦的特征蒸馏模块，将教师的特征表示分解为多尺度嵌入，使学生模型能够明确模仿小对象的特征；2) 一个跨尺度辅助模块，用于精炼学生模型的噪声和无信息边界框预测，这些预测可能会误导学生模型并损害知识蒸馏的效果。建立了一个多尺度跨注意力层，以捕获多尺度语义信息，从而提高学生模型的性能。<br>
                    效果：在COCO和VisDrone数据集上进行了实验，使用不同类型的模型（如两阶段和单阶段检测器），评估了提出的方法。结果表明，我们的ScaleKD在通用检测性能上表现优越，并在SOD性能方面取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite the prominent success of general object detection, the performance and efficiency of Small Object Detection (SOD) are still unsatisfactory. Unlike existing works that struggle to balance the trade-off between inference speed and SOD performance, in this paper, we propose a novel Scale-aware Knowledge Distillation (ScaleKD), which transfers knowledge of a complex teacher model to a compact student model. We design two novel modules to boost the quality of knowledge transfer in distillation for SOD: 1) a scale-decoupled feature distillation module that disentangled teacher's feature representation into multi-scale embedding that enables explicit feature mimicking of the student model on small objects. 2) a cross-scale assistant to refine the noisy and uninformative bounding boxes prediction student models, which can mislead the student model and impair the efficacy of knowledge distillation. A multi-scale cross-attention layer is established to capture the multi-scale semantic information to improve the student model. We conduct experiments on COCO and VisDrone datasets with diverse types of models, i.e., two-stage and one-stage detectors, to evaluate our proposed method. Our ScaleKD achieves superior performance on general detection performance and obtains spectacular improvement regarding the SOD performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2034.EFEM: Equivariant Neural Field Expectation Maximization for 3D Object Segmentation Without Scene Supervision</span><br>
                <span class="as">Lei, JiahuiandDeng, CongyueandSchmeckpeper, KarlandGuibas, LeonidasandDaniilidis, Kostas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_EFEM_Equivariant_Neural_Field_Expectation_Maximization_for_3D_Object_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4902-4912.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种无需标注或训练，即可在3D场景中分割对象的简单、有效且鲁棒的几何算法。<br>
                    动机：现有的分割方法需要大量的标注数据和复杂的训练过程，而我们的方法通过利用单个对象的形状先验知识，可以在没有标注或训练的情况下进行分割。<br>
                    方法：我们引入了等变形状表示来消除物体配置变化带来的复杂性，并提出了一种新的EM算法，该算法可以使用等变形状先验迭代地细化分割掩模。<br>
                    效果：我们在"Chairs and Mugs"数据集上进行了实验，结果表明，我们的方法在不同的场景中都能实现稳定且鲁棒的性能，而弱监督的方法可能会失败。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Equivariant Neural Field Expectation Maximization (EFEM), a simple, effective, and robust geometric algorithm that can segment objects in 3D scenes without annotations or training on scenes. We achieve such unsupervised segmentation by exploiting single object shape priors. We make two novel steps in that direction. First, we introduce equivariant shape representations to this problem to eliminate the complexity induced by the variation in object configuration. Second, we propose a novel EM algorithm that can iteratively refine segmentation masks using the equivariant shape prior. We collect a novel real dataset Chairs and Mugs that contains various object configurations and novel scenes in order to verify the effectiveness and robustness of our method. Experimental results demonstrate that our method achieves consistent and robust performance across different scenes where the (weakly) supervised methods may fail. Code and data available at https://www.cis.upenn.edu/ leijh/projects/efem</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2035.Learning To Detect and Segment for Open Vocabulary Object Detection</span><br>
                <span class="as">Wang, Tao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_To_Detect_and_Segment_for_Open_Vocabulary_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7051-7060.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用视觉语言预训练模型识别新的对象类别。<br>
                    动机：现有的开放词汇目标检测主要关注知识转移，但这种方法在处理新对象类别时效果不佳。<br>
                    方法：提出CondHead，一种动态网络设计，通过将网络头部条件参数化在语义嵌入上来更好地进行盒子回归和掩膜分割，从而更好地检测新的对象类别。<br>
                    效果：CondHead显著提高了开放词汇目标检测的性能，例如，在新的类别上超过了RegionClip模型3.0的检测AP，而计算开销仅增加了1.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open vocabulary object detection has been greately advanced by the recent development of vision-language pre-trained model, which helps recognizing the novel objects with only semantic categories. The prior works mainly focus on knowledge transferring to the object proposal classification and employ class-agnostic box and mask prediction. In this work, we propose CondHead, a principled dynamic network design to better generalize the box regression and mask segmentation for open vocabulary setting. The core idea is to conditionally parametrize the network heads on semantic embedding and thus the model is guided with class-specific knowledge to better detect novel categories. Specifically, CondHead is composed of two streams of network heads, the dynamically aggregated heads and dynamically generated heads. The former is instantiated with a set of static heads that are conditionally aggregated, these heads are optimized as experts and are expected to learn sophisticated prediction. The Latter is instantiated with dynamically generated parameters and encodes general class-specific information. With such conditional design, the detection model is bridged by the semantic embedding to offer strongly generalizable class-wise box and mask prediction. Our method brings significant improvement to the prior state-of-the-art open vocabulary object detection methods with very minor overhead, e.g., it surpasses a RegionClip model by 3.0 detection AP on novel categories, with only 1.1% more computation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2036.Box-Level Active Detection</span><br>
                <span class="as">Lyu, MengyaoandZhou, JundongandChen, HuiandHuang, YijieandYu, DongdongandLi, YaqianandGuo, YandongandGuo, YuchenandXiang, LiuyuandDing, Guiguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lyu_Box-Level_Active_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23766-23775.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用标注预算进行主动学习，以减少冗余标签并提高模型性能。<br>
                    动机：现有的主动检测基准在评估过程中存在图像级别的偏见和工作量估计不准确的问题。<br>
                    方法：提出了一种框级别主动检测框架，通过控制每个周期的框预算，优先选择信息量大的目标，避免冗余标签，实现公平比较和有效应用。同时设计了一种名为“互补伪主动策略”（ComPAS）的新流程，充分利用人类注释和模型智能。<br>
                    效果：在统一的代码库中，ComPAS在四种设置下均优于10个竞争对手。仅使用标注数据进行监督，它在VOC0712上实现了100%的监督性能，但只标注了19%的框。在COCO数据集上，它比第二名的方法提高了4.3%的mAP。此外，ComPAS还支持使用未标注的数据集进行训练，标注减少85%的情况下，其监督性能超过了90%的COCO。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Active learning selects informative samples for annotation within budget, which has proven efficient recently on object detection. However, the widely used active detection benchmarks conduct image-level evaluation, which is unrealistic in human workload estimation and biased towards crowded images. Furthermore, existing methods still perform image-level annotation, but equally scoring all targets within the same image incurs waste of budget and redundant labels. Having revealed above problems and limitations, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes informative targets and avoids redundancy for fair comparison and efficient application. Under the proposed box-level setting, we devise a novel pipeline, namely Complementary Pseudo Active Strategy (ComPAS). It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only; meantime well-learned targets are identified by the model and compensated with pseudo-labels. ComPAS consistently outperforms 10 competitors under 4 settings in a unified codebase. With supervision from labeled data only, it achieves 100% supervised performance of VOC0712 with merely 19% box annotations. On the COCO dataset, it yields up to 4.3% mAP improvement over the second-best method. ComPAS also supports training with the unlabeled pool, where it surpasses 90% COCO supervised performance with 85% label reduction. Our source code is publicly available at https://github.com/lyumengyao/blad.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2037.Generative Semantic Segmentation</span><br>
                <span class="as">Chen, JiaqiandLu, JiachenandZhu, XiatianandZhang, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Generative_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7111-7120.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种生成式语义分割（GSS）方法，将语义分割转化为图像条件掩膜生成问题。<br>
                    动机：传统的像素判别学习方法在语义分割任务中存在局限性，因此作者提出了一种新的潜在先验学习过程来替代。<br>
                    方法：通过替换传统的像素判别学习方法为潜在先验学习过程，将语义分割转化为图像条件掩膜生成问题。具体来说，我们模型化了给定分割掩膜的潜在变量的后验分布。为了实现对给定图像的语义分割，我们还引入了一个条件网络。<br>
                    效果：大量的实验表明，我们的GSS在标准的语义分割设置中可以与现有技术相媲美，同时在更具挑战性的跨领域设置中实现了新的最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Generative Semantic Segmentation (GSS), a generative learning approach for semantic segmentation. Uniquely, we cast semantic segmentation as an image-conditioned mask generation problem. This is achieved by replacing the conventional per-pixel discriminative learning with a latent prior learning process. Specifically, we model the variational posterior distribution of latent variables given the segmentation mask. To that end, the segmentation mask is expressed with a special type of image (dubbed as maskige). This posterior distribution allows to generate segmentation masks unconditionally. To achieve semantic segmentation on a given image, we further introduce a conditioning network. It is optimized by minimizing the divergence between the posterior distribution of maskige (i.e., segmentation masks) and the latent prior distribution of input training images. Extensive experiments on standard benchmarks show that our GSS can perform competitively to prior art alternatives in the standard semantic segmentation setting, whilst achieving a new state of the art in the more challenging cross-domain setting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2038.SDC-UDA: Volumetric Unsupervised Domain Adaptation Framework for Slice-Direction Continuous Cross-Modality Medical Image Segmentation</span><br>
                <span class="as">Shin, HyungseobandKim, HyeongyuandKim, SewonandJun, YohanandEo, TaejoonandHwang, Dosik</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_SDC-UDA_Volumetric_Unsupervised_Domain_Adaptation_Framework_for_Slice-Direction_Continuous_Cross-Modality_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7412-7421.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用无监督领域适应（UDA）在医学图像分割中实现跨模态的切片方向连续分割。<br>
                    动机：获取像素级专家注释在医学成像领域非常昂贵和费力，而现有的UDA方法无法保证切片方向的连续性。<br>
                    方法：提出SDC-UDA，一种简单有效的体积UDA框架，结合了切片内和切片间的自我注意力图像转换、不确定性约束的伪标签细化和体积自训练。<br>
                    效果：通过多个公开的跨模态医学图像分割数据集进行验证，SDC-UDA实现了最先进的分割性能，并且与以往研究相比，预测的切片方向连续性更高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in deep learning-based medical image segmentation studies achieve nearly human-level performance in fully supervised manner. However, acquiring pixel-level expert annotations is extremely expensive and laborious in medical imaging fields. Unsupervised domain adaptation (UDA) can alleviate this problem, which makes it possible to use annotated data in one imaging modality to train a network that can successfully perform segmentation on target imaging modality with no labels. In this work, we propose SDC-UDA, a simple yet effective volumetric UDA framework for Slice-Direction Continuous cross-modality medical image segmentation which combines intra- and inter-slice self-attentive image translation, uncertainty-constrained pseudo-label refinement, and volumetric self-training. Our method is distinguished from previous methods on UDA for medical image segmentation in that it can obtain continuous segmentation in the slice direction, thereby ensuring higher accuracy and potential in clinical practice. We validate SDC-UDA with multiple publicly available cross-modality medical image segmentation datasets and achieve state-of-the-art segmentation performance, not to mention the superior slice-direction continuity of prediction compared to previous studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2039.DoNet: Deep De-Overlapping Network for Cytology Instance Segmentation</span><br>
                <span class="as">Jiang, HaoandZhang, RushanandZhou, YanningandWang, YumengandChen, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_DoNet_Deep_De-Overlapping_Network_for_Cytology_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15641-15650.png><br>
            
            <span class="tt"><span class="t0">研究问题：细胞分割在细胞学图像中对生物分析和癌症筛查有重要意义，但因为1）大量重叠的半透明细胞簇导致边界模糊，2）模仿物和碎片混淆为核，所以仍然具有挑战性。<br>
                    动机：提出一种去重叠网络（DoNet），采用分解-重组策略，解决细胞分割问题。<br>
                    方法：设计了一个双路径区域分割模块（DRM）来明确地将细胞簇分解为交集和互补区域，然后通过语义一致性引导的重组模块（CRM）进行整合。为了进一步引入细胞核在细胞质中的包含关系，设计了一个掩膜引导的区域提案策略（MRP），整合了细胞注意力图进行内部细胞实例预测。<br>
                    效果：实验表明，提出的DoNet显著优于其他最先进的细胞实例分割方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cell instance segmentation in cytology images has significant importance for biology analysis and cancer screening, while remains challenging due to 1) the extensive overlapping translucent cell clusters that cause the ambiguous boundaries, and 2) the confusion of mimics and debris as nuclei. In this work, we proposed a De-overlapping Network (DoNet) in a decompose-and-recombined strategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposes the cell clusters into intersection and complement regions, followed by a Semantic Consistency-guided Recombination Module (CRM) for integration. To further introduce the containment relationship of the nucleus in the cytoplasm, we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cell attention maps for inner-cell instance prediction. We validate the proposed approach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNet significantly outperforms other state-of-the-art (SOTA) cell instance segmentation methods. The code is available at https://github.com/DeepDoNet/DoNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2040.DATE: Domain Adaptive Product Seeker for E-Commerce</span><br>
                <span class="as">Li, HaoyuanandJiang, HaoandJin, TaoandLi, MengyanandChen, YanandLin, ZhijieandZhao, YangandZhao, Zhou</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DATE_Domain_Adaptive_Product_Seeker_for_E-Commerce_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19315-19324.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决商品检索和目标定位问题，以改善购物体验。<br>
                    动机：由于缺乏相关数据集，我们收集了淘宝商城和直播领域的两个大规模基准数据集，并手动标注了每个图像中的对象边界框，以实现无监督的领域适应。<br>
                    方法：我们设计了一个语义聚合的特征提取器来获取集中和全面的特征，然后提出了两个合作搜索器来同时进行图像检索和精细的目标定位。此外，我们还设计了一个领域对齐器来缓解源域和目标域之间的单模态边际和多模态条件分布偏移，并设计了一个伪框生成器来动态选择可靠的实例并生成边界框以进一步进行知识转移。<br>
                    效果：实验表明，我们的DATE在全监督的商品检索、目标定位和无监督的目标定位领域适应方面都取得了满意的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Product Retrieval (PR) and Grounding (PG), aiming to seek image and object-level products respectively according to a textual query, have attracted great interest recently for better shopping experience. Owing to the lack of relevant datasets, we collect two large-scale benchmark datasets from Taobao Mall and Live domains with about 474k and 101k image-query pairs for PR, and manually annotate the object bounding boxes in each image for PG. As annotating boxes is expensive and time-consuming, we attempt to transfer knowledge from annotated domain to unannotated for PG to achieve un-supervised Domain Adaptation (PG-DA). We propose a Domain Adaptive producT sEeker (DATE) framework, regarding PR and PG as Product Seeking problem at different levels, to assist the query date the product. Concretely, we first design a semantics-aggregated feature extractor for each modality to obtain concentrated and comprehensive features for following efficient retrieval and fine-grained grounding tasks. Then, we present two cooperative seekers to simultaneously search the image for PR and localize the product for PG. Besides, we devise a domain aligner for PG-DA to alleviate uni-modal marginal and multi-modal conditional distribution shift between source and target domains, and design a pseudo box generator to dynamically select reliable instances and generate bounding boxes for further knowledge transfer. Extensive experiments show that our DATE achieves satisfactory performance in fully-supervised PR, PG and un-supervised PG-DA. Our desensitized datasets will be publicly available here https://github.com/Taobao-live/Product-Seeking.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2041.Sparse Multi-Modal Graph Transformer With Shared-Context Processing for Representation Learning of Giga-Pixel Images</span><br>
                <span class="as">Nakhli, RaminandMoghadam, PuriaAzadiandMi, HaoyangandFarahani, HosseinandBaras, AlexanderandGilks, BlakeandBashashati, Ali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nakhli_Sparse_Multi-Modal_Graph_Transformer_With_Shared-Context_Processing_for_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11547-11557.png><br>
            
            <span class="tt"><span class="t0">研究问题：处理十亿像素全幅病理图像（WSI）是一项计算密集型任务，现有的多实例学习（MIL）方法忽略了细胞级别的信息。<br>
                    动机：本文提出了共享上下文处理的新概念，设计了一种多模态图转换器，利用组织中的细胞图来为患者提供单一表示，同时利用组织的层次结构，实现细胞级别和组织级别信息的动态关注。<br>
                    方法：通过将图像分割成更小的补丁进行进一步处理，我们的方法能够充分利用细胞级别的信息，并通过对生存预测的基准测试，证明了其优越性。<br>
                    效果：实验结果表明，该方法在生存预测方面显著优于包括分层视觉转换器（ViT）在内的所有最先进的方法。更重要的是，该方法对缺失信息具有很强的鲁棒性，即使只有20%的数据也能实现相同的性能。在两个不同的癌症数据集上，该方法能够将患者分为低风险和高风险组，而其他最先进的方法则无法实现这一目标。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Processing giga-pixel whole slide histopathology images (WSI) is a computationally expensive task. Multiple instance learning (MIL) has become the conventional approach to process WSIs, in which these images are split into smaller patches for further processing. However, MIL-based techniques ignore explicit information about the individual cells within a patch. In this paper, by defining the novel concept of shared-context processing, we designed a multi-modal Graph Transformer that uses the cellular graph within the tissue to provide a single representation for a patient while taking advantage of the hierarchical structure of the tissue, enabling a dynamic focus between cell-level and tissue-level information. We benchmarked the performance of our model against multiple state-of-the-art methods in survival prediction and showed that ours can significantly outperform all of them including hierarchical vision Transformer (ViT). More importantly, we show that our model is strongly robust to missing information to an extent that it can achieve the same performance with as low as 20% of the data. Finally, in two different cancer datasets, we demonstrated that our model was able to stratify the patients into low-risk and high-risk groups while other state-of-the-art methods failed to achieve this goal. We also publish a large dataset of immunohistochemistry (IHC) images containing 1,600 tissue microarray (TMA) cores from 188 patients along with their survival information, making it one of the largest publicly available datasets in this context.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2042.Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures</span><br>
                <span class="as">Wu, LinshanandZhong, ZhunandFang, LeyuanandHe, XingxinandLiu, QiangandMa, JiayiandChen, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Sparsely_Annotated_Semantic_Segmentation_With_Adaptive_Gaussian_Mixtures_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15454-15464.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决稀疏标注的语义分割问题，即通过图像中的稀疏标签（例如点或涂鸦）来学习分割模型。<br>
                    动机：现有的方法主要关注引入低层次亲和力或生成伪标签以加强监督，但在很大程度上忽略了已标记和未标记像素之间的固有关系。<br>
                    方法：本文提出了一种新的SASS框架，配备了自适应高斯混合模型（AGMM）。AGMM能够根据已标记和未标记像素的分布为未标记像素提供可靠的监督。具体来说，我们首先使用已标记像素及其相对相似的未标记像素构建高斯混合模型，其中已标记像素作为质心，用于对每个类别的特征分布进行建模。然后，我们利用来自已标记像素的可靠信息和自适应生成的GMM预测来监督未标记像素的训练，实现在线、动态和鲁棒的自我监督。此外，通过捕获类别特定的高斯混合模型，AGMM鼓励模型以端到端对比学习的方式学习判别性类别决策边界。<br>
                    效果：在PASCAL VOC 2012和Cityscapes数据集上进行的实验结果表明，我们的AGMM可以建立新的最先进的SASS性能。代码可在https://github.com/Luffy03/AGMM-SASS获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Sparsely annotated semantic segmentation (SASS) aims to learn a segmentation model by images with sparse labels (i.e., points or scribbles). Existing methods mainly focus on introducing low-level affinity or generating pseudo labels to strengthen supervision, while largely ignoring the inherent relation between labeled and unlabeled pixels. In this paper, we observe that pixels that are close to each other in the feature space are more likely to share the same class. Inspired by this, we propose a novel SASS framework, which is equipped with an Adaptive Gaussian Mixture Model (AGMM). Our AGMM can effectively endow reliable supervision for unlabeled pixels based on the distributions of labeled and unlabeled pixels. Specifically, we first build Gaussian mixtures using labeled pixels and their relatively similar unlabeled pixels, where the labeled pixels act as centroids, for modeling the feature distribution of each class. Then, we leverage the reliable information from labeled pixels and adaptively generated GMM predictions to supervise the training of unlabeled pixels, achieving online, dynamic, and robust self-supervision. In addition, by capturing category-wise Gaussian mixtures, AGMM encourages the model to learn discriminative class decision boundaries in an end-to-end contrastive learning manner. Experimental results conducted on the PASCAL VOC 2012 and Cityscapes datasets demonstrate that our AGMM can establish new state-of-the-art SASS performance. Code is available at https://github.com/Luffy03/AGMM-SASS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2043.InstMove: Instance Motion for Object-Centric Video Segmentation</span><br>
                <span class="as">Liu, QihaoandWu, JunfengandJiang, YiandBai, XiangandYuille, AlanL.andBai, Song</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_InstMove_Instance_Motion_for_Object-Centric_Video_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6344-6354.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管已经做出了大量努力，但尖端的视频分割方法仍然对遮挡和快速移动敏感。<br>
                    动机：目前的解决方法主要依赖于对象嵌入的外观，这在遮挡和快速移动的情况下容易受到影响。<br>
                    方法：我们提出了一种名为InstMove的新方法，该方法主要依赖于实例级别的运动信息，这种信息不受图像特征嵌入的影响，并且具有物理解释性，使其在遮挡和快速移动的对象面前更加准确和鲁棒。<br>
                    效果：实验结果表明，实例级别的运动是强大且准确的，可以有效解决复杂场景下的视频分割问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Despite significant efforts, cutting-edge video segmentation methods still remain sensitive to occlusion and rapid movement, due to their reliance on the appearance of objects in the form of object embeddings, which are vulnerable to these disturbances. A common solution is to use optical flow to provide motion information, but essentially it only considers pixel-level motion, which still relies on appearance similarity and hence is often inaccurate under occlusion and fast movement. In this work, we study the instance-level motion and present InstMove, which stands for Instance Motion for Object-centric Video Segmentation. In comparison to pixel-wise motion, InstMove mainly relies on instance-level motion information that is free from image feature embeddings, and features physical interpretations, making it more accurate and robust toward occlusion and fast-moving objects. To better fit in with the video segmentation tasks, InstMove uses instance masks to model the physical presence of an object and learns the dynamic model through a memory network to predict its position and shape in the next frame. With only a few lines of code, InstMove can be integrated into current SOTA methods for three different video segmentation tasks and boost their performance. Specifically, we improve the previous arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and 4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects. These results suggest that instance-level motion is robust and accurate, and hence serving as a powerful solution in complex scenarios for object-centric video segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2044.GRES: Generalized Referring Expression Segmentation</span><br>
                <span class="as">Liu, ChangandDing, HenghuiandJiang, Xudong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23592-23601.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有参照表达式分割（RES）只能处理单目标表达式的问题，提出研究问题：本文旨在解决现有参照表达式分割（RES）只能处理单目标表达式的问题，提出一种通用参照表达式分割（GRES）方法，允许表达式引用任意数量的目标对象。<br>
                    动机：现有的RES数据集和方法仅支持单目标表达式，限制了其在实际应用中的使用。<br>
                    方法：构建了第一个大规模的GRES数据集gRefCOCO，包含多目标、无目标和单目标表达式。设计了一种新的基于区域的GRES基线ReLA，通过自适应地将图像划分为具有子实例线索的区域，并显式地建模区域-区域和区域-语言依赖关系。<br>
                    效果：实验结果表明，提出的ReLA方法在新的GRES任务和传统的RES任务上均取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring Expression Segmentation (RES) aims to generate a segmentation mask for the object described by a given language expression. Existing classic RES datasets and methods commonly support single-target expressions only, i.e., one expression refers to one target object. Multi-target and no-target expressions are not considered. This limits the usage of RES in practice. In this paper, we introduce a new benchmark called Generalized Referring Expression Segmentation (GRES), which extends the classic RES to allow expressions to refer to an arbitrary number of target objects. Towards this, we construct the first large-scale GRES dataset called gRefCOCO that contains multi-target, no-target, and single-target expressions. GRES and gRefCOCO are designed to be well-compatible with RES, facilitating extensive experiments to study the performance gap of the existing RES methods on the GRES task. In the experimental study, we find that one of the big challenges of GRES is complex relationship modeling. Based on this, we propose a region-based GRES baseline ReLA that adaptively divides the image into regions with sub-instance clues, and explicitly models the region-region and region-language dependencies. The proposed approach ReLA achieves new state-of-the-art performance on the both newly proposed GRES and classic RES tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GRES.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2045.Iterative Proposal Refinement for Weakly-Supervised Video Grounding</span><br>
                <span class="as">Cao, MengandWei, FangyunandXu, CanandGeng, XiuboandChen, LongandZhang, CanandZou, YuexianandShen, TaoandJiang, Daxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6524-6534.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督视频定位（WSVG）中的问题，即在只有视频级别标注的情况下，对未剪辑的视频中的感兴趣事件进行定位。<br>
                    动机：尽管现有的弱监督视频定位方法已经取得了一些进展，但是它们存在两个主要问题：1）缺乏显式对应建模；2）复杂事件的覆盖不全面。<br>
                    方法：为了解决这些问题，作者提出了一种新的迭代提案精炼网络（IRON）。具体来说，作者设置了两个轻量级的蒸馏分支，以在语义和概念层面上揭示跨模态的对应关系。然后，设计了一个迭代的标签传播策略，防止网络过度关注最具鉴别性的事件，而不是整个句子的内容。<br>
                    效果：通过对两个具有挑战性的WSVG数据集进行大量的实验和消融研究，证明了IRON的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-Supervised Video Grounding (WSVG) aims to localize events of interest in untrimmed videos with only video-level annotations. To date, most of the state-of-the-art WSVG methods follow a two-stage pipeline, i.e., firstly generating potential temporal proposals and then grounding with these proposal candidates. Despite the recent progress, existing proposal generation methods suffer from two drawbacks: 1) lack of explicit correspondence modeling; and 2) partial coverage of complex events. To this end, we propose a novel IteRative prOposal refiNement network (dubbed as IRON) to gradually distill the prior knowledge into each proposal and encourage proposals with more complete coverage. Specifically, we set up two lightweight distillation branches to uncover the cross-modal correspondence on both the semantic and conceptual levels. Then, an iterative Label Propagation (LP) strategy is devised to prevent the network from focusing excessively on the most discriminative events instead of the whole sentence content. Precisely, during each iteration, the proposal with the minimal distillation loss and its adjacent ones are regarded as the positive samples, which refines proposal confidence scores in a cascaded manner. Extensive experiments and ablation studies on two challenging WSVG datasets have attested to the effectiveness of our IRON.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2046.FAC: 3D Representation Learning via Foreground Aware Feature Contrast</span><br>
                <span class="as">Liu, KangchengandXiao, AoranandZhang, XiaoqinandLu, ShijianandShao, Ling</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FAC_3D_Representation_Learning_via_Foreground_Aware_Feature_Contrast_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9476-9485.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高对比学习在无监督预训练3D场景理解任务中的效果。<br>
                    动机：现有的对比学习方法存在对背景点选择的偏见，忽视了物体意识和前景与背景的区分，导致效果不佳。<br>
                    方法：提出一种通用的前景色感知特征对比（FAC）框架，通过构建更有效和信息丰富的对比对来学习更高效的点云表示。<br>
                    效果：实验证明，FAC在各种下游3D语义分割和对象检测任务上实现了优越的知识转移和数据效率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Contrastive learning has recently demonstrated great potential for unsupervised pre-training in 3D scene understanding tasks. However, most existing work randomly selects point features as anchors while building contrast, leading to a clear bias toward background points that often dominate in 3D scenes. Also, object awareness and foreground-to-background discrimination are neglected, making contrastive learning less effective. To tackle these issues, we propose a general foreground-aware feature contrast (FAC) framework to learn more effective point cloud representations in pre-training. FAC consists of two novel contrast designs to construct more effective and informative contrast pairs. The first is building positive pairs within the same foreground segment where points tend to have the same semantics. The second is that we prevent over-discrimination between 3D segments/objects and encourage foreground-to-background distinctions at the segment level with adaptive feature learning in a Siamese correspondence network, which adaptively learns feature correlations within and across point cloud views effectively. Visualization with point activation maps shows that our contrast pairs capture clear correspondences among foreground regions during pre-training. Quantitative experiments also show that FAC achieves superior knowledge transfer and data efficiency in various downstream 3D semantic segmentation and object detection tasks. All codes, data, and models are available at:https://github.com/KangchengLiu/FAC_Foreground_Aware_Contrast.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2047.SIM: Semantic-Aware Instance Mask Generation for Box-Supervised Instance Segmentation</span><br>
                <span class="as">Li, RuihuangandHe, ChenhangandZhang, YabinandLi, ShuaiandChen, LiyiandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SIM_Semantic-Aware_Instance_Mask_Generation_for_Box-Supervised_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7193-7203.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用仅有的边界框注释进行弱监督实例分割。<br>
                    动机：目前的弱监督实例分割方法大多依赖低层次图像特征作为额外监督，没有明确利用物体的高级别语义信息，当前景物体与背景或其他附近的物体外观相似时，这种方法将变得无效。<br>
                    方法：提出一种新的基于盒子的监督实例分割方法，通过开发一个语义感知实例掩膜（SIM）生成范式。不依赖于相邻像素之间的局部成对亲和力，而是构建一组类别特定的特征质心作为原型来识别前景物体并分配语义级别的伪标签。考虑到语义感知原型无法区分同一语义的不同实例，提出了一种自我修正机制来纠正错误激活的区域，同时增强正确的区域。此外，为了处理对象之间的遮挡，为弱监督实例分割任务量身定制了复制粘贴操作以增强具有挑战性的训练数据。<br>
                    效果：大量实验结果表明，所提出的SIM方法优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly supervised instance segmentation using only bounding box annotations has recently attracted much research attention. Most of the current efforts leverage low-level image features as extra supervision without explicitly exploiting the high-level semantic information of the objects, which will become ineffective when the foreground objects have similar appearances to the background or other objects nearby. We propose a new box-supervised instance segmentation approach by developing a Semantic-aware Instance Mask (SIM) generation paradigm. Instead of heavily relying on local pair-wise affinities among neighboring pixels, we construct a group of category-wise feature centroids as prototypes to identify foreground objects and assign them semantic-level pseudo labels. Considering that the semantic-aware prototypes cannot distinguish different instances of the same semantics, we propose a self-correction mechanism to rectify the falsely activated regions while enhancing the correct ones. Furthermore, to handle the occlusions between objects, we tailor the Copy-Paste operation for the weakly-supervised instance segmentation task to augment challenging training data. Extensive experimental results demonstrate the superiority of our proposed SIM approach over other state-of-the-art methods. The source code: https://github.com/lslrh/SIM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2048.VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision</span><br>
                <span class="as">Liu, MengyinandJiang, JieandZhu, ChaoandYin, Xu-Cheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_VLPD_Context-Aware_Pedestrian_Detection_via_Vision-Language_Semantic_Self-Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6662-6671.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在城市场景中准确检测行人，特别是在小尺度或严重遮挡的情况下。<br>
                    动机：目前的行人检测方法主要依赖物体区域，对于小尺度或严重遮挡的行人检测效果不佳。同时，现有的上下文感知行人检测方法要么只学习视觉线索的潜在上下文，要么需要大量的注释来获取明确的语义上下文。<br>
                    方法：本文提出了一种新的视觉语言语义自我监督的上下文感知行人检测（VLPD）方法，无需额外注释即可显式建模语义上下文。首先，提出了一种自我监督的视觉语言语义（VLS）分割方法，通过视觉语言模型生成的语义类别的明确标签，学习全监督行人检测和上下文分割。其次，提出了一种自我监督的原型语义对比（PSC）学习方法，基于从VLS获得的更明确和语义的上下文，更好地区分行人和其他类别。<br>
                    效果：在流行的基准测试上的大量实验表明，所提出的VLPD在小尺度和严重遮挡等具有挑战性的情况下，优于先前最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestrian detectors either only learn latent contexts with visual clues, or need laborious annotations to obtain explicit and semantic contexts. Therefore, we propose in this paper a novel approach via Vision-Language semantic self-supervision for context-aware Pedestrian Detection (VLPD) to model explicitly semantic contexts without any extra annotations. Firstly, we propose a self-supervised Vision-Language Semantic (VLS) segmentation method, which learns both fully-supervised pedestrian detection and contextual segmentation via self-generated explicit labels of semantic classes by vision-language models. Furthermore, a self-supervised Prototypical Semantic Contrastive (PSC) learning method is proposed to better discriminate pedestrians and other classes, based on more explicit and semantic contexts obtained from VLS. Extensive experiments on popular benchmarks show that our proposed VLPD achieves superior performances over the previous state-of-the-arts, particularly under challenging circumstances like small scale and heavy occlusion. Code is available at https://github.com/lmy98129/VLPD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2049.Unsupervised Object Localization: Observing the Background To Discover Objects</span><br>
                <span class="as">Sim\&#x27;eoni, OrianeandSekkat, Chlo\&#x27;eandPuy, GillesandVobeck\&#x27;y, Anton{\&#x27;\i</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3176-3186.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现无监督的图像对象发现和实例分割。<br>
                    动机：现有的自监督视觉表示学习方法为解决无监督任务（如对象发现和实例分割）铺平了道路，但如何在没有监督的情况下在图像中发现对象是一项非常困难的任务。<br>
                    方法：我们提出了一种寻找背景的新方法，突出的对象作为副产品出现，无需对对象应有的样子做任何强假设。我们提出了FOUND模型，该模型由一个单独的conv 1x1组成，使用从自监督基于补丁的表示中提取的粗糙背景掩码进行初始化。<br>
                    效果：经过快速训练和精炼这些种子掩码后，该模型在无监督显著性检测和对象发现基准测试中达到了最先进的结果。此外，我们的方法是有效的，可以在无监督语义分割检索任务中获得良好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in self-supervised visual representation learning have paved the way for unsupervised methods tackling tasks such as object discovery and instance segmentation. However, discovering objects in an image with no supervision is a very hard task; what are the desired objects, when to separate them into parts, how many are there, and of what classes? The answers to these questions depend on the tasks and datasets of evaluation. In this work, we take a different approach and propose to look for the background instead. This way, the salient objects emerge as a by-product without any strong assumption on what an object should be. We propose FOUND, a simple model made of a single conv 1x1 initialized with coarse background masks extracted from self-supervised patch-based representations. After fast training and refining these seed masks, the model reaches state-of-the-art results on unsupervised saliency detection and object discovery benchmarks. Moreover, we show that our approach yields good results in the unsupervised semantic segmentation retrieval task. The code to reproduce our results is available at https://github.com/valeoai/FOUND.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2050.Exemplar-FreeSOLO: Enhancing Unsupervised Instance Segmentation With Exemplars</span><br>
                <span class="as">Ishtiak, TaoseefandEn, QingandGuo, Yuhong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ishtiak_Exemplar-FreeSOLO_Enhancing_Unsupervised_Instance_Segmentation_With_Exemplars_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15424-15433.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用无标注的实例进行图像分割，以减轻模型训练的负担。<br>
                    动机：为了解决传统实例分割方法需要大量密集标注的问题，提出了一种无需标注的无监督实例分割方法。<br>
                    方法：提出了一种新的无监督实例分割方法Exemplar-FreeSOLO，通过使用少量未标注和未分割的实例来提取有用的自上而下的指导知识。<br>
                    效果：实验结果表明，该方法在三个图像实例分割数据集上都优于现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Instance segmentation seeks to identify and segment each object from images, which often relies on a large number of dense annotations for model training. To alleviate this burden, unsupervised instance segmentation methods have been developed to train class-agnostic instance segmentation models without any annotation. In this paper, we propose a novel unsupervised instance segmentation approach, Exemplar-FreeSOLO, to enhance unsupervised instance segmentation by exploiting a limited number of unannotated and unsegmented exemplars. The proposed framework offers a new perspective on directly perceiving top-down information without annotations. Specifically, Exemplar-FreeSOLO introduces a novel exemplarknowledge abstraction module to acquire beneficial top-down guidance knowledge for instances using unsupervised exemplar object extraction. Moreover, a new exemplar embedding contrastive module is designed to enhance the discriminative capability of the segmentation model by exploiting the contrastive exemplar-based guidance knowledge in the embedding space. To evaluate the proposed ExemplarFreeSOLO, we conduct comprehensive experiments and perform in-depth analyses on three image instance segmentation datasets. The experimental results demonstrate that the proposed approach is effective and outperforms the state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2051.Spatiotemporal Self-Supervised Learning for Point Clouds in the Wild</span><br>
                <span class="as">Wu, YanhaoandZhang, TongandKe, WeiandS\&quot;usstrunk, SabineandSalzmann, Mathieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Spatiotemporal_Self-Supervised_Learning_for_Point_Clouds_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5251-5260.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning (SSL) has the potential to benefit many applications, particularly those where manually annotating data is cumbersome. One such situation is the semantic segmentation of point clouds. In this context, existing methods employ contrastive learning strategies and define positive pairs by performing various augmentation of point clusters in a single frame. As such, these methods do not exploit the temporal nature of LiDAR data. In this paper, we introduce an SSL strategy that leverages positive pairs in both the spatial and temporal domains. To this end, we design (i) a point-to-cluster learning strategy that aggregates spatial information to distinguish objects; and (ii) a cluster-to-cluster learning strategy based on unsupervised object tracking that exploits temporal correspondences. We demonstrate the benefits of our approach via extensive experiments performed by self-supervised training on two large-scale LiDAR datasets and transferring the resulting models to other point cloud segmentation benchmarks. Our results evidence that our method outperforms the state-of-the-art point cloud SSL methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2052.Semi-Supervised Learning Made Simple With Self-Supervised Clustering</span><br>
                <span class="as">Fini, EnricoandAstolfi, PietroandAlahari, KarteekandAlameda-Pineda, XavierandMairal, JulienandNabi, MoinandRicci, Elisa</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fini_Semi-Supervised_Learning_Made_Simple_With_Self-Supervised_Clustering_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3187-3197.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将基于聚类的自监督学习方法转化为半监督学习模型。<br>
                    动机：在许多实际情况中，标签是部分可用的，这激发了从自监督原则出发的半监督方法的研究。<br>
                    方法：提出了一种概念简单但实证有效的方法，将基于聚类的自监督方法（如SwAV或DINO）转化为半监督学习器。具体来说，通过单一交叉熵损失引入了一个结合了使用真实标签的有监督目标和依赖聚类分配的自监督目标的多任务框架。<br>
                    效果：实验结果表明，该方法非常有效，并在CIFAR100和ImageNet上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-supervised learning models have been shown to learn rich visual representations without requiring human annotations. However, in many real-world scenarios, labels are partially available, motivating a recent line of work on semi-supervised methods inspired by self-supervised principles. In this paper, we propose a conceptually simple yet empirically powerful approach to turn clustering-based self-supervised methods such as SwAV or DINO into semi-supervised learners. More precisely, we introduce a multi-task framework merging a supervised objective using ground-truth labels and a self-supervised objective relying on clustering assignments with a single cross-entropy loss. This approach may be interpreted as imposing the cluster centroids to be class prototypes. Despite its simplicity, we provide empirical evidence that our approach is highly effective and achieves state-of-the-art performance on CIFAR100 and ImageNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2053.Harmonious Teacher for Cross-Domain Object Detection</span><br>
                <span class="as">Deng, JinhongandXu, DongliandLi, WenandDuan, Lixin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_Harmonious_Teacher_for_Cross-Domain_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23829-23838.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决跨领域目标检测中自我训练方法的问题，即如何提高伪标签的质量。<br>
                    动机：现有的自我训练方法在跨领域目标检测中取得了良好的效果，但伪标签的质量直接影响了模型的训练效果。<br>
                    方法：提出了一种新的和谐教师方法，通过在训练检测模型时对分类和定位得分的一致性进行正则化来提高伪标签的质量，并采用基于分类和定位得分一致性的样本重选策略来改进预测的排序。<br>
                    效果：实验结果表明，该方法在各种跨领域场景中均优于现有的最佳基线，验证了和谐教师方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-training approaches recently achieved promising results in cross-domain object detection, where people iteratively generate pseudo labels for unlabeled target domain samples with a model, and select high-confidence samples to refine the model. In this work, we reveal that the consistency of classification and localization predictions are crucial to measure the quality of pseudo labels, and propose a new Harmonious Teacher approach to improve the self-training for cross-domain object detection. In particular, we first propose to enhance the quality of pseudo labels by regularizing the consistency of the classification and localization scores when training the detection model. The consistency losses are defined for both labeled source samples and the unlabeled target samples. Then, we further remold the traditional sample selection method by a sample reweighing strategy based on the consistency of classification and localization scores to improve the ranking of predictions. This allows us to fully exploit all instance predictions from the target domain without abandoning valuable hard examples. Without bells and whistles, our method shows superior performance in various cross-domain scenarios compared with the state-of-the-art baselines, which validates the effectiveness of our Harmonious Teacher. Our codes will be available at https://github.com/kinredon/Harmonious-Teacher.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2054.Semi-Supervised Video Inpainting With Cycle Consistency Constraints</span><br>
                <span class="as">Wu, ZhiliangandXuan, HanyuandSun, ChangchangandGuan, WeiliandZhang, KangandYan, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Semi-Supervised_Video_Inpainting_With_Cycle_Consistency_Constraints_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22586-22595.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量标注数据实现视频修复任务。<br>
                    动机：现有的深度学习方法需要大量标注数据，但标注过程耗时且成本高，限制了实际应用。<br>
                    方法：提出一种半监督的视频修复框架，通过一个已标注的掩码预测下一帧需要修复的区域，并训练完成网络生成当前帧的修复内容。同时引入循环一致性损失约束两个网络的训练参数，使它们相互制约，提高模型性能。<br>
                    效果：在模拟真实场景的数据集上进行实验，证明该方法在视频修复任务上表现优越，即使使用半监督方式训练，其性能也可与全监督方法相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning-based video inpainting has yielded promising results and gained increasing attention from researchers. Generally, these methods usually assume that the corrupted region masks of each frame are known and easily obtained. However, the annotation of these masks are labor-intensive and expensive, which limits the practical application of current methods. Therefore, we expect to relax this assumption by defining a new semi-supervised inpainting setting, making the networks have the ability of completing the corrupted regions of the whole video using the annotated mask of only one frame. Specifically, in this work, we propose an end-to-end trainable framework consisting of completion network and mask prediction network, which are designed to generate corrupted contents of the current frame using the known mask and decide the regions to be filled of the next frame, respectively. Besides, we introduce a cycle consistency loss to regularize the training parameters of these two networks. In this way, the completion network and the mask prediction network can constrain each other, and hence the overall performance of the trained model can be maximized. Furthermore, due to the natural existence of prior knowledge (e.g., corrupted contents and clear borders), current video inpainting datasets are not suitable in the context of semi-supervised video inpainting. Thus, we create a new dataset by simulating the corrupted video of real-world scenarios. Extensive experimental results are reported to demonstrate the superiority of our model in the video inpainting task. Remarkably, although our model is trained in a semi-supervised manner, it can achieve comparable performance as fully-supervised methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2055.Out-of-Candidate Rectification for Weakly Supervised Semantic Segmentation</span><br>
                <span class="as">Cheng, ZesenandQiao, PengchongandLi, KehanandLi, SihengandWei, PengxuandJi, XiangyangandYuan, LiandLiu, ChangandChen, Jie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Out-of-Candidate_Rectification_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23673-23684.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督语义分割中存在的Out-of-Candidate错误预测问题。<br>
                    动机：现有的方法在处理这类错误预测时，由于无法有效检测到其与图像级别类别标签的矛盾，导致误判情况频繁发生。<br>
                    方法：本文提出了一种基于组排序的Out-of-Candidate Rectification（OCR）机制。首先，根据像素点的先验标注关联性和后验预测关联性，将语义类别自适应地划分为In-Candidate（IC）和Out-of-Candidate（OC）两组。然后，通过设计一个可微分的修正损失函数，迫使OC像素点向IC组转移。<br>
                    效果：将OCR机制应用于AffinityNet、SEAM、MCTformer等基础模型后，在Pascal VOC和MS COCO数据集上取得了显著的性能提升，且训练开销几乎无增加。这证明了OCR机制的有效性和通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly supervised semantic segmentation is typically inspired by class activation maps, which serve as pseudo masks with class-discriminative regions highlighted. Although tremendous efforts have been made to recall precise and complete locations for each class, existing methods still commonly suffer from the unsolicited Out-of-Candidate (OC) error predictions that do not belong to the label candidates, which could be avoidable since the contradiction with image-level class tags is easy to be detected. In this paper, we develop a group ranking-based Out-of-Candidate Rectification (OCR) mechanism in a plug-and-play fashion. Firstly, we adaptively split the semantic categories into In-Candidate (IC) and OC groups for each OC pixel according to their prior annotation correlation and posterior prediction correlation. Then, we derive a differentiable rectification loss to force OC pixels to shift to the IC group. Incorporating OCR with seminal baselines (e.g., AffinityNet, SEAM, MCTformer), we can achieve remarkable performance gains on both Pascal VOC (+3.2%, +3.3%, +0.8% mIoU) and MS COCO (+1.0%, +1.3%, +0.5% mIoU) datasets with negligible extra training overhead, which justifies the effectiveness and generality of OCR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2056.Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection</span><br>
                <span class="as">Wang, LutingandLiu, YiandDu, PenghuiandDing, ZihanandLiao, YueandQi, QiaosongandChen, BiaolongandLiu, Si</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11186-11196.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放词汇目标检测中的问题，即如何让物体检测器具有对任意文本查询描述的物体进行检测的泛化能力。<br>
                    动机：现有的方法采用知识蒸馏从预训练的视觉-语言模型（PVLMs）中提取知识并转移到检测器上，但由于非适应性的提案裁剪和单级特征模仿过程，这些方法在知识提取和知识转移过程中存在信息破坏和效率低下的问题。<br>
                    方法：本文提出了一个对象感知蒸馏金字塔（OADP）框架，包括一个对象感知知识提取（OAKE）模块和一个蒸馏金字塔（DP）机制。在从PVLMs中提取对象知识时，前者自适应地转换对象提案并采用对象感知的掩码注意力以获取精确和完整的对象知识。后者引入了全局和块蒸馏以进行更全面的知识转移，以补偿对象蒸馏中缺失的关系信息。<br>
                    效果：大量实验表明，该方法比当前的方法取得了显著的改进。特别是在MS-COCO数据集上，我们的OADP框架达到了35.6 mAP^N_50，超过了当前最先进的方法3.3 mAP^N_50。代码在补充材料中匿名提供。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-vocabulary object detection aims to provide object detectors trained on a fixed set of object categories with the generalizability to detect objects described by arbitrary text queries. Previous methods adopt knowledge distillation to extract knowledge from Pretrained Vision-and-Language Models (PVLMs) and transfer it to detectors. However, due to the non-adaptive proposal cropping and single-level feature mimicking processes, they suffer from information destruction during knowledge extraction and inefficient knowledge transfer. To remedy these limitations, we propose an Object-Aware Distillation Pyramid (OADP) framework, including an Object-Aware Knowledge Extraction (OAKE) module and a Distillation Pyramid (DP) mechanism. When extracting object knowledge from PVLMs, the former adaptively transforms object proposals and adopts object-aware mask attention to obtain precise and complete knowledge of objects. The latter introduces global and block distillation for more comprehensive knowledge transfer to compensate for the missing relation information in object distillation. Extensive experiments show that our method achieves significant improvement compared to current methods. Especially on the MS-COCO dataset, our OADP framework reaches 35.6 mAP^N_50, surpassing the current state-of-the-art method by 3.3 mAP^N_50. Code is anonymously provided in the supplementary materials.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2057.Vision Transformers Are Good Mask Auto-Labelers</span><br>
                <span class="as">Lan, ShiyiandYang, XitongandYu, ZhidingandWu, ZuxuanandAlvarez, JoseM.andAnandkumar, Anima</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lan_Vision_Transformers_Are_Good_Mask_Auto-Labelers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23745-23755.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种高质量的基于Transformer的遮罩自动标注框架，用于仅使用框注释进行实例分割。<br>
                    动机：当前的实例分割模型需要大量的标注数据，而手动标注成本高且耗时。我们希望通过自动生成高质量的遮罩来减少这种需求。<br>
                    方法：我们提出了Mask Auto-Labeler（MAL）框架，该框架接受裁剪后的图像作为输入，并有条件地生成其遮罩伪标签。<br>
                    效果：实验结果表明，我们的遮罩自动标注器在质量上与人工标注相当，使用MAL生成的遮罩训练的实例分割模型性能接近全监督模型，最高可达44.1% mAP，优于现有的最先进的框监督方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Mask Auto-Labeler (MAL), a high-quality Transformer-based mask auto-labeling framework for instance segmentation using only box annotations. MAL takes box-cropped images as inputs and conditionally generates their mask pseudo-labels.We show that Vision Transformers are good mask auto-labelers. Our method significantly reduces the gap between auto-labeling and human annotation regarding mask quality. Instance segmentation models trained using the MAL-generated masks can nearly match the performance of their fully-supervised counterparts, retaining up to 97.4% performance of fully supervised models. The best model achieves 44.1% mAP on COCO instance segmentation (test-dev 2017), outperforming state-of-the-art box-supervised methods by significant margins. Qualitative results indicate that masks produced by MAL are, in some cases, even better than human annotations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2058.MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition</span><br>
                <span class="as">Xie, RonaldandPang, KuanandBader, GaryD.andWang, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_MAESTER_Masked_Autoencoder_Guided_Segmentation_at_Pixel_Resolution_for_Accurate_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3292-3301.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确分割细胞图像，特别是在生物结构形态的固有变异性下。<br>
                    动机：由于生物结构的形态固有变异性，完全手动分割大型数据集是不可行的。虽然有监督方法被提出以自动化分割，但它们通常依赖于手动生成的地面真值，这在生物学中特别具有挑战性和耗时，因为需要领域专业知识。此外，这些方法的泛化能力有限，需要为每个数据集和使用情况生成额外的手动标签。<br>
                    方法：我们引入了MAESTER（Masked AutoEncoder guided SegmenTation at pixEl Resolution），一种用于在像素级别准确分割亚细胞结构的自监督方法。MAESTER将分割视为表示学习和聚类问题。具体来说，MAESTER在学习多像素图像补丁的语义有意义的令牌表示的同时，保持足够大的视场进行上下文学习。我们还开发了一种覆盖和步进推理策略，以实现像素级别的亚细胞结构分割。<br>
                    效果：我们在公开的老鼠胰腺胰岛beta细胞的体积电子显微镜（VEM）数据集上评估了MAESTER，并在相同的评估标准下实现了超过29.1%的性能提升。此外，我们的结果与在同一任务上训练的有监督方法相当，缩小了自监督和有监督方法之间的差距。MAESTER显示出有望缓解成像相关数据分析的关键瓶颈——地面真值生成，从而大大提高生物发现的速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Accurate segmentation of cellular images remains an elusive task due to the intrinsic variability in morphology of biological structures. Complete manual segmentation is unfeasible for large datasets, and while supervised methods have been proposed to automate segmentation, they often rely on manually generated ground truths which are especially challenging and time consuming to generate in biology due to the requirement of domain expertise. Furthermore, these methods have limited generalization capacity, requiring additional manual labels to be generated for each dataset and use case. We introduce MAESTER (Masked AutoEncoder guided SegmenTation at pixEl Resolution), a self-supervised method for accurate, subcellular structure segmentation at pixel resolution. MAESTER treats segmentation as a representation learning and clustering problem. Specifically, MAESTER learns semantically meaningful token representations of multi-pixel image patches while simultaneously maintaining a sufficiently large field of view for contextual learning. We also develop a cover-and-stride inference strategy to achieve pixel-level subcellular structure segmentation. We evaluated MAESTER on a publicly available volumetric electron microscopy (VEM) dataset of primary mouse pancreatic islets beta cells and achieved upwards of 29.1% improvement over state-of-the-art under the same evaluation criteria. Furthermore, our results are competitive against supervised methods trained on the same tasks, closing the gap between self-supervised and supervised approaches. MAESTER shows promise for alleviating the critical bottleneck of ground truth generation for imaging related data analysis and thereby greatly increasing the rate of biological discovery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2059.OCELOT: Overlapped Cell on Tissue Dataset for Histopathology</span><br>
                <span class="as">Ryu, JeongunandPuche, AaronValeroandShin, JaeWoongandPark, SeonwookandBrattoli, BiagioandLee, JinheeandJung, WonkyungandCho, SooIckandPaeng, KyunghyunandOck, Chan-YoungandYoo, DonggeunandPereira, S\&#x27;ergio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ryu_OCELOT_Overlapped_Cell_on_Tissue_Dataset_for_Histopathology_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23902-23912.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何更准确地进行细胞检测，特别是在组织层面结构和细胞形态及其周围环境方面。<br>
                    动机：目前缺乏同时考虑细胞和组织关系的数据集，限制了计算病理学中细胞检测模型的发展。<br>
                    方法：提出并公开OCELOT数据集，该数据集包含多个器官的图像，具有重叠的细胞和组织标注。同时，提出多任务学习方法，同时学习细胞和组织任务。<br>
                    效果：在3个数据集上进行实验，与只训练细胞检测任务的模型相比，所提出的多任务学习方法在OCELOT测试集上F1分数提高了6.79%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Cell detection is a fundamental task in computational pathology that can be used for extracting high-level medical information from whole-slide images. For accurate cell detection, pathologists often zoom out to understand the tissue-level structures and zoom in to classify cells based on their morphology and the surrounding context. However, there is a lack of efforts to reflect such behaviors by pathologists in the cell detection models, mainly due to the lack of datasets containing both cell and tissue annotations with overlapping regions. To overcome this limitation, we propose and publicly release OCELOT, a dataset purposely dedicated to the study of cell-tissue relationships for cell detection in histopathology. OCELOT provides overlapping cell and tissue annotations on images acquired from multiple organs. Within this setting, we also propose multi-task learning approaches that benefit from learning both cell and tissue tasks simultaneously. When compared against a model trained only for the cell detection task, our proposed approaches improve cell detection performance on 3 datasets: proposed OCELOT, public TIGER, and internal CARP datasets. On the OCELOT test set in particular, we show up to 6.79 improvement in F1-score. We believe the contributions of this paper, including the release of the OCELOT dataset at https://lunit-io.github.io/research/publications/ocelot are a crucial starting point toward the important research direction of incorporating cell-tissue relationships in computation pathology.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2060.Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss</span><br>
                <span class="as">Mahmoud, AnasandHu, JordanS.K.andKuai, TianshuandHarakeh, AliandPaull, LiamandWaslander, StevenL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mahmoud_Self-Supervised_Image-to-Point_Distillation_via_Semantically_Tolerant_Contrastive_Loss_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7102-7110.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过对比学习提炼丰富的自我监督图像特征，以解决自动驾驶数据集的图像到点表示学习面临的主要挑战。<br>
                    动机：图像到点表示学习在自动驾驶数据集中面临两个主要挑战：1）自我相似性的丰富性导致对比损失将语义相似的点和图像区域推开，从而扰乱了学习的表示的局部语义结构；2）严重的类别不平衡，预训练被过度表示的类别主导。<br>
                    方法：我们提出了一种新的语义容忍的图像到点对比损失来解决自我相似性问题，该损失考虑了正负图像区域之间的语义距离，以最小化对比语义相似的点和图像区域。此外，我们通过设计一种类无关的平衡损失来解决类别不平衡问题，该损失通过一个样本对样本的语义相似度度量来近似类别不平衡的程度。<br>
                    效果：我们的实验表明，我们的语义容忍的对比损失与类别平衡在所有的3D语义分割评估设置中都改进了最先进的2D到3D表示学习方法。我们的方法在所有类型的2D自我监督预训练模型上都优于最先进的2D到3D表示学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>An effective framework for learning 3D representations for perception tasks is distilling rich self-supervised image features via contrastive learning. However, image-to-point representation learning for autonomous driving datasets faces two main challenges: 1) the abundance of self-similarity, which results in the contrastive losses pushing away semantically similar point and image regions and thus disturbing the local semantic structure of the learned representations, and 2) severe class imbalance as pretraining gets dominated by over-represented classes. We propose to alleviate the self-similarity problem through a novel semantically tolerant image-to-point contrastive loss that takes into consideration the semantic distance between positive and negative image regions to minimize contrasting semantically similar point and image regions. Additionally, we address class imbalance by designing a class-agnostic balanced loss that approximates the degree of class imbalance through an aggregate sample-to-samples semantic similarity measure. We demonstrate that our semantically-tolerant contrastive loss with class balancing improves state-of-the-art 2D-to-3D representation learning in all evaluation settings on 3D semantic segmentation. Our method consistently outperforms state-of-the-art 2D-to-3D representation learning frameworks across a wide range of 2D self-supervised pretrained models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2061.ProtoCon: Pseudo-Label Refinement via Online Clustering and Prototypical Consistency for Efficient Semi-Supervised Learning</span><br>
                <span class="as">Nassar, IslamandHayat, MunawarandAbbasnejad, EhsanandRezatofighi, HamidandHaffari, Gholamreza</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nassar_ProtoCon_Pseudo-Label_Refinement_via_Online_Clustering_and_Prototypical_Consistency_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11641-11650.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决半监督学习中标签稀缺的问题，特别是在高置信度伪标签方法通常表现不佳的情况下。<br>
                    动机：现有的半监督学习方法主要依赖于高置信度的伪标签进行训练，但在标签稀缺的情况下，这种方法往往效果不佳。<br>
                    方法：本文提出了一种新的半监督学习方法ProtoCon，通过利用伪标签最近邻的信息来优化伪标签。该方法采用在线聚类的方式在嵌入空间中识别邻居，并通过原型损失来鼓励形成良好的聚类。ProtoCon的在线特性使其能够在一个训练周期内利用整个数据集的标签历史来优化下一个周期的标签，而无需存储图像嵌入，从而能够无缝扩展到更大的数据集。<br>
                    效果：实验结果表明，ProtoCon在5个数据集上（包括CIFARs、ImageNet和DomainNet）相比最先进的方法取得了显著的改进和更快的收敛速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Confidence-based pseudo-labeling is among the dominant approaches in semi-supervised learning (SSL). It relies on including high-confidence predictions made on unlabeled data as additional targets to train the model. We propose ProtoCon, a novel SSL method aimed at the less-explored label-scarce SSL where such methods usually underperform. ProtoCon refines the pseudo-labels by leveraging their nearest neighbours' information. The neighbours are identified as the training proceeds using an online clustering approach operating in an embedding space trained via a prototypical loss to encourage well-formed clusters. The online nature of ProtoCon allows it to utilise the label history of the entire dataset in one training cycle to refine labels in the following cycle without the need to store image embeddings. Hence, it can seamlessly scale to larger datasets at a low cost. Finally, ProtoCon addresses the poor training signal in the initial phase of training (due to fewer confident predictions) by introducing an auxiliary self-supervised loss. It delivers significant gains and faster convergence over state-of-the-art across 5 datasets, including CIFARs, ImageNet and DomainNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2062.BKinD-3D: Self-Supervised 3D Keypoint Discovery From Multi-View Videos</span><br>
                <span class="as">Sun, JenniferJ.andKarashchuk, LiliandDravid, AmilandRyou, SerimandFereidooni, SoniaandTuthill, JohnC.andKatsaggelos, AggelosandBrunton, BingniW.andGkioxari, GeorgiaandKennedy, AnnandYue, YisongandPerona, Pietro</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_BKinD-3D_Self-Supervised_3D_Keypoint_Discovery_From_Multi-View_Videos_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9001-9010.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在3D空间中进行自我监督的关键发现，以估计没有注释的3D姿势。<br>
                    动机：手动姿态注释获取成本高且耗时，而现有的关键点发现方法通常只处理单个2D视图，无法在3D空间中操作。<br>
                    方法：提出一种新方法，通过多视角的行为代理视频进行自我监督的3D关键点发现，无需任何2D或3D关键点或边界框监督。该方法使用编码器-解码器架构和3D体积热图，训练重建多个视图之间的时空差异，以及对学习的主体的3D骨架的关节长度约束。<br>
                    效果：在人类和大鼠的视频中发现了关键点，无需手动监督，展示了3D关键点发现在研究行为方面的潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Quantifying motion in 3D is important for studying the behavior of humans and other animals, but manual pose annotations are expensive and time-consuming to obtain. Self-supervised keypoint discovery is a promising strategy for estimating 3D poses without annotations. However, current keypoint discovery approaches commonly process single 2D views and do not operate in the 3D space. We propose a new method to perform self-supervised keypoint discovery in 3D from multi-view videos of behaving agents, without any keypoint or bounding box supervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoder architecture with a 3D volumetric heatmap, trained to reconstruct spatiotemporal differences across multiple views, in addition to joint length constraints on a learned 3D skeleton of the subject. In this way, we discover keypoints without requiring manual supervision in videos of humans and rats, demonstrating the potential of 3D keypoint discovery for studying behavior.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2063.Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data</span><br>
                <span class="as">Chen, YuhaoandTan, XinandZhao, BoruiandChen, ZhaoweiandSong, RenjieandLiang, JiajunandLu, Xuequan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boosting_Semi-Supervised_Learning_by_Exploiting_All_Unlabeled_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7548-7557.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督学习方法在处理复杂例子时存在浪费，因为所有伪标签都需要通过高阈值筛选以过滤掉噪声。<br>
                    动机：为了更好的利用所有未标记的例子，提出了两种新的技术：熵意义损失（EML）和自适应负学习（ANL）。<br>
                    方法：EML将非目标类别的预测分布纳入优化目标，避免与目标类别竞争，从而为选择伪标签生成更高置信度的预测。ANL为所有未标记的数据引入额外的负伪标签，以利用低置信度的例子。这两种方法都没有引入任何额外的参数和超参数。<br>
                    效果：在几个常见的半监督学习基准测试（CIFAR-10/100，SVHN，STL-10和ImageNet）上进行的大量实验表明，FullMatch大大超过了FixMatch。与FlexMatch（一个基于FixMatch的先进框架）结合使用时，实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised learning (SSL) has attracted enormous attention due to its vast potential of mitigating the dependence on large labeled datasets. The latest methods (e.g., FixMatch) use a combination of consistency regularization and pseudo-labeling to achieve remarkable successes. However, these methods all suffer from the waste of complicated examples since all pseudo-labels have to be selected by a high threshold to filter out noisy ones. Hence, the examples with ambiguous predictions will not contribute to the training phase. For better leveraging all unlabeled examples, we propose two novel techniques: Entropy Meaning Loss (EML) and Adaptive Negative Learning (ANL). EML incorporates the prediction distribution of non-target classes into the optimization objective to avoid competition with target class, and thus generating more high-confidence predictions for selecting pseudo-label. ANL introduces the additional negative pseudo-label for all unlabeled data to leverage low-confidence examples. It adaptively allocates this label by dynamically evaluating the top-k performance of the model. EML and ANL do not introduce any additional parameter and hyperparameter. We integrate these techniques with FixMatch, and develop a simple yet powerful framework called FullMatch. Extensive experiments on several common SSL benchmarks (CIFAR-10/100, SVHN, STL-10 and ImageNet) demonstrate that FullMatch exceeds FixMatch by a large margin. Integrated with FlexMatch (an advanced FixMatch-based framework), we achieve state-of-the-art performance. Source code is available at https://github.com/megvii-research/FullMatch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2064.CHMATCH: Contrastive Hierarchical Matching and Robust Adaptive Threshold Boosted Semi-Supervised Learning</span><br>
                <span class="as">Wu, JianlongandYang, HaozheandGan, TianandDing, NingandJiang, FeijunandNie, Liqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CHMATCH_Contrastive_Hierarchical_Matching_and_Robust_Adaptive_Threshold_Boosted_Semi-Supervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15762-15772.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督学习方法FixMatch和FlexMatch在处理少量标注样本时，存在阈值固定或自适应但不稳定的问题，且特征表示不具有区分性。<br>
                    动机：提出一种新的CHMatch方法，通过对比分层匹配学习鲁棒的自适应阈值和区别性特征。<br>
                    方法：首先，使用基于记忆库的鲁棒阈值学习策略选择高置信度样本；其次，充分利用层次标签中的结构化信息，学习准确的亲和力图进行对比学习。<br>
                    效果：实验结果表明，CHMatch在多个常用基准测试上表现出非常稳定和优越的结果。例如，在CIFAR-100上，仅每个类别有4个和25个标注样本时，CHMatch分别比FlexMatch减少了8.44%和9.02%的错误率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recently proposed FixMatch and FlexMatch have achieved remarkable results in the field of semi-supervised learning. But these two methods go to two extremes as FixMatch and FlexMatch use a pre-defined constant threshold for all classes and an adaptive threshold for each category, respectively. By only investigating consistency regularization, they also suffer from unstable results and indiscriminative feature representation, especially under the situation of few labeled samples. In this paper, we propose a novel CHMatch method, which can learn robust adaptive thresholds for instance-level prediction matching as well as discriminative features by contrastive hierarchical matching. We first present a memory-bank based robust threshold learning strategy to select highly-confident samples. In the meantime, we make full use of the structured information in the hierarchical labels to learn an accurate affinity graph for contrastive learning. CHMatch achieves very stable and superior results on several commonly-used benchmarks. For example, CHMatch achieves 8.44% and 9.02% error rate reduction over FlexMatch on CIFAR-100 under WRN-28-2 with only 4 and 25 labeled samples per class, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2065.Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers</span><br>
                <span class="as">Lu, ChenyanganddeGeus, DaanandDubbelman, Gijs</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23631-23640.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提高使用视觉转换器的语义分割网络的计算效率。<br>
                    动机：现有的令牌减少方法主要用于改进基于ViT的图像分类网络的效率，但这些方法不能直接应用于语义分割。<br>
                    方法：本文提出了一种内容感知令牌共享（CTS）方法，该方法通过预测图像补丁是否包含相同的语义类别并让它们共享一个令牌来利用这种冗余信息。<br>
                    效果：实验表明，使用内容感知令牌共享，我们可以将处理的令牌数量减少多达44%，而不降低分割质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper introduces Content-aware Token Sharing (CTS), a token reduction approach that improves the computational efficiency of semantic segmentation networks that use Vision Transformers (ViTs). Existing works have proposed token reduction approaches to improve the efficiency of ViT-based image classification networks, but these methods are not directly applicable to semantic segmentation, which we address in this work. We observe that, for semantic segmentation, multiple image patches can share a token if they contain the same semantic class, as they contain redundant information. Our approach leverages this by employing an efficient, class-agnostic policy network that predicts if image patches contain the same semantic class, and lets them share a token if they do. With experiments, we explore the critical design choices of CTS and show its effectiveness on the ADE20K, Pascal Context and Cityscapes datasets, various ViT backbones, and different segmentation decoders. With Content-aware Token Sharing, we are able to reduce the number of processed tokens by up to 44%, without diminishing the segmentation quality.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2066.Incrementer: Transformer for Class-Incremental Semantic Segmentation With Knowledge Distillation Focusing on Old Class</span><br>
                <span class="as">Shang, ChaoandLi, HongliangandMeng, FanmanandWu, QingboandQiu, HeqianandWang, Lanxiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Incrementer_Transformer_for_Class-Incremental_Semantic_Segmentation_With_Knowledge_Distillation_Focusing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7214-7224.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在学习新类别的同时，保持对旧类别的分割能力，并解决灾难性遗忘的问题。<br>
                    动机：现有的方法主要基于卷积神经网络，通过知识蒸馏防止遗忘，但这需要添加额外的卷积层来预测新类别，并在知识蒸馏过程中忽略了区分新旧类别的不同区域，限制了新类别的学习。<br>
                    方法：提出了一种新的Transformer框架——Incrementer，用于类别增量语义分割。只需在Transformer解码器中添加新类别的令牌即可学习新类别。同时，提出了一种新的知识蒸馏方案，专注于旧类别区域的蒸馏，降低了旧模型对新类别学习的约束，提高了可塑性。此外，还提出了一种类别去混淆策略，以减轻对新类别的过拟合和相似类别的混淆。<br>
                    效果：实验结果表明，该方法比现有技术有显著改进（在Pascal VOC和ADE20k上分别提升了5-15个百分点）。希望Incrementer能成为类别增量语义分割的新的强大管道。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Class-incremental semantic segmentation aims to incrementally learn new classes while maintaining the capability to segment old ones, and suffers catastrophic forgetting since the old-class labels are unavailable. Most existing methods are based on convolutional networks and prevent forgetting through knowledge distillation, which (1) need to add additional convolutional layers to predict new classes, and (2) ignore to distinguish different regions corresponding to old and new classes during knowledge distillation and roughly distill all the features, thus limiting the learning of new classes. Based on the above observations, we propose a new transformer framework for class-incremental semantic segmentation, dubbed Incrementer, which only needs to add new class tokens to the transformer decoder for new-class learning. Based on the Incrementer, we propose a new knowledge distillation scheme that focuses on the distillation in the old-class regions, which reduces the constraints of the old model on the new-class learning, thus improving the plasticity. Moreover, we propose a class deconfusion strategy to alleviate the overfitting to new classes and the confusion of similar classes. Our method is simple and effective, and extensive experiments show that our method outperforms the SOTAs by a large margin (5 15 absolute points boosts on both Pascal VOC and ADE20k). We hope that our Incrementer can serve as a new strong pipeline for class-incremental semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2067.Zero-Shot Object Counting</span><br>
                <span class="as">Xu, JingyiandLe, HieuandNguyen, VuandRanjan, VireshandSamaras, Dimitris</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Zero-Shot_Object_Counting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15548-15557.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现在测试阶段对任意类别的对象进行计数，特别是在没有人类注释示例的情况下。<br>
                    动机：现有的方法需要人类注释的示例作为输入，这在新的类别，特别是自主系统中通常是不可用的。因此，我们提出了零样本对象计数（ZSC），这是一种在测试阶段仅使用类名的新设置。<br>
                    方法：我们从类名开始，提出一种方法来准确识别最优的补丁，然后将其用作计数示例。具体来说，我们首先构建一个类原型来选择可能包含目标对象的补丁，即与类相关的补丁。此外，我们还引入了一个模型，可以定量测量任意补丁作为计数示例的合适程度。通过将此模型应用于所有候选补丁，我们可以选择最合适的补丁作为计数示例。<br>
                    效果：我们在最近的类别无关计数数据集FSC-147上进行的实验结果验证了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Class-agnostic object counting aims to count object instances of an arbitrary class at test time. It is challenging but also enables many potential applications. Current methods require human-annotated exemplars as inputs which are often unavailable for novel categories, especially for autonomous systems. Thus, we propose zero-shot object counting (ZSC), a new setting where only the class name is available during test time. Such a counting system does not require human annotators in the loop and can operate automatically. Starting from a class name, we propose a method that can accurately identify the optimal patches which can then be used as counting exemplars. Specifically, we first construct a class prototype to select the patches that are likely to contain the objects of interest, namely class-relevant patches. Furthermore, we introduce a model that can quantitatively measure how suitable an arbitrary patch is as a counting exemplar. By applying this model to all the candidate patches, we can select the most suitable patches as exemplars for counting. Experimental results on a recent class-agnostic counting dataset, FSC-147, validate the effectiveness of our method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2068.CORA: Adapting CLIP for Open-Vocabulary Detection With Region Prompting and Anchor Pre-Matching</span><br>
                <span class="as">Wu, XiaoshiandZhu, FengandZhao, RuiandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7031-7040.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决开放词汇检测（OVD）任务中的问题，即如何从训练模型的基础类别之外识别新的对象类别。<br>
                    动机：现有的OVD方法依赖于大规模视觉-语言预训练模型，如CLIP，来识别新的对象。然而，这些模型在应用于检测器训练时存在两个核心障碍：1）将整个图像上训练的视觉-语言模型应用于区域识别任务时产生的分布不匹配；2）定位未见过类别的对象的困难。<br>
                    方法：为了克服这些障碍，作者提出了CORA，这是一个基于DETR的框架，通过区域提示和锚点预匹配来调整CLIP以适应开放词汇检测。区域提示通过提示基于CLIP的区域分类器的区域特征来缓解整个到区域的分布差距。锚点预匹配通过类感知匹配机制帮助学习可泛化的对象定位。<br>
                    效果：在COCO OVD基准测试中，CORA实现了41.7 AP50的新类别，比之前的SOTA高出2.4 AP50，即使没有使用额外的训练数据。当有额外的训练数据可用时，作者训练了CORA+，它在COCO OVD基准测试上实现了43.1 AP50，在LVIS OVD基准测试上实现了28.1 box APr。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories beyond the base categories on which the detector is trained. Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detector training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adapts CLIP for Open-vocabulary detection by Region prompting and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the region features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localization by a class-aware matching mechanism. We evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel classes, which outperforms the previous SOTA by 2.4 AP50 even without resorting to extra training data. When extra training data is available, we train CORA+ on both ground-truth base-category annotations and additional pseudo bounding box labels computed by CORA. CORA+ achieves 43.1 AP50 on the COCO OVD benchmark and 28.1 box APr on the LVIS OVD benchmark. The code is available at https://github.com/tgxs002/CORA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2069.FedSeg: Class-Heterogeneous Federated Learning for Semantic Segmentation</span><br>
                <span class="as">Miao, JiaxuandYang, ZongxinandFan, LeileiandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Miao_FedSeg_Class-Heterogeneous_Federated_Learning_for_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8042-8052.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在保护数据隐私的同时，进行分布式学习以改进语义分割任务。<br>
                    动机：尽管存在许多用于分类任务的联邦学习算法，但很少有工作关注更具挑战性的语义分割任务，特别是在类别异构的联邦学习情况下。<br>
                    方法：我们提出了FedSeg，一种基本的联邦学习方法，用于处理类别异构的语义分割。我们首先提出了一种简单但强大的修改过的交叉熵损失函数，以修正局部优化并解决前景-背景不一致的问题。在此基础上，我们引入了像素级的对比学习，以强制本地像素嵌入属于全局语义空间。<br>
                    效果：我们在四个语义分割基准（Cityscapes，CamVID，PascalVOC和ADE20k）上进行了大量实验，证明了我们的FedSeg的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Federated Learning (FL) is a distributed learning paradigm that collaboratively learns a global model across multiple clients with data privacy-preserving. Although many FL algorithms have been proposed for classification tasks, few works focus on more challenging semantic seg-mentation tasks, especially in the class-heterogeneous FL situation. Compared with classification, the issues from heterogeneous FL for semantic segmentation are more severe: (1) Due to the non-IID distribution, different clients may contain inconsistent foreground-background classes, resulting in divergent local updates. (2) Class-heterogeneity for complex dense prediction tasks makes the local optimum of clients farther from the global optimum. In this work, we propose FedSeg, a basic federated learning approach for class-heterogeneous semantic segmentation. We first propose a simple but strong modified cross-entropy loss to correct the local optimization and address the foreground-background inconsistency problem. Based on it, we introduce pixel-level contrastive learning to enforce local pixel embeddings belonging to the global semantic space. Extensive experiments on four semantic segmentation benchmarks (Cityscapes, CamVID, PascalVOC and ADE20k) demonstrate the effectiveness of our FedSeg. We hope this work will attract more attention from the FL community to the challenging semantic segmentation federated learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2070.DPF: Learning Dense Prediction Fields With Weak Supervision</span><br>
                <span class="as">Chen, XiaoxueandZheng, YuhangandZheng, YupengandZhou, QiangandZhao, HaoandZhou, GuyueandZhang, Ya-Qin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DPF_Learning_Dense_Prediction_Fields_With_Weak_Supervision_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15347-15357.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用廉价的点级弱监督进行视觉场景理解。<br>
                    动机：像素级的密集标注既昂贵又不可能，因此需要利用便宜的点级弱监督。<br>
                    方法：提出一种新的预测范式，对点坐标查询进行预测，该方法被称为密集预测场（DPFs）。<br>
                    效果：通过三个大规模的公共数据集PascalContext、ADE20k和IIW进行基准测试，DPFs在所有数据集上都取得了新的最先进的性能，且优势显著。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Nowadays, many visual scene understanding problems are addressed by dense prediction networks. But pixel-wise dense annotations are very expensive (e.g., for scene parsing) or impossible (e.g., for intrinsic image decomposition), motivating us to leverage cheap point-level weak supervision. However, existing pointly-supervised methods still use the same architecture designed for full supervision. In stark contrast to them, we propose a new paradigm that makes predictions for point coordinate queries, as inspired by the recent success of implicit representations, like distance or radiance fields. As such, the method is named as dense prediction fields (DPFs). DPFs generate expressive intermediate features for continuous sub-pixel locations, thus allowing outputs of an arbitrary resolution. DPFs are naturally compatible with point-level supervision. We showcase the effectiveness of DPFs using two substantially different tasks: high-level semantic parsing and low-level intrinsic image decomposition. In these two cases, supervision comes in the form of single-point semantic category and two-point relative reflectance, respectively. As benchmarked by three large-scale public datasets PascalContext, ADE20k and IIW, DPFs set new state-of-the-art performance on all of them with significant margins. Code can be accessed at https://github.com/cxx226/DPF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2071.Task-Specific Fine-Tuning via Variational Information Bottleneck for Weakly-Supervised Pathology Whole Slide Image Classification</span><br>
                <span class="as">Li, HonglinandZhu, ChengluandZhang, YunlongandSun, YuxuanandShui, ZhongyiandKuang, WenweiandZheng, SunyiandYang, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Task-Specific_Fine-Tuning_via_Variational_Information_Bottleneck_for_Weakly-Supervised_Pathology_Whole_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7454-7463.png><br>
            
            <span class="tt"><span class="t0">研究问题：多重实例学习（MIL）在数字病理全幅幻灯片图像（WSI）分析中表现出了良好的效果，但因为高计算成本和对千万像素WSI的有限监督，这种范例仍然面临着性能和泛化的问题。<br>
                    动机：为了解决计算问题，先前的方法利用从ImageNet预训练的冻结模型来获取表示，但由于大领域差距，可能会丢失关键信息，且没有图像级别的训练时间增强会阻碍泛化能力。尽管自我监督学习（SSL）提出了可行的表示学习方案，但通过部分标签微调的特定任务特征尚未被探索。<br>
                    方法：我们提出了一种有效的WSI微调框架，该框架受到信息瓶颈理论的启发，能够找到WSI的最小充分统计量，从而将主干网络微调为仅依赖于WSI级别弱标签的特定任务表示。<br>
                    效果：我们在五个病理WSI数据集上的各种WSI头部进行了实验，结果表明，与先前的工作相比，该方法在准确性和泛化性方面都有显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While Multiple Instance Learning (MIL) has shown promising results in digital Pathology Whole Slide Image (WSI) analysis, such a paradigm still faces performance and generalization problems due to high computational costs and limited supervision of Gigapixel WSIs. To deal with the computation problem, previous methods utilize a frozen model pretrained from ImageNet to obtain representations, however, it may lose key information owing to the large domain gap and hinder the generalization ability without image-level training-time augmentation. Though Self-supervised Learning (SSL) proposes viable representation learning schemes, the downstream task-specific features via partial label tuning are not explored. To alleviate this problem, we propose an efficient WSI fine-tuning framework motivated by the Information Bottleneck theory. The theory enables the framework to find the minimal sufficient statistics of WSI, thus supporting us to fine-tune the backbone into a task-specific representation only depending on WSI-level weak labels. The WSI-MIL problem is further analyzed to theoretically deduce our fine-tuning method. We evaluate the method on five pathological WSI datasets on various WSI heads. The experimental results show significant improvements in both accuracy and generalization compared with previous works. Source code will be available at https://github.com/invoker-LL/WSI-finetuning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2072.Detecting Everything in the Open World: Towards Universal Object Detection</span><br>
                <span class="as">Wang, ZhenyuandLi, YaliandChen, XiandLim, Ser-NamandTorralba, AntonioandZhao, HengshuangandWang, Shengjin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11433-11443.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决通用物体检测问题，目标是检测每个场景并预测每个类别。<br>
                    动机：传统的物体检测器对人工标注的依赖、视觉信息的局限性以及开放世界中的新类别严重限制了其通用性。<br>
                    方法：我们提出了UniDetector，一种能够识别开放世界中大量类别的通用物体检测器。关键之处在于：1）通过图像和文本空间的对齐，利用多源图像和异构标签空间进行训练，保证了通用表示的充分信息；2）由于视觉和语言模态的丰富信息，易于扩展到开放世界，同时保持已见和未见类别之间的平衡；3）通过提出的解耦训练方式和概率校准进一步提升对新类别的泛化能力。<br>
                    效果：这些贡献使UniDetector能够检测到超过7k个类别，是目前可测量的最大类别大小，而只需要大约500个类别参与训练。在LVIS、ImageNetBoxes和VisualGenome等大型词汇表数据集上，UniDetector表现出强大的零样本泛化能力——在没有看到任何相应图像的情况下，平均超越传统监督基线4%以上。在包含各种场景的13个公共检测数据集上，UniDetector也仅用3%的训练数据就实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we formally address universal object detection, which aims to detect every scene and predict every category. The dependence on human annotations, the limited visual information, and the novel categories in the open world severely restrict the universality of traditional detectors. We propose UniDetector, a universal object detector that has the ability to recognize enormous categories in the open world. The critical points for the universality of UniDetector are: 1) it leverages images of multiple sources and heterogeneous label spaces for training through the alignment of image and text spaces, which guarantees sufficient information for universal representations. 2) it generalizes to the open world easily while keeping the balance between seen and unseen classes, thanks to abundant information from both vision and language modalities. 3) it further promotes the generalization ability to novel categories through our proposed decoupling training manner and probability calibration. These contributions allow UniDetector to detect over 7k categories, the largest measurable category size so far, with only about 500 classes participating in training. Our UniDetector behaves the strong zero-shot generalization ability on large-vocabulary datasets like LVIS, ImageNetBoxes, and VisualGenome - it surpasses the traditional supervised baselines by more than 4% on average without seeing any corresponding images. On 13 public detection datasets with various scenes, UniDetector also achieves state-of-the-art performance with only a 3% amount of training data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2073.Look Before You Match: Instance Understanding Matters in Video Object Segmentation</span><br>
                <span class="as">Wang, JunkeandChen, DongdongandWu, ZuxuanandLuo, ChongandTang, ChuanxinandDai, XiyangandZhao, YuchengandXie, YujiaandYuan, LuandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Look_Before_You_Match_Instance_Understanding_Matters_in_Video_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2268-2278.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频对象分割（VOS）中，如何有效地处理目标和摄像机的移动带来的外观变化和视角变化。<br>
                    动机：当前的记忆基础方法在视频对象分割任务上取得了显著的效果，但由于缺乏实例理解能力，对目标和摄像机的移动导致的大外观变化或视角变化较为敏感。<br>
                    方法：提出了一种双分支网络进行视频对象分割，一个是基于查询的视频对象分割分支，用于获取当前帧的实例详细信息；另一个是视频对象分割分支，用于执行与记忆库的空间-时间匹配。同时，引入了多路径融合块来有效地结合记忆读取和来自实例分割解码器的多尺度特征。<br>
                    效果：该方法在DAVIS 2016/2017验证集、DAVIS 2017测试集以及YouTube-VOS 2018/2019验证集上均取得了最先进的性能，明显优于其他替代方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Exploring dense matching between the current frame and past frames for long-range context modeling, memory-based methods have demonstrated impressive results in video object segmentation (VOS) recently. Nevertheless, due to the lack of instance understanding ability, the above approaches are oftentimes brittle to large appearance variations or viewpoint changes resulted from the movement of objects and cameras. In this paper, we argue that instance understanding matters in VOS, and integrating it with memory-based matching can enjoy the synergy, which is intuitively sensible from the definition of VOS task, i.e., identifying and segmenting object instances within the video. Towards this goal, we present a two-branch network for VOS, where the query-based instance segmentation (IS) branch delves into the instance details of the current frame and the VOS branch performs spatial-temporal matching with the memory bank. We employ the well-learned object queries from IS branch to inject instance-specific information into the query key, with which the instance-augmented matching is further performed. In addition, we introduce a multi-path fusion block to effectively combine the memory readout with multi-scale features from the instance segmentation decoder, which incorporates high-resolution instance-aware features to produce final segmentation results. Our method achieves state-of-the-art performance on DAVIS 2016/2017 val (92.6% and 87.1%), DAVIS 2017 test-dev (82.8%), and YouTube-VOS 2018/2019 val (86.3% and 86.3%), outperforming alternative methods by clear margins.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2074.Orthogonal Annotation Benefits Barely-Supervised Medical Image Segmentation</span><br>
                <span class="as">Cai, HengandLi, ShumengandQi, LeiandYu, QianandShi, YinghuanandGao, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Orthogonal_Annotation_Benefits_Barely-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3302-3311.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高3D医疗图像分割的半监督学习性能。<br>
                    动机：与2D图像相比，3D医疗体积包含来自不同方向的信息，如横截面、矢状面和冠状面，可以提供互补的视图。这些互补的视图和相邻3D切片的内在相似性激发了我们开发一种新的注释方式和相应的半监督模型进行有效分割。<br>
                    方法：我们首先提出了正交标注，只标注一个已标记体积的两个正交切片，大大减轻了标注负担。然后，通过注册获取稀疏标记体积的初始伪标签。接着，引入未标记体积，我们提出了一种名为密集-稀疏共同训练（DeSCO）的双重网络范式，该范式在早期阶段利用密集伪标签，在后期阶段利用稀疏标签，并同时强制两个网络的输出一致。<br>
                    效果：我们在三个基准数据集上的实验结果验证了我们的方法在性能和效率上的效果。例如，仅使用10个标注切片，我们的方法在KiTS19数据集上达到86.93%的Dice。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent trends in semi-supervised learning have significantly boosted the performance of 3D semi-supervised medical image segmentation. Compared with 2D images, 3D medical volumes involve information from different directions, e.g., transverse, sagittal, and coronal planes, so as to naturally provide complementary views. These complementary views and the intrinsic similarity among adjacent 3D slices inspire us to develop a novel annotation way and its corresponding semi-supervised model for effective segmentation. Specifically, we firstly propose the orthogonal annotation by only labeling two orthogonal slices in a labeled volume, which significantly relieves the burden of annotation. Then, we perform registration to obtain the initial pseudo labels for sparsely labeled volumes. Subsequently, by introducing unlabeled volumes, we propose a dual-network paradigm named Dense-Sparse Co-training (DeSCO) that exploits dense pseudo labels in early stage and sparse labels in later stage and meanwhile forces consistent output of two networks. Experimental results on three benchmark datasets validated our effectiveness in performance and efficiency in annotation. For example, with only 10 annotated slices, our method reaches a Dice up to 86.93% on KiTS19 dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2075.Knowledge Distillation for 6D Pose Estimation by Aligning Distributions of Local Predictions</span><br>
                <span class="as">Guo, ShuxuanandHu, YinlinandAlvarez, JoseM.andSalzmann, Mathieu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Knowledge_Distillation_for_6D_Pose_Estimation_by_Aligning_Distributions_of_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18633-18642.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像基6D物体姿态估计中的知识蒸馏问题。<br>
                    动机：尽管知识蒸馏在许多任务中取得了巨大成功，但在图像基6D物体姿态估计中尚未得到研究。<br>
                    方法：提出一种由6D姿态估计任务驱动的知识蒸馏方法，将教师网络的局部预测分布蒸馏到学生网络中，以帮助其训练。<br>
                    效果：实验结果表明，该方法在不同的紧凑型学生模型和基于关键点和密集预测的架构上均取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Knowledge distillation facilitates the training of a compact student network by using a deep teacher one. While this has achieved great success in many tasks, it remains completely unstudied for image-based 6D object pose estimation. In this work, we introduce the first knowledge distillation method driven by the 6D pose estimation task. To this end, we observe that most modern 6D pose estimation frameworks output local predictions, such as sparse 2D keypoints or dense representations, and that the compact student network typically struggles to predict such local quantities precisely. Therefore, instead of imposing prediction-to-prediction supervision from the teacher to the student, we propose to distill the teacher's distribution of local predictions into the student network, facilitating its training. Our experiments on several benchmarks show that our distillation method yields state-of-the-art results with different compact student models and for both keypoint-based and dense prediction-based architectures.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2076.Three Guidelines You Should Know for Universally Slimmable Self-Supervised Learning</span><br>
                <span class="as">Cao, Yun-HaoandSun, PeiqinandZhou, Shuchang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Three_Guidelines_You_Should_Know_for_Universally_Slimmable_Self-Supervised_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15742-15751.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现一种通用的可瘦身自监督学习（US3L），以在不同的设备上部署自监督模型时获得更好的精度-效率权衡。<br>
                    动机：直接将自监督学习（SSL）适应到通用可瘦身网络会导致训练过程频繁崩溃，因此需要找到一种方法来保证SSL的成功。<br>
                    方法：提出了三个损失设计指南，从统一梯度的角度确保了时间一致性，并提出了动态采样和组正则化策略，以提高训练效率和准确性。<br>
                    效果：在卷积神经网络和视觉变换器上进行了实证验证，仅进行一次训练和权重复制，该方法在识别、目标检测和实例分割等基准测试中优于各种最先进的方法（单独训练或未训练）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose universally slimmable self-supervised learning (dubbed as US3L) to achieve better accuracy-efficiency trade-offs for deploying self-supervised models across different devices. We observe that direct adaptation of self-supervised learning (SSL) to universally slimmable networks misbehaves as the training process frequently collapses. We then discover that temporal consistent guidance is the key to the success of SSL for universally slimmable networks, and we propose three guidelines for the loss design to ensure this temporal consistency from a unified gradient perspective. Moreover, we propose dynamic sampling and group regularization strategies to simultaneously improve training efficiency and accuracy. Our US3L method has been empirically validated on both convolutional neural networks and vision transformers. With only once training and one copy of weights, our method outperforms various state-of-the-art methods (individually trained or not) on benchmarks including recognition, object detection and instance segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2077.Learning Articulated Shape With Keypoint Pseudo-Labels From Web Images</span><br>
                <span class="as">Stathopoulos, AnastasisandPavlakos, GeorgiosandHan, LigongandMetaxas, DimitrisN.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Stathopoulos_Learning_Articulated_Shape_With_Keypoint_Pseudo-Labels_From_Web_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13092-13101.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用少量标注的2D关键点图像进行具有关节结构的物体（如马、牛、羊）的单目3D重建模型学习。<br>
                    动机：目前的模型需要大量的标注数据才能进行3D重建，而本文提出的方法只需要50-150张标注图像即可实现。<br>
                    方法：通过训练特定类别的2D关键点估计器，在未标注的网络图像上生成2D关键点伪标签，并使用已标注和自标注的数据集来训练3D重建模型。<br>
                    效果：实验结果表明，该方法能够有效地利用网络图像，提高了几种具有关节结构的物体类别的3D重建性能，超过了全监督基线。此外，该方法可以快速启动一个模型，并且只需要少量的标注2D关键点图像。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper shows that it is possible to learn models for monocular 3D reconstruction of articulated objects (e.g. horses, cows, sheep), using as few as 50-150 images labeled with 2D keypoints. Our proposed approach involves training category-specific keypoint estimators, generating 2D keypoint pseudo-labels on unlabeled web images, and using both the labeled and self-labeled sets to train 3D reconstruction models. It is based on two key insights: (1) 2D keypoint estimation networks trained on as few as 50-150 images of a given object category generalize well and generate reliable pseudo-labels; (2) a data selection mechanism can automatically create a "curated" subset of the unlabeled web images that can be used for training -- we evaluate four data selection methods. Coupling these two insights enables us to train models that effectively utilize web images, resulting in improved 3D reconstruction performance for several articulated object categories beyond the fully-supervised baseline. Our approach can quickly bootstrap a model and requires only a few images labeled with 2D keypoints. This requirement can be easily satisfied for any new object category. To showcase the practicality of our approach for predicting the 3D shape of arbitrary object categories, we annotate 2D keypoints on 250 giraffe and bear images from COCO in just 2.5 hours per category.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2078.Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation</span><br>
                <span class="as">Shi, MinandHuang, ZihaoandMa, XianzhengandHu, XiaoweiandCao, Zhiguo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7308-7317.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确预测任意类别的姿态关键点？<br>
                    动机：现有的方法依赖于图像匹配进行定位，但这种单阶段匹配模式的准确性较差，且易受开放集特性影响产生噪声。<br>
                    方法：提出了一种两阶段框架，将第一阶段匹配的关键点视为相似性感知的位置建议，然后模型学习提取相关特征以修正初始建议。并使用专为CAPE设计的变压器模型进行实例化。<br>
                    效果：该方法在CAPE基准MP-100上的准确性和效率上都大幅超越了之前的最佳方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Category-agnostic pose estimation (CAPE) aims to predict keypoints for arbitrary categories given support images with keypoint annotations. Existing approaches match the keypoints across the image for localization. However, such a one-stage matching paradigm shows inferior accuracy: the prediction heavily relies on the matching results, which can be noisy due to the open set nature in CAPE. For example, two mirror-symmetric keypoints (e.g., left and right eyes) in the query image can both trigger high similarity on certain support keypoints (eyes), which leads to duplicated or opposite predictions. To calibrate the inaccurate matching results, we introduce a two-stage framework, where matched keypoints from the first stage are viewed as similarity-aware position proposals. Then, the model learns to fetch relevant features to correct the initial proposals in the second stage. We instantiate the framework with a transformer model tailored for CAPE. The transformer encoder incorporates specific designs to improve the representation and similarity modeling in the first matching stage. In the second stage, similarity-aware proposals are packed as queries in the decoder for refinement via cross-attention. Our method surpasses the previous best approach by large margins on CAPE benchmark MP-100 on both accuracy and efficiency. Code available at https://github.com/flyinglynx/CapeFormer</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2079.LaserMix for Semi-Supervised LiDAR Semantic Segmentation</span><br>
                <span class="as">Kong, LingdongandRen, JiaweiandPan, LiangandLiu, Ziwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21705-21715.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用未标记的数据进行激光雷达语义分割。<br>
                    动机：由于标注激光雷达点云数据的成本较高，这限制了全监督学习方法的可扩展性。<br>
                    方法：提出了一种名为LaserMix的半监督学习方法，通过混合不同激光雷达扫描的激光束，并鼓励模型在混合前后做出一致和自信的预测。<br>
                    效果：在流行的激光雷达分割数据集（nuScenes、SemanticKITTI和ScribbleKITTI）上进行的全面实验分析表明，该方法具有高效性和优越性。特别是在有2到5倍更少标签的情况下，其结果与全监督方法相当，并在有监督的基准线上取得了显著的10.8%的提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Densely annotating LiDAR point clouds is costly, which often restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR semantic segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties. 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we achieve competitive results over fully-supervised counterparts with 2x to 5x fewer labels and improve the supervised-only baseline significantly by relatively 10.8%. We hope this concise yet high-performing framework could facilitate future research in semi-supervised LiDAR segmentation. Code is publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2080.Category Query Learning for Human-Object Interaction Classification</span><br>
                <span class="as">Xie, ChiandZeng, FangaoandHu, YueandLiang, ShuangandWei, Yichen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Category_Query_Learning_for_Human-Object_Interaction_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15275-15284.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的方法，通过学习类别查询来改善人类-物体交互特征。<br>
                    动机：大多数先前的HOI方法都集中在学习更好的人类-物体特征上，而我们提出了一种新颖且互补的方法，称为类别查询学习。<br>
                    方法：将查询明确地关联到交互类别，通过转换器解码器转换为图像特定的类别表示，并通过辅助的图像级分类任务进行学习。<br>
                    效果：该方法在三个代表性的HOI基线上进行了验证，并在两个基准测试中实现了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unlike most previous HOI methods that focus on learning better human-object features, we propose a novel and complementary approach called category query learning. Such queries are explicitly associated to interaction categories, converted to image specific category representation via a transformer decoder, and learnt via an auxiliary image-level classification task. This idea is motivated by an earlier multi-label image classification method, but is for the first time applied for the challenging human-object interaction classification task. Our method is simple, general and effective. It is validated on three representative HOI baselines and achieves new state-of-the-art results on two benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2081.MDQE: Mining Discriminative Query Embeddings To Segment Occluded Instances on Challenging Videos</span><br>
                <span class="as">Li, MinghanandLi, ShuaiandXiang, WangmengandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MDQE_Mining_Discriminative_Query_Embeddings_To_Segment_Occluded_Instances_on_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10524-10533.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频实例分割（VIS）方法在处理遮挡物体和拥挤场景的挑战性视频时，由于实例查询无法有效编码实例的判别嵌入，导致难以区分“困难”实例。<br>
                    动机：为了解决这些问题，我们提出了挖掘判别查询嵌入（MDQE）的方法来分割挑战性视频中的遮挡实例。<br>
                    方法：首先，通过考虑空间上下文信息和帧间物体运动，初始化对象查询的位置嵌入和内容特征。其次，提出一种实例间掩膜排斥损失，使每个实例远离其附近的非目标实例。<br>
                    效果：提出的MDQE是首个在具有挑战性的视频上取得最新成果，并在简单视频上具有竞争力表现的基于每片段输入的视频实例分割方法。具体来说，使用ResNet50的MDQE在OVIS和YouTube-VIS 2021上分别实现了33.0%和44.5%的掩膜AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While impressive progress has been achieved, video instance segmentation (VIS) methods with per-clip input often fail on challenging videos with occluded objects and crowded scenes. This is mainly because instance queries in these methods cannot encode well the discriminative embeddings of instances, making the query-based segmenter difficult to distinguish those 'hard' instances. To address these issues, we propose to mine discriminative query embeddings (MDQE) to segment occluded instances on challenging videos. First, we initialize the positional embeddings and content features of object queries by considering their spatial contextual information and the inter-frame object motion. Second, we propose an inter-instance mask repulsion loss to distance each instance from its nearby non-target instances. The proposed MDQE is the first VIS method with per-clip input that achieves state-of-the-art results on challenging videos and competitive performance on simple videos. In specific, MDQE with ResNet50 achieves 33.0% and 44.5% mask AP on OVIS and YouTube-VIS 2021, respectively. Code of MDQE can be found at https://github.com/MinghanLi/MDQE_CVPR2023.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2082.Class Attention Transfer Based Knowledge Distillation</span><br>
                <span class="as">Guo, ZiyaoandYan, HaonanandLi, HuiandLin, Xiaodong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Class_Attention_Transfer_Based_Knowledge_Distillation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11868-11877.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的知识蒸馏方法在模型压缩任务上表现优秀，但其转移的知识如何帮助提升学生网络的性能难以解释。<br>
                    动机：主流的CNN模型通过识别输入的类别判别区域来进行分类，这种能力可以通过转移类激活图来获取和增强。<br>
                    方法：提出一种具有高解释性和竞争力的知识蒸馏方法——基于类注意力转移的知识蒸馏（CAT-KD）。<br>
                    效果：CAT-KD不仅提高了解释性，也有助于更好地理解CNN，同时在多个基准测试中实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous knowledge distillation methods have shown their impressive performance on model compression tasks, however, it is hard to explain how the knowledge they transferred helps to improve the performance of the student network. In this work, we focus on proposing a knowledge distillation method that has both high interpretability and competitive performance. We first revisit the structure of mainstream CNN models and reveal that possessing the capacity of identifying class discriminative regions of input is critical for CNN to perform classification. Furthermore, we demonstrate that this capacity can be obtained and enhanced by transferring class activation maps. Based on our findings, we propose class attention transfer based knowledge distillation (CAT-KD). Different from previous KD methods, we explore and present several properties of the knowledge transferred by our method, which not only improve the interpretability of CAT-KD but also contribute to a better understanding of CNN. While having high interpretability, CAT-KD achieves state-of-the-art performance on multiple benchmarks. Code is available at: https://github.com/GzyAftermath/CAT-KD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2083.Weakly Supervised Segmentation With Point Annotations for Histopathology Images via Contrast-Based Variational Model</span><br>
                <span class="as">Zhang, HongrunandBurrows, LiamandMeng, YandaandSculthorpe, DeclanandMukherjee, AbhikandCoupland, SarahE.andChen, KeandZheng, Yalin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Weakly_Supervised_Segmentation_With_Point_Annotations_for_Histopathology_Images_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15630-15640.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低标注成本，提高图像分割效果。<br>
                    动机：对于形态变化大、形状不规则的组织病理学图像，获取标注数据的成本高且困难。<br>
                    方法：提出一种基于对比的变分模型生成分割结果，作为训练深度分割模型的可靠补充监督。<br>
                    效果：该方法能生成更一致的区域和更平滑的边界分割，对未标记的“新”区域更具鲁棒性。在两个不同的组织病理学数据集上进行的实验表明，其效果和效率优于先前的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image segmentation is a fundamental task in the field of imaging and vision. Supervised deep learning for segmentation has achieved unparalleled success when sufficient training data with annotated labels are available. However, annotation is known to be expensive to obtain, especially for histopathology images where the target regions are usually with high morphology variations and irregular shapes. Thus, weakly supervised learning with sparse annotations of points is promising to reduce the annotation workload. In this work, we propose a contrast-based variational model to generate segmentation results, which serve as reliable complementary supervision to train a deep segmentation model for histopathology images. The proposed method considers the common characteristics of target regions in histopathology images and can be trained in an end-to-end manner. It can generate more regionally consistent and smoother boundary segmentation, and is more robust to unlabeled 'novel' regions. Experiments on two different histology datasets demonstrate its effectiveness and efficiency in comparison to previous models. Code is available at: https://github.com/hrzhang1123/CVM_WS_Segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2084.Compositor: Bottom-Up Clustering and Compositing for Robust Part and Object Segmentation</span><br>
                <span class="as">He, JuandChen, JienengandLin, Ming-XianandYu, QihangandYuille, AlanL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Compositor_Bottom-Up_Clustering_and_Compositing_for_Robust_Part_and_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11259-11268.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种鲁棒的联合部分和对象分割方法。<br>
                    动机：现有的部分和对象分割方法往往忽视了低语义层次信息到高语义层次信息的整合，本研究希望通过自底向上的聚类方式解决这个问题。<br>
                    方法：将部分和对象分割转化为优化问题，构建包括像素、部分和对象级别的嵌入特征表示的分层特征表示，以自底向上的聚类方式进行解决。通过这种方式，像素被分为几个簇，其中部分级别的嵌入作为簇中心。然后，通过合成部分建议获得对象掩码。<br>
                    效果：实验结果表明，该方法在PartImageNet和Pascal-Part上取得了最先进的性能，相比之前的方法提高了约0.9%和1.3%的部分和对象mIoU，对遮挡的鲁棒性也提高了约4.4%和7.1%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we present a robust approach for joint part and object segmentation. Specifically, we reformulate object and part segmentation as an optimization problem and build a hierarchical feature representation including pixel, part, and object-level embeddings to solve it in a bottom-up clustering manner. Pixels are grouped into several clusters where the part-level embeddings serve as cluster centers. Afterwards, object masks are obtained by compositing the part proposals. This bottom-up interaction is shown to be effective in integrating information from lower semantic levels to higher semantic levels. Based on that, our novel approach Compositor produces part and object segmentation masks simultaneously while improving the mask quality. Compositor achieves state-of-the-art performance on PartImageNet and Pascal-Part by outperforming previous methods by around 0.9% and 1.3% on PartImageNet, 0.4% and 1.7% on Pascal-Part in terms of part and object mIoU and demonstrates better robustness against occlusion by around 4.4% and 7.1% on part and object respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2085.ZBS: Zero-Shot Background Subtraction via Instance-Level Background Modeling and Foreground Selection</span><br>
                <span class="as">An, YongqiandZhao, XuandYu, TaoandGuo, HaiyunandZhao, ChaoyangandTang, MingandWang, Jinqiao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/An_ZBS_Zero-Shot_Background_Subtraction_via_Instance-Level_Background_Modeling_and_Foreground_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6355-6364.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提取视频帧中的所有移动对象以获取二值前景分割掩码。<br>
                    动机：虽然深度学习已被广泛用于背景减除领域，但现有的无监督深度学习背景减除算法在复杂场景（如阴影或夜光）下表现不佳，且无法检测到预定义类别之外的对象。<br>
                    方法：提出了一种基于零样本目标检测的无监督背景减除算法，称为零样本背景减除ZBS。该方法充分利用了零样本目标检测的优点，建立了开放词汇实例级的背景模型。在此基础上，通过将新帧的检测结果与背景模型进行比较，可以有效地提取前景。<br>
                    效果：ZBS在复杂场景下表现出色，并且具有丰富和可扩展的类别。此外，我们的方法可以轻松地推广到其他任务，例如在未见过的环境中进行遗弃物体检测。实验表明，ZBS在CDnet 2014数据集上超越了最先进的无监督背景减除方法，F-Measure提高了4.70%。代码已在https://github.com/CASIA-IVA-Lab/ZBS上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Background subtraction (BGS) aims to extract all moving objects in the video frames to obtain binary foreground segmentation masks. Deep learning has been widely used in this field. Compared with supervised-based BGS methods, unsupervised methods have better generalization. However, previous unsupervised deep learning BGS algorithms perform poorly in sophisticated scenarios such as shadows or night lights, and they cannot detect objects outside the pre-defined categories. In this work, we propose an unsupervised BGS algorithm based on zero-shot object detection called Zero-shot Background Subtraction ZBS. The proposed method fully utilizes the advantages of zero-shot object detection to build the open-vocabulary instance-level background model. Based on it, the foreground can be effectively extracted by comparing the detection results of new frames with the background model. ZBS performs well for sophisticated scenarios, and it has rich and extensible categories. Furthermore, our method can easily generalize to other tasks, such as abandoned object detection in unseen environments. We experimentally show that ZBS surpasses state-of-the-art unsupervised BGS methods by 4.70% F-Measure on the CDnet 2014 dataset. The code is released at https://github.com/CASIA-IVA-Lab/ZBS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2086.Siamese DETR</span><br>
                <span class="as">Chen, ZerenandHuang, GengshiandLi, WeiandTeng, JianingandWang, KunandShao, JingandLoy, ChenChangeandSheng, Lu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Siamese_DETR_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15722-15731.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一种适用于DETR的自我监督预训练方法。<br>
                    动机：现有的自我监督方法主要针对基础模型如ResNets或ViTs进行表示学习，难以直接应用于具有任务特定Transformer模块的DETR。<br>
                    方法：提出Siamese DETR，一种用于DETR中Transformer架构的Siamese自我监督预训练方法。通过两个互补的任务，即局部化和鉴别，在一个新颖的多视图学习框架中同时学习视图不变和面向检测的表示。设计了两种自我监督预训练任务：（i）多视图区域检测，旨在学习在输入的增强视图之间定位感兴趣区域；（ii）多视图语义鉴别，尝试提高每个区域的物体级鉴别能力。<br>
                    效果：所提出的Siamese DETR在所有设置下，使用不同的DETR变体，在COCO和PASCAL VOC检测上实现了最先进的迁移性能。代码可在https://github.com/Zx55/SiameseDETR获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent self-supervised methods are mainly designed for representation learning with the base model, e.g., ResNets or ViTs. They cannot be easily transferred to DETR, with task-specific Transformer modules. In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture in DETR. We consider learning view-invariant and detection-oriented representations simultaneously through two complementary tasks, i.e., localization and discrimination, in a novel multi-view learning framework. Two self-supervised pretext tasks are designed: (i) Multi-View Region Detection aims at learning to localize regions-of-interest between augmented views of the input, and (ii) Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposed Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL VOC detection using different DETR variants in all setups. Code is available at https://github.com/Zx55/SiameseDETR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2087.Center Focusing Network for Real-Time LiDAR Panoptic Segmentation</span><br>
                <span class="as">Li, XiaoyanandZhang, GangandWang, BoyueandHu, YongliandYin, Baocai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Center_Focusing_Network_for_Real-Time_LiDAR_Panoptic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13425-13434.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现一种准确且实时的LiDAR全景分割方法，使自动驾驶车辆能够全面理解周围物体和场景。<br>
                    动机：现有的无提议方法虽然加速了算法，但由于难以建模不存在的实例中心以及昂贵的基于中心的聚类模块，其效果和效率仍然有限。<br>
                    方法：提出了一种新的中心聚焦网络（CFNet），其中包括中心聚焦特征编码（CFFE）和快速中心去重模块（CDM）。CFFE通过移动激光雷达点并填充中心点，明确理解原始激光雷达点与虚拟实例中心之间的关系。CDM则用于利用冗余检测的中心，为每个实例只选择一个中心。<br>
                    效果：在SemanticKITTI和nuScenes全景分割基准测试中，实验证明我们的CFNet比所有现有方法都有大幅度的提升，并且比最高效的方法快1.6倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR panoptic segmentation facilitates an autonomous vehicle to comprehensively understand the surrounding objects and scenes and is required to run in real time. The recent proposal-free methods accelerate the algorithm, but their effectiveness and efficiency are still limited owing to the difficulty of modeling non-existent instance centers and the costly center-based clustering modules. To achieve accurate and real-time LiDAR panoptic segmentation, a novel center focusing network (CFNet) is introduced. Specifically, the center focusing feature encoding (CFFE) is proposed to explicitly understand the relationships between the original LiDAR points and virtual instance centers by shifting the LiDAR points and filling in the center points. Moreover, to leverage the redundantly detected centers, a fast center deduplication module (CDM) is proposed to select only one center for each instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation benchmarks demonstrate that our CFNet outperforms all existing methods by a large margin and is 1.6 times faster than the most efficient method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2088.Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense Object Localization</span><br>
                <span class="as">Xu, LianandOuyang, WanliandBennamoun, MohammedandBoussaid, FaridandXu, Dan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Multi-Modal_Class-Specific_Tokens_for_Weakly_Supervised_Dense_Object_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19596-19605.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督密集物体定位（WSDOL）通常依赖类激活映射（CAM），但其在处理类别内变化时的能力有限，导致像素特征关联不准确，影响密集定位图的准确性。<br>
                    动机：为了解决上述问题，本文提出利用对比语言-图像预训练（CLIP）显式构建多模态类别表示，以指导密集定位。<br>
                    方法：提出了一个统一的转换器框架来学习两类特定模态的类别标记，即特定于类别的视觉和文本标记。前者从目标视觉数据中捕获语义，后者利用来自CLIP的与类别相关的语言先验知识，提供互补信息以更好地感知类别内的多样性。此外，还提出了用样本特定的上下文（包括视觉上下文和图像-语言上下文）丰富多模态类别特定标记的方法，使类别表示学习更具适应性，进一步促进密集定位。<br>
                    效果：大量实验表明，该方法在两个多标签数据集（PASCAL VOC和MS COCO）和一个单标签数据集（OpenImages）上的WSDOL性能优越。密集定位图还在PASCAL VOC和MS COCO上实现了最先进的弱监督语义分割（WSSS）结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly supervised dense object localization (WSDOL) relies generally on Class Activation Mapping (CAM), which exploits the correlation between the class weights of the image classifier and the pixel-level features. Due to the limited ability to address intra-class variations, the image classifier cannot properly associate the pixel features, leading to inaccurate dense localization maps. In this paper, we propose to explicitly construct multi-modal class representations by leveraging the Contrastive Language-Image Pre-training (CLIP), to guide dense localization. More specifically, we propose a unified transformer framework to learn two-modalities of class-specific tokens, i.e., class-specific visual and textual tokens. The former captures semantics from the target visual data while the latter exploits the class-related language priors from CLIP, providing complementary information to better perceive the intra-class diversities. In addition, we propose to enrich the multi-modal class-specific tokens with sample-specific contexts comprising visual context and image-language context. This enables more adaptive class representation learning, which further facilitates dense localization. Extensive experiments show the superiority of the proposed method for WSDOL on two multi-label datasets, i.e., PASCAL VOC and MS COCO, and one single-label dataset, i.e., OpenImages. Our dense localization maps also lead to the state-of-the-art weakly supervised semantic segmentation (WSSS) results on PASCAL VOC and MS COCO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2089.Decoupled Semantic Prototypes Enable Learning From Diverse Annotation Types for Semi-Weakly Segmentation in Expert-Driven Domains</span><br>
                <span class="as">Rei{\ss</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Reiss_Decoupled_Semantic_Prototypes_Enable_Learning_From_Diverse_Annotation_Types_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15495-15506.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将现有的图像分割解决方案扩展到专家驱动的领域，如显微镜应用或医疗健康？<br>
                    动机：由于专家数量有限且难以提供像素级标注，因此将现有的图像分割解决方案扩展到专家驱动的领域仍然具有挑战性。<br>
                    方法：通过分析现有训练算法的灵活性和可扩展性，提出了一种新的训练方法——解耦语义原型（DSP），该方法可以处理各种类型的标注，包括图像级别、点、边界框和像素级标注。<br>
                    效果：在细胞器分割这一具有挑战性的领域中进行广泛评估后发现，DSP方法比现有的半监督和弱监督分割算法能够更好地利用各种类型的标注，并显著提高了准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A vast amount of images and pixel-wise annotations allowed our community to build scalable segmentation solutions for natural domains. However, the transfer to expert-driven domains like microscopy applications or medical healthcare remains difficult as domain experts are a critical factor due to their limited availability for providing pixel-wise annotations. To enable affordable segmentation solutions for such domains, we need training strategies which can simultaneously handle diverse annotation types and are not bound to costly pixel-wise annotations. In this work, we analyze existing training algorithms towards their flexibility for different annotation types and scalability to small annotation regimes. We conduct an extensive evaluation in the challenging domain of organelle segmentation and find that existing semi- and semi-weakly supervised training algorithms are not able to fully exploit diverse annotation types. Driven by our findings, we introduce Decoupled Semantic Prototypes (DSP) as a training method for semantic segmentation which enables learning from annotation types as diverse as image-level-, point-, bounding box-, and pixel-wise annotations and which leads to remarkable accuracy gains over existing solutions for semi-weakly segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2090.Iterative Next Boundary Detection for Instance Segmentation of Tree Rings in Microscopy Images of Shrub Cross Sections</span><br>
                <span class="as">Gillert, AlexanderandResente, GiuliaandAnadon-Rosell, AlbaandWilmking, MartinandvonLukas, UweFreiherr</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gillert_Iterative_Next_Boundary_Detection_for_Instance_Segmentation_of_Tree_Rings_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14540-14548.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在显微图像中检测灌木截面的树环的问题。<br>
                    动机：这是一种特殊的实例分割任务，由于物体的同心圆形环状形状和高精度要求，现有方法的性能不佳。<br>
                    方法：我们提出了一种新的迭代方法，称为“迭代下一步边界检测”（INBD）。它直观地模拟了自然生长方向，从灌木截面的中心开始，并在每个迭代步骤中检测下一个环边界。<br>
                    效果：实验结果表明，INBD的性能优于通用实例分割方法，并且是唯一一个具有内置时间顺序概念的方法。我们的数据集和源代码可在http://github.com/alexander-g/INBD上获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the problem of detecting tree rings in microscopy images of shrub cross sections. This can be regarded as a special case of the instance segmentation task with several unique challenges such as the concentric circular ring shape of the objects and high precision requirements that result in inadequate performance of existing methods. We propose a new iterative method which we term Iterative Next Boundary Detection (INBD). It intuitively models the natural growth direction, starting from the center of the shrub cross section and detecting the next ring boundary in each iteration step. In our experiments, INBD shows superior performance to generic instance segmentation methods and is the only one with a built-in notion of chronological order. Our dataset and source code are available at http://github.com/alexander-g/INBD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2091.Universal Instance Perception As Object Discovery and Retrieval</span><br>
                <span class="as">Yan, BinandJiang, YiandWu, JiannanandWang, DongandLuo, PingandYuan, ZehuanandLu, Huchuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15325-15336.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将各种实例感知任务统一，以更有效地利用大量数据和减少冗余计算。<br>
                    动机：目前的实例感知任务被分割成多个独立的子任务，缺乏统一的处理方式。<br>
                    方法：提出了一种下一代通用实例感知模型UNINEXT，将不同的实例感知任务统一为对象发现和检索模式，通过改变输入提示灵活地感知不同类型的对象。<br>
                    效果：UNINEXT在10个实例级任务的20个挑战性基准测试中表现出优越的性能，包括经典的图像级任务（目标检测和实例分割）、视觉语言任务（指代表达式理解和分割）以及六个视频级对象跟踪任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks. In this work, we present a universal instance perception model of the next generation, termed UNINEXT. UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2) the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously. UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. Code is available at https://github.com/MasterBin-IIAU/UNINEXT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2092.MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation</span><br>
                <span class="as">Wang, YongchaoandXiao, BinandBi, XiuliandLi, WeishengandGao, Xinbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15651-15660.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在有限的标注下，通过半监督学习方法提高医学图像分割的准确性，并解决模型认知偏差对边缘区域分割性能的影响。<br>
                    动机：目前的半监督医学图像分割（SSMIS）方法缺乏处理模型认知偏差的设计，而这种偏差会随着训练的进行逐渐加深，难以自我纠正。<br>
                    方法：提出一种新的互校正框架（MCF），引入两个不同的子网络来探索和利用子网络之间的差异以纠正模型的认知偏差。具体包括提出对比差异回顾（CDR）模块来找出预测不一致的区域并进行回顾训练，以及动态竞争伪标签生成（DCPLG）模块实时评估子网络的性能，动态选择更可靠的伪标签。<br>
                    效果：在两种不同模态（CT和MRI）的医学图像数据库上的实验结果表明，该方法比几种最先进的方法具有更好的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised learning is a promising method for medical image segmentation under limited annotation. However, the model cognitive bias impairs the segmentation performance, especially for edge regions. Furthermore, current mainstream semi-supervised medical image segmentation (SSMIS) methods lack designs to handle model bias. The neural network has a strong learning ability, but the cognitive bias will gradually deepen during the training, and it is difficult to correct itself. We propose a novel mutual correction framework (MCF) to explore network bias correction and improve the performance of SSMIS. Inspired by the plain contrast idea, MCF introduces two different subnets to explore and utilize the discrepancies between subnets to correct cognitive bias of the model. More concretely, a contrastive difference review (CDR) module is proposed to find out inconsistent prediction regions and perform a review training. Additionally, a dynamic competitive pseudo-label generation (DCPLG) module is proposed to evaluate the performance of subnets in real-time, dynamically selecting more reliable pseudo-labels. Experimental results on two medical image databases with different modalities (CT and MRI) show that our method achieves superior performance compared to several state-of-the-art methods. The code will be available at https://github.com/WYC-321/MCF.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2093.Semantic Human Parsing via Scalable Semantic Transfer Over Multiple Label Domains</span><br>
                <span class="as">Yang, JieandWang, ChaoqunandLi, ZhenandWang, JunleandZhang, Ruimao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Semantic_Human_Parsing_via_Scalable_Semantic_Transfer_Over_Multiple_Label_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19424-19433.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用不同标签领域的数据（即不同级别的标签粒度）的互惠互利，训练强大的人体解析网络。<br>
                    动机：在实际应用中，存在两种常见的应用场景，即通用解析和专用解析，前者旨在从多个标签领域学习同质的人体表示，并通过仅使用不同的分割头来切换预测；后者则旨在学习特定领域的预测，同时从其他领域提炼语义知识。<br>
                    方法：提出了可扩展的语义转移（SST）训练范式，将多个标签领域的人体部位语义关联嵌入到人体表示学习过程中。<br>
                    效果：实验结果表明，SST能够有效地实现通用人体解析性能，并在三个人体解析基准测试（即PASCAL-Person-Part、ATR和CIHP）上取得了显著的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents Scalable Semantic Transfer (SST), a novel training paradigm, to explore how to leverage the mutual benefits of the data from different label domains (i.e. various levels of label granularity) to train a powerful human parsing network. In practice, two common application scenarios are addressed, termed universal parsing and dedicated parsing, where the former aims to learn homogeneous human representations from multiple label domains and switch predictions by only using different segmentation heads, and the latter aims to learn a specific domain prediction while distilling the semantic knowledge from other domains. The proposed SST has the following appealing benefits: (1) it can capably serve as an effective training scheme to embed semantic associations of human body parts from multiple label domains into the human representation learning process; (2) it is an extensible semantic transfer framework without predetermining the overall relations of multiple label domains, which allows continuously adding human parsing datasets to promote the training. (3) the relevant modules are only used for auxiliary training and can be removed during inference, eliminating the extra reasoning cost. Experimental results demonstrate SST can effectively achieve promising universal human parsing performance as well as impressive improvements compared to its counterparts on three human parsing benchmarks (i.e., PASCAL-Person-Part, ATR, and CIHP). Code is available at https://github.com/yangjie-cv/SST.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2094.RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension</span><br>
                <span class="as">Jin, LeiandLuo, GenandZhou, YiyiandSun, XiaoshuaiandJiang, GuannanandShu, AnnanandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2681-2690.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何降低指代表达式理解（REC）任务的开发成本，并提高其性能？<br>
                    动机：现有的弱监督方法主要基于两阶段检测网络，计算量大且昂贵。<br>
                    方法：提出了一种名为RefCLIP的弱监督模型，将REC定义为锚文本匹配问题，避免了现有方法中的复杂后处理。通过引入基于锚点的对比损失来优化RefCLIP。<br>
                    效果：在四个REC基准测试中，RefCLIP不仅显著提高了现有弱监督模型的性能，如在RefCOCO上提高了24.87%，而且推理速度比现有模型快5倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring Expression Comprehension (REC) is a task of grounding the referent based on an expression, and its development is greatly limited by expensive instance-level annotations. Most existing weakly supervised methods are built based on two-stage detection networks, which are computationally expensive. In this paper, we resort to the efficient one-stage detector and propose a novel weakly supervised model called RefCLIP. Specifically, RefCLIP redefines weakly supervised REC as an anchor-text matching problem, which can avoid the complex post-processing in existing methods. To achieve weakly supervised learning, we introduce anchor-based contrastive loss to optimize RefCLIP via numerous anchor-text pairs. Based on RefCLIP, we further propose the first model-agnostic weakly supervised training scheme for existing REC models, where RefCLIP acts as a mature teacher to generate pseudo-labels for teaching common REC models. With our careful designs, this scheme can even help existing REC models achieve better weakly supervised performance than RefCLIP, e.g., TransVG and SimREC. To validate our approaches, we conduct extensive experiments on four REC benchmarks, i.e., RefCOCO, RefCOCO+, RefCOCOg and ReferItGame. Experimental results not only report our significant performance gains over existing weakly supervised models, e.g., +24.87% on RefCOCO, but also show the 5x faster inference speed. Project: https://refclip.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2095.Boundary-Enhanced Co-Training for Weakly Supervised Semantic Segmentation</span><br>
                <span class="as">Rong, ShenghaiandTu, BohaiandWang, ZileiandLi, Junjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19574-19584.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的弱监督语义分割方法主要关注生成准确完整的类激活图作为伪标签，却忽视了训练分割网络的重要性。<br>
                    动机：研究发现，伪标签的质量与最终分割模型的性能存在不一致性，且错误标记的像素主要位于边界区域。因此，弱监督语义分割的重点应转向对噪声伪标签的鲁棒学习。<br>
                    方法：提出一种基于边界增强的协同训练（BECO）方法来训练分割模型。具体来说，首先使用两个交互网络的协同训练范式来提高不确定像素的学习；然后提出一种边界增强策略，利用可靠的预测结果构建人工边界，以提升困难边界区域的预测精度。<br>
                    效果：实验证明，该方法在PASCAL VOC 2012和MS COCO 2014数据集上的表现优于其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The existing weakly supervised semantic segmentation (WSSS) methods pay much attention to generating accurate and complete class activation maps (CAMs) as pseudo-labels, while ignoring the importance of training the segmentation networks. In this work, we observe that there is an inconsistency between the quality of the pseudo-labels in CAMs and the performance of the final segmentation model, and the mislabeled pixels mainly lie on the boundary areas. Inspired by these findings, we argue that the focus of WSSS should be shifted to robust learning given the noisy pseudo-labels, and further propose a boundary-enhanced co-training (BECO) method for training the segmentation model. To be specific, we first propose to use a co-training paradigm with two interactive networks to improve the learning of uncertain pixels. Then we propose a boundary-enhanced strategy to boost the prediction of difficult boundary areas, which utilizes reliable predictions to construct artificial boundaries. Benefiting from the design of co-training and boundary enhancement, our method can achieve promising segmentation performance for different CAMs. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 validate the superiority of our BECO over other state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2096.Uni3D: A Unified Baseline for Multi-Dataset 3D Object Detection</span><br>
                <span class="as">Zhang, BoandYuan, JiakangandShi, BotianandChen, TaoandLi, YikangandQiao, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Uni3D_A_Unified_Baseline_for_Multi-Dataset_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9253-9262.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前3D物体检测模型在单一数据集上进行训练和测试，当直接部署到另一个数据集时，检测精度会大幅下降。<br>
                    动机：由于不同LiDAR类型和数据获取标准导致的数据级差异和分类级别变化，使得从多个数据集中训练统一3D检测器成为一项具有挑战性的任务。<br>
                    方法：提出Uni3D模型，通过简单的数据级校正操作和设计的语义级耦合-重接模块，分别缓解不可避免的数据级和分类级别差异。该方法简单易行，可与许多3D物体检测基线（如PV-RCNN和Voxel-RCNN）结合，使它们能有效地从多个现成的3D数据集中学习，获得更具辨别力和泛化能力的特征表示。<br>
                    效果：实验表明，Uni3D在许多数据集整合设置中超越了单个数据集上训练的一系列独立检测器，其参数增加量比选定的基线检测器多1.04倍。这项工作预计将推动3D通用性的研究，因为它将推动感知性能的极限。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current 3D object detection models follow a single dataset-specific training and testing paradigm, which often faces a serious detection accuracy drop when they are directly deployed in another dataset. In this paper, we study the task of training a unified 3D detector from multiple datasets. We observe that this appears to be a challenging task, which is mainly due to that these datasets present substantial data-level differences and taxonomy-level variations caused by different LiDAR types and data acquisition standards. Inspired by such observation, we present a Uni3D which leverages a simple data-level correction operation and a designed semantic-level coupling-and-recoupling module to alleviate the unavoidable data-level and taxonomy-level differences, respectively. Our method is simple and easily combined with many 3D object detection baselines such as PV-RCNN and Voxel-RCNN, enabling them to effectively learn from multiple off-the-shelf 3D datasets to obtain more discriminative and generalizable representations. Experiments are conducted on many dataset consolidation settings. Their results demonstrate that Uni3D exceeds a series of individual detectors trained on a single dataset, with a 1.04x parameter increase over a selected baseline detector. We expect this work will inspire the research of 3D generalization since it will push the limits of perceptual performance. Our code is available at: https://github.com/PJLab-ADG/3DTrans</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2097.Devil&#x27;s on the Edges: Selective Quad Attention for Scene Graph Generation</span><br>
                <span class="as">Jung, DeunsolandKim, SanghyunandKim, WonHwaandCho, Minsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_Devils_on_the_Edges_Selective_Quad_Attention_for_Scene_Graph_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18664-18674.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从图像中构造语义图结构，同时处理图像中的干扰对象和关系。<br>
                    动机：场景图生成任务的主要挑战在于图像中存在大量无关的对象和关系，这严重干扰了上下文推理。<br>
                    方法：提出选择性四元注意力网络（SQUAT），通过学习选择相关对象对并通过多样化的上下文交互来消除歧义。<br>
                    效果：实验表明，SQUAT在视觉基因组和Open Images v6基准测试上表现出强大的性能和鲁棒性，达到了最先进的水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scene graph generation aims to construct a semantic graph structure from an image such that its nodes and edges respectively represent objects and their relationships. One of the major challenges for the task lies in the presence of distracting objects and relationships in images; contextual reasoning is strongly distracted by irrelevant objects or backgrounds and, more importantly, a vast number of irrelevant candidate relations. To tackle the issue, we propose the Selective Quad Attention Network (SQUAT) that learns to select relevant object pairs and disambiguate them via diverse contextual interactions. SQUAT consists of two main components: edge selection and quad attention. The edge selection module selects relevant object pairs, i.e., edges in the scene graph, which helps contextual reasoning, and the quad attention module then updates the edge features using both edge-to-node and edge-to-edge cross-attentions to capture contextual information between objects and object pairs. Experiments demonstrate the strong performance and robustness of SQUAT, achieving the state of the art on the Visual Genome and Open Images v6 benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2098.NIFF: Alleviating Forgetting in Generalized Few-Shot Object Detection via Neural Instance Feature Forging</span><br>
                <span class="as">Guirguis, KarimandMeier, JohannesandEskandar, GeorgeandKayser, MatthiasandYang, BinandBeyerer, J\&quot;urgen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guirguis_NIFF_Alleviating_Forgetting_in_Generalized_Few-Shot_Object_Detection_via_Neural_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24193-24202.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何缓解AI训练中对大量数据的需求，特别是在需要共享和存储数据有问题的情况下。<br>
                    动机：目前的通用少样本目标检测（G-FSOD）方法需要访问旧类别（即基础类别）的图像以学习新类别，但当共享和存储数据存在问题时，这种方法就无法使用。<br>
                    方法：提出了一种无需访问基础图像的数据自由知识蒸馏（DFKD）方法。该方法利用基础模型感兴趣区域（RoI）特征的统计数据来生成实例级特征。<br>
                    效果：这种方法可以显著减少基础内存需求，同时在具有挑战性的MS-COCO和PASCAL-VOC基准测试中为G-FSOD设定了新的标准。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Privacy and memory are two recurring themes in a broad conversation about the societal impact of AI. These concerns arise from the need for huge amounts of data to train deep neural networks. A promise of Generalized Few-shot Object Detection (G-FSOD), a learning paradigm in AI, is to alleviate the need for collecting abundant training samples of novel classes we wish to detect by leveraging prior knowledge from old classes (i.e., base classes). G-FSOD strives to learn these novel classes while alleviating catastrophic forgetting of the base classes. However, existing approaches assume that the base images are accessible, an assumption that does not hold when sharing and storing data is problematic. In this work, we propose the first data-free knowledge distillation (DFKD) approach for G-FSOD that leverages the statistics of the region of interest (RoI) features from the base model to forge instance-level features without accessing the base images. Our contribution is three-fold: (1) we design a standalone lightweight generator with (2) class-wise heads (3) to generate and replay diverse instance-level base features to the RoI head while finetuning on the novel data. This stands in contrast to standard DFKD approaches in image classification, which invert the entire network to generate base images. Moreover, we make careful design choices in the novel finetuning pipeline to regularize the model. We show that our approach can dramatically reduce the base memory requirements, all while setting a new standard for G-FSOD on the challenging MS-COCO and PASCAL-VOC benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2099.PartDistillation: Learning Parts From Instance Segmentation</span><br>
                <span class="as">Cho, JangHyunandKr\&quot;ahenb\&quot;uhl, PhilippandRamanathan, Vignesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cho_PartDistillation_Learning_Parts_From_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7152-7161.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从物体实例标签中学习部分分割。<br>
                    动机：现有的实例分割模型包含了大量隐藏的部分信息，但这部分信息通常噪声大、不完整且不一致。<br>
                    方法：通过在大型数据集上进行自我监督的自我训练，将实例分割模型的部分信息转移到部分分割模型中，形成的结果模型鲁棒、准确且泛化能力强。<br>
                    效果：在各种部分分割数据集上评估该模型，结果显示，该模型在零射击一般化性能上优于监督部分分割，并在目标数据集上的微调性能上也优于监督对应和其他基线，特别是在少数射击区域。此外，当评估超过10K个物体类别时，该模型提供了更广泛的罕见部分覆盖。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a scalable framework to learn part segmentation from object instance labels. State-of-the-art instance segmentation models contain a surprising amount of part information. However, much of this information is hidden from plain view. For each object instance, the part information is noisy, inconsistent, and incomplete. PartDistillation transfers the part information of an instance segmentation model into a part segmentation model through self-supervised self-training on a large dataset. The resulting segmentation model is robust, accurate, and generalizes well. We evaluate the model on various part segmentation datasets. Our model outperforms supervised part segmentation in zero-shot generalization performance by a large margin. Our model outperforms when finetuned on target datasets compared to supervised counterpart and other baselines especially in few-shot regime. Finally, our model provides a wider coverage of rare parts when evaluated over 10K object classes. Code is at https://github.com/facebookresearch/PartDistillation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2100.Boosting Video Object Segmentation via Space-Time Correspondence Learning</span><br>
                <span class="as">Zhang, YurongandLi, LiuleiandWang, WenguanandXie, RongandSong, LiandZhang, Wenjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Boosting_Video_Object_Segmentation_via_Space-Time_Correspondence_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2246-2256.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频对象分割（VOS）中，如何更好地进行空间-时间对应匹配。<br>
                    动机：当前主流的VOS方法仅利用地面真值掩码进行学习，没有对空间-时间对应匹配施加任何约束，这是其基础构建模块中的一个关键但常被忽视的问题。<br>
                    方法：设计了一种对应感知训练框架，通过在网络学习过程中明确鼓励稳健的对应匹配，提升基于匹配的VOS解决方案。<br>
                    效果：在四个广泛使用的基准测试（DAVIS2016&2017，YouTube-VOS2018&2019）上，该算法在著名的基于匹配的VOS解决方案之上取得了显著的性能提升，且无需额外的训练标注成本，也不会增加部署速度延迟或需要修改架构。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Current top-leading solutions for video object segmentation (VOS) typically follow a matching-based regime: for each query frame, the segmentation mask is inferred according to its correspondence to previously processed and the first annotated frames. They simply exploit the supervisory signals from the groundtruth masks for learning mask prediction only, without posing any constraint on the space-time correspondence matching, which, however, is the fundamental building block of such regime. To alleviate this crucial yet commonly ignored issue, we devise a correspondence-aware training framework, which boosts matching-based VOS solutions by explicitly encouraging robust correspondence matching during network learning. Through comprehensively exploring the intrinsic coherence in videos on pixel and object levels, our algorithm reinforces the standard, fully supervised training of mask segmentation with label-free, contrastive correspondence learning. Without neither requiring extra annotation cost during training, nor causing speed delay during deployment, nor incurring architectural modification, our algorithm provides solid performance gains on four widely used benchmarks, i.e., DAVIS2016&2017, and YouTube-VOS2018&2019, on the top of famous matching-based VOS solutions. Our implementation will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2101.You Only Segment Once: Towards Real-Time Panoptic Segmentation</span><br>
                <span class="as">Hu, JieandHuang, LinyanandRen, TianheandZhang, ShengchuanandJi, RongrongandCao, Liujuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17819-17829.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种实时全景分割框架YOSO。<br>
                    动机：为了解决实例和语义分割任务需要分别处理的问题，减少计算开销。<br>
                    方法：通过在图像特征图和全景内核之间进行动态卷积来预测掩码，设计了特征金字塔聚合器进行特征图提取，以及可分离的动态解码器生成全景内核。<br>
                    效果：实验结果表明，YOSO在COCO、Cityscapes、ADE20K和Mapillary Vistas等数据集上的表现优于最先进的模型，同时具有较高的效率和准确性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose YOSO, a real-time panoptic segmentation framework. YOSO predicts masks via dynamic convolutions between panoptic kernels and image feature maps, in which you only need to segment once for both instance and semantic segmentation tasks. To reduce the computational overhead, we design a feature pyramid aggregator for the feature map extraction, and a separable dynamic decoder for the panoptic kernel generation. The aggregator re-parameterizes interpolation-first modules in a convolution-first way, which significantly speeds up the pipeline without any additional costs. The decoder performs multi-head cross-attention via separable dynamic convolution for better efficiency and accuracy. To the best of our knowledge, YOSO is the first real-time panoptic segmentation framework that delivers competitive performance compared to state-of-the-art models. Specifically, YOSO achieves 46.4 PQ, 45.6 FPS on COCO; 52.5 PQ, 22.6 FPS on Cityscapes; 38.0 PQ, 35.4 FPS on ADE20K; and 34.1 PQ, 7.1 FPS on Mapillary Vistas. Code is available at https://github.com/hujiecpp/YOSO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2102.CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection</span><br>
                <span class="as">Ma, ShuaileiandWang, YuefengandWei, YingandFan, JiaqiandLi, ThomasH.andLiu, HongliandLv, Fanbing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19681-19690.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一个模型，使其能识别已知和未知的对象，并逐步学习识别这些未知对象。<br>
                    动机：现有的检测框架和固定伪标签机制存在以下问题：(i) 检测未知对象会显著降低模型检测已知对象的能力；(ii) 伪标签机制没有充分利用输入的先验知识；(iii) 伪标签机制的固定选择方式不能保证模型在正确的方向上进行训练。<br>
                    方法：提出一种新的解决方案，称为CAT（Localization and Identification Cascade Detection Transformer），通过级联解码的方式将检测过程解耦。同时，提出自适应伪标签机制，结合模型驱动和输入驱动的伪标签机制，自适应地为未知对象生成鲁棒的伪标签，显著提高CAT检索未知对象的能力。<br>
                    效果：在MS-COCO和PASCAL VOC两个基准数据集上的全面实验表明，我们的模型在开放世界目标检测、增量目标检测和开放集检测任务中的所有指标上都优于最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Open-world object detection (OWOD), as a more general and challenging goal, requires the model trained from data on known objects to detect both known and unknown objects and incrementally learn to identify these unknown objects. The existing works which employ standard detection framework and fixed pseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion of detecting unknown objects substantially reduces the model's ability to detect known ones. (ii) The PLM does not adequately utilize the priori knowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee that the model is trained in the right direction. We observe that humans subconsciously prefer to focus on all foreground objects and then identify each one in detail, rather than localize and identify a single object simultaneously, for alleviating the confusion. This motivates us to propose a novel solution called CAT: LoCalization and IdentificAtion Cascade Detection Transformer which decouples the detection process via the shared decoder in the cascade decoding way. In the meanwhile, we propose the self-adaptive pseudo-labelling mechanism which combines the model-driven with input-driven PLM and self-adaptively generates robust pseudo-labels for unknown objects, significantly improving the ability of CAT to retrieve unknown objects. Comprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL VOC, show that our model outperforms the state-of-the-art in terms of all metrics in the task of OWOD, incremental object detection (IOD) and open-set detection.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2103.LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding</span><br>
                <span class="as">Li, GenandJampani, VarunandSun, DeqingandSevilla-Lara, Laura</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LOCATE_Localize_and_Transfer_Object_Parts_for_Weakly_Supervised_Affordance_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10922-10931.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过观察识别物体的可执行部分，即“affordance grounding”。<br>
                    动机：人类通过观察学习使用新工具的能力是智能系统与世界互动的基础。<br>
                    方法：提出一个名为LOCATE的框架，通过在图像中寻找匹配的对象部分，将知识从被动对象（用于测试的自身中心图像）转移到主动对象（用于学习的外中心图像）。<br>
                    效果：实验证明，该方法在已知和未知对象的处理上都大幅超越了现有技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans excel at acquiring knowledge through observation. For example, we can learn to use new tools by watching demonstrations. This skill is fundamental for intelligent systems to interact with the world. A key step to acquire this skill is to identify what part of the object affords each action, which is called affordance grounding. In this paper, we address this problem and propose a framework called LOCATE that can identify matching object parts across images, to transfer knowledge from images where an object is being used (exocentric images used for learning), to images where the object is inactive (egocentric ones used to test). To this end, we first find interaction areas and extract their feature embeddings. Then we learn to aggregate the embeddings into compact prototypes (human, object part, and background), and select the one representing the object part. Finally, we use the selected prototype to guide affordance grounding. We do this in a weakly supervised manner, learning only from image-level affordance and object labels. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a large margin on both seen and unseen objects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2104.Cut and Learn for Unsupervised Object Detection and Instance Segmentation</span><br>
                <span class="as">Wang, XudongandGirdhar, RohitandYu, StellaX.andMisra, Ishan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3124-3134.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种名为Cut-and-LEaRn（CutLER）的简单方法，用于训练无监督的对象检测和分割模型。<br>
                    动机：利用自监督模型的特性，无需人工标签即可发现并定位对象，从而训练出先进的本地化模型。<br>
                    方法：首先使用提出的MaskCut方法为图像中的多个对象生成粗略的遮罩，然后在这些遮罩上使用稳健的损失函数学习检测器。通过在预测结果上进行自我训练，进一步提高性能。<br>
                    效果：与先前的工作相比，CutLER更简单，可与不同的检测架构兼容，并能检测到多个对象。同时，CutLER是一种零次无监督检测器，并在视频帧、绘画、草图等不同领域的11个基准测试中，将检测性能AP_50提高了超过2.7倍。经过微调后，当使用5%的标签进行训练时，CutLER可以作为低次无监督检测器，在COCO上超越MoCo-v2 7.3%的AP^box和6.6%的AP^mask。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Cut-and-LEaRn (CutLER), a simple approach for training unsupervised object detection and segmentation models. We leverage the property of self-supervised models to 'discover' objects without supervision and amplify it to train a state-of-the-art localization model without any human labels. CutLER first uses our proposed MaskCut approach to generate coarse masks for multiple objects in an image, and then learns a detector on these masks using our robust loss function. We further improve performance by self-training the model on its predictions. Compared to prior work, CutLER is simpler, compatible with different detection architectures, and detects multiple objects. CutLER is also a zero-shot unsupervised detector and improves detection performance AP_50 by over 2.7x on 11 benchmarks across domains like video frames, paintings, sketches, etc. With finetuning, CutLER serves as a low-shot detector surpassing MoCo-v2 by 7.3% AP^box and 6.6% AP^mask on COCO when training with 5% labels.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2105.Side Adapter Network for Open-Vocabulary Semantic Segmentation</span><br>
                <span class="as">Xu, MengdeandZhang, ZhengandWei, FangyunandHu, HanandBai, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Side_Adapter_Network_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2945-2954.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的开放词汇语义分割框架，名为SAN。<br>
                    动机：将语义分割任务模型化为区域识别问题，并利用预训练的视觉语言模型CLIP进行改进。<br>
                    方法：在冻结的CLIP模型上附加一个侧边网络，用于预测掩膜建议和注意力偏差。这种解耦设计使CLIP能够识别掩膜建议的类别。整个网络可以端到端训练，使侧边网络适应冻结的CLIP模型，使预测的掩膜建议具有CLIP意识。<br>
                    效果：在多个语义分割基准测试中，该方法显著优于其他方法，训练参数减少了18倍，推理速度提高了19倍。希望这种方法能成为坚实的基线，有助于未来开放词汇语义分割的研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a new framework for open-vocabulary semantic segmentation with the pre-trained vision-language model, named SAN. Our approach models the semantic segmentation task as a region recognition problem. A side network is attached to a frozen CLIP model with two branches: one for predicting mask proposals, and the other for predicting attention bias which is applied in the CLIP model to recognize the class of masks. This decoupled design has the benefit CLIP in recognizing the class of mask proposals. Since the attached side network can reuse CLIP features, it can be very light. In addition, the entire network can be trained end-to-end, allowing the side network to be adapted to the frozen CLIP model, which makes the predicted mask proposals CLIP-aware. Our approach is fast, accurate, and only adds a few additional trainable parameters. We evaluate our approach on multiple semantic segmentation benchmarks. Our method significantly outperforms other counterparts, with up to 18 times fewer trainable parameters and 19 times faster inference speed. We hope our approach will serve as a solid baseline and help ease future research in open-vocabulary semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2106.MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence</span><br>
                <span class="as">Sun, YixuanandHuang, YiwenandGuo, HaijingandZhao, YuzhouandWu, RunminandYu, YizhouandGe, WeifengandZhang, Wenqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_MISC210K_A_Large-Scale_Dataset_for_Multi-Instance_Semantic_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7121-7130.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的单对象匹配模式难以发现类别之间的共性，无法满足真实世界识别任务的需求。<br>
                    动机：为了填补这一空白，我们设计了多实例语义对应任务，旨在构建图像对中多个对象的对应关系。<br>
                    方法：我们从COCO Detection 2017任务中构建了一个名为MISC210K的多实例语义对应（MISC）数据集。我们通过三个步骤构建数据集：（1）类别选择和数据清理；（2）基于3D模型和对象描述规则的关键点设计；（3）人机协作注释。然后，我们选择了34个类别的对象，使用精心设计的半自动工作流程对4812张具有挑战性的图片进行了标注，最终获得了218,179个带有实例掩码和实例级关键点对的图像对。我们设计了一个双路径协同学习流程，同时训练实例级共同分割任务和细粒度级别对应任务。<br>
                    效果：我们提供了基准评估和进一步的消融结果分析，并提出了三个未来方向。我们的项目可以在https://github.com/YXSUNMADMAX/MISC210K上找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic correspondence have built up a new way for object recognition. However current single-object matching schema can be hard for discovering commonalities for a category and far from the real-world recognition tasks. To fill this gap, we design the multi-instance semantic correspondence task which aims at constructing the correspondence between multiple objects in an image pair. To support this task, we build a multi-instance semantic correspondence (MISC) dataset from COCO Detection 2017 task called MISC210K. We construct our dataset as three steps: (1) category selection and data cleaning; (2) keypoint design based on 3D models and object description rules; (3) human-machine collaborative annotation. Following these steps, we select 34 classes of objects with 4,812 challenging images annotated via a well designed semi-automatic workflow, and finally acquire 218,179 image pairs with instance masks and instance-level keypoint pairs annotated. We design a dual-path collaborative learning pipeline to train instance-level co-segmentation task and fine-grained level correspondence task together. Benchmark evaluation and further ablation results with detailed analysis are provided with three future directions proposed. Our project is available on https://github.com/YXSUNMADMAX/MISC210K.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2107.GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds</span><br>
                <span class="as">Zhang, ZihuiandYang, BoandWang, BingandLi, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_GrowSP_Unsupervised_Semantic_Segmentation_of_3D_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17619-17629.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从原始点云中进行3D语义分割的问题。<br>
                    动机：现有的方法主要依赖于大量的人工标注来训练神经网络，而我们提出了第一个完全无监督的方法GrowSP，无需任何类型的人工标签或预训练模型，就能成功识别3D场景中每个点的复杂语义类别。<br>
                    方法：我们的方法通过逐步增长超级点的3D语义元素发现，主要由三个部分组成：1）特征提取器，从输入的点云中学习每个点的特征；2）超级点构造器，逐步增长超级点的大小；3）语义原语聚类模块，将超级点分组为最终的语义分割的语义元素。<br>
                    效果：我们在多个数据集上广泛评估了我们的方法，证明其在所有无监督基线上的性能优越，并接近经典的全监督PointNet。我们希望我们的研究能激发更多先进的无监督3D语义学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on multiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fully supervised PointNet. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2108.ConQueR: Query Contrast Voxel-DETR for 3D Object Detection</span><br>
                <span class="as">Zhu, BenjinandWang, ZheandShi, ShaoshuaiandXu, HangandHong, LanqingandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ConQueR_Query_Contrast_Voxel-DETR_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9296-9305.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管基于DETR的3D检测器简化了检测流程并实现了直接稀疏预测，但其性能仍然落后于具有后处理的密集检测器。<br>
                    动机：大多数3D对象检测中的误报是由于缺乏对局部相似查询的显式监督而导致的。<br>
                    方法：提出了一种名为Query Contrast Voxel-DETR（ConQueR）的简单而有效的稀疏3D检测器，通过构建每个GT的正负GT-查询对，并基于特征相似性使用对比损失来增强正GT-查询对与负对之间的差异，以消除挑战性的误报。<br>
                    效果：ConQueR缩小了稀疏和密集3D检测器之间的差距，并将误报减少了60%。在具有挑战性的Waymo开放数据集验证集上，单帧ConQueR实现了71.6 mAPH/L2，比先前的最佳方法高出2.0 mAPH/L2。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although DETR-based 3D detectors simplify the detection pipeline and achieve direct sparse predictions, their performance still lags behind dense detectors with post-processing for 3D object detection from point clouds. DETRs usually adopt a larger number of queries than GTs (e.g., 300 queries v.s.  40 objects in Waymo) in a scene, which inevitably incur many false positives during inference. In this paper, we propose a simple yet effective sparse 3D detector, named Query Contrast Voxel-DETR (ConQueR), to eliminate the challenging false positives, and achieve more accurate and sparser predictions. We observe that most false positives are highly overlapping in local regions, caused by the lack of explicit supervision to discriminate locally similar queries. We thus propose a Query Contrast mechanism to explicitly enhance queries towards their best-matched GTs over all unmatched query predictions. This is achieved by the construction of positive and negative GT-query pairs for each GT, and a contrastive loss to enhance positive GT-query pairs against negative ones based on feature similarities. ConQueR closes the gap of sparse and dense 3D detectors, and reduces  60% false positives. Our single-frame ConQueR achieves 71.6 mAPH/L2 on the challenging Waymo Open Dataset validation set, outperforming previous sota methods by over 2.0 mAPH/L2. Code: https://github.com/poodarchu/EFG.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2109.BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation</span><br>
                <span class="as">Cheng, TianhengandWang, XinggangandChen, ShaoyuandZhang, QianandLiu, Wenyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_BoxTeacher_Exploring_High-Quality_Pseudo_Labels_for_Weakly_Supervised_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3145-3154.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用弱监督实例分割中的边界框生成高质量的分割掩码，以提高实例分割的性能。<br>
                    动机：现有的弱监督实例分割方法主要依赖于设计启发式损失函数，而忽略了边界框中可能包含的精细分割信息。<br>
                    方法：提出BoxTeacher框架，通过一个复杂的教师模型生成高质量的伪标签作为分割掩码，同时引入噪声感知像素损失和噪声减少亲和性损失来优化学生模型。<br>
                    效果：在具有挑战性的COCO数据集上，BoxTeacher无需复杂的操作就能达到35.0的mask AP和36.5的mask AP，显著超过了之前最先进的方法，缩小了边界框监督和掩码监督之间的差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Labeling objects with pixel-wise segmentation requires a huge amount of human labor compared to bounding boxes. Most existing methods for weakly supervised instance segmentation focus on designing heuristic losses with priors from bounding boxes. While, we find that box-supervised methods can produce some fine segmentation masks and we wonder whether the detectors could learn from these fine masks while ignoring low-quality masks. To answer this question, we present BoxTeacher, an efficient and end-to-end training framework for high-performance weakly supervised instance segmentation, which leverages a sophisticated teacher to generate high-quality masks as pseudo labels. Considering the massive noisy masks hurt the training, we present a mask-aware confidence score to estimate the quality of pseudo masks and propose the noise-aware pixel loss and noise-reduced affinity loss to adaptively optimize the student with pseudo masks. Extensive experiments can demonstrate the effectiveness of the proposed BoxTeacher. Without bells and whistles, BoxTeacher remarkably achieves 35.0 mask AP and 36.5 mask AP with ResNet-50 and ResNet-101 respectively on the challenging COCO dataset, which outperforms the previous state-of-the-art methods by a significant margin and bridges the gap between box-supervised and mask-supervised methods. The code and models will be available later.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2110.Devil Is in the Queries: Advancing Mask Transformers for Real-World Medical Image Segmentation and Out-of-Distribution Localization</span><br>
                <span class="as">Yuan, MingzeandXia, YingdaandDong, HexinandChen, ZifanandYao, JiawenandQiu, MingyanandYan, KeandYin, XiaoliandShi, YuandChen, XinandLiu, ZaiyiandDong, BinandZhou, JingrenandLu, LeandZhang, LingandZhang, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Devil_Is_in_the_Queries_Advancing_Mask_Transformers_for_Real-World_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23879-23889.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高医疗图像分割算法在长尾复杂对象（即罕见疾病）上的有效性，避免在分布外（OOD）情况下产生临床危险的损害。<br>
                    动机：现实世界的医疗图像分割具有巨大的长尾复杂性，尾部条件与相对罕见的疾病相关，并具有临床意义。一个值得信赖的医疗AI算法应该在尾部条件下展示其效果，以避免在这些分布外（OOD）情况下造成临床危险的损害。<br>
                    方法：采用Mask Transformers中的对象查询概念将语义分割 formulate 为软聚类分配。查询在训练过程中拟合内群特征级的聚类中心。因此，在现实世界的医疗图像上进行推理时，像素与查询之间的相似性可以检测和定位OOD区域。我们称这种OOD定位为MaxQuery。此外，现实世界的医疗图像前景，无论是OOD对象还是内群，都是病变。它们之间的差异明显小于前景和背景之间的差异，导致对象查询可能会冗余地集中在背景上。因此，我们提出了一种查询分布（QD）损失来强制在查询级别上明确分割目标和其他区域之间的边界，提高内群分割和OOD指示。<br>
                    效果：我们的框架在两个现实世界的分割任务上进行了测试，即胰腺和肝脏肿瘤的分割，比之前的领先算法平均提高了7.39%的AUROC，14.69%的AUPR和13.79%的FPR95 OOD定位。另一方面，我们的框架通过与nnUNet相比，平均提高了5.27%的DSC，从而提高了内群分割的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Real-world medical image segmentation has tremendous long-tailed complexity of objects, among which tail conditions correlate with relatively rare diseases and are clinically significant. A trustworthy medical AI algorithm should demonstrate its effectiveness on tail conditions to avoid clinically dangerous damage in these out-of-distribution (OOD) cases. In this paper, we adopt the concept of object queries in Mask transformers to formulate semantic segmentation as a soft cluster assignment. The queries fit the feature-level cluster centers of inliers during training. Therefore, when performing inference on a medical image in real-world scenarios, the similarity between pixels and the queries detects and localizes OOD regions. We term this OOD localization as MaxQuery. Furthermore, the foregrounds of real-world medical images, whether OOD objects or inliers, are lesions. The difference between them is obviously less than that between the foreground and background, resulting in the object queries may focus redundantly on the background. Thus, we propose a query-distribution (QD) loss to enforce clear boundaries between segmentation targets and other regions at the query level, improving the inlier segmentation and OOD indication. Our proposed framework is tested on two real-world segmentation tasks, i.e., segmentation of pancreatic and liver tumors, outperforming previous leading algorithms by an average of 7.39% on AUROC, 14.69% on AUPR, and 13.79% on FPR95 for OOD localization. On the other hand, our framework improves the performance of inlier segmentation by an average of 5.27% DSC compared with nnUNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2111.Token Contrast for Weakly-Supervised Semantic Segmentation</span><br>
                <span class="as">Ru, LixiangandZheng, HeliangandZhan, YibingandDu, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ru_Token_Contrast_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3093-3102.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督语义分割中，图像级标签通常使用类激活映射（CAM）生成伪标签的问题。<br>
                    动机：由于卷积神经网络（CNN）对局部结构感知的限制，CAM通常无法识别完整的目标区域。虽然最近的视觉变换器（ViT）可以弥补这一缺陷，但作者观察到它也带来了过度平滑的问题，即最终的补丁令牌倾向于均匀。<br>
                    方法：作者提出了令牌对比（ToCo）来解决这个问题，并进一步探索了ViT在弱监督语义分割中的应用价值。首先，受到ViT中间层仍能保留语义多样性的观察启发，设计了一个补丁令牌对比模块（PTC）。PTC使用从中间层派生的伪令牌关系来监督最终的补丁令牌，使它们能够对齐语义区域，从而产生更准确的CAM。其次，为了进一步区分CAM中的低置信度区域，作者设计了一个类令牌对比模块（CTC），灵感来自于ViT中的类令牌可以捕获高级语义的事实。CTC通过对比其类令牌来促进不确定局部区域和全局对象之间的表示一致性。<br>
                    效果：在PASCAL VOC和MS COCO数据集上的实验表明，所提出的ToCo可以显著超越其他单阶段竞争对手，并达到与最先进的多阶段方法相当的性能。代码可在https://github.com/rulixiang/ToCo获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-Supervised Semantic Segmentation (WSSS) using image-level labels typically utilizes Class Activation Map (CAM) to generate the pseudo labels. Limited by the local structure perception of CNN, CAM usually cannot identify the integral object regions. Though the recent Vision Transformer (ViT) can remedy this flaw, we observe it also brings the over-smoothing issue, ie, the final patch tokens incline to be uniform. In this work, we propose Token Contrast (ToCo) to address this issue and further explore the virtue of ViT for WSSS. Firstly, motivated by the observation that intermediate layers in ViT can still retain semantic diversity, we designed a Patch Token Contrast module (PTC). PTC supervises the final patch tokens with the pseudo token relations derived from intermediate layers, allowing them to align the semantic regions and thus yield more accurate CAM. Secondly, to further differentiate the low-confidence regions in CAM, we devised a Class Token Contrast module (CTC) inspired by the fact that class tokens in ViT can capture high-level semantics. CTC facilitates the representation consistency between uncertain local regions and global objects by contrasting their class tokens. Experiments on the PASCAL VOC and MS COCO datasets show the proposed ToCo can remarkably surpass other single-stage competitors and achieve comparable performance with state-of-the-art multi-stage methods. Code is available at https://github.com/rulixiang/ToCo.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2112.MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation</span><br>
                <span class="as">Hoyer, LukasandDai, DengxinandWang, HaoranandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11721-11732.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让预训练的语言模型更好地利用结构化知识，以提高语言理解能力？<br>
                    动机：现有的预训练语言模型往往忽视了知识图谱中的有信息量的实体，而这些实体可以增强语言表示。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，该模型同时利用大规模文本语料库和知识图谱进行训练，以充分利用词汇、句法和知识信息。<br>
                    效果：实验结果显示，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In unsupervised domain adaptation (UDA), a model trained on source data (e.g. synthetic) is adapted to target data (e.g. real-world) without access to target annotation. Most previous UDA methods struggle with classes that have a similar visual appearance on the target domain as no ground truth is available to learn the slight appearance differences. To address this problem, we propose a Masked Image Consistency (MIC) module to enhance UDA by learning spatial context relations of the target domain as additional clues for robust visual recognition. MIC enforces the consistency between predictions of masked target images, where random patches are withheld, and pseudo-labels that are generated based on the complete image by an exponential moving average teacher. To minimize the consistency loss, the network has to learn to infer the predictions of the masked regions from their context. Due to its simple and universal concept, MIC can be integrated into various UDA methods across different visual recognition tasks such as image classification, semantic segmentation, and object detection. MIC significantly improves the state-of-the-art performance across the different recognition tasks for synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8% on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an improvement of +2.1 and +3.0 percent points over the previous state of the art. The implementation is available at https://github.com/lhoyer/MIC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2113.SkyEye: Self-Supervised Bird&#x27;s-Eye-View Semantic Mapping Using Monocular Frontal View Images</span><br>
                <span class="as">Gosala, NikhilandPetek, K\&quot;ursatandDrews-Jr, PauloL.J.andBurgard, WolframandValada, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gosala_SkyEye_Self-Supervised_Birds-Eye-View_Semantic_Mapping_Using_Monocular_Frontal_View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14901-14910.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用单目前视图像生成BEV语义地图，而无需依赖大量标注的BEV数据。<br>
                    动机：现有的生成BEV语义地图的方法仍遵循全监督训练范式，需要大量的标注数据。<br>
                    方法：提出首个使用单目前视图像进行自我监督生成BEV语义地图的方法，即SkyEye架构。通过两种自我监督模式——隐式监督和显式监督进行训练。隐式监督根据FV语义序列在时间上保持场景的空间一致性；显式监督则利用从FV语义标注和自我监督深度估计生成的BEV伪标签。<br>
                    效果：在KITTI-360数据集上的广泛评估表明，该方法与最先进的全监督方法表现相当，并且仅使用1%的直接监督在BEV上就达到了竞争性的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Bird's-Eye-View (BEV) semantic maps have become an essential component of automated driving pipelines due to the rich representation they provide for decision-making tasks. However, existing approaches for generating these maps still follow a fully supervised training paradigm and hence rely on large amounts of annotated BEV data. In this work, we address this limitation by proposing the first self-supervised approach for generating a BEV semantic map using a single monocular image from the frontal view (FV). During training, we overcome the need for BEV ground truth annotations by leveraging the more easily available FV semantic annotations of video sequences. Thus, we propose the SkyEye architecture that learns based on two modes of self-supervision, namely, implicit supervision and explicit supervision. Implicit supervision trains the model by enforcing spatial consistency of the scene over time based on FV semantic sequences, while explicit supervision exploits BEV pseudolabels generated from FV semantic annotations and self-supervised depth estimates. Extensive evaluations on the KITTI-360 dataset demonstrate that our self-supervised approach performs on par with the state-of-the-art fully supervised methods and achieves competitive results using only 1% of direct supervision in BEV compared to fully supervised approaches. Finally, we publicly release both our code and the BEV datasets generated from the KITTI-360 and Waymo datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2114.Boosting Weakly-Supervised Temporal Action Localization With Text Information</span><br>
                <span class="as">Li, GuozhangandCheng, DeandDing, XinpengandWang, NannanandWang, XiaoyuandGao, Xinbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Boosting_Weakly-Supervised_Temporal_Action_Localization_With_Text_Information_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10648-10657.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于缺乏时间标注，当前的弱监督时序动作定位（WTAL）方法通常陷入过度完整或不完整的定位。<br>
                    动机：本文旨在利用文本信息从两个方面提升WTAL，即扩大类别间差异的判别目标和增强类别内完整性的生成目标。<br>
                    方法：对于判别目标，提出了一种文本段挖掘（TSM）机制，根据动作类别标签构建一个文本描述，并将文本视为查询以挖掘所有与类别相关的段。在没有动作的时间标注的情况下，TSM将文本查询与整个数据集中的视频进行比较，挖掘最佳匹配的段，同时忽略无关的段。<br>
                    效果：在THUMOS14和ActivityNet1.3上取得了最先进的性能。令人惊讶的是，我们发现该方法可以无缝地应用于现有的方法，并以明显的幅度提高其性能。代码可在https://github.com/lgzlIlIlI/Boosting-WTAL获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage the text information to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose a Text-Segment Mining (TSM) mechanism, which constructs a text description based on the action class label, and regards the text as the query to mine all class-related segments. Without the temporal annotation of actions, TSM compares the text query with the entire videos across the dataset to mine the best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-text Language Completion (VLC), which focuses on all semantic-related segments from videos to complete the text sentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2115.Weakly Supervised Class-Agnostic Motion Prediction for Autonomous Driving</span><br>
                <span class="as">Li, RuiboandShi, HanyuandFu, ZiangandWang, ZheandLin, Guosheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Weakly_Supervised_Class-Agnostic_Motion_Prediction_for_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17599-17608.png><br>
            
            <span class="tt"><span class="t0">研究问题：理解动态环境中的运动行为对于自动驾驶至关重要，这在LiDAR点云中对类别无关的运动预测引起了越来越多的关注。<br>
                    动机：户外场景通常可以被分解为移动前景和静态背景，这使得我们可以将运动理解与场景解析联系起来。<br>
                    方法：我们提出了一种新的弱监督运动预测范式，其中使用完全或部分（1%，0.1%）注释的前景/背景二进制掩码进行监督，而不是昂贵的运动注释。为此，我们设计了一个两阶段弱监督方法，其中第一阶段使用不完整的二进制掩码训练的分割模型将通过预先估计可能的移动前景来促进第二阶段的自我监督运动预测网络的学习。<br>
                    效果：实验表明，使用完全或部分二进制掩码作为监督，我们的弱监督模型大大超过了自我监督模型，并且与一些有监督的模型表现相当。这进一步证明了我们的方法在注释努力和性能之间取得了良好的平衡。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Understanding the motion behavior of dynamic environments is vital for autonomous driving, leading to increasing attention in class-agnostic motion prediction in LiDAR point clouds. Outdoor scenes can often be decomposed into mobile foregrounds and static backgrounds, which enables us to associate motion understanding with scene parsing. Based on this observation, we study a novel weakly supervised motion prediction paradigm, where fully or partially (1%, 0.1%) annotated foreground/background binary masks rather than expensive motion annotations are used for supervision. To this end, we propose a two-stage weakly supervised approach, where the segmentation model trained with the incomplete binary masks in Stage1 will facilitate the self-supervised learning of the motion prediction network in Stage2 by estimating possible moving foregrounds in advance. Furthermore, for robust self-supervised motion learning, we design a Consistency-aware Chamfer Distance loss by exploiting multi-frame information and explicitly suppressing potential outliers. Comprehensive experiments show that, with fully or partially binary masks as supervision, our weakly supervised models surpass the self-supervised models by a large margin and perform on par with some supervised ones. This further demonstrates that our approach achieves a good compromise between annotation effort and performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2116.Mask DINO: Towards a Unified Transformer-Based Framework for Object Detection and Segmentation</span><br>
                <span class="as">Li, FengandZhang, HaoandXu, HuaizheandLiu, ShilongandZhang, LeiandNi, LionelM.andShum, Heung-Yeung</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3041-3050.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种统一的物体检测和分割框架Mask DINO。<br>
                    动机：通过添加一个支持所有图像分割任务（实例、全景和语义）的掩码预测分支，扩展了DINO（具有改进的去噪锚框的DETR）。<br>
                    方法：使用来自DINO的查询嵌入点积高分辨率像素嵌入地图来预测一组二进制掩码。通过共享架构和训练过程，将DINO中的一些关键组件扩展到分割。<br>
                    效果：实验表明，Mask DINO在ResNet-50主干和预训练模型SwinL主干上，显著优于所有现有的专用分割方法。特别是在实例分割（COCO上54.5 AP）、全景分割（COCO上59.4 PQ）和语义分割（ADE20K上60.8 mIoU）方面，Mask DINO在十亿参数以下的模型中建立了迄今为止最好的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, scalable, and benefits from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K) among models under one billion parameters. We will release the code after the blind review.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2117.Self-Supervised AutoFlow</span><br>
                <span class="as">Huang, Hsin-PingandHerrmann, CharlesandHur, JunhwaandLu, ErikaandSargent, KyleandStone, AustinandYang, Ming-HsuanandSun, Deqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Self-Supervised_AutoFlow_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11412-11421.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用自我监督学习在没有目标领域真实标签的情况下，训练光学流的训练集。<br>
                    动机：观察到真实标签搜索度量与自我监督损失之间的强相关性，提出自我监督的AutoFlow来处理没有真实标签的真实世界视频。<br>
                    方法：使用自我监督损失作为搜索度量，我们的自我监督AutoFlow在Sintel和KITTI上的表现与有真实标签的AutoFlow相当，并在真实的DAVIS数据集上表现更好。<br>
                    效果：在半监督设置中使用自我监督AutoFlow，并与最先进的技术进行比较，得到了具有竞争力的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, AutoFlow has shown promising results on learning a training set for optical flow, but requires ground truth labels in the target domain to compute its search metric. Observing a strong correlation between the ground truth search metric and self-supervised losses, we introduce self-supervised AutoFlow to handle real-world videos without ground truth labels. Using self-supervised loss as the search metric, our self-supervised AutoFlow performs on par with AutoFlow on Sintel and KITTI where ground truth is available, and performs better on the real-world DAVIS dataset. We further explore using self-supervised AutoFlow in the (semi-)supervised setting and obtain competitive results against the state of the art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2118.MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube Partition and Recovery</span><br>
                <span class="as">Chen, DuowenandBai, YunhaoandShen, WeiandLi, QingliandYu, LequanandWang, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MagicNet_Semi-Supervised_Multi-Organ_Segmentation_via_Magic-Cube_Partition_and_Recovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23869-23878.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的教师-学生模型用于半监督多器官分割。<br>
                    动机：利用先验解剖结构作为强大的工具来引导数据增强，减少标记和未标记图像之间的不匹配，以进行半监督学习。<br>
                    方法：提出了一种基于分区-恢复N^3立方体交叉和跨标注与未标注图像的数据增强策略。<br>
                    效果：在两个公共CT多器官数据集上进行的大量实验表明，MagicNet的有效性，并显著优于最先进的半监督医学图像分割方法，在10%标记图像的MACT数据集上提高了+7%的DSC。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel teacher-student model for semi-supervised multi-organ segmentation. In the teacher-student model, data augmentation is usually adopted on unlabeled data to regularize the consistent training between teacher and student. We start from a key perspective that fixed relative locations and variable sizes of different organs can provide distribution information where a multi-organ CT scan is drawn. Thus, we treat the prior anatomy as a strong tool to guide the data augmentation and reduce the mismatch between labeled and unlabeled images for semi-supervised learning. More specifically, we propose a data augmentation strategy based on partition-and-recovery N^3 cubes cross- and within- labeled and unlabeled images. Our strategy encourages unlabeled images to learn organ semantics in relative locations from the labeled images (cross-branch) and enhances the learning ability for small organs (within-branch). For within-branch, we further propose to refine the quality of pseudo labels by blending the learned representations from small cubes to incorporate local attributes. Our method is termed as MagicNet, since it treats the CT volume as a magic-cube and N^3-cube partition-and-recovery process matches with the rule of playing a magic-cube. Extensive experiments on two public CT multi-organ datasets demonstrate the effectiveness of MagicNet, and noticeably outperforms state-of-the-art semi-supervised medical image segmentation approaches, with +7% DSC improvement on MACT dataset with 10% labeled images.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2119.Few-Shot Geometry-Aware Keypoint Localization</span><br>
                <span class="as">He, XingzheandBharaj, GauravandFerman, DavidandRhodin, HelgeandGarrido, Pablo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Few-Shot_Geometry-Aware_Keypoint_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21337-21348.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前的监督关键点定位方法依赖于大量手动标注的图像数据集，但创建这样的大型关键点标签既耗时又昂贵，且由于标签不一致而容易出错。<br>
                    动机：因此，我们希望有一种方法可以在较少但一致标注的图像上学习关键点定位。<br>
                    方法：我们提出了一种新的方法，通过自我监督使用更大的未标注数据集来扩展用户标记的2D图像，学习定位语义一致的关键点定义，甚至对遮挡区域也可以进行定位。<br>
                    效果：我们的方法在几个数据集上取得了竞争或最先进的准确性，包括人脸、眼睛、动物、汽车以及从未尝试过的嘴部内部（牙齿）定位任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Supervised keypoint localization methods rely on large manually labeled image datasets, where objects can deform, articulate, or occlude. However, creating such large keypoint labels is time-consuming and costly, and is often error-prone due to inconsistent labeling. Thus, we desire an approach that can learn keypoint localization with fewer yet consistently annotated images. To this end, we present a novel formulation that learns to localize semantically consistent keypoint definitions, even for occluded regions, for varying object categories. We use a few user-labeled 2D images as input examples, which are extended via self-supervision using a larger unlabeled dataset. Unlike unsupervised methods, the few-shot images act as semantic shape constraints for object localization. Furthermore, we introduce 3D geometry-aware constraints to uplift keypoints, achieving more accurate 2D localization. Our general-purpose formulation paves the way for semantically conditioned generative modeling and attains competitive or state-of-the-art accuracy on several datasets, including human faces, eyes, animals, cars, and never-before-seen mouth interior (teeth) localization tasks, not attempted by the previous few-shot methods. Project page: https://xingzhehe.github.io/FewShot3DKP/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2120.PEFAT: Boosting Semi-Supervised Medical Image Classification via Pseudo-Loss Estimation and Feature Adversarial Training</span><br>
                <span class="as">Zeng, QingjieandXie, YutongandLu, ZilinandXia, Yong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_PEFAT_Boosting_Semi-Supervised_Medical_Image_Classification_via_Pseudo-Loss_Estimation_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15671-15680.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高半监督学习在计算机视觉和医学影像分类任务中的性能。<br>
                    动机：现有的半监督学习方法主要从模型预测概率的角度寻找高置信度的伪标签样本，但这种方法可能会引入错误伪标签数据，且常常忽视低置信度样本的潜力。<br>
                    方法：提出一种新颖的Pseudo-loss Estimation and Feature Adversarial Training（PEFAT）半监督框架，通过损失分布建模和对抗训练来提升多类别和多标签的医疗图像分类性能。具体包括开发一个可信赖的数据选择方案来分割高质量的伪标签集，以及通过在特征级别注入对抗性噪声来学习区分信息，从而平滑决策边界。<br>
                    效果：在三个医疗和两个自然图像基准测试集上的实验结果表明，PEFAT能够取得良好的性能并超越其他最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Pseudo-labeling approaches have been proven beneficial for semi-supervised learning (SSL) schemes in computer vision and medical imaging. Most works are dedicated to finding samples with high-confidence pseudo-labels from the perspective of model predicted probability. Whereas this way may lead to the inclusion of incorrectly pseudo-labeled data if the threshold is not carefully adjusted. In addition, low-confidence probability samples are frequently disregarded and not employed to their full potential. In this paper, we propose a novel Pseudo-loss Estimation and Feature Adversarial Training semi-supervised framework, termed as PEFAT, to boost the performance of multi-class and multi-label medical image classification from the point of loss distribution modeling and adversarial training. Specifically, we develop a trustworthy data selection scheme to split a high-quality pseudo-labeled set, inspired by the dividable pseudo-loss assumption that clean data tend to show lower loss while noise data is the opposite. Instead of directly discarding these samples with low-quality pseudo-labels, we present a novel regularization approach to learn discriminate information from them via injecting adversarial noises at the feature-level to smooth the decision boundary. Experimental results on three medical and two natural image benchmarks validate that our PEFAT can achieve a promising performance and surpass other state-of-the-art methods. The code is available at https://github.com/maxwell0027/PEFAT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2121.Learning To Segment Every Referring Object Point by Point</span><br>
                <span class="as">Qu, MengxueandWu, YuandWei, YunchaoandLiu, WuandLiang, XiaodanandZhao, Yao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3021-3030.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决参照表达式分割（RES）的问题，即如何实现视觉和语言在像素级别的语义对齐。<br>
                    动机：现有的RES方法大多需要大量的像素级标注，这既昂贵又繁琐。<br>
                    方法：本文提出了一种新的部分监督训练模式，即使用丰富的参照边界框和少量（如1%）的像素级参照掩码进行训练。为了最大限度地提高REC模型的可转移性，我们构建了一个基于点序列预测模型的模型。我们还提出了共同内容教师强制策略，使模型明确地将点坐标（尺度值）与被参照的空间特征关联起来，以减轻由于有限的分割掩码引起的暴露偏差。<br>
                    效果：实验表明，当我们只使用1%的掩码标注时，我们的模型在RefCOCO+@testA上达到了52.06%的准确率（全监督设置下为58.93%）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring Expression Segmentation (RES) can facilitate pixel-level semantic alignment between vision and language. Most of the existing RES approaches require massive pixel-level annotations, which are expensive and exhaustive. In this paper, we propose a new partially supervised training paradigm for RES, i.e., training using abundant referring bounding boxes and only a few (e.g., 1%) pixel-level referring masks. To maximize the transferability from the REC model, we construct our model based on the point-based sequence prediction model. We propose the co-content teacher-forcing to make the model explicitly associate the point coordinates (scale values) with the referred spatial features, which alleviates the exposure bias caused by the limited segmentation masks. To make the most of referring bounding box annotations, we further propose the resampling pseudo points strategy to select more accurate pseudo-points as supervision. Extensive experiments show that our model achieves 52.06% in terms of accuracy (versus 58.93% in fully supervised setting) on RefCOCO+@testA, when only using 1% of the mask annotations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2122.Bootstrapping Objectness From Videos by Relaxed Common Fate and Visual Grouping</span><br>
                <span class="as">Lian, LongandWu, ZhirongandYu, StellaX.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lian_Bootstrapping_Objectness_From_Videos_by_Relaxed_Common_Fate_and_Visual_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14582-14591.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从无标签视频中学习物体分割。<br>
                    动机：人类可以轻易地分割移动的物体，而无需知道它们是什么。基于运动的分割启发了基于共同命运的无监督物体发现。然而，共同命运并不是对象性的可靠指标。<br>
                    方法：首先通过放松的共同命运学习图像特征，然后根据图像本身和跨图像的视觉外观分组进行统计细化。具体来说，我们首先在近似光学流与常数段流加上小段内残差流的循环中学习图像分割器，然后通过更连贯的外观和统计形状-背景相关性进行细化。<br>
                    效果：在无监督视频物体分割上，仅使用ResNet和卷积头，我们的模型在DAVIS16 / STv2 / FBMS59上分别以绝对增益7/9/5%超越现有技术，证明了我们的想法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study learning object segmentation from unlabeled videos. Humans can easily segment moving objects without knowing what they are. The Gestalt law of common fate, i.e., what move at the same speed belong together, has inspired unsupervised object discovery based on motion segmentation. However, common fate is not a reliable indicator of objectness: Parts of an articulated / deformable object may not move at the same speed, whereas shadows / reflections of an object always move with it but are not part of it. Our insight is to bootstrap objectness by first learning image features from relaxed common fate and then refining them based on visual appearance grouping within the image itself and across images statistically. Specifically, we learn an image segmenter first in the loop of approximating optical flow with constant segment flow plus small within-segment residual flow, and then by refining it for more coherent appearance and statistical figure-ground relevance. On unsupervised video object segmentation, using only ResNet and convolutional heads, our model surpasses the state-of-the-art by absolute gains of 7/9/5% on DAVIS16 / STv2 / FBMS59 respectively, demonstrating the effectiveness of our ideas. Our code is publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2123.Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification \&amp; Segmentation</span><br>
                <span class="as">Kang, DahyunandKoniusz, PiotrandCho, MinsuandMurray, Naila</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Distilling_Self-Supervised_Vision_Transformers_for_Weakly-Supervised_Few-Shot_Classification__Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19627-19638.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督的少样本图像分类和分割任务。<br>
                    动机：利用预训练的自我监督视觉转换器（ViT）进行弱监督的少样本图像分类和分割。<br>
                    方法：通过自我注意力机制，利用自我监督ViT的标记表示及其相关性，通过单独的任务头产生分类和分割预测。<br>
                    效果：在各种监督设置下，特别是在几乎没有像素级标签的情况下，实验结果在Pascal-5i和COCO-20i上表现出显著的性能提升。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We address the task of weakly-supervised few-shot image classification and segmentation, by leveraging a Vision Transformer (ViT) pretrained with self-supervision. Our proposed method takes token representations from the self-supervised ViT and leverages their correlations, via self-attention, to produce classification and segmentation predictions through separate task heads. Our model is able to effectively learn to perform classification and segmentation in the absence of pixel-level labels during training, using only image-level labels. To do this it uses attention maps, created from tokens generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We also explore a practical setup with "mixed" supervision, where a small number of training images contains ground-truth pixel-level labels and the remaining images have only image-level labels. For this mixed setup, we propose to improve the pseudo-labels using a pseudo-label enhancer that was trained using the available ground-truth pixel-level labels. Experiments on Pascal-5i and COCO-20i demonstrate significant performance gains in a variety of supervision settings, and in particular when little-to-no pixel-level labels are available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2124.Collaborative Noisy Label Cleaner: Learning Scene-Aware Trailers for Multi-Modal Highlight Detection in Movies</span><br>
                <span class="as">Gan, BeiandShu, XiujunandQiao, RuizhiandWu, HaoqianandChen, KeyuandLi, HanjunandRen, Bo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gan_Collaborative_Noisy_Label_Cleaner_Learning_Scene-Aware_Trailers_for_Multi-Modal_Highlight_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18898-18907.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从电影预告片中检测出电影的精彩片段，并处理不同标注者在标注过程中的不确定性。<br>
                    动机：目前的电影精彩片段检测方法需要大量的手动标注，且存在标注不准确和耗时的问题。此外，现有的视频语料库虽然可以用于训练，但往往信息杂乱且不完整。<br>
                    方法：本文提出了一种新的学习方式，即将精彩片段检测视为“学习有噪声的标签”。首先，利用场景分割从电影预告片中获取完整的镜头作为有噪声的标签。然后，提出一个协作式噪声标签清理（CLC）框架来从有噪声的精彩片段中学习。CLC包括两个模块：增强型交叉传播（ACP）和多模态清理（MMC）。前者旨在利用紧密相关的视听信号并融合它们以学习统一的多模态表示。后者旨在通过观察不同模态之间的损失变化来实现更清晰的精彩片段标签。<br>
                    效果：在MovieLights和YouTube Highlights数据集上进行的全面实验证明了该方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Movie highlights stand out of the screenplay for efficient browsing and play a crucial role on social media platforms. Based on existing efforts, this work has two observations: (1) For different annotators, labeling highlight has uncertainty, which leads to inaccurate and time-consuming annotations. (2) Besides previous supervised or unsupervised settings, some existing video corpora can be useful, e.g., trailers, but they are often noisy and incomplete to cover the full highlights. In this work, we study a more practical and promising setting, i.e., reformulating highlight detection as "learning with noisy labels". This setting does not require time-consuming manual annotations and can fully utilize existing abundant video corpora. First, based on movie trailers, we leverage scene segmentation to obtain complete shots, which are regarded as noisy labels. Then, we propose a Collaborative noisy Label Cleaner (CLC) framework to learn from noisy highlight moments. CLC consists of two modules: augmented cross-propagation (ACP) and multi-modality cleaning (MMC). The former aims to exploit the closely related audio-visual signals and fuse them to learn unified multi-modal representations. The latter aims to achieve cleaner highlight labels by observing the changes in losses among different modalities. To verify the effectiveness of CLC, we further collect a large-scale highlight dataset named MovieLights. Comprehensive experiments on MovieLights and YouTube Highlights datasets demonstrate the effectiveness of our approach. Code has been made available at: https://github.com/TencentYoutuResearch/HighlightDetection-CLC</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2125.Contrastive Mean Teacher for Domain Adaptive Object Detectors</span><br>
                <span class="as">Cao, ShengcaoandJoshi, DhirajandGui, Liang-YanandWang, Yu-Xiong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Contrastive_Mean_Teacher_for_Domain_Adaptive_Object_Detectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23839-23848.png><br>
            
            <span class="tt"><span class="t0">研究问题：目标检测器在训练（源领域）和现实世界应用（目标领域）之间存在域差距。<br>
                    动机：均值教师自我训练是对象检测中强大的无监督领域适应范例，但难以处理低质量的伪标签。<br>
                    方法：我们提出了对比性均值教师（CMT），这是一个统一的通用框架，自然地整合了两种范例，以最大化有益的学习信号。<br>
                    效果：当与最新的均值教师自我训练方法结合使用时，CMT在目标领域的性能达到了新的最高水平，例如在Foggy Cityscapes上达到51.9% mAP，比之前的最佳水平高出2.1% mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object detectors often suffer from the domain gap between training (source domain) and real-world applications (target domain). Mean-teacher self-training is a powerful paradigm in unsupervised domain adaptation for object detection, but it struggles with low-quality pseudo-labels. In this work, we identify the intriguing alignment and synergy between mean-teacher self-training and contrastive learning. Motivated by this, we propose Contrastive Mean Teacher (CMT) -- a unified, general-purpose framework with the two paradigms naturally integrated to maximize beneficial learning signals. Instead of using pseudo-labels solely for final predictions, our strategy extracts object-level features using pseudo-labels and optimizes them via contrastive learning, without requiring labels in the target domain. When combined with recent mean-teacher self-training methods, CMT leads to new state-of-the-art target-domain performance: 51.9% mAP on Foggy Cityscapes, outperforming the previously best by 2.1% mAP. Notably, CMT can stabilize performance and provide more significant gains as pseudo-label noise increases.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2126.Primitive Generation and Semantic-Related Alignment for Universal Zero-Shot Segmentation</span><br>
                <span class="as">He, ShutingandDing, HenghuiandJiang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Primitive_Generation_and_Semantic-Related_Alignment_for_Universal_Zero-Shot_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11238-11247.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在实现无需任何训练样本的通用零样本分割，以实现对新类别的全景、实例和语义分割。<br>
                    动机：现有的零样本分割能力依赖于语义空间中的类间关系，将从已见类别中学习到的视觉知识转移到未见类别上。因此，需要很好地连接语义-视觉空间，并将语义关系应用于视觉特征学习。<br>
                    方法：我们引入了一个生成模型来为未见类别合成特征，该模型连接了语义和视觉空间，并解决了缺乏未见训练数据的问题。此外，为了缓解语义和视觉空间之间的领域差距，我们首先通过学习与类别相关的细粒度属性来增强基本的生成器，然后通过选择性地组装这些原语来合成未见特征。其次，我们提出将视觉特征分解为与语义相关部分和与语义无关部分，后者包含有用的视觉分类线索，但对语义表示的相关性较低。然后需要使与语义相关的视觉特征的类间关系与语义空间中的那些对齐，从而将语义知识转移到视觉特征学习中。所提出的方法在零样本全景分割、实例分割和语义分割方面取得了令人印象深刻的最先进的性能。<br>
                    效果：当与最新的均值教师自我训练方法结合使用时，CMT在目标领域的性能达到了新的最高水平，例如在Foggy Cityscapes上达到51.9% mAP，比之前的最佳水平高出2.1% mAP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study universal zero-shot segmentation in this work to achieve panoptic, instance, and semantic segmentation for novel categories without any training samples. Such zero-shot segmentation ability relies on inter-class relationships in semantic space to transfer the visual knowledge learned from seen categories to unseen ones. Thus, it is desired to well bridge semantic-visual spaces and apply the semantic relationships to visual feature learning. We introduce a generative model to synthesize features for unseen categories, which links semantic and visual spaces as well as address the issue of lack of unseen training data. Furthermore, to mitigate the domain gap between semantic and visual spaces, firstly, we enhance the vanilla generator with learned primitives, each of which contains fine-grained attributes related to categories, and synthesize unseen features by selectively assembling these primitives. Secondly, we propose to disentangle the visual feature into the semantic-related part and the semantic-unrelated part that contains useful visual classification clues but is less relevant to semantic representation. The inter-class relationships of semantic-related visual features are then required to be aligned with those in semantic space, thereby transferring semantic knowledge to visual feature learning. The proposed approach achieves impressively state-of-the-art performance on zero-shot panoptic segmentation, instance segmentation, and semantic segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2127.HandsOff: Labeled Dataset Generation With No Additional Human Annotations</span><br>
                <span class="as">Xu, AustinandVasileva, MariyaI.andDave, AchalandSeshadri, Arjun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_HandsOff_Labeled_Dataset_Generation_With_No_Additional_Human_Annotations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7991-8000.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成大量带标签的合成数据集，同时避免依赖人工标注和保证生成标签的质量。<br>
                    动机：现有的基于生成对抗网络（GANs）的数据集生成方法需要新的合成图像标注，这限制了其应用。<br>
                    方法：提出HandsOff框架，该框架通过在少量预先标记的图像上进行训练，可以生成无限数量的合成图像和相应的标签。<br>
                    效果：该方法在多个具有挑战性的领域（如人脸、汽车、全身人体姿态和城市驾驶场景）中生成了带有丰富像素级标签的数据集，并在语义分割、关键点检测和深度估计等任务上取得了优于现有数据集生成方法和迁移学习基线的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent work leverages the expressive power of genera- tive adversarial networks (GANs) to generate labeled syn- thetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We in- troduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and cor- responding labels after being trained on less than 50 pre- existing labeled images. Our framework avoids the practi- cal drawbacks of prior work by unifying the field of GAN in- version with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth es- timation compared to prior dataset generation approaches and transfer learning baselines. We additionally showcase its ability to address broad challenges in model develop- ment which stem from fixed, hand-annotated datasets, such as the long-tail problem in semantic segmentation. Project page: austinxu87.github.io/handsoff.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2128.Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module</span><br>
                <span class="as">Huang, LinzhiandLi, YulongandTian, HongboandYang, YueandLi, XiangangandDeng, WeihongandYe, Jieping</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Semi-Supervised_2D_Human_Pose_Estimation_Driven_by_Position_Inconsistency_Pseudo_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/693-703.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决半监督二维人体姿态估计中的问题，包括大研究问题：本文旨在解决半监督二维人体姿态估计中的问题，包括大模型和小模型交互训练时小模型的伪标签对大模型的引导问题，以及噪声伪标签对训练的负面影响。<br>
                    动机：目前的2D人体姿态估计方法忽视了这两个问题，并且其使用的标签（关键点类别和位置）相对复杂。<br>
                    方法：本文提出了一个由位置不一致伪标签修正模块驱动的半监督二维人体姿态估计框架（SSPCM）。通过引入额外的辅助教师和使用两个教师在不同时期生成的伪标签来计算不一致性分数并去除异常值，然后通过交互式训练更新两个教师模型，使用两个教师生成的伪标签更新学生模型。此外，还使用了基于伪关键点感知的半监督切割遮挡来生成更多困难且有效的样本。<br>
                    效果：实验表明，该方法优于之前的最好的半监督二维人体姿态估计方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we delve into semi-supervised 2D human pose estimation. The previous method ignored two problems: (i) When conducting interactive training between large model and lightweight model, the pseudo label of lightweight model will be used to guide large models. (ii) The negative impact of noise pseudo labels on training. Moreover, the labels used for 2D human pose estimation are relatively complex: keypoint category and keypoint position. To solve the problems mentioned above, we propose a semi-supervised 2D human pose estimation framework driven by a position inconsistency pseudo label correction module (SSPCM). We introduce an additional auxiliary teacher and use the pseudo labels generated by the two teacher model in different periods to calculate the inconsistency score and remove outliers. Then, the two teacher models are updated through interactive training, and the student model is updated using the pseudo labels generated by two teachers. To further improve the performance of the student model, we use the semi-supervised Cut-Occlude based on pseudo keypoint perception to generate more hard and effective samples. In addition, we also proposed a new indoor overhead fisheye human keypoint dataset WEPDTOF-Pose. Extensive experiments demonstrate that our method outperforms the previous best semi-supervised 2D human pose estimation method. We will release the code and dataset at https://github.com/hlz0606/SSPCM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2129.X3KD: Knowledge Distillation Across Modalities, Tasks and Stages for Multi-Camera 3D Object Detection</span><br>
                <span class="as">Klingner, MarvinandBorse, ShubhankarandKumar, VarunRaviandRezaei, BehnazandNarayanan, VenkatramanandYogamani, SenthilandPorikli, Fatih</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Klingner_X3KD_Knowledge_Distillation_Across_Modalities_Tasks_and_Stages_for_Multi-Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13343-13353.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决基于多相机图像的环视3D物体检测（3DOD）在特征视图转换过程中由于缺失深度信息导致的模糊问题。<br>
                    动机：目前的多相机3DOD模型在将特征从透视视图转换为3D世界表示时，由于缺失深度信息，导致结果模糊不清。<br>
                    方法：本文提出了一种跨模态、任务和阶段的全面知识蒸馏框架X3KD，包括跨任务蒸馏、跨模态特征蒸馏、对抗训练和跨模态输出蒸馏等步骤。<br>
                    效果：实验结果表明，X3KD模型在nuScenes和Waymo数据集上的表现优于先前最先进的方法，并能推广到雷达基的3DOD。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in 3D object detection (3DOD) have obtained remarkably strong results for LiDAR-based models. In contrast, surround-view 3DOD models based on multiple camera images underperform due to the necessary view transformation of features from perspective view (PV) to a 3D world representation which is ambiguous due to missing depth information. This paper introduces X3KD, a comprehensive knowledge distillation framework across different modalities, tasks, and stages for multi-camera 3DOD. Specifically, we propose cross-task distillation from an instance segmentation teacher (X-IS) in the PV feature extraction stage providing supervision without ambiguous error backpropagation through the view transformation. After the transformation, we apply cross-modal feature distillation (X-FD) and adversarial training (X-AT) to improve the 3D world representation of multi-camera features through the information contained in a LiDAR-based 3DOD teacher. Finally, we also employ this teacher for cross-modal output distillation (X-OD), providing dense supervision at the prediction stage. We perform extensive ablations of knowledge distillation at different stages of multi-camera 3DOD. Our final X3KD model outperforms previous state-of-the-art approaches on the nuScenes and Waymo datasets and generalizes to RADAR-based 3DOD. Qualitative results video at https://youtu.be/1do9DPFmr38.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2130.Optimal Transport Minimization: Crowd Localization on Density Maps for Semi-Supervised Counting</span><br>
                <span class="as">Lin, WeiandChan, AntoniB.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Optimal_Transport_Minimization_Crowd_Localization_on_Density_Maps_for_Semi-Supervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21663-21673.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高人群密度地图中人群定位的准确性。<br>
                    动机：尽管深度神经网络在预测人群密度图方面取得了显著进步，但大多数方法并未进一步探索在密度图中定位人群的能力。<br>
                    方法：本文提出了一种基于最优传输最小化（OT-M）算法的人群密度图定位方法。该方法的目标是找到与输入密度图具有最小Sinkhorn距离的目标点图，并提出了计算解决方案的迭代算法。<br>
                    效果：通过将OT-M应用于生成硬伪标签（点图），而不是先前方法中使用的软伪标签（密度图），我们的方法在人群定位和半监督计数方面都取得了出色的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The accuracy of crowd counting in images has improved greatly in recent years due to the development of deep neural networks for predicting crowd density maps. However, most methods do not further explore the ability to localize people in the density map, with those few works adopting simple methods, like finding the local peaks in the density map. In this paper, we propose the optimal transport minimization (OT-M) algorithm for crowd localization with density maps. The objective of OT-M is to find a target point map that has the minimal Sinkhorn distance with the input density map, and we propose an iterative algorithm to compute the solution. We then apply OT-M to generate hard pseudo-labels (point maps) for semi-supervised counting, rather than the soft pseudo-labels (density maps) used in previous methods. Our hard pseudo-labels provide stronger supervision, and also enable the use of recent density-to-point loss functions for training. We also propose a confidence weighting strategy to give higher weight to the more reliable unlabeled data. Extensive experiments show that our methods achieve outstanding performance on both crowd localization and semi-supervised counting. Code is available at https://github.com/Elin24/OT-M.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2131.L-CoIns: Language-Based Colorization With Instance Awareness</span><br>
                <span class="as">Chang, ZhengandWeng, ShuchenandZhang, PeixuanandLi, YuandLi, SiandShi, Boxin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19221-19230.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过语言描述自动生成与图像内容一致的颜色，并解决颜色-对象耦合和不匹配的问题。<br>
                    动机：现有的方法在处理同一对象词时仍存在困难，需要一种能够实现实例感知的方法。<br>
                    方法：提出一种基于变压器的框架，通过自动聚合相似的图像块来实现实例感知，同时应用亮度增强和反色损失来打破亮度和颜色词之间的统计相关性。<br>
                    效果：收集了一个具有独特视觉特征和详细语言描述的数据集，实验证明该方法可以生成视觉上令人愉悦且与描述一致的实例感知颜色化结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Language-based colorization produces plausible colors consistent with the language description provided by the user. Recent studies introduce additional annotation to prevent color-object coupling and mismatch issues, but they still have difficulty in distinguishing instances corresponding to the same object words. In this paper, we propose a transformer-based framework to automatically aggregate similar image patches and achieve instance awareness without any additional knowledge. By applying our presented luminance augmentation and counter-color loss to break down the statistical correlation between luminance and color words, our model is driven to synthesize colors with better descriptive consistency. We further collect a dataset to provide distinctive visual characteristics and detailed language descriptions for multiple instances in the same image. Extensive experiments demonstrate our advantages of synthesizing visually pleasing and description-consistent results of instance-aware colorization.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2132.MixTeacher: Mining Promising Labels With Mixed Scale Teacher for Semi-Supervised Object Detection</span><br>
                <span class="as">Liu, LiangandZhang, BoshenandZhang, JiangningandZhang, WuhaoandGan, ZhenyeandTian, GuanzhongandZhu, WenbingandWang, YabiaoandWang, Chengjie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7370-7379.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决目标检测中物体实例尺度变化的关键挑战，特别是在半监督情况下。<br>
                    动机：虽然现代检测模型在处理尺度变化方面取得了显著进展，但在半监督情况下，尺度变化仍然会带来麻烦。大多数现有的半监督目标检测方法依赖于严格的条件来从网络预测中筛选出高质量的伪标签。然而，我们发现尺度极端的物体往往具有低置信度，这使得这些物体缺乏积极的监督。<br>
                    方法：我们深入研究了尺度变化问题，并提出了一个新颖的框架，通过引入混合尺度教师来改进伪标签生成和尺度不变学习。此外，受益于混合尺度特征的更好预测，我们提出了通过跨尺度预测的得分提升来挖掘伪标签。<br>
                    效果：我们在MS COCO和PASCAL VOC基准测试集上进行了大量实验，结果表明我们的方法在各种半监督设置下都达到了新的最先进的性能。代码和模型将公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Scale variation across object instances is one of the key challenges in object detection. Although modern detection models have achieved remarkable progress in dealing with the scale variation, it still brings trouble in the semi-supervised case. Most existing semi-supervised object detection methods rely on strict conditions to filter out high-quality pseudo labels from the network predictions. However, we observe that objects with extreme scale tend to have low confidence, which makes the positive supervision missing for these objects. In this paper, we delve into the scale variation problem, and propose a novel framework by introducing a mixed scale teacher to improve the pseudo labels generation and scale invariant learning. In addition, benefiting from the better predictions from mixed scale features, we propose to mine pseudo labels with the score promotion of predictions across scales. Extensive experiments on MS COCO and PASCAL VOC benchmarks under various semi-supervised settings demonstrate that our method achieves new state-of-the-art performance. The code and models will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2133.Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation</span><br>
                <span class="as">Bai, YunhaoandChen, DuowenandLi, QingliandShen, WeiandWang, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Bidirectional_Copy-Paste_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11514-11524.png><br>
            
            <span class="tt"><span class="t0">研究问题：半监督医疗图像分割中存在标记和未标记数据分布的经验不匹配问题。<br>
                    动机：如果将标记和未标记的数据分开处理，或者以不一致的方式训练标记和未标记的数据，那么从标记数据中学到的知识可能会被大量丢弃。<br>
                    方法：提出一种直接的方法来缓解这个问题——在简单的Mean Teacher架构中，双向复制粘贴标记和未标记的数据。这种方法鼓励未标记的数据从标记的数据中学习全面的共同语义。<br>
                    效果：实验结果表明，与其他最先进的半监督医疗图像分割数据集相比，该方法可以获得坚实的收益（例如，在ACDC数据集上获得超过21%的Dice改进，仅使用5%的标记数据）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In semi-supervised medical image segmentation, there exist empirical mismatch problems between labeled and unlabeled data distribution. The knowledge learned from the labeled data may be largely discarded if treating labeled and unlabeled data separately or training labeled and unlabeled data in an inconsistent manner. We propose a straightforward method for alleviating the problem -- copy-pasting labeled and unlabeled data bidirectionally, in a simple Mean Teacher architecture. The method encourages unlabeled data to learn comprehensive common semantics from the labeled data in both inward and outward directions. More importantly, the consistent learning procedure for labeled and unlabeled data can largely reduce the empirical distribution gap. In detail, we copy-paste a random crop from a labeled image (foreground) onto an unlabeled image (background) and an unlabeled image (foreground) onto a labeled image (background), respectively. The two mixed images are fed into a Student network. It is trained by the generated supervisory signal via bidirectional copy-pasting between the predictions of the unlabeled images from the Teacher and the label maps of the labeled images. We explore several design choices of how to copy-paste to make it more effective for minimizing empirical distribution gaps between labeled and unlabeled data. We reveal that the simple mechanism of copy-pasting bidirectionally between labeled and unlabeled data is good enough and the experiments show solid gains (e.g., over 21% Dice improvement on ACDC dataset with 5% labeled data) compared with other state-of-the-arts on various semi-supervised medical image segmentation datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2134.Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation</span><br>
                <span class="as">He, ShutingandDing, HenghuiandJiang, Wei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19498-19507.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高零样本实例分割的性能，特别是在未见过的类别上。<br>
                    动机：现有的模型在训练中存在对已见类别的强烈偏向，并且难以区分背景和未见过的对象。<br>
                    方法：提出D^2Zero模型，结合语义提升去偏和背景消歧技术，利用类间语义关系进行视觉特征训练，并学习基于输入图像的动态分类器，同时产生适应图像的背景表示以避免将新对象误认为背景。<br>
                    效果：实验表明，该方法显著优于现有最先进的方法，例如在COCO数据集上提高了16.86%的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Zero-shot instance segmentation aims to detect and precisely segment objects of unseen categories without any training samples. Since the model is trained on seen categories, there is a strong bias that the model tends to classify all the objects into seen categories. Besides, there is a natural confusion between background and novel objects that have never shown up in training. These two challenges make novel objects hard to be raised in the final instance segmentation results. It is desired to rescue novel objects from background and dominated seen categories. To this end, we propose D^2Zero with Semantic-Promoted Debiasing and Background Disambiguation to enhance the performance of Zero-shot instance segmentation. Semantic-promoted debiasing utilizes inter-class semantic relationships to involve unseen categories in visual feature training and learns an input-conditional classifier to conduct dynamical classification based on the input image. Background disambiguation produces image-adaptive background representation to avoid mistaking novel objects for background. Extensive experiments show that we significantly outperform previous state-of-the-art methods by a large margin, e.g., 16.86% improvement on COCO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2135.CoMFormer: Continual Learning in Semantic and Panoptic Segmentation</span><br>
                <span class="as">Cermelli, FabioandCord, MatthieuandDouillard, Arthur</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cermelli_CoMFormer_Continual_Learning_in_Semantic_and_Panoptic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3010-3020.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决语义分割和全景分割的持续学习问题。<br>
                    动机：现有的研究主要关注于语义分割，忽视了具有实际影响的全景分割。<br>
                    方法：提出CoMFormer模型，利用转换器架构的特性进行类别学习，并设计了一种新的自适应蒸馏损失函数和基于掩码的伪标签技术来有效防止遗忘。<br>
                    效果：在ADE20K数据集上进行的实验表明，CoMFormer在遗忘旧类和学习新类方面都优于现有方法，并在大规模持续语义分割场景中也显著优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Continual learning for segmentation has recently seen increasing interest. However, all previous works focus on narrow semantic segmentation and disregard panoptic segmentation, an important task with real-world impacts. In this paper, we present the first continual learning model capable of operating on both semantic and panoptic segmentation. Inspired by recent transformer approaches that consider segmentation as a mask-classification problem, we design CoMFormer. Our method carefully exploits the properties of transformer architectures to learn new classes over time. Specifically, we propose a novel adaptive distillation loss along with a mask-based pseudo-labeling technique to effectively prevent forgetting. To evaluate our approach, we introduce a novel continual panoptic segmentation benchmark on the challenging ADE20K dataset. Our CoMFormer outperforms all the existing baselines by forgetting less old classes but also learning more effectively new classes. In addition, we also report an extensive evaluation in the large-scale continual semantic segmentation scenario showing that CoMFormer also significantly outperforms state-of-the-art methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2136.Towards Effective Visual Representations for Partial-Label Learning</span><br>
                <span class="as">Xia, ShiyuandLv, JiaqiandXu, NingandNiu, GangandGeng, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_Towards_Effective_Visual_Representations_for_Partial-Label_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15589-15598.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在部分标签学习（PLL）中提高视觉任务的性能，特别是在没有真实标签的情况下。<br>
                    动机：在部分标签学习中，由于只有模糊的候选标签集包含未知的真实标签，因此需要通过对比相同/不同类别的实体来学习表示以提高性能。<br>
                    方法：本文提出了一种名为PaPi的新框架，该框架通过共享特征编码器的线性分类器来指导原型分类器的优化，从而明确鼓励表示反映类别之间的视觉相似性。<br>
                    效果：实验结果表明，PaPi在各种图像分类任务上显著优于其他PLL方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Under partial-label learning (PLL) where, for each training instance, only a set of ambiguous candidate labels containing the unknown true label is accessible, contrastive learning has recently boosted the performance of PLL on vision tasks, attributed to representations learned by contrasting the same/different classes of entities. Without access to true labels, positive points are predicted using pseudolabels that are inherently noisy, and negative points often require large batches or momentum encoders, resulting in unreliable similarity information and a high computational overhead. In this paper, we rethink a state-of-the-art contrastive PLL method PiCO [24], inspiring the design of a simple framework termed PaPi (Partial-label learning with a guided Prototypical classifier), which demonstrates significant scope for improvement in representation learning, thus contributing to label disambiguation. PaPi guides the optimization of a prototypical classifier by a linear classifier with which they share the same feature encoder, thus explicitly encouraging the representation to reflect visual similarity between categories. It is also technically appealing, as PaPi requires only a few components in PiCO with the opposite direction of guidance, and directly eliminates the contrastive learning module that would introduce noise and consume computational resources. We empirically demonstrate that PaPi significantly outperforms other PLL methods on various image classification tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2137.A Loopback Network for Explainable Microvascular Invasion Classification</span><br>
                <span class="as">Zhang, ShengxumingandShi, TianqiandJiang, YangandZhang, XiumingandLei, JieandFeng, ZunleiandSong, Mingli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_A_Loopback_Network_for_Explainable_Microvascular_Invasion_Classification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7443-7453.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种准确、客观和可解释的微血管侵犯（MVI）诊断工具。<br>
                    动机：目前，MVI的诊断依赖于病理学家手动从数百条血管中找出癌细胞，这既耗时又繁琐，且具有主观性。深度学习在医学影像分析任务上取得了令人鼓舞的成果，但其黑箱模型的不透明性和对大量标注样本的需求限制了基于深度学习的诊断方法在临床上的应用。<br>
                    方法：本文提出了一种名为Loopback Network（LoopNet）的循环网络，用于高效地分类MVI。通过收集的病理血管图像数据集（PVID）的图像级类别注释，LoopNet被设计为由二进制分类分支和细胞定位分支组成。后者被设计来定位癌细胞区域、正常非癌细胞区域和背景。对于健康样本，细胞的定位分支受到细胞伪掩模的监督，以区分正常非癌细胞区域和背景。对于每个MVI样本，细胞定位分支预测癌细胞的掩模。然后，同一样本的掩蔽癌细胞和非癌细胞区域分别输入到二元分类分支中。两个分支之间的回环使得类别标签能够监督细胞定位分支学习癌细胞区域的定位能力。<br>
                    效果：实验结果表明，提出的LoopNet在MVI分类上实现了97.5%的准确率。令人惊讶的是，提出的回环机制不仅使LoopNet能够预测癌细胞区域，还有助于分类主干实现更好的分类性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Microvascular invasion (MVI) is a critical factor for prognosis evaluation and cancer treatment. The current diagnosis of MVI relies on pathologists to manually find out cancerous cells from hundreds of blood vessels, which is time-consuming, tedious, and subjective. Recently, deep learning has achieved promising results in medical image analysis tasks. However, the unexplainability of black box models and the requirement of massive annotated samples limit the clinical application of deep learning based diagnostic methods. In this paper, aiming to develop an accurate, objective, and explainable diagnosis tool for MVI, we propose a Loopback Network (LoopNet) for classifying MVI efficiently. With the image-level category annotations of the collected Pathologic Vessel Image Dataset (PVID), LoopNet is devised to be composed binary classification branch and cell locating branch. The latter is devised to locate the area of cancerous cells, regular non-cancerous cells, and background. For healthy samples, the pseudo masks of cells supervise the cell locating branch to distinguish the area of regular non-cancerous cells and background. For each MVI sample, the cell locating branch predicts the mask of cancerous cells. Then the masked cancerous and non-cancerous areas of the same sample are inputted back to the binary classification branch separately. The loopback between two branches enables the category label to supervise the cell locating branch to learn the locating ability for cancerous areas. Experiment results show that the proposed LoopNet achieves 97.5% accuracy on MVI classification. Surprisingly, the proposed loopback mechanism not only enables LoopNet to predict the cancerous area but also facilitates the classification backbone to achieve better classification performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2138.Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Yang, LiheandQi, LeiandFeng, LitongandZhang, WayneandShi, Yinghuan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7236-7246.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在改进弱-强一致性框架，并将其应用于图像分割任务。<br>
                    动机：目前的弱-强一致性框架在图像分割任务中的效果有待提高，且其依赖于手动设计的强大数据增强方法，限制了其在更广泛的扰动空间中的探索。<br>
                    方法：作者提出了一种辅助特征扰动流作为补充，以扩大扰动空间，并提出了双流扰动技术，使得两个强大的视图可以同时由一个共同的弱视图指导。<br>
                    效果：这种统一的双流扰动方法（UniMatch）在所有评估协议上均显著超越了现有的所有方法，并在遥感解释和医学图像分析中也表现出优越性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we revisit the weak-to-strong consistency framework, popularized by FixMatch from semi-supervised classification, where the prediction of a weakly perturbed image serves as supervision for its strongly perturbed version. Intriguingly, we observe that such a simple pipeline already achieves competitive results against recent advanced works, when transferred to our segmentation scenario. Its success heavily relies on the manual design of strong data augmentations, however, which may be limited and inadequate to explore a broader perturbation space. Motivated by this, we propose an auxiliary feature perturbation stream as a supplement, leading to an expanded perturbation space. On the other, to sufficiently probe original image-level augmentations, we present a dual-stream perturbation technique, enabling two strong views to be simultaneously guided by a common weak view. Consequently, our overall Unified Dual-Stream Perturbations approach (UniMatch) surpasses all existing methods significantly across all evaluation protocols on the Pascal, Cityscapes, and COCO benchmarks. Its superiority is also demonstrated in remote sensing interpretation and medical image analysis. We hope our reproduced FixMatch and our results can inspire more future works. Code and logs are available at https://github.com/LiheYoung/UniMatch.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2139.MP-Former: Mask-Piloted Transformer for Image Segmentation</span><br>
                <span class="as">Zhang, HaoandLi, FengandXu, HuaizheandHuang, ShijiaandLiu, ShilongandNi, LionelM.andZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MP-Former_Mask-Piloted_Transformer_for_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18074-18083.png><br>
            
            <span class="tt"><span class="t0">研究问题：改进Mask2Former在图像分割中的遮罩注意力，解决连续解码器层之间遮罩预测不一致的问题。<br>
                    动机：由于Mask2Former在连续解码器层之间的遮罩预测不一致，导致优化目标不一致和解码器查询利用率低。<br>
                    方法：提出一种遮罩引导的训练方法，额外引入噪声的地面真值遮罩到遮罩注意力中，训练模型重构原始遮罩。<br>
                    效果：与遮罩注意力中使用的预测遮罩相比，地面真值遮罩作为引导可以有效缓解Mask2Former中不准确遮罩预测的负面影响。基于此技术，MP-Former在所有三项图像分割任务（实例、全景和语义）上取得了显著的性能提升，使用ResNet-50主干网络在Cityscapes实例和语义分割任务上获得了+2.3 AP和+1.6 mIoU。该方法还大大加快了训练速度，在ADE20K上使用ResNet-50和Swin-L主干网络，一半的训练周期就超过了Mask2Former。此外，该方法在训练期间仅引入少量计算，推理期间没有额外计算。代码将在https://github.com/IDEA-Research/MP-Former上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a mask-piloted Transformer which improves masked-attention in Mask2Former for image segmentation. The improvement is based on our observation that Mask2Former suffers from inconsistent mask predictions between consecutive decoder layers, which leads to inconsistent optimization goals and low utilization of decoder queries. To address this problem, we propose a mask-piloted training approach, which additionally feeds noised ground-truth masks in masked-attention and trains the model to reconstruct the original ones. Compared with the predicted masks used in mask-attention, the ground-truth masks serve as a pilot and effectively alleviate the negative impact of inaccurate mask predictions in Mask2Former. Based on this technique, our MP-Former achieves a remarkable performance improvement on all three image segmentation tasks (instance, panoptic, and semantic), yielding +2.3 AP and +1.6 mIoU on the Cityscapes instance and semantic segmentation tasks with a ResNet-50 backbone. Our method also significantly speeds up the training, outperforming Mask2Former with half of the number of training epochs on ADE20K with both a ResNet-50 and a Swin-L backbones. Moreover, our method only introduces little computation during training and no extra computation during inference. Our code will be released at https://github.com/IDEA-Research/MP-Former.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2140.Learning Orthogonal Prototypes for Generalized Few-Shot Semantic Segmentation</span><br>
                <span class="as">Liu, Sun-AoandZhang, YihengandQiu, ZhaofanandXie, HongtaoandZhang, YongdongandYao, Ting</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learning_Orthogonal_Prototypes_for_Generalized_Few-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11319-11328.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同时区分基本类别和新颖类别的像素，而不牺牲基础类别的性能。<br>
                    动机：目前的一般化少镜头语义分割方法在更新过程中往往会损害已学习的特征，导致基础类别的性能下降。<br>
                    方法：提出一种新的投影到正交原型（POP）方法，通过构建一组代表每个语义类别的正交原型，并在其上进行特征投影来进行预测，从而在不损害基础类别性能的情况下更新特征以识别新颖类别。<br>
                    效果：实验结果表明，POP在新颖类别上取得了优异的性能，同时对基础类别的性能影响不大。在PASCAL-5i的5次拍摄场景中，POP的整体mIoU比最先进的微调方法高出3.93%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalized few-shot semantic segmentation (GFSS) distinguishes pixels of base and novel classes from the background simultaneously, conditioning on sufficient data of base classes and a few examples from novel class. A typical GFSS approach has two training phases: base class learning and novel class updating. Nevertheless, such a stand-alone updating process often compromises the well-learnt features and results in performance drop on base classes. In this paper, we propose a new idea of leveraging Projection onto Orthogonal Prototypes (POP), which updates features to identify novel classes without compromising base classes. POP builds a set of orthogonal prototypes, each of which represents a semantic class, and makes the prediction for each class separately based on the features projected onto its prototype. Technically, POP first learns prototypes on base data, and then extends the prototype set to novel classes. The orthogonal constraint of POP encourages the orthogonality between the learnt prototypes and thus mitigates the influence on base class features when generalizing to novel prototypes. Moreover, we capitalize on the residual of feature projection as the background representation to dynamically fit semantic shifting (i.e., background no longer includes the pixels of novel classes in updating phase). Extensive experiments on two benchmarks demonstrate that our POP achieves superior performances on novel classes without sacrificing much accuracy on base classes. Notably, POP outperforms the state-of-the-art fine-tuning by 3.93% overall mIoU on PASCAL-5i in 5-shot scenario.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2141.Few-Shot Semantic Image Synthesis With Class Affinity Transfer</span><br>
                <span class="as">Careil, Marl\`eneandVerbeek, JakobandLathuili\`ere, St\&#x27;ephane</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Careil_Few-Shot_Semantic_Image_Synthesis_With_Class_Affinity_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23611-23620.png><br>
            
            <span class="tt"><span class="t0">研究问题：语义图像合成旨在根据语义分割图生成逼真的图像，但需要大量带有逐像素标签图的图像数据集进行训练，获取这些数据非常繁琐。<br>
                    动机：为了减轻高标注成本，我们提出了一种转移学习方法，利用大型源数据集训练的模型通过估计源类别和目标类别之间的成对关系来提高在小目标数据集上的学习能力。<br>
                    方法：我们将类亲和矩阵作为源模型的第一层，使其与目标标签图兼容，然后对目标领域进一步微调源模型。为了估计类别亲和性，我们考虑了不同的方法来利用先验知识：源领域的语义分割、文本标签嵌入和自监督视觉特征。我们将这种方法应用于基于GAN和扩散的语义合成架构。<br>
                    效果：实验表明，不同的方式来估计类别亲和性可以有效地结合，我们的方法显著改善了现有的最先进的生成式图像模型的转移学习方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic image synthesis aims to generate photo realistic images given a semantic segmentation map. Despite much recent progress, training them still requires large datasets of images annotated with per-pixel label maps that are extremely tedious to obtain. To alleviate the high annotation cost, we propose a transfer method that leverages a model trained on a large source dataset to improve the learning ability on small target datasets via estimated pairwise relations between source and target classes. The class affinity matrix is introduced as a first layer to the source model to make it compatible with the target label maps, and the source model is then further fine-tuned for the target domain. To estimate the class affinities we consider different approaches to leverage prior knowledge: semantic segmentation on the source domain, textual label embeddings, and self-supervised vision features. We apply our approach to GAN-based and diffusion-based architectures for semantic synthesis. Our experiments show that the different ways to estimate class affinity can effectively combined, and that our approach significantly improves over existing state-of-the-art transfer approaches for generative image models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2142.One-to-Few Label Assignment for End-to-End Dense Detection</span><br>
                <span class="as">Li, ShuaiandLi, MinghanandLi, RuihuangandHe, ChenhangandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7350-7359.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的一对一（o2o）标签分配在完全卷积的端到端密集检测中，由于正样本数量有限，可能会降低特征学习性能。<br>
                    动机：尽管可以通过引入额外的正样本来缓解这个问题，但在锚点之间的自我和交叉注意力计算阻止了其在密集和全卷积检测器中的实际应用。<br>
                    方法：我们提出了一种简单而有效的一对多（o2f）标签分配策略，除了为每个对象定义一个正锚和多个负锚外，我们还定义了几个软锚点，它们同时作为正负样本。这些软锚点的正负权重在训练过程中会动态调整，以便它们在早期阶段更多地参与“表示学习”，在后期阶段更多地参与“重复预测消除”。<br>
                    效果：在COCO和CrowdHuman数据集上的实验表明，所提出的o2f方案是有效的。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>One-to-one (o2o) label assignment plays a key role for transformer based end-to-end detection, and it has been recently introduced in fully convolutional detectors for lightweight end-to-end dense detection. However, o2o can largely degrade the feature learning performance due to the limited number of positive samples. Though extra positive samples can be introduced to mitigate this issue, the computation of self- and cross- attentions among anchors prevents its practical application to dense and fully convolutional detectors. In this work, we propose a simple yet effective one-to-few (o2f) label assignment strategy for end-to-end dense detection. Apart from defining one positive and many negative anchors for each object, we define several soft anchors, which serve as positive and negative samples simultaneously. The positive and negative weights of these soft anchors are dynamically adjusted during training so that they can contribute more to 'representation learning' in the early training stage and contribute more to 'duplicated prediction removal' in the later stage. The detector trained in this way can not only learn a strong feature representation but also perform end-to-end detection. Experiments on COCO and CrowdHuman datasets demonstrate the effectiveness of the proposed o2f scheme.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2143.Hierarchical Supervision and Shuffle Data Augmentation for 3D Semi-Supervised Object Detection</span><br>
                <span class="as">Liu, ChuandongandGao, ChenqiangandLiu, FangcenandLi, PengchengandMeng, DeyuandGao, Xinbo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Hierarchical_Supervision_and_Shuffle_Data_Augmentation_for_3D_Semi-Supervised_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23819-23828.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用有限的标注样本和大量的未标注样本进行半监督学习，提高三维物体检测的性能。<br>
                    动机：现有的三维物体检测模型通常需要大量高质量的三维标注数据进行训练，但这种标注过程既昂贵又耗时，不适合实际应用。<br>
                    方法：提出一种新的分层监督和洗牌数据增强（HSSDA）方法，通过设计动态的双阈值策略，使教师网络为学生网络生成更合理的监督信号，同时通过洗牌数据增强策略强化学生网络的特征表示能力。<br>
                    效果：实验结果表明，HSSDA在各种数据集上始终优于最新的最先进技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>State-of-the-art 3D object detectors are usually trained on large-scale datasets with high-quality 3D annotations. However, such 3D annotations are often expensive and time-consuming, which may not be practical for real applications. A natural remedy is to adopt semi-supervised learning (SSL) by leveraging a limited amount of labeled samples and abundant unlabeled samples. Current pseudo-labeling-based SSL object detection methods mainly adopt a teacher-student framework, with a single fixed threshold strategy to generate supervision signals, which inevitably brings confused supervision when guiding the student network training. Besides, the data augmentation of the point cloud in the typical teacher-student framework is too weak, and only contains basic down sampling and flip-and-shift (i.e., rotate and scaling), which hinders the effective learning of feature information. Hence, we address these issues by introducing a novel approach of Hierarchical Supervision and Shuffle Data Augmentation (HSSDA), which is a simple yet effective teacher-student framework. The teacher network generates more reasonable supervision for the student network by designing a dynamic dual-threshold strategy. Besides, the shuffle data augmentation strategy is designed to strengthen the feature representation ability of the student network. Extensive experiments show that HSSDA consistently outperforms the recent state-of-the-art methods on different datasets. The code will be released at https://github.com/azhuantou/HSSDA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2144.ZegCLIP: Towards Adapting CLIP for Zero-Shot Semantic Segmentation</span><br>
                <span class="as">Zhou, ZiqinandLei, YinjieandZhang, BowenandLiu, LingqiaoandLiu, Yifan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11175-11185.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将CLIP的零样本学习能力从图像扩展到像素级别，同时避免复杂性和高计算成本。<br>
                    动机：现有的两阶段方法需要两个图像编码器，导致流程复杂且计算成本高。<br>
                    方法：提出了一种更简单、更高效的单阶段解决方案，直接扩展CLIP的零样本预测能力。通过比较文本和CLIP提取的补丁嵌入之间的相似性生成语义掩码。<br>
                    效果：在公开的三个基准测试中，ZegCLIP表现出优越的性能，在"归纳式"和"转导式"零样本设置下均大幅超过现有最佳方法。与两阶段方法相比，ZegCLIP的推理速度提高了约5倍。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a wo-stage scheme. The general idea is to first generate class-agnostic region proposals and then feed the cropped proposal regions to CLIP to utilize its image-level zero-shot classification capability. While effective, such a scheme requires two image encoders, one for proposal generation and one for CLIP, leading to a complicated pipeline and high computational cost. In this work, we pursue a simpler-and-efficient one-stage solution that directly extends CLIP's zero-shot prediction capability from image to pixel level. Our investigation starts with a straightforward extension as our baseline that generates semantic masks by comparing the similarity between text and patch embeddings extracted from CLIP. However, such a paradigm could heavily overfit the seen classes and fail to generalize to unseen classes. To handle this issue, we propose three simple-but-effective designs and figure out that they can significantly retain the inherent zero-shot capacity of CLIP and improve pixel-level generalization ability. Incorporating those modifications leads to an efficient zero-shot semantic segmentation system called ZegCLIP. Through extensive experiments on three public benchmarks, ZegCLIP demonstrates superior performance, outperforming the state-of-the-art methods by a large margin under both "inductive" and "transductive" zero-shot settings. In addition, compared with the two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 times faster during inference. We release the code at https://github.com/ZiqinZhou66/ZegCLIP.git.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2145.CLIP Is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation</span><br>
                <span class="as">Lin, YuqiandChen, MinghaoandWang, WenxiaoandWu, BoxiandLi, KeandLin, BinbinandLiu, HaifengandHe, Xiaofei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15305-15314.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用对比语言-图像预训练模型（CLIP）进行弱监督语义分割，仅使用图像级别标签，无需进一步训练。<br>
                    动机：主流的多阶段框架在弱监督语义分割中成本高昂，我们探索了使用CLIP进行无额外训练的图像分类的可能性。<br>
                    方法：我们提出了一种新的弱监督语义分割框架CLIP-ES，通过引入softmax函数到GradCAM和使用CLIP的零样本能力来抑制非目标类别和背景引起的混淆。同时，我们还重新探索了文本输入在弱监督语义分割设置下的应用，并定制了两种基于文本的策略：锐度为基础的提示选择和同义词融合。<br>
                    效果：我们的CLIP-ES在Pascal VOC 2012和MS COCO 2014上取得了最先进的性能，同时生成伪蒙版的所需时间仅为以前方法的10%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware attention-based affinity (CAA) module based on the inherent multi-head self-attention (MHSA) in CLIP-ViTs. 3) When training the final segmentation model with the masks generated by CLIP, we introduced a confidence-guided loss (CGL) focus on confident regions. Our CLIP-ES achieves SOTA performance on Pascal VOC 2012 and MS COCO 2014 while only taking 10% time of previous methods for the pseudo mask generation. Code is available at https://github.com/linyq2117/CLIP-ES.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2146.Mask-Free Video Instance Segmentation</span><br>
                <span class="as">Ke, LeiandDanelljan, MartinandDing, HenghuiandTai, Yu-WingandTang, Chi-KeungandYu, Fisher</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_Mask-Free_Video_Instance_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22857-22866.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频实例分割（VIS）的发展主要受到深度和数据驱动的转换器模型的推动，但视频遮罩的标注既繁琐又昂贵，限制了现有VIS数据集的规模和多样性。<br>
                    动机：本文旨在消除遮罩标注的需求，提出了MaskFreeVIS，仅使用边界框注释即可实现高度竞争性的VIS性能。<br>
                    方法：通过引入时间KNN-patch损失（TK-Loss），利用视频中丰富的时间遮罩一致性约束，无需任何标签即可提供强大的遮罩监督。<br>
                    效果：在YouTube-VIS 2019/2021、OVIS和BDD100K MOTS基准测试中验证了MaskFreeVIS，结果清楚地表明了该方法的有效性，大大缩小了全监督和弱监督VIS性能之间的差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recent advancement in Video Instance Segmentation (VIS) has largely been driven by the use of deeper and increasingly data-hungry transformer-based models. However, video masks are tedious and expensive to annotate, limiting the scale and diversity of existing VIS datasets. In this work, we aim to remove the mask-annotation requirement. We propose MaskFreeVIS, achieving highly competitive VIS performance, while only using bounding box annotations for the object state. We leverage the rich temporal mask consistency constraints in videos by introducing the Temporal KNN-patch Loss (TK-Loss), providing strong mask supervision without any labels. Our TK-Loss finds one-to-many matches across frames, through an efficient patch-matching step followed by a K-nearest neighbor selection. A consistency loss is then enforced on the found matches. Our mask-free objective is simple to implement, has no trainable parameters, is computationally efficient, yet outperforms baselines employing, e.g., state-of-the-art optical flow to enforce temporal mask consistency. We validate MaskFreeVIS on the YouTube-VIS 2019/2021, OVIS and BDD100K MOTS benchmarks. The results clearly demonstrate the efficacy of our method by drastically narrowing the gap between fully and weakly-supervised VIS performance. Our code and trained models are available at http://vis.xyz/pub/maskfreevis.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2147.Continual Detection Transformer for Incremental Object Detection</span><br>
                <span class="as">Liu, YaoyaoandSchiele, BerntandVedaldi, AndreaandRupprecht, Christian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23799-23808.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练一种增量物体检测器，使其能够逐步学习新的物体类别，同时避免灾难性遗忘？<br>
                    动机：现有的增量物体检测方法在处理最新的基于变换器的物体检测器时，如Deformable DETR和UP-DETR，知识蒸馏和示例重放等技术效果不佳。<br>
                    方法：提出一种新的基于变换器的增量物体检测方法ContinuaL DEtection TRansformer（CL-DETR），该方法能有效利用知识蒸馏和示例重放。具体包括引入Detector Knowledge Distillation（DKD）损失，关注旧模型中最具信息性和可靠性的预测，忽略冗余的背景预测，并与可用的地面真值标签兼容；同时改进示例重放策略，通过校准策略保持训练集的标签分布，使训练和测试统计更好地匹配。<br>
                    效果：在COCO 2017数据集上进行大量实验，结果表明CL-DETR在增量物体检测设置中取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Incremental object detection (IOD) aims to train an object detector in phases, each with annotations for new object categories. As other incremental settings, IOD is subject to catastrophic forgetting, which is often addressed by techniques such as knowledge distillation (KD) and exemplar replay (ER). However, KD and ER do not work well if applied directly to state-of-the-art transformer-based object detectors such as Deformable DETR and UP-DETR. In this paper, we solve these issues by proposing a ContinuaL DEtection TRansformer (CL-DETR), a new method for transformer-based IOD which enables effective usage of KD and ER in this context. First, we introduce a Detector Knowledge Distillation (DKD) loss, focusing on the most informative and reliable predictions from old versions of the model, ignoring redundant background predictions, and ensuring compatibility with the available ground-truth labels. We also improve ER by proposing a calibration strategy to preserve the label distribution of the training set, therefore better matching training and testing statistics. We conduct extensive experiments on COCO 2017 and demonstrate that CL-DETR achieves state-of-the-art results in the IOD setting.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2148.HyperMatch: Noise-Tolerant Semi-Supervised Learning via Relaxed Contrastive Constraint</span><br>
                <span class="as">Zhou, BeitongandLu, JingandLiu, KeruiandXu, YunluandCheng, ZhanzhanandNiu, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_HyperMatch_Noise-Tolerant_Semi-Supervised_Learning_via_Relaxed_Contrastive_Constraint_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24017-24026.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督学习方法存在由于不准确的伪标签导致的实例对不匹配问题，这会加剧半监督学习的知名确认偏差。<br>
                    动机：为了解决这个问题，研究人员提出了一种新的半监督学习方法——HyperMatch，该方法能够容忍噪声并有效利用未标记的数据。<br>
                    方法：通过结合置信度预测和语义相似性生成更客观的类别分布，然后使用高斯混合模型将伪标签分为“可信”和“不太可信”的子集。此外，还引入了放松对比损失，将“不太可信”的样本分配给一个超类（即前K个最近邻类的并集），从而有效地规范不正确伪标签的干扰，甚至提高将“不太可信”的样本拉向其真实类别的概率。<br>
                    效果：实验和深入研究证明，HyperMatch表现出卓越的最先进性能，在CIFAR100上以400和2500个标记样本分别比FixMatch高出11.86%和4.88%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent developments of the application of Contrastive Learning in Semi-Supervised Learning (SSL) have demonstrated significant advancements, as a result of its exceptional ability to learn class-aware cluster representations and the full exploitation of massive unlabeled data. However, mismatched instance pairs caused by inaccurate pseudo labels would assign an unlabeled instance to the incorrect class in feature space, hence exacerbating SSL's renowned confirmation bias. To address this issue, we introduced a novel SSL approach, HyperMatch, which is a plug-in to several SSL designs enabling noise-tolerant utilization of unlabeled data. In particular, confidence predictions are combined with semantic similarities to generate a more objective class distribution, followed by a Gaussian Mixture Model to divide pseudo labels into a 'confident' and a 'less confident' subset. Then, we introduce Relaxed Contrastive Loss by assigning the 'less-confident' samples to a hyper-class, i.e. the union of top-K nearest classes, which effectively regularizes the interference of incorrect pseudo labels and even increases the probability of pulling a 'less confident' sample close to its true class. Experiments and in-depth studies demonstrate that HyperMatch delivers remarkable state-of-the-art performance, outperforming FixMatch on CIFAR100 with 400 and 2500 labeled samples by 11.86% and 4.88%, respectively.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2149.Mask-Free OVIS: Open-Vocabulary Instance Segmentation Without Manual Mask Annotations</span><br>
                <span class="as">VS, VibashanandYu, NingandXing, ChenandQin, CanandGao, MingfeiandNiebles, JuanCarlosandPatel, VishalM.andXu, Ran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/VS_Mask-Free_OVIS_Open-Vocabulary_Instance_Segmentation_Without_Manual_Mask_Annotations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23539-23549.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的实例分割模型需要通过人工标注从基础类别中学习特定任务信息，这需要大量的人力，限制了对新类别的可扩展性。<br>
                    动机：为了解决这个问题，开放词汇（OV）方法利用大规模的图像-标题对和视觉语言模型来学习新类别。然而，这种强监督和弱监督之间的差异会导致对基础类别的过拟合，从而影响对新类别的泛化能力。<br>
                    方法：我们提出了一种无掩码OVIS管道，通过使用预训练的视觉语言模型生成的伪掩码注释进行弱监督学习，来克服这个问题。这种方法自动生成伪掩码注释，然后使用这些注释来监督实例分割模型，使整个流程无需任何劳动密集型的实例级注释和过拟合。<br>
                    效果：我们的大量实验表明，与最近使用手动掩码训练的最新方法相比，仅使用伪掩码训练的方法在MS-COCO数据集和OpenImages数据集上的mAP得分显著提高。代码和模型可以在https://vibashan.github.io/ovis-web/获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing instance segmentation models learn task-specific information using manual mask annotations from base (training) categories. These mask annotations require tremendous human effort, limiting the scalability to annotate novel (new) categories. To alleviate this problem, Open-Vocabulary (OV) methods leverage large-scale image-caption pairs and vision-language models to learn novel categories. In summary, an OV method learns task-specific information using strong supervision from base annotations and novel category information using weak supervision from image-captions pairs. This difference between strong and weak supervision leads to overfitting on base categories, resulting in poor generalization towards novel categories. In this work, we overcome this issue by learning both base and novel categories from pseudo-mask annotations generated by the vision-language model in a weakly supervised manner using our proposed Mask-free OVIS pipeline. Our method automatically generates pseudo-mask annotations by leveraging the localization ability of a pre-trained vision-language model for objects present in image-caption pairs. The generated pseudo-mask annotations are then used to supervise an instance segmentation model, freeing the entire pipeline from any labour-expensive instance-level annotations and overfitting. Our extensive experiments show that our method trained with just pseudo-masks significantly improves the mAP scores on the MS-COCO dataset and OpenImages dataset compared to the recent state-of-the-art methods trained with manual masks. Codes and models are provided in https://vibashan.github.io/ovis-web/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2150.Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</span><br>
                <span class="as">Zhang, ZhuoyangandDong, YuhaoandLiu, YunzeandYi, Li</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Complete-to-Partial_4D_Distillation_for_Self-Supervised_Point_Cloud_Sequence_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17661-17670.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效利用未标记的原始数据进行4D点云序列的学习。<br>
                    动机：获取全面标注的4D数据集既昂贵又费力，因此研究如何利用未标记的原始数据至关重要。<br>
                    方法：提出一种名为“从完整到部分的4D蒸馏”的新型4D自监督预训练方法，将4D自监督表示学习构建为教师-学生的知识蒸馏框架，让学生在教师的指导下学习有用的4D表示。<br>
                    效果：实验证明，这种方法在各种4D点云序列理解任务上显著优于先前的预训练方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent work on 4D point cloud sequences has attracted a lot of attention. However, obtaining exhaustively labeled 4D datasets is often very expensive and laborious, so it is especially important to investigate how to utilize raw unlabeled data. However, most existing self-supervised point cloud representation learning methods only consider geometry from a static snapshot omitting the fact that sequential observations of dynamic scenes could reveal more comprehensive geometric details. To overcome such issues, this paper proposes a new 4D self-supervised pre-training method called Complete-to-Partial 4D Distillation. Our key idea is to formulate 4D self-supervised representation learning as a teacher-student knowledge distillation framework and let the student learn useful 4D representations with the guidance of the teacher. Experiments show that this approach significantly outperforms previous pre-training approaches on a wide range of 4D point cloud sequence understanding tasks. Code is available at: https://github.com/dongyh20/C2P.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2151.Texture-Guided Saliency Distilling for Unsupervised Salient Object Detection</span><br>
                <span class="as">Zhou, HuajunandQiao, BoandYang, LingxiaoandLai, JianhuangandXie, Xiaohua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Texture-Guided_Saliency_Distilling_for_Unsupervised_Salient_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7257-7267.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习进行无监督显著性目标检测，特别是在存在噪声标签的情况下。<br>
                    动机：现有的方法主要依赖传统手工制作的方法或预训练网络生成的噪声显著性伪标签，忽视了困难样本中有价值的知识。<br>
                    方法：提出了一种新的显著性目标检测方法，该方法从易和难的样本中挖掘丰富和准确的显著性知识。首先，提出了一种基于置信度的显著性蒸馏策略，该策略根据样本的置信度对样本进行评分，指导模型逐步将显著性知识从易样本蒸馏到难样本。其次，提出了一种边界感知纹理匹配策略，通过匹配预测边界周围的纹理来细化噪声标签的边界。<br>
                    效果：在RGB、RGB-D、RGB-T和视频SOD基准测试上的大量实验证明，该方法实现了最先进的USOD性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep Learning-based Unsupervised Salient Object Detection (USOD) mainly relies on the noisy saliency pseudo labels that have been generated from traditional handcraft methods or pre-trained networks. To cope with the noisy labels problem, a class of methods focus on only easy samples with reliable labels but ignore valuable knowledge in hard samples. In this paper, we propose a novel USOD method to mine rich and accurate saliency knowledge from both easy and hard samples. First, we propose a Confidence-aware Saliency Distilling (CSD) strategy that scores samples conditioned on samples' confidences, which guides the model to distill saliency knowledge from easy samples to hard samples progressively. Second, we propose a Boundary-aware Texture Matching (BTM) strategy to refine the boundaries of noisy labels by matching the textures around the predicted boundaries. Extensive experiments on RGB, RGB-D, RGB-T, and video SOD benchmarks prove that our method achieves state-of-the-art USOD performance. Code is available at www.github.com/moothes/A2S-v2.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2152.Network-Free, Unsupervised Semantic Segmentation With Synthetic Images</span><br>
                <span class="as">Feng, QianliandGadde, RaghudeepandLiao, WentongandRamon, EduardandMartinez, Aleix</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Network-Free_Unsupervised_Semantic_Segmentation_With_Synthetic_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23602-23610.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何不使用任何额外的神经网络、层、手动标注的训练数据或监督训练，生成高度准确的语义分割图。<br>
                    动机：观察到在生成图像的GAN风格混合方法中，同一语义分割的一组像素的相关性不会改变。<br>
                    方法：利用GAN反转，对合成和真实的照片进行准确的语义分割，并为后续任务生成大量的训练图像-语义分割掩码对。<br>
                    效果：实验证明该方法能准确进行语义分割，并可为后续任务生成大量的训练数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We derive a method that yields highly accurate semantic segmentation maps without the use of any additional neural network, layers, manually annotated training data, or supervised training. Our method is based on the observation that the correlation of a set of pixels belonging to the same semantic segment do not change when generating synthetic variants of an image using the style mixing approach in GANs. We show how we can use GAN inversion to accurately semantically segment synthetic and real photos as well as generate large training image-semantic segmentation mask pairs for downstream tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2153.Hierarchical Dense Correlation Distillation for Few-Shot Segmentation</span><br>
                <span class="as">Peng, BohaoandTian, ZhuotaoandWu, XiaoyangandWang, ChengyaoandLiu, ShuandSu, JingyongandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23641-23651.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决小样本语义分割问题，即仅用少量标注来对未见过的类别进行分割。<br>
                    动机：现有的方法受限于语义特征和原型表示，导致分割粒度粗糙和训练集过拟合的问题。<br>
                    方法：设计了一种基于变压器架构的层次解耦匹配网络（HDMNet），利用自我注意力模块建立层次密集特征，实现查询和支持特征之间的级联匹配。同时，提出了一种匹配模块以减少训练集过拟合，并引入相关性蒸馏，利用粗分辨率的语义对应关系提升细粒度分割。<br>
                    效果：在实验中表现良好，COCO-5i数据集上的一阶段设置达到了50.0%的mIoU，五阶段分割达到了56.0%。代码可在项目网站上获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot semantic segmentation (FSS) aims to form class-agnostic models segmenting unseen classes with only a handful of annotations. Previous methods limited to the semantic feature and prototype representation suffer from coarse segmentation granularity and train-set overfitting. In this work, we design Hierarchically Decoupled Matching Network (HDMNet) mining pixel-level support correlation based on the transformer architecture. The self-attention modules are used to assist in establishing hierarchical dense features, as a means to accomplish the cascade matching between query and support features. Moreover, we propose a matching module to reduce train-set overfitting and introduce correlation distillation leveraging semantic correspondence from coarse resolution to boost fine-grained segmentation. Our method performs decently in experiments. We achieve 50.0% mIoU on COCO-5i dataset one-shot setting and 56.0% on five-shot segmentation, respectively. The code is available on the project website.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2154.PVO: Panoptic Visual Odometry</span><br>
                <span class="as">Ye, WeicaiandLan, XinyueandChen, ShuoandMing, YuhangandYu, XingyuanandBao, HujunandCui, ZhaopengandZhang, Guofeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9579-9589.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种全新的全景视觉里程计框架PVO，以实现对场景运动、几何和全景分割信息的更全面建模。<br>
                    动机：目前的视觉里程计和视频全景分割方法无法充分利用彼此的信息，导致性能受限。<br>
                    方法：PVO将视觉里程计（VO）和视频全景分割（VPS）在统一视图中进行建模，使两个任务相互受益。具体来说，我们在图像全景分割的指导下，将全景更新模块引入到VO模块中，通过全景感知动态掩码减轻动态对象对相机位姿估计的影响。同时，VO增强的VPS模块通过融合当前帧的全景分割结果到相邻帧，利用从VO模块获得的相机位姿、深度和光流等几何信息提高分割精度。这两个模块通过递归迭代优化相互促进。<br>
                    效果：实验结果表明，PVO在视觉里程计和视频全景分割任务上都优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present PVO, a novel panoptic visual odometry framework to achieve more comprehensive modeling of the scene motion, geometry, and panoptic segmentation information. Our PVO models visual odometry (VO) and video panoptic segmentation (VPS) in a unified view, which makes the two tasks mutually beneficial. Specifically, we introduce a panoptic update module into the VO Module with the guidance of image panoptic segmentation. This Panoptic-Enhanced VO Module can alleviate the impact of dynamic objects in the camera pose estimation with a panoptic-aware dynamic mask. On the other hand, the VO-Enhanced VPS Module also improves the segmentation accuracy by fusing the panoptic segmentation result of the current frame on the fly to the adjacent frames, using geometric information such as camera pose, depth, and optical flow obtained from the VO Module. These two modules contribute to each other through recurrent iterative optimization. Extensive experiments demonstrate that PVO outperforms state-of-the-art methods in both visual odometry and video panoptic segmentation tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2155.ISBNet: A 3D Point Cloud Instance Segmentation Network With Instance-Aware Sampling and Box-Aware Dynamic Convolution</span><br>
                <span class="as">Ngo, TuanDucandHua, Binh-SonandNguyen, Khoi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ngo_ISBNet_A_3D_Point_Cloud_Instance_Segmentation_Network_With_Instance-Aware_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13550-13559.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D实例分割方法主要依赖于自下而上的设计，即手动微调算法将点分组为簇，然后通过细化网络进行优化。然而，这种方法在处理具有相同语义类别的邻近对象被打包在一起或大对象具有松散连接区域的情况时，会产生不稳定的结果。<br>
                    动机：为了解决现有方法的局限性，我们提出了ISBNet，一种新的无聚类方法，将实例表示为内核，并通过动态卷积解码实例掩码。<br>
                    方法：我们提出了一种名为“实例感知的最远点采样”的简单策略，以生成高召回率和判别性的内核。同时，我们利用受PointNet++启发的局部聚合层来编码候选特征。此外，我们还展示了在动态卷积中预测和利用3D轴对齐边界框可以进一步提高性能。<br>
                    效果：在ScanNetV2、S3DIS和STPLS3D数据集上，我们的ISBNet方法在AP方面取得了新的最先进的结果（分别为55.9、60.8和49.2），并且保持了快速的推理速度（在ScanNetV2上每个场景为237ms）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing 3D instance segmentation methods are predominated by the bottom-up design -- manually fine-tuned algorithm to group points into clusters followed by a refinement network. However, by relying on the quality of the clusters, these methods generate susceptible results when (1) nearby objects with the same semantic class are packed together, or (2) large objects with loosely connected regions. To address these limitations, we introduce ISBNet, a novel cluster-free method that represents instances as kernels and decodes instance masks via dynamic convolution. To efficiently generate high-recall and discriminative kernels, we propose a simple strategy named Instance-aware Farthest Point Sampling to sample candidates and leverage the local aggregation layer inspired by PointNet++ to encode candidate features. Moreover, we show that predicting and leveraging the 3D axis-aligned bounding boxes in the dynamic convolution further boosts performance. Our method set new state-of-the-art results on ScanNetV2 (55.9), S3DIS (60.8), and STPLS3D (49.2) in terms of AP and retains fast inference time (237ms per scene on ScanNetV2). The source code and trained models are available at https://github.com/VinAIResearch/ISBNet.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2156.CLIP-S4: Language-Guided Self-Supervised Semantic Segmentation</span><br>
                <span class="as">He, WenbinandJamonnak, SuphanutandGou, LiangandRen, Liu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_CLIP-S4_Language-Guided_Self-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11207-11216.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的语义分割方法通常受限于昂贵的像素级标注和预定义的类别，本文提出了一种新的方法来解决这些问题。<br>
                    动机：本文的动机是利用自我监督的像素表示学习和视觉语言模型来进行各种语义分割任务，而无需任何人工标注和未知类别信息。<br>
                    方法：首先，我们使用像素段对比学习从图像的不同增强视图中学习像素嵌入。然后，为了进一步提高像素嵌入并实现语言驱动的语义分割，我们设计了两种类型的一致性指导，一种是嵌入一致性，将我们的像素嵌入对齐到预训练的视觉语言模型CLIP的联合特征空间；另一种是语义一致性，强制我们的模型在一组精心设计的目标类别上做出与CLIP相同的预测，这些目标类别既有已知的也有未知的原型。<br>
                    效果：实验结果表明，我们的方法在四个流行的基准测试中表现出一致且显著的性能改进，超过了最先进的无监督和语言驱动的语义分割方法。更重要的是，我们的方法在这些方法上未知类别识别的性能提高了很大一截。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S^4 that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CLIP-S^4 enables a new task of class-free semantic segmentation where no unknown class information is needed during training. As a result, our approach shows consistent and substantial performance improvement over four popular benchmarks compared with the state-of-the-art unsupervised and language-driven semantic segmentation methods. More importantly, our method outperforms these methods on unknown class recognition by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2157.Unified Mask Embedding and Correspondence Learning for Self-Supervised Video Segmentation</span><br>
                <span class="as">Li, LiuleiandWang, WenguanandZhou, TianfeiandLi, JianwuandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Unified_Mask_Embedding_and_Correspondence_Learning_for_Self-Supervised_Video_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18706-18716.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在通过自监督学习实现视频对象分割。<br>
                    动机：现有的方法通常依赖于像素级的相关性来“复制”标签，而我们的方法则直接从无标签的视频中学习执行掩码引导的序列分割。<br>
                    方法：我们开发了一个统一的框架，同时对局部判别特征学习和目标掩码解码进行跨帧密集对应建模，并嵌入对象级上下文。具体来说，我们的算法交替进行i) 对视频像素进行聚类以创建伪分割标签；ii) 利用伪标签学习VOS的掩码编码和解码。此外，我们还将无监督对应学习融入到这种自我教授的掩码嵌入方案中，以确保学习到的表示的通用性并避免聚类退化。<br>
                    效果：实验结果表明，我们的方法在两个标准基准（即DAVIS17和YouTube-VOS）上设置了最先进的技术，无论在性能还是网络架构设计方面，都在自我监督和全监督VOS之间缩小了差距。我们的完整代码将被发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The objective of this paper is self-supervised learning of video object segmentation. We develop a unified framework which simultaneously models cross-frame dense correspondence for locally discriminative feature learning and embeds object-level context for target-mask decoding. As a result, it is able to directly learn to perform mask-guided sequential segmentation from unlabeled videos, in contrast to previous efforts usually relying on an oblique solution --- cheaply "copying" labels according to pixel-wise correlations. Concretely, our algorithm alternates between i) clustering video pixels for creating pseudo segmentation labels ex nihilo; and ii) utilizing the pseudo labels to learn mask encoding and decoding for VOS. Unsupervised correspondence learning is further incorporated into this self-taught, mask embedding scheme, so as to ensure the generic nature of the learnt representation and avoid cluster degeneracy. Our algorithm sets state-of-the-arts on two standard benchmarks (i.e., DAVIS17 and YouTube-VOS), narrowing the gap between self- and fully-supervised VOS, in terms of both performance and network architecture design. Our full code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2158.Knowledge Combination To Learn Rotated Detection Without Rotated Annotation</span><br>
                <span class="as">Zhu, TianyuandFerenczi, BryceandPurkait, PulakandDrummond, TomandRezatofighi, HamidandvandenHengel, Anton</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Knowledge_Combination_To_Learn_Rotated_Detection_Without_Rotated_Annotation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15518-15527.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过更经济的轴对齐标注来预测精确的旋转框，以提高旋转检测器的使用率。<br>
                    动机：虽然旋转边界框可以显著减少细长对象的输出歧义，但由于标注过程繁琐，并未被广泛采用。<br>
                    方法：本文提出了一个框架，该框架利用神经网络在目标领域的丰富表示能力，结合源数据集的任务知识和目标数据集的领域知识，通过新颖的分配过程和投影损失进行联合训练，使模型能够在目标领域中解决更详细任务，而无需额外的计算开销。<br>
                    效果：实验结果表明，该方法在各种目标数据集上的表现与全监督方法相当，证明了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rotated bounding boxes drastically reduce output ambiguity of elongated objects, making it superior to axis-aligned bounding boxes. Despite the effectiveness, rotated detectors are not widely employed. Annotating rotated bounding boxes is such a laborious process that they are not provided in many detection datasets where axis-aligned annotations are used instead. In this paper, we propose a framework that allows the model to predict precise rotated boxes only requiring cheaper axis-aligned annotation of the target dataset. To achieve this, we leverage the fact that neural networks are capable of learning richer representation of the target domain than what is utilized by the task. The under-utilized representation can be exploited to address a more detailed task. Our framework combines task knowledge of an out-of-domain source dataset with stronger annotation and domain knowledge of the target dataset with weaker annotation. A novel assignment process and projection loss are used to enable the co-training on the source and target datasets. As a result, the model is able to solve the more detailed task in the target domain, without additional computation overhead during inference. We extensively evaluate the method on various target datasets including fresh-produce dataset, HRSC2016 and SSDD. Results show that the proposed method consistently performs on par with the fully supervised approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2159.Contrastive Grouping With Transformer for Referring Image Segmentation</span><br>
                <span class="as">Tang, JiajinandZheng, GeandShi, ChengandYang, Sibei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23570-23580.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决图像分割中目标参照物的问题，通过自然语言表达式来对图像中的参照物进行分割。<br>
                    动机：现有的一阶段方法采用像素级的分类框架，直接在像素级别上对视觉和语言进行对齐，无法捕捉到关键的物体级信息。<br>
                    方法：提出了一种掩码分类框架CGFormer，通过基于令牌的查询和分组策略显式地捕获物体级信息。具体来说，CGFormer首先引入了可学习的查询令牌来表示物体，然后交替查询语言特征并将视觉特征分组到查询令牌中进行物体感知的跨模态推理。此外，CGFormer通过每两层连续层共同更新查询令牌和解码掩码来实现跨层级交互。最后，CGFormer将对比学习与分组策略相结合，以识别参照物对应的令牌和掩码。<br>
                    效果：实验结果表明，CGFormer在分割和泛化设置中始终显著且一致地优于最先进的方法。代码可在https://github.com/Toneyaya/CGFormer获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring image segmentation aims to segment the target referent in an image conditioning on a natural language expression. Existing one-stage methods employ per-pixel classification frameworks, which attempt straightforwardly to align vision and language at the pixel level, thus failing to capture critical object-level information. In this paper, we propose a mask classification framework, Contrastive Grouping with Transformer network (CGFormer), which explicitly captures object-level information via token-based querying and grouping strategy. Specifically, CGFormer first introduces learnable query tokens to represent objects and then alternately queries linguistic features and groups visual features into the query tokens for object-aware cross-modal reasoning. In addition, CGFormer achieves cross-level interaction by jointly updating the query tokens and decoding masks in every two consecutive layers. Finally, CGFormer cooperates contrastive learning to the grouping strategy to identify the token and its mask corresponding to the referent. Experimental results demonstrate that CGFormer outperforms state-of-the-art methods in both segmentation and generalization settings consistently and significantly. Code is available at https://github.com/Toneyaya/CGFormer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2160.Cascade Evidential Learning for Open-World Weakly-Supervised Temporal Action Localization</span><br>
                <span class="as">Chen, MengyuanandGao, JunyuandXu, Changsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Cascade_Evidential_Learning_for_Open-World_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14741-14750.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在训练中仅使用视频级别标签来识别和定位动作实例，特别是在动态变化的开放世界中，未知动作不断出现的情况下。<br>
                    动机：现有的弱监督时序动作定位（WTAL）方法基于封闭集假设，但在开放世界中，这种假设无效。此外，传统开放集识别任务的标注是未知的，而已知动作实例的精细标注只能从视频类别标签中模糊推断出来。<br>
                    方法：我们首次提出了一种级联证据学习框架，针对开放世界弱监督时序动作定位（OWTAL）。该方法联合利用多尺度时间上下文和知识引导原型信息，逐步收集级联和增强的证据，用于已知动作、未知动作和背景分离。<br>
                    效果：我们在THUMOS-14和ActivityNet-v1.3上进行了大量实验，验证了我们的方法的有效性。除了以前开放集识别方法采用的分类指标外，我们还在更符合OWTAL的本地化指标上评估了我们的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Targeting at recognizing and localizing action instances with only video-level labels during training, Weakly-supervised Temporal Action Localization (WTAL) has achieved significant progress in recent years. However, living in the dynamically changing open world where unknown actions constantly spring up, the closed-set assumption of existing WTAL methods is invalid. Compared with traditional open-set recognition tasks, Open-world WTAL (OWTAL) is challenging since not only are the annotations of unknown samples unavailable, but also the fine-grained annotations of known action instances can only be inferred ambiguously from the video category labels. To address this problem, we propose a Cascade Evidential Learning framework at an evidence level, which targets at OWTAL for the first time. Our method jointly leverages multi-scale temporal contexts and knowledge-guided prototype information to progressively collect cascade and enhanced evidence for known action, unknown action, and background separation. Extensive experiments conducted on THUMOS-14 and ActivityNet-v1.3 verify the effectiveness of our method. Besides the classification metrics adopted by previous open-set recognition methods, we also evaluate our method on localization metrics which are more reasonable for OWTAL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2161.Reducing the Label Bias for Timestamp Supervised Temporal Action Segmentation</span><br>
                <span class="as">Liu, KaiyuanandLi, YunhengandLiu, ShenglanandTan, ChenweiandShao, Zihang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Reducing_the_Label_Bias_for_Timestamp_Supervised_Temporal_Action_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6503-6513.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决时间监督的动作分割中标签偏差严重的问题。<br>
                    动机：由于过度依赖稀疏的时间戳标注，现有的方法存在严重的标签偏差，导致性能不佳。<br>
                    方法：提出了去偏时间监督动作分割（D-TSTAS）框架，通过利用未标注的帧从两个阶段减轻这种偏差：1）初始化。为了减少对标注帧的依赖，提出了掩码时间预测（MTP）以确保初始化的模型捕获更多的上下文信息。2）精炼。为了克服稀疏标注时间戳的表达性限制，提出了一种以中心为导向的时间戳扩展（CTE）方法，逐步扩展包含动作段语义丰富的运动表示的伪时间戳组。然后，使用这些伪时间戳组和模型输出迭代生成伪标签，以在全监督设置中精炼模型。我们还引入了分段置信度损失，使模型能够在伪时间戳组内具有高置信度的预测以及更准确的动作边界。<br>
                    效果：我们的D-TSTAS方法优于最先进的TSTAS方法，并在三个基准数据集上与全监督方法取得了竞争性的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Timestamp supervised temporal action segmentation (TSTAS) is more cost-effective than fully supervised counterparts. However, previous approaches suffer from severe label bias due to over-reliance on sparse timestamp annotations, resulting in unsatisfactory performance. In this paper, we propose the Debiasing-TSTAS (D-TSTAS) framework by exploiting unannotated frames to alleviate this bias from two phases: 1) Initialization. To reduce the dependencies on annotated frames, we propose masked timestamp predictions (MTP) to ensure that initialized model captures more contextual information. 2) Refinement. To overcome the limitation of the expressiveness from sparsely annotated timestamps, we propose a center-oriented timestamp expansion (CTE) approach to progressively expand pseudo-timestamp groups which contain semantic-rich motion representation of action segments. Then, these pseudo-timestamp groups and the model output are used to iteratively generate pseudo-labels for refining the model in a fully supervised setup. We further introduce segmental confidence loss to enable the model to have high confidence predictions within the pseudo-timestamp groups and more accurate action boundaries. Our D-TSTAS outperforms the state-of-the-art TSTAS method as well as achieves competitive results compared with fully supervised approaches on three benchmark datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2162.SimpSON: Simplifying Photo Cleanup With Single-Click Distracting Object Segmentation Network</span><br>
                <span class="as">Huynh, ChuongandZhou, YuqianandLin, ZheandBarnes, ConnellyandShechtman, EliandAmirghodsi, SohrabandShrivastava, Abhinav</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14518-14527.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过单次点击，有效地选择和移除图像中的视觉干扰物，提高图像质量并突出主要主题。<br>
                    动机：手动选择和移除密集的干扰区域既耗时又费力。因此，需要一种交互式的方法来简化这个过程。<br>
                    方法：提出了一种交互式干扰物选择方法，只需一次点击即可完成。该方法优于传统的运行全景分割然后选择包含点击的片段的方法。我们还展示了如何使用基于变压器的模块来识别与用户点击位置相似的更多干扰区域。<br>
                    效果：实验证明，该模型可以有效地、准确地交互式地分割未知的干扰物体并进行分组。通过大大简化照片清理和修饰过程，我们提出的模型为探索使用单次点击进行稀有物体分割和分组提供了启示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In photo editing, it is common practice to remove visual distractions to improve the overall image quality and highlight the primary subject. However, manually selecting and removing these small and dense distracting regions can be a laborious and time-consuming task. In this paper, we propose an interactive distractor selection method that is optimized to achieve the task with just a single click. Our method surpasses the precision and recall achieved by the traditional method of running panoptic segmentation and then selecting the segments containing the clicks. We also showcase how a transformer-based module can be used to identify more distracting regions similar to the user's click position. Our experiments demonstrate that the model can effectively and accurately segment unknown distracting objects interactively and in groups. By significantly simplifying the photo cleaning and retouching process, our proposed model provides inspiration for exploring rare object segmentation and group selection with a single click.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2163.Discriminating Known From Unknown Objects via Structure-Enhanced Recurrent Variational AutoEncoder</span><br>
                <span class="as">Wu, AmingandDeng, Cheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Discriminating_Known_From_Unknown_Objects_via_Structure-Enhanced_Recurrent_Variational_AutoEncoder_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23956-23965.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用已知的分布内数据提高模型对未知对象的识别能力。<br>
                    动机：模拟人类区分已知和未知对象的能力，提出一种无监督的分布外物体检测任务，有助于推动物体检测器的安全部署。<br>
                    方法：提出了一种结构增强循环变分自编码器（SR-VAE）的方法，主要包括两个专门的循环变分自编码器分支。通过使用经典的拉普拉斯高斯（LoG）算子来增强提取的低层特征的结构信息，以提高物体定位的性能。同时设计了一个生成分类特征增强的变分自编码器分支，以加强物体分类器的判别能力。为了缓解缺乏未知数据的影响，还提出了一个循环一致的条件变分自编码器分支，用于合成偏离分布内特征分布的虚拟分布外特征，以提高区分分布外对象的能力。<br>
                    效果：在分布外物体检测、开放词汇检测和增量物体检测等实验中，该方法显著优于基线方法，显示出优越性。代码将在https://github.com/AmingWu/SR-VAE上发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Discriminating known from unknown objects is an important essential ability for human beings. To simulate this ability, a task of unsupervised out-of-distribution object detection (OOD-OD) is proposed to detect the objects that are never-seen-before during model training, which is beneficial for promoting the safe deployment of object detectors. Due to lacking unknown data for supervision, for this task, the main challenge lies in how to leverage the known in-distribution (ID) data to improve the detector's discrimination ability. In this paper, we first propose a method of Structure-Enhanced Recurrent Variational AutoEncoder (SR-VAE), which mainly consists of two dedicated recurrent VAE branches. Specifically, to boost the performance of object localization, we explore utilizing the classical Laplacian of Gaussian (LoG) operator to enhance the structure information in the extracted low-level features. Meanwhile, we design a VAE branch that recurrently generates the augmentation of the classification features to strengthen the discrimination ability of the object classifier. Finally, to alleviate the impact of lacking unknown data, another cycle-consistent conditional VAE branch is proposed to synthesize virtual OOD features that deviate from the distribution of ID features, which improves the capability of distinguishing OOD objects. In the experiments, our method is evaluated on OOD-OD, open-vocabulary detection, and incremental object detection. The significant performance gains over baselines show the superiorities of our method. The code will be released at https://github.com/AmingWu/SR-VAE.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2164.Fuzzy Positive Learning for Semi-Supervised Semantic Segmentation</span><br>
                <span class="as">Qiao, PengchongandWei, ZhidanandWang, YuandWang, ZhennanandSong, GuoliandXu, FanandJi, XiangyangandLiu, ChangandChen, Jie</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_Fuzzy_Positive_Learning_for_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15465-15474.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决半监督学习中对人工标注的依赖性，以及错误标签带来的干扰问题。<br>
                    动机：通过充分利用多个正确候选标签中的有信息量的语义，以减少对人工标注的依赖并降低错误标签的影响。<br>
                    方法：提出模糊正类学习（FPL）方法，包括模糊正类分配（FPA）和模糊正类正则化（FPR），以实现自适应数量的标签并为每个像素提供模糊正类预测。<br>
                    效果：在Cityscapes和VOC 2012数据集上的实验表明，该方法可以显著减轻错误标签的干扰，并逐步实现清晰的像素级语义判别。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised learning (SSL) essentially pursues class boundary exploration with less dependence on human annotations. Although typical attempts focus on ameliorating the inevitable error-prone pseudo-labeling, we think differently and resort to exhausting informative semantics from multiple probably correct candidate labels. In this paper, we introduce Fuzzy Positive Learning (FPL) for accurate SSL semantic segmentation in a plug-and-play fashion, targeting adaptively encouraging fuzzy positive predictions and suppressing highly-probable negatives. Being conceptually simple yet practically effective, FPL can remarkably alleviate interference from wrong pseudo labels and progressively achieve clear pixel-level semantic discrimination. Concretely, our FPL approach consists of two main components, including fuzzy positive assignment (FPA) to provide an adaptive number of labels for each pixel and fuzzy positive regularization (FPR) to restrict the predictions of fuzzy positive categories to be larger than the rest under different perturbations. Theoretical analysis and extensive experiments on Cityscapes and VOC 2012 with consistent performance gain justify the superiority of our approach. Codes are available in https://github.com/qpc1611094/FPL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2165.Improving Weakly Supervised Temporal Action Localization by Bridging Train-Test Gap in Pseudo Labels</span><br>
                <span class="as">Zhou, JingqiuandHuang, LinjiangandWang, LiangandLiu, SiandLi, Hongsheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Improving_Weakly_Supervised_Temporal_Action_Localization_by_Bridging_Train-Test_Gap_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23003-23012.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱监督的时间动作定位任务，即生成目标动作的时序边界并分类其类别。<br>
                    动机：现有的伪标签方法在训练和测试阶段使用不同的流程或设置，导致训练和测试之间的差距。<br>
                    方法：提出从预测的动作边界生成高质量的伪标签的方法。设计了高斯加权融合模块来保留动作实例的信息并获得高质量的动作边界；将伪标签生成定义为一个优化问题，考虑动作实例的置信度得分约束；引入Delta伪标签的概念，使模型具有自我校正的能力。<br>
                    效果：在THUMOS14和ActivityNet1.3两个基准测试中，该方法的表现优于现有方法，平均mAP分别提高了1.9%和3.7%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of weakly supervised temporal action localization targets at generating temporal boundaries for actions of interest, meanwhile the action category should also be classified. Pseudo-label-based methods, which serve as an effective solution, have been widely studied recently. However, existing methods generate pseudo labels during training and make predictions during testing under different pipelines or settings, resulting in a gap between training and testing. In this paper, we propose to generate high-quality pseudo labels from the predicted action boundaries. Nevertheless, we note that existing post-processing, like NMS, would lead to information loss, which is insufficient to generate high-quality action boundaries. More importantly, transforming action boundaries into pseudo labels is quite challenging, since the predicted action instances are generally overlapped and have different confidence scores. Besides, the generated pseudo-labels can be fluctuating and inaccurate at the early stage of training. It might repeatedly strengthen the false predictions if there is no mechanism to conduct self-correction. To tackle these issues, we come up with an effective pipeline for learning better pseudo labels. Firstly, we propose a Gaussian weighted fusion module to preserve information of action instances and obtain high-quality action boundaries. Second, we formulate the pseudo-label generation as an optimization problem under the constraints in terms of the confidence scores of action instances. Finally, we introduce the idea of Delta pseudo labels, which enables the model with the ability of self-correction. Our method achieves superior performance to existing methods on two benchmarks, THUMOS14 and ActivityNet1.3, achieving gains of 1.9% on THUMOS14 and 3.7% on ActivityNet1.3 in terms of average mAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2166.SOOD: Towards Semi-Supervised Oriented Object Detection</span><br>
                <span class="as">Hua, WeiandLiang, DingkangandLi, JingyuandLiu, XiaolongandZou, ZhikangandYe, XiaoqingandBai, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hua_SOOD_Towards_Semi-Supervised_Oriented_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15558-15567.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督目标检测方法主要关注水平方向的目标，对航空图像中常见的多方向目标尚未进行探索。<br>
                    动机：为了利用未标记的数据提升目标检测器的性能，本文提出了一种新的半监督有向目标检测模型SOOD。<br>
                    方法：在主流的伪标签框架上构建SOOD，设计了两种损失函数来提供更好的监督。第一种损失函数根据对象的方向差距调整自适应权重，以保持每个伪标签预测对的一致性；第二种损失函数则通过建立伪标签集和预测集之间的多对多关系，显式地考虑图像布局，增强了全局一致性约束。<br>
                    效果：实验表明，使用这两种提出的损失函数训练后，SOOD在DOTA-v1.5基准测试的各种设置下超越了现有的半监督目标检测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-Supervised Object Detection (SSOD), aiming to explore unlabeled data for boosting object detectors, has become an active task in recent years. However, existing SSOD approaches mainly focus on horizontal objects, leaving multi-oriented objects that are common in aerial images unexplored. This paper proposes a novel Semi-supervised Oriented Object Detection model, termed SOOD, built upon the mainstream pseudo-labeling framework. Towards oriented objects in aerial scenes, we design two loss functions to provide better supervision. Focusing on the orientations of objects, the first loss regularizes the consistency between each pseudo-label-prediction pair (includes a prediction and its corresponding pseudo label) with adaptive weights based on their orientation gap. Focusing on the layout of an image, the second loss regularizes the similarity and explicitly builds the many-to-many relation between the sets of pseudo-labels and predictions. Such a global consistency constraint can further boost semi-supervised learning. Our experiments show that when trained with the two proposed losses, SOOD surpasses the state-of-the-art SSOD methods under various settings on the DOTA-v1.5 benchmark. The code will be available at https://github.com/HamPerdredes/SOOD.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2167.Semi-DETR: Semi-Supervised Object Detection With Detection Transformers</span><br>
                <span class="as">Zhang, JiachengandLin, XiangruandZhang, WeiandWang, KuoandTan, XiaoandHan, JunyuandDing, ErruiandWang, JingdongandLi, Guanbin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23809-23818.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的基于DETR的半监督目标检测（SSOD）框架存在一些问题，如一对一匹配策略在伪真值边界框不准确时会产生错误的匹配，导致训练效率低下；以及基于DETR的检测器在其输入查询和预测输出之间缺乏确定性对应关系，阻碍了一致性正则化方法在当前SSOD方法中的广泛应用。<br>
                    动机：为了解决上述问题，我们提出了一种新的基于变压器的端到端半监督目标检测器——Semi-DETR。<br>
                    方法：我们设计了一种阶段混合匹配策略，结合一对一和一对多分配策略，以提高第一阶段的训练效率，并为第二阶段的训练提供高质量的伪标签。此外，我们还引入了跨视图查询一致性方法，以学习来自不同视图的目标查询的语义特征不变性，同时避免了寻找确定性查询对应关系的需求。最后，我们提出了一种基于成本的伪标签挖掘模块，根据伪真值边界框的匹配成本动态挖掘更多的伪框进行一致性训练。<br>
                    效果：我们在COCO和Pascal VOC基准数据集的所有SSOD设置上进行了广泛的实验，结果显示我们的Semi-DETR方法在所有最先进的方法上都取得了明显的优势。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We analyze the DETR-based framework on semi-supervised object detection (SSOD) and observe that (1) the one-to-one assignment strategy generates incorrect matching when the pseudo ground-truth bounding box is inaccurate, leading to training inefficiency; (2) DETR-based detectors lack deterministic correspondence between the input query and its prediction output, which hinders the applicability of the consistency-based regularization widely used in current SSOD methods. We present Semi-DETR, the first transformer-based end-to-end semi-supervised object detector, to tackle these problems. Specifically, we propose a Stage-wise Hybrid Matching strategy that com- bines the one-to-many assignment and one-to-one assignment strategies to improve the training efficiency of the first stage and thus provide high-quality pseudo labels for the training of the second stage. Besides, we introduce a Cross-view Query Consistency method to learn the semantic feature invariance of object queries from different views while avoiding the need to find deterministic query correspondence. Furthermore, we propose a Cost-based Pseudo Label Mining module to dynamically mine more pseudo boxes based on the matching cost of pseudo ground truth bounding boxes for consistency training. Extensive experiments on all SSOD settings of both COCO and Pascal VOC benchmark datasets show that our Semi-DETR method outperforms all state-of-the-art methods by clear margins.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2168.Hierarchical Semantic Contrast for Scene-Aware Video Anomaly Detection</span><br>
                <span class="as">Sun, ShengyangandGong, Xiaojin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Hierarchical_Semantic_Contrast_for_Scene-Aware_Video_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22846-22856.png><br>
            
            <span class="tt"><span class="t0">研究问题：视频异常检测中场景感知性增强的关键挑战。<br>
                    动机：现有的视频异常检测模型缺乏对场景的理解和利用，通过引入预训练的视频解析模型和对比学习，可以更好地捕捉视频中的语义信息。<br>
                    方法：提出一种分层语义对比（HSC）方法，通过自动编码器重建框架，结合前景物体和背景场景特征进行高级别的语义整合，同时在场景级别和物体级别进行对比学习，以提高模型的判别能力。<br>
                    效果：实验结果表明，该方法在多个公共数据集和场景依赖混合数据集上均取得了良好的效果，能有效提高视频异常检测的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Increasing scene-awareness is a key challenge in video anomaly detection (VAD). In this work, we propose a hierarchical semantic contrast (HSC) method to learn a scene-aware VAD model from normal videos. We first incorporate foreground object and background scene features with high-level semantics by taking advantage of pre-trained video parsing models. Then, building upon the autoencoder-based reconstruction framework, we introduce both scene-level and object-level contrastive learning to enforce the encoded latent features to be compact within the same semantic classes while being separable across different classes. This hierarchical semantic contrast strategy helps to deal with the diversity of normal patterns and also increases their discrimination ability. Moreover, for the sake of tackling rare normal activities, we design a skeleton-based motion augmentation to increase samples and refine the model further. Extensive experiments on three public datasets and scene-dependent mixture datasets validate the effectiveness of our proposed method.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2169.Self-Guided Diffusion Models</span><br>
                <span class="as">Hu, VincentTaoandZhang, DavidW.andAsano, YukiM.andBurghouts, GertjanJ.andSnoek, CeesG.M.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Self-Guided_Diffusion_Models_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18413-18422.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决传统扩散模型在图像生成过程中需要大量标注对的问题。<br>
                    动机：传统的扩散模型在图像生成质量上取得了显著的进步，但需要大量的图像标注对进行训练，且依赖于其可用性和准确性。<br>
                    方法：本文提出了一种自我指导的扩散模型框架，利用特征提取函数和自我标注函数，在不同的图像粒度上提供指导信号，包括整体图像、对象框甚至分割掩码。<br>
                    效果：实验结果表明，自我标注的指导在所有情况下都优于没有指导的扩散模型，甚至可能超过基于地面真值标签的指导。当配备自我监督的框或掩码建议时，该方法可以生成视觉上多样而语义一致的图像，无需任何类别、框或分割标签的标注。自我指导的扩散模型简单、灵活，并有望在大规模部署中获益。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability and correctness. In this paper, we eliminate the need for such annotation by instead exploiting the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2170.Dense Distinct Query for End-to-End Object Detection</span><br>
                <span class="as">Zhang, ShilongandWang, XinjiangandWang, JiaqiandPang, JiangmiaoandLyu, ChengqiandZhang, WenweiandLuo, PingandChen, Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7329-7338.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在对象检测中进行端到端的一对一标签分配，同时解决现有稀疏查询无法保证高召回率和密集查询带来的相似查询过多和优化困难的问题。<br>
                    动机：现有的稀疏查询和密集查询在对象检测中都有其问题，因此需要寻找一种新的查询方式来提高检测性能。<br>
                    方法：提出了一种称为Dense Distinct Queries（DDQ）的方法。首先像传统检测器一样放置密集查询，然后选择不同的查询进行一对一的分配。DDQ结合了传统方法和最新端到端检测器的优点，显著提高了各种检测器的性能。<br>
                    效果：实验结果表明，DDQ-DETR在MS-COCO数据集上使用ResNet-50主干网络在12个epoch内达到了52.1的AP，超过了同一设置下的所有现有检测器。DDQ还具有在拥挤场景中的优势，并在CrowdHuman数据集上实现了93.8的AP。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>One-to-one label assignment in object detection has successfully obviated the need of non-maximum suppression (NMS) as a postprocessing and makes the pipeline end-to-end. However, it triggers a new dilemma as the widely used sparse queries cannot guarantee a high recall, while dense queries inevitably bring more similar queries and encounters optimization difficulty. As both sparse and dense queries are problematic, then what are the expected queries in end-to-end object detection? This paper shows that the solution should be Dense Distinct Queries (DDQ). Concretely, we first lay dense queries like traditional detectors and then select distinct ones for one-to-one assignments. DDQ blends the advantages of traditional and recent end-to-end detectors and significantly improves the performance of various detectors including FCN, R-CNN, and DETRs. Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming all existing detectors in the same setting. DDQ also shares the benefit of end-to-end detectors in crowded scenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers to consider the complementarity between traditional methods and end-to-end detectors. The source code can be found at https://github.com/jshilong/DDQ.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2171.DETR With Additional Global Aggregation for Cross-Domain Weakly Supervised Object Detection</span><br>
                <span class="as">Tang, ZonghengandSun, YifanandLiu, SiandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_DETR_With_Additional_Global_Aggregation_for_Cross-Domain_Weakly_Supervised_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11422-11432.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种基于DETR的跨领域弱监督物体检测（CDWSOD）方法，旨在通过弱监督将检测器从源领域适应到目标领域。<br>
                    动机：由于DETR的编码器和解码器都基于注意力机制，因此能够在整个图像上聚合语义，这使DETR具有在CDWSOD中应用的巨大潜力。<br>
                    方法：我们提出了带有额外全局聚合的DETR（DETR-GA），这是一种同时进行“实例级别+图像级别”预测并利用“强+弱”监督的CDWSOD检测器。关键思想非常简单：对于编码器/解码器，我们分别添加多个类别查询/前景查询以将语义聚合成图像级别的预测。<br>
                    效果：在四个流行的跨领域基准测试集上的大量实验表明，DETR-GA显著提高了CSWSOD的性能，并在几个数据集上超越了现有技术。例如，在PASCAL VOC到Clipart_all数据集上，mAP从29.0%提高到79.4%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents a DETR-based method for cross-domain weakly supervised object detection (CDWSOD), aiming at adapting the detector from source to target domain through weak supervision. We think DETR has strong potential for CDWSOD due to an insight: the encoder and the decoder in DETR are both based on the attention mechanism and are thus capable of aggregating semantics across the entire image. The aggregation results, i.e., image-level predictions, can naturally exploit the weak supervision for domain alignment. Such motivated, we propose DETR with additional Global Aggregation (DETR-GA), a CDWSOD detector that simultaneously makes "instance-level + image-level" predictions and utilizes "strong + weak" supervisions. The key point of DETR-GA is very simple: for the encoder / decoder, we respectively add multiple class queries / a foreground query to aggregate the semantics into image-level predictions. Our query-based aggregation has two advantages. First, in the encoder, the weakly-supervised class queries are capable of roughly locating the corresponding positions and excluding the distraction from non-relevant regions. Second, through our design, the object queries and the foreground query in the decoder share consensus on the class semantics, therefore making the strong and weak supervision mutually benefit each other for domain alignment. Extensive experiments on four popular cross-domain benchmarks show that DETR-GA significantly improves CSWSOD and advances the states of the art (e.g., 29.0% --> 79.4% mAP on PASCAL VOC --> Clipart_all dataset).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2172.Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning</span><br>
                <span class="as">Liu, KangningandZhu, WeichengandShen, YiqiuandLiu, ShengandRazavian, NargesandGeras, KrzysztofJ.andFernandez-Granda, Carlos</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Multiple_Instance_Learning_via_Iterative_Self-Paced_Supervised_Contrastive_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3355-3365.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在只有包级别标签的情况下学习单个实例的表示，这是多实例学习（MIL）的基本挑战。<br>
                    动机：在现实世界的应用中，如医学图像分类，通常存在类别不平衡的问题，导致随机选择的实例大多属于同一多数类，这阻碍了对比自监督学习（CSSL）学习类别间差异。<br>
                    方法：提出一种新的框架——迭代自我步调监督对比学习用于MIL表示（ItS2CLR），通过利用从包级别标签派生的实例级伪标签来改进学习的表示。该框架采用一种新的自我步调采样策略以确保伪标签的准确性。<br>
                    效果：在三个医疗数据集上评估ItS2CLR，结果显示，它提高了实例级伪标签和表示的质量，并在包级别和实例级别准确性方面优于现有的MIL方法。代码可在https://github.com/Kangningthu/ItS2CLR获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning representations for individual instances when only bag-level labels are available is a fundamental challenge in multiple instance learning (MIL). Recent works have shown promising results using contrastive self-supervised learning (CSSL), which learns to push apart representations corresponding to two different randomly-selected instances. Unfortunately, in real-world applications such as medical image classification, there is often class imbalance, so randomly-selected instances mostly belong to the same majority class, which precludes CSSL from learning inter-class differences. To address this issue, we propose a novel framework, Iterative Self-paced Supervised Contrastive Learning for MIL Representations (ItS2CLR), which improves the learned representation by exploiting instance-level pseudo labels derived from the bag-level labels. The framework employs a novel self-paced sampling strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three medical datasets, showing that it improves the quality of instance-level pseudo labels and representations, and outperforms existing MIL methods in terms of both bag and instance level accuracy. Code is available at https://github.com/Kangningthu/ItS2CLR</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2173.Weak-Shot Object Detection Through Mutual Knowledge Transfer</span><br>
                <span class="as">Du, XuanyiandWan, WeitaoandSun, ChongandLi, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19671-19680.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决弱目标检测中只有图像级标签的目标任务集的问题。<br>
                    动机：通过在源数据集和目标任务集之间双向转移对象知识，提高目标任务集的检测性能。<br>
                    方法：提出了一种新颖的知识转移损失函数，同时从源数据集上训练的建议生成器中提炼出对象性和类别熵的知识，优化目标任务集上的多实例学习模块。<br>
                    效果：通过共同优化分类损失和提出的知识转移损失，多实例学习模块有效地学习将建议划分为目标任务集中的新类别，并利用源数据集中的基本类别的知识。实验表明，该方法在不增加模型参数或推理计算复杂度的情况下，显著提高了目标任务集的检测性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weak-shot Object Detection methods exploit a fully-annotated source dataset to facilitate the detection performance on the target dataset which only contains image-level labels for novel categories. To bridge the gap between these two datasets, we aim to transfer the object knowledge between the source (S) and target (T) datasets in a bi-directional manner. We propose a novel Knowledge Transfer (KT) loss which simultaneously distills the knowledge of objectness and class entropy from a proposal generator trained on the S dataset to optimize a multiple instance learning module on the T dataset. By jointly optimizing the classification loss and the proposed KT loss, the multiple instance learning module effectively learns to classify object proposals into novel categories in the T dataset with the transferred knowledge from base categories in the S dataset. Noticing the predicted boxes on the T dataset can be regarded as an extension for the original annotations on the S dataset to refine the proposal generator in return, we further propose a novel Consistency Filtering (CF) method to reliably remove inaccurate pseudo labels by evaluating the stability of the multiple instance learning module upon noise injections. Via mutually transferring knowledge between the S and T datasets in an iterative manner, the detection performance on the target dataset is significantly improved. Extensive experiments on public benchmarks validate that the proposed method performs favourably against the state-of-the-art methods without increasing the model parameters or inference computational complexity.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2174.CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model</span><br>
                <span class="as">Liang, DingkangandXie, JiahaoandZou, ZhikangandYe, XiaoqingandXu, WeiandBai, Xiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2893-2903.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决监督式人群计数需要大量昂贵手动标注的问题。<br>
                    动机：监督式人群计数方法在高密度场景中，由于需要大量的人工标注，既困难又昂贵。<br>
                    方法：提出一种名为CrowdCLIP的新型无监督框架进行人群计数。利用对比预训练的视觉语言模型（CLIP）和人群补丁与计数文本之间的自然映射关系，通过构建排名文本提示来匹配大小排序的人群补丁，引导图像编码器学习。<br>
                    效果：实验结果表明，CrowdCLIP在五个具有挑战性的数据集上的表现优于先前的无监督最先进计数方法，甚至在某些跨数据集设置下超过了一些流行的全监督方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Supervised crowd counting relies heavily on costly manual labeling, which is difficult and expensive, especially in dense scenes. To alleviate the problem, we propose a novel unsupervised framework for crowd counting, named CrowdCLIP. The core idea is built on two observations: 1) the recent contrastive pre-trained vision-language model (CLIP) has presented impressive performance on various downstream tasks; 2) there is a natural mapping between crowd patches and count text. To the best of our knowledge, CrowdCLIP is the first to investigate the vision-language knowledge to solve the counting problem. Specifically, in the training stage, we exploit the multi-modal ranking loss by constructing ranking text prompts to match the size-sorted crowd patches to guide the image encoder learning. In the testing stage, to deal with the diversity of image patches, we propose a simple yet effective progressive filtering strategy to first select the highly potential crowd patches and then map them into the language space with various counting intervals. Extensive experiments on five challenging datasets demonstrate that the proposed CrowdCLIP achieves superior performance compared to previous unsupervised state-of-the-art counting methods. Notably, CrowdCLIP even surpasses some popular fully-supervised methods under the cross-dataset setting. The source code will be available at https://github.com/dk-liang/CrowdCLIP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2175.Geometric Visual Similarity Learning in 3D Medical Image Self-Supervised Pre-Training</span><br>
                <span class="as">He, YutingandYang, GuanyuandGe, RongjunandChen, YangandCoatrieux, Jean-LouisandWang, BoyuandLi, Shuo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Geometric_Visual_Similarity_Learning_in_3D_Medical_Image_Self-Supervised_Pre-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9538-9547.png><br>
            
            <span class="tt"><span class="t0">研究问题：学习图像间的相似性对于3D医疗图像的自监督预训练至关重要，但由于缺乏语义先验和语义无关的3D医疗图像变化，获取可靠的图像间相似性测量变得困难，阻碍了相同语义的学习。<br>
                    动机：由于3D医疗图像中存在大量相同的语义区域，因此学习图像间的相似性对于其自我监督预训练至关重要。然而，现有的方法在度量标准上缺乏语义先验，且3D医疗图像中的语义无关变化使得获取可靠的图像间相似性测量变得困难，这阻碍了相同语义的学习。<br>
                    方法：我们提出了一种新的视觉相似性学习范式——几何视觉相似性学习（GVSL），它将拓扑不变性的先验嵌入到图像间相似性的测量中，以实现相同语义区域的一致表示。为了推动这一范式，我们还构建了一个新颖的几何匹配头——Z匹配头，它协同学习了语义区域的全局和局部相似性，引导了不同尺度级别的图像间语义特征的有效表示学习。<br>
                    效果：我们的实验表明，使用我们的图像间相似性学习方法进行预训练，在四个具有挑战性的3D医疗图像任务上获得了更强的内部场景、场景间和全局-局部转移能力。我们的代码和预训练模型将在https://github.com/YutingHe-list/GVSL上公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning inter-image similarity is crucial for 3D medical images self-supervised pre-training, due to their sharing of numerous same semantic regions. However, the lack of the semantic prior in metrics and the semantic-independent variation in 3D medical images make it challenging to get a reliable measurement for the inter-image similarity, hindering the learning of consistent representation for same semantics. We investigate the challenging problem of this task, i.e., learning a consistent representation between images for a clustering effect of same semantic features. We propose a novel visual similarity learning paradigm, Geometric Visual Similarity Learning, which embeds the prior of topological invariance into the measurement of the inter-image similarity for consistent representation of semantic regions. To drive this paradigm, we further construct a novel geometric matching head, the Z-matching head, to collaboratively learn the global and local similarity of semantic regions, guiding the efficient representation learning for different scale-level inter-image semantic features. Our experiments demonstrate that the pre-training with our learning of inter-image similarity yields more powerful inner-scene, inter-scene, and global-local transferring ability on four challenging 3D medical image tasks. Our codes and pre-trained models will be publicly available in https://github.com/YutingHe-list/GVSL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2176.Enhanced Training of Query-Based Object Detection via Selective Query Recollection</span><br>
                <span class="as">Chen, FangyiandZhang, HanandHu, KaiandHuang, Yu-KaiandZhu, ChenchenandSavvides, Marios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23756-23765.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文调查了基于查询的目标检测器在预测的最后解码阶段误判，但在中间阶段预测正确的现象。<br>
                    动机：作者认为这种现象是由于训练过程中的两个限制造成的：缺乏训练重点和解码序列的级联错误。<br>
                    方法：作者设计并提出了选择性查询回收（SQR）策略，这是一种简单而有效的训练策略。它会随着解码阶段的深入逐步收集中间查询，并选择性地将查询转发到下游阶段，而不是按照序列结构进行。这样，SQR就可以将训练重点放在后期阶段，并允许后期阶段直接使用早期阶段的中间查询。<br>
                    效果：作者将SQR应用于Adamixer、DAB-DETR和Deformable-DETR等不同的基于查询的目标检测器，并在各种设置（主干网络、查询数量、调度）下进行了测试，结果普遍提高了1.4至2.8个AP值。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper investigates a phenomenon where query-based object detectors mispredict at the last decoding stage while predicting correctly at an intermediate stage. We review the training process and attribute the overlooked phenomenon to two limitations: lack of training emphasis and cascading errors from decoding sequence. We design and present Selective Query Recollection (SQR), a simple and effective training strategy for query-based object detectors. It cumulatively collects intermediate queries as decoding stages go deeper and selectively forwards the queries to the downstream stages aside from the sequential structure. Such-wise, SQR places training emphasis on later stages and allows later stages to work with intermediate queries from earlier stages directly. SQR can be easily plugged into various query-based object detectors and significantly enhances their performance while leaving the inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR, and Deformable-DETR across various settings (backbone, number of queries, schedule) and consistently brings 1.4   2.8 AP improvement.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2177.Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly Detection</span><br>
                <span class="as">Lv, HuiandYue, ZhongqiandSun, QianruandLuo, BinandCui, ZhenandZhang, Hanwang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lv_Unbiased_Multiple_Instance_Learning_for_Weakly_Supervised_Video_Anomaly_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8022-8031.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督视频异常检测（WSVAD）面临的挑战是，虽然异常标签只在视频级别给出，但输出需要片段级别的预测。<br>
                    动机：现有的多实例学习（MIL）方法在WSVAD中普遍存在，但因为片段级别的检测器容易受到简单上下文的异常片段的影响，对相同偏见的正常片段感到困惑，并错过具有不同模式的异常，因此会产生许多误报。<br>
                    方法：为此，我们提出了一种新的MIL框架——无偏MIL（UMIL），以学习改善WSVAD的无偏异常特征。在每次MIL训练迭代中，我们使用当前的检测器将样本分为两组具有不同上下文偏见的样本：最有信心的异常/正常片段和其余的模糊片段。然后，通过寻找两个样本组之间的不变特征，我们可以消除可变的上下文偏见。<br>
                    效果：我们在UCF-Crime和TAD基准测试上的大量实验表明了我们的UMIL的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly Supervised Video Anomaly Detection (WSVAD) is challenging because the binary anomaly label is only given on the video level, but the output requires snippet-level predictions. So, Multiple Instance Learning (MIL) is prevailing in WSVAD. However, MIL is notoriously known to suffer from many false alarms because the snippet-level detector is easily biased towards the abnormal snippets with simple context, confused by the normality with the same bias, and missing the anomaly with a different pattern. To this end, we propose a new MIL framework: Unbiased MIL (UMIL), to learn unbiased anomaly features that improve WSVAD. At each MIL training iteration, we use the current detector to divide the samples into two groups with different context biases: the most confident abnormal/normal snippets and the rest ambiguous ones. Then, by seeking the invariant features across the two sample groups, we can remove the variant context biases. Extensive experiments on benchmarks UCF-Crime and TAD demonstrate the effectiveness of our UMIL. Our code is provided at https://github.com/ktr-hubrt/UMIL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2178.Rethinking the Correlation in Few-Shot Segmentation: A Buoys View</span><br>
                <span class="as">Wang, YuanandSun, RuiandZhang, Tianzhu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7183-7192.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何减少由像素级相关性引起的错误匹配，以在给定的查询图像中仅使用少数标注的支持图像来分割新的对象。<br>
                    动机：大多数先前的最佳方法（无论是原型学习还是亲和力学习方法）都忽视了由于其自身的像素级相关性而引起的错误匹配。<br>
                    方法：从代表性参考特征（称为浮标）的角度重新思考如何减轻错误匹配，并提出一种新的自适应浮标关联网络（ABC）来纠正直接的像素级相关性，包括一个浮标挖掘模块和一个自适应关联模块。<br>
                    效果：通过在两个具有挑战性的基准测试上使用两种不同的骨干网络进行广泛的实验，结果表明，我们的ABC作为一种通用插件，在1-shot和5-shot设置上均比几种领先方法取得了一致的改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Few-shot segmentation (FSS) aims to segment novel objects in a given query image with only a few annotated support images. However, most previous best-performing methods, whether prototypical learning methods or affinity learning methods, neglect to alleviate false matches caused by their own pixel-level correlation. In this work, we rethink how to mitigate the false matches from the perspective of representative reference features (referred to as buoys), and propose a novel adaptive buoys correlation (ABC) network to rectify direct pairwise pixel-level correlation, including a buoys mining module and an adaptive correlation module. The proposed ABC enjoys several merits. First, to learn the buoys well without any correspondence supervision, we customize the buoys mining module according to the three characteristics of representativeness, task awareness and resilience. Second, the proposed adaptive correlation module is responsible for further endowing buoy-correlation-based pixel matching with an adaptive ability. Extensive experimental results with two different backbones on two challenging benchmarks demonstrate that our ABC, as a general plugin, achieves consistent improvements over several leading methods on both 1-shot and 5-shot settings.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2179.Proposal-Based Multiple Instance Learning for Weakly-Supervised Temporal Action Localization</span><br>
                <span class="as">Ren, HuanandYang, WenfeiandZhang, TianzhuandZhang, Yongdong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Proposal-Based_Multiple_Instance_Learning_for_Weakly-Supervised_Temporal_Action_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2394-2404.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督的时间动作定位旨在在训练期间仅使用视频级别的类别标签对未修剪的视频进行动作的定位和识别。<br>
                    动机：大多数现有方法遵循基于片段的多实例学习（S-MIL）框架，其中片段的预测由视频的标签进行监督。然而，训练阶段获取片段级分数的目标与测试阶段获取提案级分数的目标不一致，导致结果不佳。<br>
                    方法：我们提出了一种新的基于提案的多实例学习（P-MIL）框架，该框架直接在训练和测试阶段对候选提案进行分类，包括三个关键设计：1) 一个考虑周围对比信息的周边对比特征提取模块，以抑制具有区分性的短提案；2) 一个提案完整性评估模块，通过完整性伪标签的指导抑制低质量提案；3) 一个实例级别排名一致性损失，通过利用RGB和FLOW模态的互补性实现稳健检测。<br>
                    效果：我们在THUMOS14和ActivityNet两个具有挑战性的基准上进行了广泛的实验，结果表明我们的方法性能优越。我们的代码可以在github.com/OpenSpaceAI/CVPR2023_P-MIL上找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Weakly-supervised temporal action localization aims to localize and recognize actions in untrimmed videos with only video-level category labels during training. Without instance-level annotations, most existing methods follow the Segment-based Multiple Instance Learning (S-MIL) framework, where the predictions of segments are supervised by the labels of videos. However, the objective for acquiring segment-level scores during training is not consistent with the target for acquiring proposal-level scores during testing, leading to suboptimal results. To deal with this problem, we propose a novel Proposal-based Multiple Instance Learning (P-MIL) framework that directly classifies the candidate proposals in both the training and testing stages, which includes three key designs: 1) a surrounding contrastive feature extraction module to suppress the discriminative short proposals by considering the surrounding contrastive information, 2) a proposal completeness evaluation module to inhibit the low-quality proposals with the guidance of the completeness pseudo labels, and 3) an instance-level rank consistency loss to achieve robust detection by leveraging the complementarity of RGB and FLOW modalities. Extensive experimental results on two challenging benchmarks including THUMOS14 and ActivityNet demonstrate the superior performance of our method. Our code is available at github.com/OpenSpaceAI/CVPR2023_P-MIL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2180.Interventional Bag Multi-Instance Learning on Whole-Slide Pathological Images</span><br>
                <span class="as">Lin, TianchengandYu, ZhimiaoandHu, HongyuandXu, YiandChen, Chang-Wen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Interventional_Bag_Multi-Instance_Learning_on_Whole-Slide_Pathological_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19830-19839.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改进现有的多实例学习（MIL）方法，以解决在处理全幅病理图像（WSIs）分类时，由于巨大的像素分辨率和幻灯片级别的标签导致的困难。<br>
                    动机：现有的MIL方法主要关注于提高特征提取器和聚合器的性能，但一个主要的不足是袋子的上下文先验可能会误导模型捕获袋子和标签之间的虚假相关性，这限制了现有MIL方法的性能。<br>
                    方法：提出了一种新的策略——干预性袋子多实例学习（IBMIL），通过后门调整实现干预训练，从而抑制由袋子的上下文先验引起的偏差。<br>
                    效果：实验结果表明，IBMIL能够显著提升现有方案的性能，达到新的最先进的性能水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-instance learning (MIL) is an effective paradigm for whole-slide pathological images (WSIs) classification to handle the gigapixel resolution and slide-level label. Prevailing MIL methods primarily focus on improving the feature extractor and aggregator. However, one deficiency of these methods is that the bag contextual prior may trick the model into capturing spurious correlations between bags and labels. This deficiency is a confounder that limits the performance of existing MIL methods. In this paper, we propose a novel scheme, Interventional Bag Multi-Instance Learning (IBMIL), to achieve deconfounded bag-level prediction. Unlike traditional likelihood-based strategies, the proposed scheme is based on the backdoor adjustment to achieve the interventional training, thus is capable of suppressing the bias caused by the bag contextual prior. Note that the principle of IBMIL is orthogonal to existing bag MIL methods. Therefore, IBMIL is able to bring consistent performance boosting to existing schemes, achieving new state-of-the-art performance. Code is available at https://github.com/HHHedo/IBMIL.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2181.SegLoc: Learning Segmentation-Based Representations for Privacy-Preserving Visual Localization</span><br>
                <span class="as">Pietrantoni, MaximeandHumenberger, MartinandSattler, TorstenandCsurka, Gabriela</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pietrantoni_SegLoc_Learning_Segmentation-Based_Representations_for_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15380-15391.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探讨如何利用强大的图像分割技术在保护隐私的视觉定位中发挥作用。<br>
                    动机：现有的视觉定位方法缺乏对个人隐私的保护，而图像分割技术可以创建强大、紧凑且保护隐私的场景表示。<br>
                    方法：提出一种新的定位框架SegLoc，利用图像分割技术创建稳健、紧凑且保护隐私的场景表示，即3D地图。通过学习一组具有判别性的聚类标签、额外的一致性正则化项以及联合学习全局和局部密集表示，使Larson等人（ICCV'19）的对应监督细粒度分割方法更具鲁棒性。<br>
                    效果：实验结果表明，所提出的表示法能够在仅使用不包含足够信息以供攻击者重建个人信息的紧凑3D地图的情况下实现（接近）最先进的姿态估计结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inspired by properties of semantic segmentation, in this paper we investigate how to leverage robust image segmentation in the context of privacy-preserving visual localization. We propose a new localization framework, SegLoc, that leverages image segmentation to create robust, compact, and privacy-preserving scene representations, i.e., 3D maps. We build upon the correspondence-supervised, fine-grained segmentation approach from Larsson et al (ICCV'19), making it more robust by learning a set of cluster labels with discriminative clustering, additional consistency regularization terms and we jointly learn a global image representation along with a dense local representation. In our localization pipeline, the former will be used for retrieving the most similar images, the latter to refine the retrieved poses by minimizing the label inconsistency between the 3D points of the map and their projection onto the query image. In various experiments, we show that our proposed representation allows to achieve (close-to) state-of-the-art pose estimation results while only using a compact 3D map that does not contain enough information about the original images for an attacker to reconstruct personal information.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2182.RankMix: Data Augmentation for Weakly Supervised Learning of Classifying Whole Slide Images With Diverse Sizes and Imbalanced Categories</span><br>
                <span class="as">Chen, Yuan-ChihandLu, Chun-Shien</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_RankMix_Data_Augmentation_for_Weakly_Supervised_Learning_of_Classifying_Whole_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23936-23945.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对大规模、不平衡的全幻灯片图像（WSIs）进行分类，这是一类弱监督学习问题。<br>
                    动机：全幻灯片图像（WSIs）通常具有千兆像素的大小，缺乏像素级别的注释，且类别分布极度不平衡，这给其分类带来了挑战。<br>
                    方法：我们提出了一种名为RankMix的数据增强方法，该方法通过混合成对WSIs中的特征排名来提取关键区域，并引入了伪标签和排名的概念以提升模型性能。<br>
                    效果：RankMix方法在处理缺乏训练数据和类别不平衡的WSI分类问题上表现出良好的效果，为解决此类问题提供了新的视角。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Whole Slide Images (WSIs) are usually gigapixel in size and lack pixel-level annotations. The WSI datasets are also imbalanced in categories. These unique characteristics, significantly different from the ones in natural images, pose the challenge of classifying WSI images as a kind of weakly supervise learning problems. In this study, we propose, RankMix, a data augmentation method of mixing ranked features in a pair of WSIs. RankMix introduces the concepts of pseudo labeling and ranking in order to extract key WSI regions in contributing to the WSI classification task. A two-stage training is further proposed to boost stable training and model performance. To our knowledge, the study of weakly supervised learning from the perspective of data augmentation to deal with the WSI classification problem that suffers from lack of training data and imbalance of categories is relatively unexplored.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2183.Mask-Guided Matting in the Wild</span><br>
                <span class="as">Park, KwanyongandWoo, SanghyunandOh, SeoungWugandKweon, InSoandLee, Joon-Young</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Mask-Guided_Matting_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1992-2001.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将掩码引导的抠图技术扩展到实际场景中，处理各种复杂背景下的多种类别？<br>
                    动机：传统的基于三值映射的方法在实际应用中表现不佳，而掩码引导的抠图方法虽然实用，但需要处理更广泛的类别和复杂的背景。<br>
                    方法：提出了一种简单有效的学习框架，包括1) 学习一个能理解给定掩码指导的通用抠图模型；2) 利用弱监督数据集（如实例分割数据集）来缓解现有抠图数据集的多样性和规模限制。<br>
                    效果：在多个基准测试上进行了大量实验，包括一个新的合成基准（Composition-Wild）和现有的自然数据集，证明了该方法的优越性。同时，在新的实际应用（如全景抠图和掩码引导的视频抠图）上也取得了良好的效果，显示出模型的强大通用性和潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Mask-guided matting has shown great practicality compared to traditional trimap-based methods. The mask-guided approach takes an easily-obtainable coarse mask as guidance and produces an accurate alpha matte. To extend the success toward practical usage, we tackle mask-guided matting in the wild, which covers a wide range of categories in their complex context robustly. To this end, we propose a simple yet effective learning framework based on two core insights: 1) learning a generalized matting model that can better understand the given mask guidance and 2) leveraging weak supervision datasets (e.g., instance segmentation dataset) to alleviate the limited diversity and scale of existing matting datasets. Extensive experimental results on multiple benchmarks, consisting of a newly proposed synthetic benchmark (Composition-Wild) and existing natural datasets, demonstrate the superiority of the proposed method. Moreover, we provide appealing results on new practical applications (e.g., panoptic matting and mask-guided video matting), showing the great generality and potential of our model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2184.Dynamic Conceptional Contrastive Learning for Generalized Category Discovery</span><br>
                <span class="as">Pu, NanandZhong, ZhunandSebe, Nicu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pu_Dynamic_Conceptional_Contrastive_Learning_for_Generalized_Category_Discovery_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7579-7588.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决广义类别发现（GCD）的问题，即如何自动对部分标记的数据进行聚类。<br>
                    动机：传统的新类别发现（NCD）方法由于假设未标记的数据只来自新的类别，因此在面对GCD时显得力不从心。<br>
                    方法：本文提出了一种动态概念对比学习（DCCL）框架，通过交替估计潜在的视觉概念和学习概念表示，以改善聚类准确性。同时设计了一种动态概念生成和更新机制，以确保概念学习的一致性，进一步优化DCCL。<br>
                    效果：实验表明，DCCL在六个通用和细粒度的视觉识别数据集上取得了新的最先进的性能，特别是在细粒度数据集上表现突出。例如，在CUB-200数据集的新类别上，该方法比最佳竞争对手高出16.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalized category discovery (GCD) is a recently proposed open-world problem, which aims to automatically cluster partially labeled data. The main challenge is that the unlabeled data contain instances that are not only from known categories of the labeled data but also from novel categories. This leads traditional novel category discovery (NCD) methods to be incapacitated for GCD, due to their assumption of unlabeled data are only from novel categories. One effective way for GCD is applying self-supervised learning to learn discriminate representation for unlabeled data. However, this manner largely ignores underlying relationships between instances of the same concepts (e.g., class, super-class, and sub-class), which results in inferior representation learning. In this paper, we propose a Dynamic Conceptional Contrastive Learning (DCCL) framework, which can effectively improve clustering accuracy by alternately estimating underlying visual conceptions and learning conceptional representation. In addition, we design a dynamic conception generation and update mechanism, which is able to ensure consistent conception learning and thus further facilitate the optimization of DCCL. Extensive experiments show that DCCL achieves new state-of-the-art performances on six generic and fine-grained visual recognition datasets, especially on fine-grained ones. For example, our method significantly surpasses the best competitor by 16.2% on the new classes for the CUB-200 dataset. Code is available at https://github.com/TPCD/DCCL</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2185.Zero-Shot Referring Image Segmentation With Global-Local Context Features</span><br>
                <span class="as">Yu, SeonghoonandSeo, PaulHongsuckandSon, Jeany</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19456-19465.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过利用大规模文本语料库和知识图谱训练一种增强的语言表示模型（ERNIE），以同时充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring image segmentation (RIS) aims to find a segmentation mask given a referring expression grounded to a region of the input image. Collecting labelled datasets for this task, however, is notoriously costly and labor-intensive. To overcome this issue, we propose a simple yet effective zero-shot referring image segmentation method by leveraging the pre-trained cross-modal knowledge from CLIP. In order to obtain segmentation masks grounded to the input text, we propose a mask-guided visual encoder that captures global and local contextual information of an input image. By utilizing instance masks obtained from off-the-shelf mask proposal techniques, our method is able to segment fine-detailed instance-level groundings. We also introduce a global-local text encoder where the global feature captures complex sentence-level semantics of the entire input expression while the local feature focuses on the target noun phrase extracted by a dependency parser. In our experiments, the proposed method outperforms several zero-shot baselines of the task and even the weakly supervised referring expression segmentation method with substantial margins. Our code is available at https://github.com/Seonghoon-Yu/Zero-shot-RIS.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2186.Weakly Supervised Monocular 3D Object Detection Using Multi-View Projection and Direction Consistency</span><br>
                <span class="as">Tao, RunzhouandHan, WenchengandQiu, ZhongyingandXu, Cheng-ZhongandShen, Jianbing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Weakly_Supervised_Monocular_3D_Object_Detection_Using_Multi-View_Projection_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17482-17492.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决单目3D物体检测中训练和推断阶段数据不一致的问题，即研究问题：本文旨在解决单目3D物体检测中训练和推断阶段数据不一致的问题，即训练阶段需要依赖3D点云数据标注真值，而推断阶段不需要。<br>
                    动机：目前大多数的单目3D物体检测方法在训练阶段依赖3D点云数据标注真值，这增加了数据收集成本，且与推断阶段的实际应用需求不符。<br>
                    方法：提出一种新的弱监督单目3D物体检测方法，仅使用2D图像上的标签进行模型训练。通过探索投影、多视角和方向一致性三种类型的一致性，设计了一种基于这些一致性的弱监督架构。同时，提出了一种新的2D方向标签方法来指导模型进行准确的旋转方向预测。<br>
                    效果：实验表明，该方法的性能与一些全监督方法相当。当用作预训练方法时，仅使用1/3的3D标签，就可以显著超越相应的全监督基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular 3D object detection has become a mainstream approach in automatic driving for its easy application. A prominent advantage is that it does not need LiDAR point clouds during the inference. However, most current methods still rely on 3D point cloud data for labeling the ground truths used in the training phase. This inconsistency between the training and inference makes it hard to utilize the large-scale feedback data and increases the data collection expenses. To bridge this gap, we propose a new weakly supervised monocular 3D objection detection method, which can train the model with only 2D labels marked on images. To be specific, we explore three types of consistency in this task, i.e. the projection, multi-view and direction consistency, and design a weakly-supervised architecture based on these consistencies. Moreover, we propose a new 2D direction labeling method in this task to guide the model for accurate rotation direction prediction. Experiments show that our weakly-supervised method achieves comparable performance with some fully supervised methods. When used as a pre-training method, our model can significantly outperform the corresponding fully-supervised baseline with only 1/3 3D labels.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2187.Towards Open-World Segmentation of Parts</span><br>
                <span class="as">Pan, Tai-YuandLiu, QingandChao, Wei-LunandPrice, Brian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Towards_Open-World_Segmentation_of_Parts_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15392-15401.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行对象部分分割，特别是在未见过的对象上？<br>
                    动机：现有的最大数据集只包含200个物体类别，难以扩展到无约束的设置。<br>
                    方法：提出一种看似简单但实用且可扩展的任务——类别无关的部分分割。在训练中忽略部分类别标签，将所有部分视为单个部分类别。同时，使模型具有对象感知能力，并利用模型提取的像素级特征对未见过的物体进行部分分割。<br>
                    效果：通过在PartImageNet和Pascal-Part上的大量实验，证明了该方法的有效性，为开放世界的部分分割迈出了关键一步。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Segmenting object parts such as cup handles and animal bodies is important in many real-world applications but requires more annotation effort. The largest dataset nowadays contains merely two hundred object categories, implying the difficulty to scale up part segmentation to an unconstrained setting. To address this, we propose to explore a seemingly simplified but empirically useful and scalable task, class-agnostic part segmentation. In this problem, we disregard the part class labels in training and instead treat all of them as a single part class. We argue and demonstrate that models trained without part classes can better localize parts and segment them on objects unseen in training. We then present two further improvements. First, we propose to make the model object-aware, leveraging the fact that parts are "compositions" whose extents are bounded by objects, whose appearances are by nature not independent but bundled. Second, we introduce a novel approach to improve part segmentation on unseen objects, inspired by an interesting finding --- for unseen objects, the pixel-wise features extracted by the model often reveal high-quality part segments. To this end, we propose a novel self-supervised procedure that iterates between pixel clustering and supervised contrastive learning that pulls pixels closer or pushes them away. Via extensive experiments on PartImageNet and Pascal-Part, we show notable and consistent gains by our approach, essentially a critical step towards open-world part segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2188.DualRel: Semi-Supervised Mitochondria Segmentation From a Prototype Perspective</span><br>
                <span class="as">Mai, HuayuandSun, RuiandZhang, TianzhuandXiong, ZhiweiandWu, Feng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mai_DualRel_Semi-Supervised_Mitochondria_Segmentation_From_a_Prototype_Perspective_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19617-19626.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地进行半监督线粒体分割，降低手动标注成本。<br>
                    动机：现有的线粒体图像分割方法严重依赖经验丰富的领域专家的手动收集，且简单地将自然图像领域的半监督分割方法应用于线粒体图像分割并不理想。<br>
                    方法：我们分析了线粒体图像和自然图像之间的差距，并从可靠的原型级别监督的角度重新思考了如何实现有效的半监督线粒体分割。我们提出了一种新的端到端双可靠（DualRel）网络，包括一个可靠的像素聚合模块和一个可靠的原型选择模块。<br>
                    效果：在三个具有挑战性的基准测试中，我们的方法表现优于最先进的半监督分割方法。重要的是，即使只使用极少的训练样本，DualRel也能与当前最先进的全监督方法相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Automatic mitochondria segmentation enjoys great popularity with the development of deep learning. However, existing methods rely heavily on the labor-intensive manual gathering by experienced domain experts. And naively applying semi-supervised segmentation methods in the natural image field to mitigate the labeling cost is undesirable. In this work, we analyze the gap between mitochondrial images and natural images and rethink how to achieve effective semi-supervised mitochondria segmentation, from the perspective of reliable prototype-level supervision. We propose a novel end-to-end dual-reliable (DualRel) network, including a reliable pixel aggregation module and a reliable prototype selection module. The proposed DualRel enjoys several merits. First, to learn the prototypes well without any explicit supervision, we carefully design the referential correlation to rectify the direct pairwise correlation. Second, the reliable prototype selection module is responsible for further evaluating the reliability of prototypes in constructing prototype-level consistency regularization. Extensive experimental results on three challenging benchmarks demonstrate that our method performs favorably against state-of-the-art semi-supervised segmentation methods. Importantly, with extremely few samples used for training, DualRel is also on par with current state-of-the-art fully supervised methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2189.Progressive Semantic-Visual Mutual Adaption for Generalized Zero-Shot Learning</span><br>
                <span class="as">Liu, ManandLi, FengandZhang, ChunjieandWei, YunchaoandBai, HuihuiandZhao, Yao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Progressive_Semantic-Visual_Mutual_Adaption_for_Generalized_Zero-Shot_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15337-15346.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过知识转移从已知领域识别未见过的种类，并解决视觉外观对应相同属性时产生的语义模糊问题。<br>
                    动机：现有的工作主要定位共享属性对应的区域，当多种视觉外观对应同一属性时，共享属性会引入语义模糊，阻碍准确语义-视觉交互的探索。<br>
                    方法：采用双重语义-视觉转换器模块（DSVTM）逐步对属性原型和视觉特征之间的对应关系进行建模，构建一个渐进的语义-视觉相互适应（PSVMA）网络进行语义消歧和提高知识可转移性。<br>
                    效果：实验结果表明，PSVMA在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generalized Zero-Shot Learning (GZSL) identifies unseen categories by knowledge transferred from the seen domain, relying on the intrinsic interactions between visual and semantic information. Prior works mainly localize regions corresponding to the sharing attributes. When various visual appearances correspond to the same attribute, the sharing attributes inevitably introduce semantic ambiguity, hampering the exploration of accurate semantic-visual interactions. In this paper, we deploy the dual semantic-visual transformer module (DSVTM) to progressively model the correspondences between attribute prototypes and visual features, constituting a progressive semantic-visual mutual adaption (PSVMA) network for semantic disambiguation and knowledge transferability improvement. Specifically, DSVTM devises an instance-motivated semantic encoder that learns instance-centric prototypes to adapt to different images, enabling the recast of the unmatched semantic-visual pair into the matched one. Then, a semantic-motivated instance decoder strengthens accurate cross-domain interactions between the matched pair for semantic-related instance adaption, encouraging the generation of unambiguous visual representations. Moreover, to mitigate the bias towards seen classes in GZSL, a debiasing loss is proposed to pursue response consistency between seen and unseen predictions. The PSVMA consistently yields superior performances against other state-of-the-art methods. Code will be available at: https://github.com/ManLiuCoder/PSVMA.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2190.Unknown Sniffer for Object Detection: Don&#x27;t Turn a Blind Eye to Unknown Objects</span><br>
                <span class="as">Liang, WentengandXue, FengandLiu, YihaoandZhong, GuofengandMing, Anlong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Unknown_Sniffer_for_Object_Detection_Dont_Turn_a_Blind_Eye_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3230-3239.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高开放世界对象和开放集检测在发现从未见过的对象以及区分它们与已知对象方面的能力。<br>
                    动机：现有的开放世界对象和开放集检测方法对从已知类别到未知类别的知识转移研究不够深入，导致隐藏在背景中的未知对象检测能力不足。<br>
                    方法：提出未知嗅探器（UnSniffer）来发现已知和未知的对象。首先引入通用对象置信度（GOC）得分，仅使用已知样本进行监督，避免抑制背景中的未知对象。然后提出负能量抑制损失进一步抑制背景中的非对象样本。最后，引入基于图的确定方案代替人工设计的非最大抑制（NMS）后处理，以解决训练中缺乏未知对象的语义信息的问题。<br>
                    效果：实验表明，该方法远优于现有的最先进技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The recently proposed open-world object and open-set detection have achieved a breakthrough in finding never-seen-before objects and distinguishing them from known ones. However, their studies on knowledge transfer from known classes to unknown ones are not deep enough, resulting in the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further suppress the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this issue, we introduce a graph-based determination scheme to replace hand-designed non-maximum suppression (NMS) post-processing. Finally, we present the Unknown Object Detection Benchmark, the first publicly benchmark that encompasses precision evaluation for unknown detection to our knowledge. Experiments show that our method is far better than the existing state-of-the-art methods. Code is available at: https://github.com/Went-Liang/UnSniffer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2191.Where Is My Wallet? Modeling Object Proposal Sets for Egocentric Visual Query Localization</span><br>
                <span class="as">Xu, MengmengandLi, YanghaoandFu, Cheng-YangandGhanem, BernardandXiang, TaoandP\&#x27;erez-R\&#x27;ua, Juan-Manuel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Where_Is_My_Wallet_Modeling_Object_Proposal_Sets_for_Egocentric_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2593-2603.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决从视觉范例中定位图像和视频数据集中的对象的问题，特别是关注具有挑战性的自我中心视觉查询定位问题。<br>
                    动机：当前基于查询的条件模型设计和视觉查询数据集存在严重的隐含偏见。<br>
                    方法：通过扩展有限的注释并在训练过程中动态丢弃对象提议来直接解决这些问题。此外，提出了一种新的基于变压器的模块，该模块在引入查询信息的同时考虑了对象提议集的上下文。<br>
                    效果：实验表明，提出的调整提高了自我中心查询检测的性能，从而在2D和3D配置中都改进了视觉查询定位系统。因此，我们在AP上将帧级检测性能从26.28%提高到31.26%，相应地显著提高了VQ2D和VQ3D的定位分数。改进的上下文感知查询对象检测器在VQ2D和VQ3D任务中分别排名第一和第二。此外，我们还展示了所提出模型在少样本检测（FSD）任务中的相关性，并在那里也取得了最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper deals with the problem of localizing objects in image and video datasets from visual exemplars. In particular, we focus on the challenging problem of egocentric visual query localization. We first identify grave implicit biases in current query-conditioned model design and visual query datasets. Then, we directly tackle such biases at both frame and object set levels. Concretely, our method solves these issues by expanding limited annotations and dynamically dropping object proposals during training. Additionally, we propose a novel transformer-based module that allows for object-proposal set context to be considered while incorporating query information. We name our module Conditioned Contextual Transformer or CocoFormer. Our experiments show that the proposed adaptations improve egocentric query detection, leading to a better visual query localization system in both 2D and 3D configurations. Thus, we are able to improve frame-level detection performance from 26.28% to 31.26% in AP, which correspondingly improves the VQ2D and VQ3D localization scores by significant margins. Our improved context-aware query object detector ranked first and second in the VQ2D and VQ3D tasks in the 2nd Ego4D challenge. In addition, we showcase the relevance of our proposed model in the Few-Shot Detection (FSD) task, where we also achieve SOTA results.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2192.Boosting Low-Data Instance Segmentation by Unsupervised Pre-Training With Saliency Prompt</span><br>
                <span class="as">Li, HaoandZhang, DingwenandLiu, NianandCheng, LechaoandDai, YalunandZhang, ChaoandWang, XinggangandHan, Junwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Boosting_Low-Data_Instance_Segmentation_by_Unsupervised_Pre-Training_With_Saliency_Prompt_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15485-15494.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在小数据集环境下提升基于查询的端到端实例分割（QEIS）模型的性能。<br>
                    动机：在小数据集环境中，现有的QEIS方法由于难以学习定位和形状先验，性能会下降。<br>
                    方法：提出一种新的预训练方法，通过为查询/核提供显著性提示来增强QEIS模型。该方法包括三个部分：1) 显著性掩码提案，根据显著性机制从无标签图像生成伪掩码；2) 提示-核匹配，将伪掩码转换为提示，并将相应的定位和形状先验注入最佳匹配的核；3) 核监督，在核级别提供监督以进行稳健学习。<br>
                    效果：实验结果表明，该方法在小数据集环境中显著提升了几种QEIS模型的性能，使其达到与CNN基模型相当的收敛速度和性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, inspired by DETR variants, query-based end-to-end instance segmentation (QEIS) methods have outperformed CNN-based models on large-scale datasets. Yet they would lose efficacy when only a small amount of training data is available since it's hard for the crucial queries/kernels to learn localization and shape priors. To this end, this work offers a novel unsupervised pre-training solution for low-data regimes. Inspired by the recent success of the Prompting technique, we introduce a new pre-training method that boosts QEIS models by giving Saliency Prompt for queries/kernels. Our method contains three parts: 1) Saliency Masks Proposal is responsible for generating pseudo masks from unlabeled images based on the saliency mechanism. 2) Prompt-Kernel Matching transfers pseudo masks into prompts and injects the corresponding localization and shape priors to the best-matched kernels. 3) Kernel Supervision is applied to supply supervision at the kernel level for robust learning. From a practical perspective, our pre-training method helps QEIS models achieve a similar convergence speed and comparable performance with CNN-based models in low-data regimes. Experimental results show that our method significantly boosts several QEIS models on three datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2193.Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis</span><br>
                <span class="as">Zhang, YunfeiandHuo, XiaoyangandChen, TianyiandWu, SiandWong, HauSan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7392-7401.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的半监督条件图像合成方法通常通过推断和注入类标签到条件生成对抗网络（GAN）中进行，但这种形式的监督可能不足以对具有多样化视觉外观的类别进行建模。<br>
                    动机：为了解决上述问题，本文提出了一种基于可学习聚类提示的GAN（LCP-GAN），以更广泛的监督源捕获类特征和类内变化因素。<br>
                    方法：首先，对每个类别进行软分区，然后探索将类内聚类与预训练的语言-视觉模型（如CLIP）的特征空间中的可学习视觉概念关联的可能性。对于条件图像生成，设计了一种基于聚类条件的生成器，通过注入类内聚类标签嵌入的组合，并在CLIP的基础上进一步引入真实-假分类头，以区分真实实例和合成实例。<br>
                    效果：实验结果表明，LCP-GAN不仅具有优越的生成能力，而且在多个标准基准上与基础模型BigGAN和StyleGAN2-ADA的全监督版本相匹配。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semi-supervised class-conditional image synthesis is typically performed by inferring and injecting class labels into a conditional Generative Adversarial Network (GAN). The supervision in the form of class identity may be inadequate to model classes with diverse visual appearances. In this paper, we propose a Learnable Cluster Prompt-based GAN (LCP-GAN) to capture class-wise characteristics and intra-class variation factors with a broader source of supervision. To exploit partially labeled data, we perform soft partitioning on each class, and explore the possibility of associating intra-class clusters with learnable visual concepts in the feature space of a pre-trained language-vision model, e.g., CLIP. For class-conditional image generation, we design a cluster-conditional generator by injecting a combination of intra-class cluster label embeddings, and further incorporate a real-fake classification head on top of CLIP to distinguish real instances from the synthesized ones, conditioned on the learnable cluster prompts. This significantly strengthens the generator with more semantic language supervision. LCP-GAN not only possesses superior generation capability but also matches the performance of the fully supervised version of the base models: BigGAN and StyleGAN2-ADA, on multiple standard benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2194.Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding</span><br>
                <span class="as">Meng, LingchenandDai, XiyangandChen, YinpengandZhang, PengchuanandChen, DongdongandLiu, MengchenandWang, JianfengandWu, ZuxuanandYuan, LuandJiang, Yu-Gang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_Detection_Hub_Unifying_Object_Detection_Datasets_via_Query_Adaptation_on_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11402-11411.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决对象检测中多个数据集组合时存在的分类差异和领域差距问题。<br>
                    动机：由于检测数据集的分类差异和领域差距，多数据集组合在对象检测中并未产生显著效果提升。<br>
                    方法：本文提出了一种名为“检测中心”的新设计，该设计具有数据集意识和类别对齐的特点。它通过学习一个数据集嵌入来适应对象查询以及检测头的卷积核，以减轻数据集的不一致性。同时，通过将一维热编码类别表示替换为词嵌入并利用语言嵌入的语义连贯性，将跨数据集的类别在统一空间中进行语义对齐。<br>
                    效果：实验证明，多数据集联合训练比单独在每个数据集上训练取得了显著的性能提升。"检测中心"在具有广泛多样数据集的UODB基准测试中实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Combining multiple datasets enables performance boost on many computer vision tasks. But similar trend has not been witnessed in object detection when combining multiple datasets due to two inconsistencies among detection datasets: taxonomy difference and domain gap. In this paper, we address these challenges by a new design (named Detection Hub) that is dataset-aware and category-aligned. It not only mitigates the dataset inconsistency but also provides coherent guidance for the detector to learn across multiple datasets. In particular, the dataset-aware design is achieved by learning a dataset embedding that is used to adapt object queries as well as convolutional kernels in detection heads. The categories across datasets are semantically aligned into a unified space by replacing one-hot category representations with word embedding and leveraging the semantic coherence of language embedding. Detection Hub fulfills the benefits of large data on object detection. Experiments demonstrate that joint training on multiple datasets achieves significant performance gains over training on each dataset alone. Detection Hub further achieves SoTA performance on UODB benchmark with wide variety of datasets.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2195.Referring Multi-Object Tracking</span><br>
                <span class="as">Wu, DongmingandHan, WenchengandWang, TiancaiandDong, XingpingandZhang, XiangyuandShen, Jianbing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14633-14642.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的通用指代理解任务，即多对象追踪的指代理解（RMOT）。<br>
                    动机：现有的指代理解任务通常只涉及单个文本指代对象的检测，而新的RMOT任务则通过语言表达作为语义线索来预测多个对象的追踪。<br>
                    方法：构建了一个基于KITTI的可扩展表达式基准Refer-KITTI，并开发了一种在线处理新任务的基于变压器的架构TransRMOT。<br>
                    效果：实验结果表明，TransRMOT在Refer-KITTI数据集上取得了令人印象深刻的检测性能，优于其他同类模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing referring understanding tasks tend to involve the detection of a single text-referred object. In this paper, we propose a new and general referring understanding task, termed referring multi-object tracking (RMOT). Its core idea is to employ a language expression as a semantic cue to guide the prediction of multi-object tracking. To the best of our knowledge, it is the first work to achieve an arbitrary number of referent object predictions in videos. To push forward RMOT, we construct one benchmark with scalable expressions based on KITTI, named Refer-KITTI. Specifically, it provides 18 videos with 818 expressions, and each expression in a video is annotated with an average of 10.7 objects. Further, we develop a transformer-based architecture TransRMOT to tackle the new task in an online manner, which achieves impressive detection performance and outperforms other counterparts. The Refer-KITTI dataset and the code are released at https://referringmot.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2196.Weakly Supervised Temporal Sentence Grounding With Uncertainty-Guided Self-Training</span><br>
                <span class="as">Huang, YifeiandYang, LijinandSato, Yoichi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/18908-18918.png><br>
            
            <span class="tt"><span class="t0">研究问题：弱监督的时间句子基础任务旨在找到语言描述在视频中的对应时间点，只给出视频级别的视频-语言对应关系。<br>
                    动机：由于视频的复杂时间结构，与负面样本不同的提议可能对应于几个视频片段，但并不一定是正确答案。<br>
                    方法：提出一种不确定性引导的自我训练技术，通过教师-学生互学和弱强增强提供额外的自我监督信号来指导弱监督学习。设计了两个技巧：（1）构建贝叶斯教师网络，利用其不确定性作为权重抑制噪声教师监督信号；（2）利用时间数据增强带来的循环一致性在两个网络之间进行互学。<br>
                    效果：实验证明该方法在Charades-STA和ActivityNet Captions数据集上表现优越，且可以应用于提高多种主干方法的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of weakly supervised temporal sentence grounding aims at finding the corresponding temporal moments of a language description in the video, given video-language correspondence only at video-level. Most existing works select mismatched video-language pairs as negative samples and train the model to generate better positive proposals that are distinct from the negative ones. However, due to the complex temporal structure of videos, proposals distinct from the negative ones may correspond to several video segments but not necessarily the correct ground truth. To alleviate this problem, we propose an uncertainty-guided self-training technique to provide extra self-supervision signals to guide the weakly-supervised learning. The self-training process is based on teacher-student mutual learning with weak-strong augmentation, which enables the teacher network to generate relatively more reliable outputs compared to the student network, so that the student network can learn from the teacher's output. Since directly applying existing self-training methods in this task easily causes error accumulation, we specifically design two techniques in our self-training method: (1) we construct a Bayesian teacher network, leveraging its uncertainty as a weight to suppress the noisy teacher supervisory signals; (2) we leverage the cycle consistency brought by temporal data augmentation to perform mutual learning between the two networks. Experiments demonstrate our method's superiority on Charades-STA and ActivityNet Captions datasets. We also show in the experiment that our self-training method can be applied to improve the performance of multiple backbone methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2197.AutoRecon: Automated 3D Object Discovery and Reconstruction</span><br>
                <span class="as">Wang, YuangandHe, XingyiandPeng, SidaandLin, HaotongandBao, HujunandZhou, Xiaowei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AutoRecon_Automated_3D_Object_Discovery_and_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21382-21391.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从多视图图像中自动发现和重建物体？<br>
                    动机：虽然3D重建领域有了重大发展，但通过手动劳动（如边界框标注、遮罩注释和网格操作）去除背景以获取干净的物体模型仍然需要。<br>
                    方法：提出一个名为AutoRecon的新框架，利用自我监督的2D视觉转换器特征从SfM点云中鲁棒地定位和分割前景物体，然后使用分解点云提供的密集监督重建分解的神经场景表示，从而实现准确的物体重建和分割。<br>
                    效果：在DTU、BlendedMVS和CO3D-V2数据集上的实验证明了AutoRecon的有效性和鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A fully automated object reconstruction pipeline is crucial for digital content creation. While the area of 3D reconstruction has witnessed profound developments, the removal of background to obtain a clean object model still relies on different forms of manual labor, such as bounding box labeling, mask annotations, and mesh manipulations. In this paper, we propose a novel framework named AutoRecon for the automated discovery and reconstruction of an object from multi-view images. We demonstrate that foreground objects can be robustly located and segmented from SfM point clouds by leveraging self-supervised 2D vision transformer features. Then, we reconstruct decomposed neural scene representations with dense supervision provided by the decomposed point clouds, resulting in accurate object reconstruction and segmentation. Experiments on the DTU, BlendedMVS and CO3D-V2 datasets demonstrate the effectiveness and robustness of AutoRecon. The code and supplementary material are available on the project page: https://zju3dv.github.io/autorecon/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2198.Two-Shot Video Object Segmentation</span><br>
                <span class="as">Yan, KunandLi, XiaoandWei, FangyunandWang, JingluandZhang, ChenbinandWang, PingandLu, Yan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Two-Shot_Video_Object_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2257-2267.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视频对象分割（VOS）中需要密集标注的问题，即获取像素级标注既昂贵又耗时。<br>
                    动机：现有的视频对象分割模型都需要在密集标注的视频上进行训练，但获取像素级标注的成本高且耗时长。<br>
                    方法：本文提出了一种新的训练方式，称为两拍视频对象分割（two-shot VOS）。我们只需要每段训练视频中的两个标记帧，通过在训练过程中为未标记的帧生成伪标签，并将标记和伪标签的数据一起优化模型。<br>
                    效果：实验结果表明，该方法可以在稀疏标注的视频上训练出满意的VOS模型，性能与全量标注的模型相当。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Previous works on video object segmentation (VOS) are trained on densely annotated videos. Nevertheless, acquiring annotations in pixel level is expensive and time-consuming. In this work, we demonstrate the feasibility of training a satisfactory VOS model on sparsely annotated videos--we merely require two labeled frames per training video while the performance is sustained. We term this novel training paradigm as two-shot video object segmentation, or two-shot VOS for short. The underlying idea is to generate pseudo labels for unlabeled frames during training and to optimize the model on the combination of labeled and pseudo-labeled data. Our approach is extremely simple and can be applied to a majority of existing frameworks. We first pre-train a VOS model on sparsely annotated videos in a semi-supervised manner, with the first frame always being a labeled one. Then, we adopt the pre-trained VOS model to generate pseudo labels for all unlabeled frames, which are subsequently stored in a pseudo-label bank. Finally, we retrain a VOS model on both labeled and pseudo-labeled data without any restrictions on the first frame. For the first time, we present a general way to train VOS models on two-shot VOS datasets. By using 7.3% and 2.9% labeled data of YouTube-VOS and DAVIS benchmarks, our approach achieves comparable results in contrast to the counterparts trained on fully labeled set. Code and models are available at https://github.com/yk-pku/Two-shot-Video-Object-Segmentation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2199.Enhanced Multimodal Representation Learning With Cross-Modal KD</span><br>
                <span class="as">Chen, MengxiandXing, LinyuandWang, YuandZhang, Ya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Multimodal_Representation_Learning_With_Cross-Modal_KD_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11766-11775.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索如何利用训练时可用的辅助模态通过跨模态知识蒸馏（KD）增强多模态表示学习。<br>
                    动机：现有的基于互信息最大化的目标会导致弱教师的捷径解决方案，即通过使教师模型与学生模型一样弱来达到最大互信息。为了防止这种弱解决方案，我们引入了额外的目标项，即教师和辅助模态模型之间的互信息。此外，为了缩小学生和教师之间的信息差距，我们还进一步提出了最小化教师给定学生的条件熵。<br>
                    方法：设计了基于对比学习和对抗性学习的新颖训练方案，以优化互信息和条件熵。<br>
                    效果：在三个流行的多模态基准数据集上进行的实验结果表明，所提出的方法在视频识别、视频检索和情感分类等方面优于一系列最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper explores the tasks of leveraging auxiliary modalities which are only available at training to enhance multimodal representation learning through cross-modal Knowledge Distillation (KD). The widely adopted mutual information maximization-based objective leads to a short-cut solution of the weak teacher, i.e., achieving the maximum mutual information by simply making the teacher model as weak as the student model. To prevent such a weak solution, we introduce an additional objective term, i.e., the mutual information between the teacher and the auxiliary modality model. Besides, to narrow down the information gap between the student and teacher, we further propose to minimize the conditional entropy of the teacher given the student. Novel training schemes based on contrastive learning and adversarial learning are designed to optimize the mutual information and the conditional entropy, respectively. Experimental results on three popular multimodal benchmark datasets have shown that the proposed method outperforms a range of state-of-the-art approaches for video recognition, video retrieval and emotion classification.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2200.Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation</span><br>
                <span class="as">Basak, HritamandYin, Zhaozheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19786-19797.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在医学图像分割任务中，利用有限的标注信息学习出具有判别性的特征表示。<br>
                    动机：尽管半监督学习在自然图像分割上取得了显著的成功，但在医学图像分割上，如何从有限的标注信息中学习出判别性的特征表示仍是一个开放的问题。<br>
                    方法：提出了一种新的基于补丁的对比学习框架用于医学图像分割，该框架结合了半监督学习和对比学习的优点，通过半监督学习生成的伪标签为对比学习提供额外的指导，同时通过对比学习学到的判别性类别信息实现准确的多类别分割。<br>
                    效果：在三个公开的多模态数据集上的实验分析表明，该方法优于现有的最先进方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Although recent works in semi-supervised learning (SemiSL) have accomplished significant success in natural image segmentation, the task of learning discriminative representations from limited annotations has been an open problem in medical images. Contrastive Learning (CL) frameworks use the notion of similarity measure which is useful for classification problems, however, they fail to transfer these quality representations for accurate pixel-level segmentation. To this end, we propose a novel semi-supervised patch-based CL framework for medical image segmentation without using any explicit pretext task. We harness the power of both CL and SemiSL, where the pseudo-labels generated from SemiSL aid CL by providing additional guidance, whereas discriminative class information learned in CL leads to accurate multi-class segmentation. Additionally, we formulate a novel loss that synergistically encourages inter-class separability and intra-class compactness among the learned representations. A new inter-patch semantic disparity mapping using average patch entropy is employed for a guided sampling of positives and negatives in the proposed CL framework. Experimental analysis on three publicly available datasets of multiple modalities reveals the superiority of our proposed method as compared to the state-of-the-art methods. Code is available at: https://github.com/hritam-98/PatchCL-MedSeg.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2201.CrOC: Cross-View Online Clustering for Dense Visual Representation Learning</span><br>
                <span class="as">Stegm\&quot;uller, ThomasandLebailly, TimandBozorgtabar, BehzadandTuytelaars, TinneandThiran, Jean-Philippe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Stegmuller_CrOC_Cross-View_Online_Clustering_for_Dense_Visual_Representation_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7000-7009.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从场景中心的数据中学习密集的无标签视觉表示。<br>
                    动机：这是一个具有挑战性的问题，特别是在没有手工制作先验知识的情况下。<br>
                    方法：提出一种跨视图一致性目标和在线聚类机制（CrOC），以发现和分割视图的语义。这种方法不需要繁琐的预处理步骤，并且可以更好地推广。<br>
                    效果：在各种数据集上进行的线性和无监督分割转移任务以及视频对象分割任务中，该方法表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning dense visual representations without labels is an arduous task and more so from scene-centric data. We propose to tackle this challenging problem by proposing a Cross-view consistency objective with an Online Clustering mechanism (CrOC) to discover and segment the semantics of the views. In the absence of hand-crafted priors, the resulting method is more generalizable and does not require a cumbersome pre-processing step. More importantly, the clustering algorithm conjointly operates on the features of both views, thereby elegantly bypassing the issue of content not represented in both views and the ambiguous matching of objects from one crop to the other. We demonstrate excellent performance on linear and unsupervised segmentation transfer tasks on various datasets and similarly for video object segmentation. Our code and pre-trained models are publicly available at https://github.com/stegmuel/CrOC.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2202.Consistent-Teacher: Towards Reducing Inconsistent Pseudo-Targets in Semi-Supervised Object Detection</span><br>
                <span class="as">Wang, XinjiangandYang, XingyiandZhang, ShilongandLi, YijiangandFeng, LitongandFang, ShijieandLyu, ChengqiandChen, KaiandZhang, Wayne</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3240-3249.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究关注半监督目标检测中伪目标的不一致性问题。<br>
                    动机：我们发现，不稳定的伪目标会破坏准确检测器的训练，给训练注入噪声，导致严重的过拟合问题。<br>
                    方法：我们提出了一个名为NAME的系统解决方案来减少这种不一致性。首先，使用自适应锚点分配（ASA）替代静态IoU策略，使学生网络能够抵抗噪声伪边界框的影响。然后，通过设计3D特征对齐模块（FAM-3D），校准子任务预测，使每个分类特征能够在任意尺度和位置自适应地查询最优的特征向量进行回归任务。最后，使用高斯混合模型（GMM）动态修改伪边界框的得分阈值，以在早期阶段稳定真实目标的数量并解决训练过程中不可靠的监督信号问题。<br>
                    效果：NAME在各种半监督目标检测评估中表现出色。在只有10%标注的MS-COCO数据上，使用ResNet-50主干网络，NAME实现了40.0 mAP，比之前使用伪标签的基线高出约3 mAP。当在完全标注的MS-COCO上添加未标注的数据进行训练时，性能进一步提高到47.7 mAP。我们的代码可以在https://github.com/Adamdad/ConsistentTeacher获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this study, we dive deep into the inconsistency of pseudo targets in semi-supervised object detection (SSOD). Our core observation is that the oscillating pseudo-targets undermine the training of an accurate detector. It injects noise into the student's training, leading to severe overfitting problems. Therefore, we propose a systematic solution, termed NAME, to reduce the inconsistency. First, adaptive anchor assignment (ASA) substitutes the static IoU-based strategy, which enables the student network to be resistant to noisy pseudo-bounding boxes. Then we calibrate the subtask predictions by designing a 3D feature alignment module (FAM-3D). It allows each classification feature to adaptively query the optimal feature vector for the regression task at arbitrary scales and locations. Lastly, a Gaussian Mixture Model (GMM) dynamically revises the score threshold of pseudo-bboxes, which stabilizes the number of ground truths at an early stage and remedies the unreliable supervision signal during training. NAME provides strong results on a large range of SSOD evaluations. It achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotated MS-COCO data, which surpasses previous baselines using pseudo labels by around 3 mAP. When trained on fully annotated MS-COCO with additional unlabeled data, the performance further increases to 47.7 mAP. Our code is available at https://github.com/Adamdad/ConsistentTeacher.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2203.RefTeacher: A Strong Baseline for Semi-Supervised Referring Expression Comprehension</span><br>
                <span class="as">Sun, JiamuandLuo, GenandZhou, YiyiandSun, XiaoshuaiandJiang, GuannanandWang, ZhiyuandJi, Rongrong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_RefTeacher_A_Strong_Baseline_for_Semi-Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19144-19154.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决参照表达式理解（REC）任务中，需要大量实例级标注的问题，这既费时又费力。<br>
                    动机：受计算机视觉领域最新进展的启发，作者提出采用教师-学生学习范式进行半监督学习，以利用大量的未标注数据。<br>
                    方法：提出了一种名为RefTeacher的强基线方法。该方法通过教师网络预测伪标签来优化学生网络，从而充分利用少量的已标注数据。同时，为了解决稀疏的监督信号和较差的伪标签噪声问题，设计了两种新的方法：基于注意力的模仿学习和自适应伪标签加权。<br>
                    效果：在三个参照表达式理解基准数据集上进行了广泛的实验，结果表明，RefTeacher明显优于全监督方法。更重要的是，仅使用10%的标注数据，该方法就能实现接近100%的全监督性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Referring expression comprehension (REC) often requires a large number of instance-level annotations for fully supervised learning, which are laborious and expensive. In this paper, we present the first attempt of semi-supervised learning for REC and propose a strong baseline method called RefTeacher. Inspired by the recent progress in computer vision, RefTeacher adopts a teacher-student learning paradigm, where the teacher REC network predicts pseudo-labels for optimizing the student one. This paradigm allows REC models to exploit massive unlabeled data based on a small fraction of labeled. In particular, we also identify two key challenges in semi-supervised REC, namely, sparse supervision signals and worse pseudo-label noise. To address these issues, we equip RefTeacher with two novel designs called Attention-based Imitation Learning (AIL) and Adaptive Pseudo-label Weighting (APW). AIL can help the student network imitate the recognition behaviors of the teacher, thereby obtaining sufficient supervision signals. APW can help the model adaptively adjust the contributions of pseudo-labels with varying qualities, thus avoiding confirmation bias. To validate RefTeacher, we conduct extensive experiments on three REC benchmark datasets. Experimental results show that RefTeacher obtains obvious gains over the fully supervised methods. More importantly, using only 10% labeled data, our approach allows the model to achieve near 100% fully supervised performance, e.g., only -2.78% on RefCOCO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2204.Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations</span><br>
                <span class="as">Gong, RuiandWang, QinandDanelljan, MartinandDai, DengxinandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_Continuous_Pseudo-Label_Rectified_Domain_Adaptive_Semantic_Segmentation_With_Implicit_Neural_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7225-7235.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用有标签的源领域来提高未标记的目标领域的语义分割模型性能。<br>
                    动机：现有的无监督领域适应方法通过在未标记的目标领域图像上使用伪标签取得了显著的进步，但由领域差异产生的低质量伪标签不可避免地阻碍了适应过程。<br>
                    方法：提出一种利用隐式神经表示来估计预测伪标签的修正值的方法。将修正值视为在连续空间域上定义的信号，以图像坐标和附近的深层特征作为输入，预测给定坐标处的修正值作为输出。<br>
                    效果：该方法在包括合成到真实和白天到黑夜在内的不同无监督领域适应基准测试中表现出色，与最先进的方法相比，取得了优越的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Unsupervised domain adaptation (UDA) for semantic segmentation aims at improving the model performance on the unlabeled target domain by leveraging a labeled source domain. Existing approaches have achieved impressive progress by utilizing pseudo-labels on the unlabeled target-domain images. Yet the low-quality pseudo-labels, arising from the domain discrepancy, inevitably hinder the adaptation. This calls for effective and accurate approaches to estimating the reliability of the pseudo-labels, in order to rectify them. In this paper, we propose to estimate the rectification values of the predicted pseudo-labels with implicit neural representations. We view the rectification value as a signal defined over the continuous spatial domain. Taking an image coordinate and the nearby deep features as inputs, the rectification value at a given coordinate is predicted as an output. This allows us to achieve high-resolution and detailed rectification values estimation, important for accurate pseudo-label generation at mask boundaries in particular. The rectified pseudo-labels are then leveraged in our rectification-aware mixture model (RMM) to be learned end-to-end and help the adaptation. We demonstrate the effectiveness of our approach on different UDA benchmarks, including synthetic-to-real and day-to-night. Our approach achieves superior results compared to state-of-the-art. The implementation is available at https://github.com/ETHRuiGong/IR2F.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2205.UniDAformer: Unified Domain Adaptive Panoptic Segmentation Transformer via Hierarchical Mask Calibration</span><br>
                <span class="as">Zhang, JingyiandHuang, JiaxingandZhang, XiaoqinandLu, Shijian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_UniDAformer_Unified_Domain_Adaptive_Panoptic_Segmentation_Transformer_via_Hierarchical_Mask_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/11227-11237.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有的领域自适应全景分割方法在实例分割和语义分割上需要两个独立网络，导致参数过多、训练和推理过程复杂且计算密集的问题。<br>
                    动机：通过利用一个或多个相关源领域的现成标注数据，减轻数据标注挑战。<br>
                    方法：设计了一个名为UniDAformer的统一领域自适应全景分割变换器，该变换器可以在单个网络中同时实现领域自适应实例分割和语义分割。UniDAformer引入了分层掩码校准（HMC），通过在线自我训练实时纠正区域、超像素和像素级别的不准确预测。<br>
                    效果：实验表明，UniDAformer在多个公共基准测试中实现了优于现有技术的领域自适应全景分割。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Domain adaptive panoptic segmentation aims to mitigate data annotation challenge by leveraging off-the-shelf annotated data in one or multiple related source domains. However, existing studies employ two separate networks for instance segmentation and semantic segmentation which lead to excessive network parameters as well as complicated and computationally intensive training and inference processes. We design UniDAformer, a unified domain adaptive panoptic segmentation transformer that is simple but can achieve domain adaptive instance segmentation and semantic segmentation simultaneously within a single network. UniDAformer introduces Hierarchical Mask Calibration (HMC) that rectifies inaccurate predictions at the level of regions, superpixels and pixels via online self-training on the fly. It has three unique features: 1) it enables unified domain adaptive panoptic adaptation; 2) it mitigates false predictions and improves domain adaptive panoptic segmentation effectively; 3) it is end-to-end trainable with a much simpler training and inference pipeline. Extensive experiments over multiple public benchmarks show that UniDAformer achieves superior domain adaptive panoptic segmentation as compared with the state-of-the-art.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2206.JacobiNeRF: NeRF Shaping With Mutual Information Gradients</span><br>
                <span class="as">Xu, XiaomengandYang, YanchaoandMo, KaichunandPan, BoxiaoandYi, LiandGuibas, Leonidas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_JacobiNeRF_NeRF_Shaping_With_Mutual_Information_Gradients_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16498-16507.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何训练神经网络辐射场（NeRF）模型，使其不仅能编码场景的外观，还能编码场景点、区域或实体之间的语义关联性。<br>
                    动机：传统的一阶光度重建目标无法捕捉高度相关的实体之间的相互协方差模式。因此，我们提出了一种新的方法，通过在随机场景扰动下最大化实体间的互信息来优化学习动态。<br>
                    方法：我们的方法显式地规范了学习动态，以对齐高度相关的实体的雅可比矩阵。通过关注这种二阶信息，我们可以塑造一个NeRF，当网络权重沿着单个实体、区域甚至点的梯度改变时，表达出具有语义意义的协同效应。<br>
                    效果：实验表明，与没有互信息塑造的NeRF相比，JacobiNeRF在2D像素和3D点之间传播注释的效率更高，特别是在极度稀疏的标签环境中，从而减轻了注释负担。此外，相同的机制还可以用于实体选择或场景修改。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a method that trains a neural radiance field (NeRF) to encode not only the appearance of the scene but also semantic correlations between scene points, regions, or entities -- aiming to capture their mutual co-variation patterns. In contrast to the traditional first-order photometric reconstruction objective, our method explicitly regularizes the learning dynamics to align the Jacobians of highly-correlated entities, which proves to maximize the mutual information between them under random scene perturbations. By paying attention to this second-order information, we can shape a NeRF to express semantically meaningful synergies when the network weights are changed by a delta along the gradient of a single entity, region, or even a point. To demonstrate the merit of this mutual information modeling, we leverage the coordinated behavior of scene entities that emerges from our shaping to perform label propagation for semantic and instance segmentation. Our experiments show that a JacobiNeRF is more efficient in propagating annotations among 2D pixels and 3D points compared to NeRFs without mutual information shaping, especially in extremely sparse label regimes -- thus reducing annotation burden. The same machinery can further be used for entity selection or scene modifications. Our code is available at https://github.com/xxm19/jacobinerf.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2207.Interactive Segmentation of Radiance Fields</span><br>
                <span class="as">Goel, RahulandSirikonda, DhawalandSaini, SaurabhandNarayanan, P.J.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goel_Interactive_Segmentation_of_Radiance_Fields_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4201-4211.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对Radiance Fields（RF）进行对象分割，以实现混合现实个人空间中的场景理解和操作。<br>
                    动机：现有的分割方法无法处理具有复杂外观的复杂对象，需要一种能够精确分割具有精细结构和外观的对象的方法。<br>
                    方法：ISRF方法通过使用蒸馏的语义特征进行最近邻特征匹配来识别高置信度种子区域，然后在联合空间-语义空间中进行双边搜索以恢复准确的分割。<br>
                    效果：ISRF方法在从RF中分割对象并将其合成到另一个场景、改变外观等方面取得了最先进的结果，同时提供了一个其他人可以使用的交互式分割工具。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Radiance Fields (RF) are popular to represent casually-captured scenes for new view synthesis and several applications beyond it. Mixed reality on personal spaces needs understanding and manipulating scenes represented as RFs, with semantic segmentation of objects as an important step. Prior segmentation efforts show promise but don't scale to complex objects with diverse appearance. We present the ISRF method to interactively segment objects with fine structure and appearance. Nearest neighbor feature matching using distilled semantic features identifies high-confidence seed regions. Bilateral search in a joint spatio-semantic space grows the region to recover accurate segmentation. We show state-of-the-art results of segmenting objects from RFs and compositing them to another scene, changing appearance, etc., and an interactive segmentation tool that others can use.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>