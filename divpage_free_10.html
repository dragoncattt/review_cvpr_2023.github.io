<!DOCTYPE html>
<html>
<head>
  <title>扫会助手</title>
  <style>
    .page {
      display: none;
    }
    .active {
      display: block;
    }
    .as {
	font-size: 12px;
	color: #900;
    }
   .ts {
	font-weight: bold;
	font-size: 14px;
    }
   .tt {
	color: #009;
	font-size: 13px;
   }
   .apaper {
  width: 1200px;
	margin-top: 10px;
	padding: 15px;
	background-color: white;
  margin:auto;
  top:0;
  left:0;
  right: 0;
  bottom: 0;
}

.apaper img {
	width: 1200px;
}

.paperdesc {
	float: left;
}

.dllinks {
	float: right;
	text-align: right;
}
.t0 { color: #000;}

#titdiv {
	width: 100%;
	height: 90px;
	background-color: #840000;
	color: white;

	padding-top: 20px;
	padding-left: 20px;

	border-bottom: 1px solid #540000;
}

.collapsible {
  color:black;
  cursor: pointer;
 
  text-align: left;
  outline: none;
  font-size: 15px;
}

.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
}

#titdiv {
	width: 100%;
	height: 95px;
	background-color: #4b2e84;
	color: #f3f3f6;
	padding-top: 15px;
	border-bottom: 1px solid #540000;
	text-align: center;
	line-height: 18px;
}

.alignleft {
    display: inline;
    float: left;
    margin: auto;
}

body {
	margin: 0;
	padding: 0;
	font-family: 'arial';
	background-color: #e2e1e8;
}


.t1 { color: #C00;}
.t2 { color: #0C0;}
.t3 { color: #00C;}
.t4 { color: #AA0;}
.t5 { color: #C0C;}
.t6 { color: #0CA;}
.t7 { color: #EBC;}
.t8 { color: #0AC;}
.t9 { color: #CAE}
.t10 { color: #C0A;}

.topicchoice {
	border: 2px solid black;
	border-radius: 5px;
	padding: 5px;
	margin-left: 5px;
	margin-right: 5px;
	cursor: pointer;
	text-decoration: underline;
}

#sortoptions {
	text-align: center;
	padding: 10px;
	line-height: 35px;
}


.pagination_p a:hover:not(.active) { 
            background-color: #031F3B; 
            color: white; 
        }

  </style>
</head>
<body>

  <div id ="titdiv">
    <h1>CVPR 2023 papers</h1>
    
    </div><br>
  
  <div id="sortoptions">
  Please select the LDA topic:
  <div class="pagination_p"> 
    <a href="index.html" >topic-1</a> 
    <a href="divpage_free_2.html" >topic-2</a> 
    <a href="divpage_free_3.html" >topic-3</a> 
    <a href="divpage_free_4.html" >topic-4</a> 
    <a href="divpage_free_5.html" >topic-5</a>
    <a href="divpage_free_6.html" >topic-6</a> 
    <a href="divpage_free_7.html" >topic-7</a> 
    <a href="divpage_free_8.html" >topic-8</a> 
    <a href="divpage_free_9.html" >topic-9</a> 
    <a href="divpage_free_10.html" >topic-10</a> 
</div>
  </div>

  <div id="page1" class="page active">
    <div id="sortoptions"><h2>topic10</h2>
      <b>Topic words : &ensp;</b>object, &ensp;dataset, &ensp;human, &ensp;detection, &ensp;objects, &ensp;lidar, &ensp;large, &ensp;scene</div>
    <p>
       
        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2208.Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving</span><br>
                <span class="as">Agro, BenandSykora, QuinlanandCasas, SergioandUrtasun, Raquel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1379-1388.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自动驾驶车辆感知和预测方法要么进行对象检测后跟踪预测，要么预测整个场景的密集占据和流量网格，这两种方法都存在一些问题。<br>
                    动机：前者由于需要保持低检测数量以提高效率，牺牲了对象召回率，存在安全风险；后者由于全卷积网络的内在限制，具有有限的接受域，且输出网格的高维度导致计算成本高。<br>
                    方法：提出了一种统一的感知和未来预测方法，用单一的神经网络隐式表示随时间的占据和流量。该方法避免了不必要的计算，因为可以直接由运动规划器在连续的空间-时间位置进行查询。此外，通过添加一个高效而有效的全局注意力机制，设计了一种能够克服先前显式占据预测方法有限接受域的架构。<br>
                    效果：在城市和高速公路环境中的大量实验表明，这种隐式模型优于当前最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the motion planner at continuous spatio-temporal locations. Moreover, we design an architecture that overcomes the limited receptive field of previous explicit occupancy prediction methods by adding an efficient yet effective global attention mechanism. Through extensive experiments in both urban and highway settings, we demonstrate that our implicit model outperforms the current state-of-the-art. For more information, visit the project website: https://waabi.ai/research/implicito.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2209.3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification</span><br>
                <span class="as">Zhang, JiazhaoandDai, LiuandMeng, FanpengandFan, QingnanandChen, XuelinandXu, KaiandWang, He</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_3D-Aware_Object_Goal_Navigation_via_Simultaneous_Exploration_and_Identification_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6672-6682.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高未见过环境中的目标导航（ObjectNav）能力。<br>
                    动机：现有的目标导航方法主要基于2D地图、场景图或图像序列，但在3D空间中进行导航时，利用精细的空间信息可以提高导航能力。然而，在底层任务中，利用3D场景表示进行策略学习可能由于样本效率低和计算成本高而不实用。<br>
                    方法：提出了一个基于两个简单子策略的3D感知目标导航框架。这两个子策略，即角点引导探索策略和类别感知识别策略，同时利用在线融合的3D点作为观察结果进行操作。<br>
                    效果：通过大量实验表明，该框架可以通过学习3D场景表示显著提高目标导航性能。该框架在所有基于模块化的方法中在Matterport3D和Gibson数据集上表现最好，同时训练所需的计算成本最多可减少30倍。代码将发布以惠及社区。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object goal navigation (ObjectNav) in unseen environments is a fundamental task for Embodied AI. Agents in existing works learn ObjectNav policies based on 2D maps, scene graphs, or image sequences. Considering this task happens in 3D space, a 3D-aware agent can advance its ObjectNav capability via learning from fine-grained spatial information. However, leveraging 3D scene representation can be prohibitively unpractical for policy learning in this floor-level task, due to low sample efficiency and expensive computational cost. In this work, we propose a framework for the challenging 3D-aware ObjectNav based on two straightforward sub-policies. The two sub-polices, namely corner-guided exploration policy and category-aware identification policy, simultaneously perform by utilizing online fused 3D points as observation. Through extensive experiments, we show that this framework can dramatically improve the performance in ObjectNav through learning from 3D scene representation. Our framework achieves the best performance among all modular-based methods on the Matterport3D and Gibson datasets while requiring (up to30x) less computational cost for training. The code will be released to benefit the community.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2210.Analyzing Physical Impacts Using Transient Surface Wave Imaging</span><br>
                <span class="as">Zhang, TianyuanandSheinin, MarkandChan, DorianandRau, MarkandO{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Analyzing_Physical_Impacts_Using_Transient_Surface_Wave_Imaging_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4339-4348.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过物体表面的振动信息，恢复物体的物理属性和其与环境的交互情况。<br>
                    动机：现有的方法或忽视了物体被干扰后立即传播的瞬态振动，或只关注局部信号的恢复，而忽略了不同物体点振动之间的时空关系。<br>
                    方法：利用双快门相机，同时从稀疏的物体点上提取瞬态表面振动的信息，模拟物体表面被干扰后产生的弹性波，并使用该模型定位各种材料的干扰源。<br>
                    效果：实验证明，瞬态物体振动包含了关于冲击力和冲击物体材料性质的额外线索。在实际应用中，如乒乓球比赛中球拍的冲击位置定位，以及通过成像地板振动恢复脚步的位置等，本方法均表现出良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The subtle vibrations on an object's surface contain information about the object's physical properties and its interaction with the environment. Prior works imaged surface vibration to recover the object's material properties via modal analysis, which discards the transient vibrations propagating immediately after the object is disturbed. Conversely, prior works that captured transient vibrations focused on recovering localized signals (e.g., recording nearby sound sources), neglecting the spatiotemporal relationship between vibrations at different object points. In this paper, we extract information from the transient surface vibrations simultaneously measured at a sparse set of object points using the dual-shutter camera described by Sheinin[31]. We model the geometry of an elastic wave generated shortly after an object's surface is disturbed (e.g., a knock or a footstep), and use the model to localize the disturbance source for various materials (e.g., wood, plastic, tile). We also show that transient object vibrations contain additional cues about the impact force and the impacting object's material properties. We demonstrate our approach in applications like localizing the strikes of a ping-pong ball on a table mid-play and recovering the footsteps' locations by imaging the floor vibrations they create.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2211.UniSim: A Neural Closed-Loop Sensor Simulator</span><br>
                <span class="as">Yang, ZeandChen, YunandWang, JingkangandManivasagam, SivabalanandMa, Wei-ChiuandYang, AnqiJoyceandUrtasun, Raquel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1389-1399.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对自动驾驶系统进行严格的测试，以实现安全驾驶的目标。<br>
                    动机：为了确保自动驾驶车辆的安全，我们需要在封闭环境中对其进行测试，模拟罕见但重要的驾驶场景。<br>
                    方法：我们提出了UniSim，一个神经传感器模拟器，它利用录制的驾驶日志生成逼真的多传感器封闭环境模拟。UniSim构建了神经网络特征网格来重建静态背景和动态角色，并将它们组合在一起，模拟新视角下的激光雷达和相机数据。<br>
                    效果：实验表明，UniSim可以生成逼真的传感器数据，并在下游任务上实现了小范围的领域差距。通过UniSim，我们首次展示了如何在现实世界中对自动驾驶系统进行封闭环境的评估。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Rigorously testing autonomy systems is essential for making safe self-driving vehicles (SDV) a reality. It requires one to generate safety critical scenarios beyond what can be collected safely in the world, as many scenarios happen rarely on our roads. To accurately evaluate performance, we need to test the SDV on these scenarios in closed-loop, where the SDV and other actors interact with each other at each timestep. Previously recorded driving logs provide a rich resource to build these new scenarios from, but for closed loop evaluation, we need to modify the sensor data based on the new scene configuration and the SDV's decisions, as actors might be added or removed and the trajectories of existing actors and the SDV will differ from the original log. In this paper, we present UniSim, a neural sensor simulator that takes a single recorded log captured by a sensor-equipped vehicle and converts it into a realistic closed-loop multi-sensor simulation. UniSim builds neural feature grids to reconstruct both the static background and dynamic actors in the scene, and composites them together to simulate LiDAR and camera data at new viewpoints, with actors added or removed and at new placements. To better handle extrapolated views, we incorporate learnable priors for dynamic objects, and leverage a convolutional network to complete unseen regions. Our experiments show UniSim can simulate realistic sensor data with small domain gap on downstream tasks. With UniSim, we demonstrate, for the first time, closed-loop evaluation of an autonomy system on safety-critical scenarios as if it were in the real world.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2212.DexArt: Benchmarking Generalizable Dexterous Manipulation With Articulated Objects</span><br>
                <span class="as">Bao, ChenandXu, HelinandQin, YuzheandWang, Xiaolong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_DexArt_Benchmarking_Generalizable_Dexterous_Manipulation_With_Articulated_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21190-21200.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使机器人像人一样操作日常的铰接物体？<br>
                    动机：目前的机器人操作主要依赖于使用平行夹具，这限制了机器人只能操作一组有限的物体。而使用多指机器人手将更好地模拟人类行为，并使机器人能够操作各种铰接物体。<br>
                    方法：我们提出了一个新的基准测试，称为DexArt，它涉及在物理模拟器中进行灵巧的铰接物体操作。在这个基准测试中，我们定义了多个复杂的操作任务，机器人手需要在每个任务中操作各种不同的铰接物体。我们的主要关注点是评估学习到的策略在未见过铰接物体上的泛化能力。<br>
                    效果：通过大量的研究，我们提供了关于3D表示学习如何影响具有3D点云输入的RL决策的新见解。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finger robot hand will allow better approximation to human behavior and enable the robot to operate on diverse articulated objects. To this end, we propose a new benchmark called DexArt, which involves Dexterous manipulation with Articulated objects in a physical simulator. In our benchmark, we define multiple complex manipulation tasks, and the robot hand will need to manipulate diverse articulated objects within each task. Our main focus is to evaluate the generalizability of the learned policy on unseen articulated objects. This is very challenging given the high degrees of freedom of both hands and objects. We use Reinforcement Learning with 3D representation learning to achieve generalization. Through extensive studies, we provide new insights into how 3D representation learning affects decision making in RL with 3D point cloud inputs. More details can be found at https://www.chenbao.tech/dexart/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2213.Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone?</span><br>
                <span class="as">Petrov, IlyaA.andMarin, RiccardoandChibane, JulianandPons-Moll, Gerard</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4726-4736.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索是否仅从人类互动中就可以推断出3D物体及其姿势。<br>
                    动机：尽管计算机视觉社区已经开发出了一些以物体为中心的方法，但是从人类互动中推断出3D物体及其姿势的研究却鲜有涉及。<br>
                    方法：通过对人类点云数据的分析，即使用户只是在模仿某种功能（如通过双筒望远镜看东西），也能从中推断出未被观察到的物体。<br>
                    效果：通过合成数据和任务序列的验证，该方法在XR/VR领域具有应用潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The intimate entanglement between objects affordances and human poses is of large interest, among others, for behavioural sciences, cognitive psychology, and Computer Vision communities. In recent years, the latter has developed several object-centric approaches: starting from items, learning pipelines synthesizing human poses and dynamics in a realistic way, satisfying both geometrical and functional expectations. However, the inverse perspective is significantly less explored: Can we infer 3D objects and their poses from human interactions alone? Our investigation follows this direction, showing that a generic 3D human point cloud is enough to pop up an unobserved object, even when the user is just imitating a functionality (e.g., looking through a binocular) without involving a tangible counterpart. We validate our method qualitatively and quantitatively, with synthetic data and sequences acquired for the task, showing applicability for XR/VR.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2214.Leapfrog Diffusion Model for Stochastic Trajectory Prediction</span><br>
                <span class="as">Mao, WeiboandXu, ChenxinandZhu, QiandChen, SihengandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5517-5526.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地预测人类行为的不确定性，实现随机轨迹预测？<br>
                    动机：现有的扩散模型虽然在生成任务中表现出强大的表示能力，但由于需要大量的去噪步骤，无法满足实时预测的需求。<br>
                    方法：提出一种新颖的基于扩散的轨迹预测模型LEapfrog Diffusion model（LED）。该模型通过训练一个可学习的跳跃初始化器直接学习未来轨迹的多模态分布，跳过大量去噪步骤，显著加速推理速度。同时，跳跃初始化器被训练以适当分配相关样本，提供多样化的未来轨迹预测，从而显著提高预测性能。<br>
                    效果：在四个真实世界数据集上的实验表明，LED在性能上持续提升，并在NFL数据集上实现了23.7%/21.9%的位置误差/方向误差改善。相比标准的扩散模型，LED在NBA/NFL/SDD/ETH-UCY数据集上的推理速度分别提高了19.3/30.8/24.3/25.1倍，满足了实时推理的需求。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To model the indeterminacy of human behaviors, stochastic trajectory prediction requires a sophisticated multi-modal distribution of future trajectories. Emerging diffusion models have revealed their tremendous representation capacities in numerous generation tasks, showing potential for stochastic trajectory prediction. However, expensive time consumption prevents diffusion models from real-time prediction, since a large number of denoising steps are required to assure sufficient representation ability. To resolve the dilemma, we present LEapfrog Diffusion model (LED), a novel diffusion-based trajectory prediction model, which provides real-time, precise, and diverse predictions. The core of the proposed LED is to leverage a trainable leapfrog initializer to directly learn an expressive multi-modal distribution of future trajectories, which skips a large number of denoising steps, significantly accelerating inference speed. Moreover, the leapfrog initializer is trained to appropriately allocate correlated samples to provide a diversity of predicted future trajectories, significantly improving prediction performances. Extensive experiments on four real-world datasets, including NBA/NFL/SDD/ETH-UCY, show that LED consistently improves performance and achieves 23.7%/21.9% ADE/FDE improvement on NFL. The proposed LED also speeds up the inference 19.3/30.8/24.3/25.1 times compared to the standard diffusion model on NBA/NFL/SDD/ETH-UCY, satisfying real-time inference needs. Code is available at https://github.com/MediaBrain-SJTU/LED.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2215.Resource-Efficient RGBD Aerial Tracking</span><br>
                <span class="as">Yang, JinyuandGao, ShangandLi, ZheandZheng, FengandLeonardis, Ale\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Resource-Efficient_RGBD_Aerial_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13374-13383.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决无人机在复杂环境中的视觉感知问题，特别是在RGBD追踪方面的挑战。<br>
                    动机：现有的研究主要关注于城市环境中的行人或车辆等有限类别的目标追踪，而对更复杂的场景和深度信息的应用还处于探索阶段。<br>
                    方法：本文提出了一个大规模的RGBD空中追踪基准，包含1000个带有密集注释的无人机捕获的RGBD视频。同时，为了应对无人机应用中有限的计算资源和实时处理的需求，作者还提出了一种高效的RGBD追踪器EMT。<br>
                    效果：实验结果表明，EMT追踪器在GPU上运行速度超过100帧/秒，在Nvidia Jetson NX Xavier的边缘平台上运行速度为25帧/秒，实现了有效的多模态融合和特征匹配，取得了良好的追踪性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Aerial robots are now able to fly in complex environments, and drone-captured data gains lots of attention in object tracking. However, current research on aerial perception has mainly focused on limited categories, such as pedestrian or vehicle, and most scenes are captured in urban environments from a birds-eye view. Recently, UAVs equipped with depth cameras have been also deployed for more complex applications, while RGBD aerial tracking is still unexplored. Compared with traditional RGB object tracking, adding depth information can more effectively deal with more challenging scenes such as target and background interference. To this end, in this paper, we explore RGBD aerial tracking in an overhead space, which can greatly enlarge the development of drone-based visual perception. To boost the research, we first propose a large-scale benchmark for RGBD aerial tracking, containing 1,000 drone-captured RGBD videos with dense annotations. Then, as drone-based applications require for real-time processing with limited computational resources, we also propose an efficient RGBD tracker named EMT. Our tracker runs at over 100 fps on GPU, and 25 fps on the edge platform of NVidia Jetson NX Xavier, benefiting from its efficient multimodal fusion and feature matching. Extensive experiments show that our EMT achieves promising tracking performance. All resources are available at https://github.com/yjybuaa/RGBDAerialTracking.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2216.PACO: Parts and Attributes of Common Objects</span><br>
                <span class="as">Ramanathan, VigneshandKalia, AnmolandPetrovic, VladanandWen, YiandZheng, BaixueandGuo, BaishanandWang, RuiandMarquez, AaronandKovvuri, RamaandKadian, AbhishekandMousavi, AmirandSong, YiwenandDubey, AbhimanyuandMahajan, Dhruv</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7141-7151.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决对象模型从预测类别标签到提供详细对象实例描述的问题。<br>
                    动机：随着对象模型的发展，需要更丰富的数据集，如部分遮罩和属性等，以提供更详细的对象实例描述。<br>
                    方法：介绍了PACO：常见物体的部分和属性，这是一个包含75个物体类别、456个物体部分类别和55个属性的图像（LVIS）和视频（Ego4D）数据集。我们提供了641K的部分遮罩，覆盖了260K的对象框，其中大约一半被详细地标注了属性。<br>
                    效果：我们在数据集上设计了评估指标，并在三个任务上提供了基准结果：部分遮罩分割、对象和部分属性预测以及零样本实例检测。数据集、模型和代码在https://github.com/facebookresearch/paco上开源。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object models are gradually progressing from predicting just category labels to providing detailed descriptions of object instances. This motivates the need for large datasets which go beyond traditional object masks and provide richer annotations such as part masks and attributes. Hence, we introduce PACO: Parts and Attributes of Common Objects. It spans 75 object categories, 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets. We provide 641K part masks annotated across 260K object boxes, with roughly half of them exhaustively annotated with attributes as well. We design evaluation metrics and provide benchmark results for three tasks on the dataset: part mask segmentation, object and part attribute prediction and zero-shot instance detection. Dataset, models, and code are open-sourced at https://github.com/facebookresearch/paco.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2217.MoDAR: Using Motion Forecasting for 3D Object Detection in Point Cloud Sequences</span><br>
                <span class="as">Li, YingweiandQi, CharlesR.andZhou, YinandLiu, ChenxiandAnguelov, Dragomir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MoDAR_Using_Motion_Forecasting_for_3D_Object_Detection_in_Point_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9329-9339.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高3D物体检测中被遮挡和远距离物体的检测效果。<br>
                    动机：点云序列数据提供了改善这类情况的独特机会，因为被遮挡或遥远的物体可以从不同的视角或随着时间的推移获得更好的可见性。<br>
                    方法：提出MoDAR方法，使用运动预测输出作为虚拟模态来增强激光雷达点云。MoDAR模态将物体信息从时间上下文传播到目标帧，表示为一组虚拟点，每个对象都有一个来自预测轨迹上的一个路标的虚拟点。然后将原始传感器点和虚拟点的融合点云输入给任何现成的基于点云的3D物体检测器。<br>
                    效果：在Waymo开放数据集上进行评估，我们的方法通过使用来自额外长序列（如18秒）的运动预测，显著提高了现有技术探测器的性能，达到了新的最先进的水平，同时没有增加太多的计算开销。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Occluded and long-range objects are ubiquitous and challenging for 3D object detection. Point cloud sequence data provide unique opportunities to improve such cases, as an occluded or distant object can be observed from different viewpoints or gets better visibility over time. However, the efficiency and effectiveness in encoding long-term sequence data can still be improved. In this work, we propose MoDAR, using motion forecasting outputs as a type of virtual modality, to augment LiDAR point clouds. The MoDAR modality propagates object information from temporal contexts to a target frame, represented as a set of virtual points, one for each object from a waypoint on a forecasted trajectory. A fused point cloud of both raw sensor points and the virtual points can then be fed to any off-the-shelf point-cloud based 3D object detector. Evaluated on the Waymo Open Dataset, our method significantly improves prior art detectors by using motion forecasting from extra-long sequences (e.g. 18 seconds), achieving new state of the arts, while not adding much computation overhead.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2218.Connecting Vision and Language With Video Localized Narratives</span><br>
                <span class="as">Voigtlaender, PaulandChangpinyo, SoravitandPont-Tuset, JordiandSoricut, RaduandFerrari, Vittorio</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Voigtlaender_Connecting_Vision_and_Language_With_Video_Localized_Narratives_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2461-2471.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种新的多模态视频标注形式，将视觉和语言连接起来。<br>
                    动机：原始的局部叙述需要注释者在图像上同时说话并移动鼠标，为每个词创建一个鼠标轨迹段，但在视频上执行此操作具有挑战性。<br>
                    方法：新的协议允许注释者使用局部叙述讲述视频的故事，捕捉到涉及多个演员相互互动以及与多个被动对象的复杂事件。<br>
                    效果：我们在OVIS、UVO和Oops数据集上标注了2万个视频，共170万字。基于这些数据，我们还构建了新的视频叙述基础和视频问答任务基准，并提供来自强大基线模型的参考结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose Video Localized Narratives, a new form of multimodal video annotations connecting vision and language. In the original Localized Narratives, annotators speak and move their mouse simultaneously on an image, thus grounding each word with a mouse trace segment. However, this is challenging on a video. Our new protocol empowers annotators to tell the story of a video with Localized Narratives, capturing even complex events involving multiple actors interacting with each other and with several passive objects. We annotated 20k videos of the OVIS, UVO, and Oops datasets, totalling 1.7M words. Based on this data, we also construct new benchmarks for the video narrative grounding and video question answering tasks, and provide reference results from strong baseline models. Our annotations are available at https://google.github.io/video-localized-narratives/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2219.OmniCity: Omnipotent City Understanding With Multi-Level and Multi-View Images</span><br>
                <span class="as">Li, WeijiaandLai, YawenandXu, LinningandXiangli, YuanboandYu, JinhuaandHe, ConghuiandXia, Gui-SongandLin, Dahua</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OmniCity_Omnipotent_City_Understanding_With_Multi-Level_and_Multi-View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17397-17407.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的多级别、多视角的城市理解数据集OmniCity。<br>
                    动机：为了解决城市理解中的复杂问题，需要一种包含多种视角和级别的大规模数据集。<br>
                    方法：OmniCity包括了卫星图像、街景全景图像和单视图图像，通过在25K个地理位置收集的超过100K像素级标注图像构建而成。同时，作者还提出了一种利用现有卫星视图标签图和不同视图间转换关系的高效街道视图图像标注流程。<br>
                    效果：OmniCity比现有的多级别、多视角基准包含更多的图像和更丰富的标注类型，提供了更多最先进的模型基准结果，并引入了一个新的细粒度建筑实例分割任务。此外，OmniCity为现有任务如跨视图图像匹配、合成、分割、检测等提供了新的设置，有助于大规模城市理解、重建和模拟的新方法的开发。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper presents OmniCity, a new dataset for omnipotent city understanding from multi-level and multi-view images. More precisely, OmniCity contains multi-view satellite images as well as street-level panorama and mono-view images, constituting over 100K pixel-wise annotated images that are well-aligned and collected from 25K geo-locations in New York City. To alleviate the substantial pixel-wise annotation efforts, we propose an efficient street-view image annotation pipeline that leverages the existing label maps of satellite view and the transformation relations between different views (satellite, panorama, and mono-view). With the new OmniCity dataset, we provide benchmarks for a variety of tasks including building footprint extraction, height estimation, and building plane/instance/fine-grained segmentation. Compared with existing multi-level and multi-view benchmarks, OmniCity contains a larger number of images with richer annotation types and more views, provides more benchmark results of state-of-the-art models, and introduces a new task for fine-grained building instance segmentation on street-level panorama images. Moreover, OmniCity provides new problem settings for existing tasks, such as cross-view image matching, synthesis, segmentation, detection, etc., and facilitates the developing of new methods for large-scale city understanding, reconstruction, and simulation. The OmniCity dataset as well as the benchmarks will be released at https://city-super.github.io/omnicity/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2220.NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions</span><br>
                <span class="as">Zhang, JuzeandLuo, HaiminandYang, HongdiandXu, XinruandWu, QianyangandShi, YeandYu, JingyiandXu, LanandWang, Jingya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8834-8845.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过构建一个包含人类与物体自由交互的数据集，解决视觉推理中的遮挡、形状和纹理模糊、运动等问题。<br>
                    动机：由于现实生活中的人与物体交互过程常常存在各种遮挡和模糊问题，因此需要构建一个能够捕捉自由视角交互的数据集来解决这个问题。<br>
                    方法：研究者构建了一个名为HODome的密集多视图穹顶，用于获取复杂的人类对象交互数据集。同时，他们还开发了一套专门处理多视图视频输入的神经网络处理流程——NeuralDome，用于对人类和物体进行准确的跟踪、几何重建和自由视角渲染。<br>
                    效果：在HODome数据集上的大量实验表明，NeuralDome在推理、建模和渲染等多种任务上都表现出了良好的效果。该数据集和NeuralDome工具将被分享给社区以供进一步开发。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans constantly interact with objects in daily life tasks. Capturing such processes and subsequently conducting visual inferences from a fixed viewpoint suffers from occlusions, shape and texture ambiguities, motions, etc. To mitigate the problem, it is essential to build a training dataset that captures free-viewpoint interactions. We construct a dense multi-view dome to acquire a complex human object interaction dataset, named HODome, that consists of  71M frames on 10 subjects interacting with 23 objects. To process the HODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline tailored for multi-view video inputs to conduct accurate tracking, geometry reconstruction and free-view rendering, for both human subjects and objects. Extensive experiments on the HODome dataset demonstrate the effectiveness of NeuralDome on a variety of inference, modeling, and rendering tasks. Both the dataset and the NeuralDome tools will be disseminated to the community for further development, which can be found at https://juzezhang.github.io/NeuralDome</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2221.Target-Referenced Reactive Grasping for Dynamic Objects</span><br>
                <span class="as">Liu, JirongandZhang, RuoandFang, Hao-ShuandGou, MinghaoandFang, HongjieandWang, ChenxiandXu, ShengandYan, HengxuandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8824-8833.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使机器人成功地抓取动态移动的对象。<br>
                    动机：目前的方法主要关注预测抓取姿态的时间平滑性，但很少考虑其语义一致性，导致在杂乱的场景中，预测的抓取位置可能无法落在同一物体的同一部位。<br>
                    方法：本文提出通过跟踪生成的抓取空间来解决目标参考设置下的反射式抓取问题。给定对象上的目标抓取姿态和在新观测中检测到的抓取姿态，该方法由两个阶段组成：1）通过注意力图神经网络发现抓取姿态对应关系，并选择与目标姿态最相似的一个；2）基于目标和历史信息细化选定的抓取姿态。<br>
                    效果：在大规模的基准GraspNet-1Billion上进行评估，收集了30个动态物体场景进行测试。实验结果表明，该方法优于其他代表性方法，实际机器人实验的平均成功率超过80%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reactive grasping, which enables the robot to successfully grasp dynamic moving objects, is of great interest in robotics. Current methods mainly focus on the temporal smoothness of the predicted grasp poses but few consider their semantic consistency. Consequently, the predicted grasps are not guaranteed to fall on the same part of the same object, especially in cluttered scenes. In this paper, we propose to solve reactive grasping in a target-referenced setting by tracking through generated grasp spaces. Given a targeted grasp pose on an object and detected grasp poses in a new observation, our method is composed of two stages: 1) discovering grasp pose correspondences through an attentional graph neural network and selecting the one with the highest similarity with respect to the target pose; 2) refining the selected grasp poses based on target and historical information. We evaluate our method on a large-scale benchmark GraspNet-1Billion. We also collect 30 scenes of dynamic objects for testing. The results suggest that our method outperforms other representative methods. Furthermore, our real robot experiments achieve an average success rate of over 80 percent.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2222.Stimulus Verification Is a Universal and Effective Sampler in Multi-Modal Human Trajectory Prediction</span><br>
                <span class="as">Sun, JianhuaandLi, YuxuanandChai, LiangandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Stimulus_Verification_Is_a_Universal_and_Effective_Sampler_in_Multi-Modal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22014-22023.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地从候选未来轨迹中采样出最终预测结果，以提高多模态人类轨迹预测的准确性。<br>
                    动机：虽然现有的研究已经开发出了各种强大的模型来预测候选轨迹，但如何有效地采样出最终的预测结果却未受到足够的关注。<br>
                    方法：本文提出了刺激验证法，作为一种通用且有效的采样过程来提高多模态预测能力。刺激验证引入了一个概率模型，称为刺激验证器，用于验证预测的未来轨迹与其相应的刺激之间的连贯性。通过突出显示具有更好刺激连贯性的预测样本，刺激验证确保采样的轨迹在刺激的角度来看是合理的，从而有助于提高多模态预测性能。<br>
                    效果：我们在五个代表性的预测框架上实施了刺激验证，并在三个广泛使用的基准上进行了详尽的实验。优越的结果证明了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To comprehensively cover the uncertainty of the future, the common practice of multi-modal human trajectory prediction is to first generate a set/distribution of candidate future trajectories and then sample required numbers of trajectories from them as final predictions. Even though a large number of previous researches develop various strong models to predict candidate trajectories, how to effectively sample the final ones has not received much attention yet. In this paper, we propose stimulus verification, serving as a universal and effective sampling process to improve the multi-modal prediction capability, where stimulus refers to the factor in the observation that may affect the future movements such as social interaction and scene context. Stimulus verification introduces a probabilistic model, denoted as stimulus verifier, to verify the coherence between a predicted future trajectory and its corresponding stimulus. By highlighting prediction samples with better stimulus-coherence, stimulus verification ensures sampled trajectories plausible from the stimulus' point of view and therefore aids in better multi-modal prediction performance. We implement stimulus verification on five representative prediction frameworks and conduct exhaustive experiments on three widely-used benchmarks. Superior results demonstrate the effectiveness of our approach.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2223.MethaneMapper: Spectral Absorption Aware Hyperspectral Transformer for Methane Detection</span><br>
                <span class="as">Kumar, SatishandArevalo, IvanandIftekhar, ASMandManjunath, BS</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumar_MethaneMapper_Spectral_Absorption_Aware_Hyperspectral_Transformer_for_Methane_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17609-17618.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确检测和量化甲烷排放，并解决现有方法对局部地形条件敏感、需要专家手动检查、容易出错且不可扩展的问题。<br>
                    动机：现有的分析数据的方法存在许多问题，因此需要开发一种新方法来解决这些问题。<br>
                    方法：提出了一种新的端到端的光谱吸收波长感知的变压器网络MethaneMapper，用于检测和量化甲烷排放。MethaneMapper引入了两个新的模块，帮助在光谱域中定位最相关的甲烷烟柱区域，并使用它们进行精确定位。<br>
                    效果：实验表明，MethaneMapper在检测方面实现了0.63 mAP，与当前最先进的技术相比，模型大小减少了5倍。此外，还引入了一个大规模的甲烷烟柱分割掩模数据集，包含超过4000个甲烷烟柱地点。这个数据集将为研究人员提供机会，开发和推进新的方法来应对这个具有重大社会影响的温室气体检测难题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Methane (CH 4 ) is the chief contributor to global climate change. Recent Airborne Visible-Infrared Imaging Spectrometer-Next Generation (AVIRIS-NG) has been very useful in quantitative mapping of methane emissions. Existing methods for analyzing this data are sensitive to local terrain conditions, often require manual inspection from domain experts, prone to significant error and hence are not scalable. To address these challenges, we propose a novel end-to-end spectral absorption wavelength aware transformer network, MethaneMapper, to detect and quantify the emissions. MethaneMapper introduces two novel modules that help to locate the most relevant methane plume regions in the spectral domain and uses them to localize these accurately. Thorough evaluation shows that MethaneMapper achieves 0.63 mAP in detection and reduces the model size (by 5x) compared to the current state of the art. In addition, we also introduce a large-scale dataset of methane plume segmentation mask for over 1200 AVIRIS-NG flightlines from 2015-2022. It contains over 4000 methane plume sites. Our dataset will provide researchers the opportunity to develop and advance new methods for tackling this challenging green-house gas detection problem with significant broader social impact. Dataset and source code link.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2224.Autonomous Manipulation Learning for Similar Deformable Objects via Only One Demonstration</span><br>
                <span class="as">Ren, YuandChen, RonghanandCong, Yang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_Autonomous_Manipulation_Learning_for_Similar_Deformable_Objects_via_Only_One_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17069-17078.png><br>
            
            <span class="tt"><span class="t0">研究问题：大多数现有的方法主要关注于刚性物体的识别和操作，而现实生活中更常见的可变形物体却未得到足够关注。<br>
                    动机：大多数现有的可变形物体操作方法存在两个问题：1）需要大量的演示：为了训练一个特定实例，机器人需要重复数千次的操作演示；2）泛化能力差：在将所学技能转移到同一类别的新实例时，往往需要重新训练。<br>
                    方法：我们提出了一种基于类别的可变形3D物体操作框架，只需要一次演示就可以操作可变形3D物体，并将所学技能泛化到新的相似实例，无需重新训练。该框架主要由两个模块组成：Nocs状态转换（NST）模块将目标的观察点云转换为预定义的统一姿态状态（即Nocs状态），这是进行类别级操作学习的基础；神经空间编码（NSE）模块通过编码类别级空间信息，将所学技能泛化到新的实例，以实现预期的抓取点，无需重新训练。然后规划相对运动路径以实现自主操作。<br>
                    效果：通过我们的Cap40数据集进行的模拟结果和实际机器人实验证明了我们框架的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In comparison with most methods focusing on 3D rigid object recognition and manipulation, deformable objects are more common in our real life but attract less attention. Generally, most existing methods for deformable object manipulation suffer two issues, 1) Massive demonstration: repeating thousands of robot-object demonstrations for model training of one specific instance; 2) Poor generalization: inevitably re-training for transferring the learned skill to a similar/new instance from the same category. Therefore, we propose a category-level deformable 3D object manipulation framework, which could manipulate deformable 3D objects with only one demonstration and generalize the learned skills to new similar instances without re-training. Specifically, our proposed framework consists of two modules. The Nocs State Transform (NST) module transfers the observed point clouds of the target to a pre-defined unified pose state (i.e., Nocs state), which is the foundation for the category-level manipulation learning; the Neural Spatial Encoding (NSE) module generalizes the learned skill to novel instances by encoding the category-level spatial information to pursue the expected grasping point without re-training. The relative motion path is then planned to achieve autonomous manipulation. Both the simulated results via our Cap40 dataset and real robotic experiments justify the effectiveness of our framework.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2225.Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion</span><br>
                <span class="as">Rempe, DavisandLuo, ZhengyiandBinPeng, XueandYuan, YeandKitani, KrisandKreis, KarstenandFidler, SanjaandLitany, Or</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rempe_Trace_and_Pace_Controllable_Pedestrian_Animation_via_Guided_Trajectory_Diffusion_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13756-13766.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何生成符合用户定义目标的真实行人轨迹和全身动画？<br>
                    动机：利用最新的引导扩散建模技术，实现轨迹的测试时可控性，通常这仅与基于规则的系统相关。<br>
                    方法：通过目标路标、速度和指定的社交群体来约束轨迹，同时考虑周围环境上下文，将此轨迹扩散模型与新颖的基于物理的人形控制器集成，形成闭环全身行人动画系统，能够在具有不同地形的模拟环境中放置大量人群。<br>
                    效果：利用在动画控制器的RL训练过程中学习到的价值函数来指导扩散，以产生更适合特定场景（如避免碰撞和穿越不平坦地形）的轨迹。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2226.Progressive Transformation Learning for Leveraging Virtual Images in Training</span><br>
                <span class="as">Shen, Yi-TingandLee, HyungtaeandKwon, HeesungandBhattacharyya, ShuvraS.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_Progressive_Transformation_Learning_for_Leveraging_Virtual_Images_in_Training_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/835-844.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地对无人机图像进行对象检测，如人类？<br>
                    动机：获取大规模的包含各种姿态和视角的人类的无人机数据集是必要的。<br>
                    方法：介绍了一种称为渐进式转换学习（PTL）的方法，通过逐渐添加增强现实感的转换虚拟图像来扩充训练数据集。<br>
                    效果：实验表明，PTL在小数据和跨领域环境下的性能显著提高，超过了基线。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To effectively interrogate UAV-based images for detecting objects of interest, such as humans, it is essential to acquire large-scale UAV-based datasets that include human instances with various poses captured from widely varying viewing angles. As a viable alternative to laborious and costly data curation, we introduce Progressive Transformation Learning (PTL), which gradually augments a training dataset by adding transformed virtual images with enhanced realism. Generally, a virtual2real transformation generator in the conditional GAN framework suffers from quality degradation when a large domain gap exists between real and virtual images. To deal with the domain gap, PTL takes a novel approach that progressively iterates the following three steps: 1) select a subset from a pool of virtual images according to the domain gap, 2) transform the selected virtual images to enhance realism, and 3) add the transformed virtual images to the training set while removing them from the pool. In PTL, accurately quantifying the domain gap is critical. To do that, we theoretically demonstrate that the feature representation space of a given object detector can be modeled as a multivariate Gaussian distribution from which the Mahalanobis distance between a virtual object and the Gaussian distribution of each object category in the representation space can be readily computed. Experiments show that PTL results in a substantial performance increase over the baseline, especially in the small data and the cross-domain regime.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2227.Localized Semantic Feature Mixers for Efficient Pedestrian Detection in Autonomous Driving</span><br>
                <span class="as">Khan, AbdulHannanandNawaz, MohammedShariqandDengel, Andreas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khan_Localized_Semantic_Feature_Mixers_for_Efficient_Pedestrian_Detection_in_Autonomous_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5476-5485.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高自动驾驶系统中的行人检测效率和准确性？<br>
                    动机：目前的行人检测器存在推理时间长、对小且被严重遮挡的行人检测效果差的问题。<br>
                    方法：提出一种名为局部化语义特征混合器（LSFM）的新型无锚行人检测架构，使用超像素金字塔池化模块进行特征编码，并采用基于MLPMixer的密集焦点检测网络作为轻量级检测头。<br>
                    效果：在Caltech、City Persons、Euro City Persons和TJU-Traffic-Pedestrian等数据集上，LSFM取得了最先进的性能，同时平均推理时间减少了55%。此外，LSFM首次在行人检测中超越了人类基线。最后，跨数据集评估证明，提出的LSFM能够很好地泛化到未见数据。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Autonomous driving systems rely heavily on the underlying perception module which needs to be both performant and efficient to allow precise decisions in real-time. Avoiding collisions with pedestrians is of topmost priority in any autonomous driving system. Therefore, pedestrian detection is one of the core parts of such systems' perception modules. Current state-of-the-art pedestrian detectors have two major issues. Firstly, they have long inference times which affect the efficiency of the whole perception module, and secondly, their performance in the case of small and heavily occluded pedestrians is poor. We propose Localized Semantic Feature Mixers (LSFM), a novel, anchor-free pedestrian detection architecture. It uses our novel Super Pixel Pyramid Pooling module instead of the, computationally costly, Feature Pyramid Networks for feature encoding. Moreover, our MLPMixer-based Dense Focal Detection Network is used as a light detection head, reducing computational effort and inference time compared to existing approaches. To boost the performance of the proposed architecture, we adapt and use mixup augmentation which improves the performance, especially in small and heavily occluded cases. We benchmark LSFM against the state-of-the-art on well-established traffic scene pedestrian datasets. The proposed LSFM achieves state-of-the-art performance in Caltech, City Persons, Euro City Persons, and TJU-Traffic-Pedestrian datasets while reducing the inference time on average by 55%. Further, LSFM beats the human baseline for the first time in the history of pedestrian detection. Finally, we conducted a cross-dataset evaluation which proved that our proposed LSFM generalizes well to unseen data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2228.Coaching a Teachable Student</span><br>
                <span class="as">Zhang, JimuyangandHuang, ZanmingandOhn-Bar, Eshed</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Coaching_a_Teachable_Student_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7805-7815.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地教导一个传感器运动学生代理从有特权的教师代理的监督下驾驶。<br>
                    动机：当前的传感器运动代理的蒸馏方法往往导致学生学习到的行为次优，我们假设这是由于两个代理的输入、建模能力和优化过程之间的内在差异造成的。<br>
                    方法：我们开发了一种新的蒸馏方案，可以解决这些限制，缩小传感器运动代理和其有特权的教师之间的差距。我们的关键洞察是设计一个学生，让他们学会将自己的输入特征与教师的特权鸟瞰图（BEV）空间对齐。然后学生可以从教师的直接监督中受益，进行内部表示学习。为了支持困难的传感器运动学习任务，学生模型通过各种辅助监督的学生步调辅导机制进行优化。我们还提出了一种高容量的模仿学习的有特权的代理，它在CARLA中超越了之前的有特权的代理，确保学生学习安全驾驶行为。<br>
                    效果：我们提出的传感器运动代理在CARLA中产生了一个鲁棒的基于图像的行为克隆代理，在驾驶分数上提高了当前模型的20.6%，而无需LiDAR、历史观察、模型集合、策略数据聚合或强化学习。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the supervision of a privileged teacher agent. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents. We develop a novel distillation scheme that can address these limitations and close the gap between the sensorimotor agent and its privileged teacher. Our key insight is to design a student which learns to align their input features with the teacher's privileged Bird's Eye View (BEV) space. The student then can benefit from direct supervision by the teacher over the internal representation learning. To scaffold the difficult sensorimotor learning task, the student model is optimized via a student-paced coaching mechanism with various auxiliary supervision. We further propose a high-capacity imitation learned privileged agent that surpasses prior privileged agents in CARLA and ensures the student learns safe driving behavior. Our proposed sensorimotor agent results in a robust image-based behavior cloning agent in CARLA, improving over current models by over 20.6% in driving score without requiring LiDAR, historical observations, ensemble of models, on-policy data aggregation or reinforcement learning.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2229.Collaboration Helps Camera Overtake LiDAR in 3D Detection</span><br>
                <span class="as">Hu, YueandLu, YifanandXu, RunshengandXie, WeidiandChen, SihengandWang, Yanfeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Collaboration_Helps_Camera_Overtake_LiDAR_in_3D_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9243-9252.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在没有直接3D测量输入的情况下，通过改进网络设计提高精确的深度估计。<br>
                    动机：解决基于激光雷达的检测系统在本地化3D空间中的对象时的问题，提供一种经济的解决方案。<br>
                    方法：提出多代理协作的相机仅3D检测（CoCa3D），使代理能够通过通信共享互补信息，并通过选择最有意义的线索优化通信效率。<br>
                    效果：在真实世界数据集和两个新的模拟数据集上进行评估，结果显示CoCa3D在DAIR-V2X、OPV2V+和CoPerception-UAVs+上的AP@70分别提高了44.21%、30.60%和12.59%。初步结果表明，在充分的协作下，相机在某些实际场景中可能超过激光雷达。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Camera-only 3D detection provides an economical solution with a simple configuration for localizing objects in 3D space compared to LiDAR-based detection systems. However, a major challenge lies in precise depth estimation due to the lack of direct 3D measurements in the input. Many previous methods attempt to improve depth estimation through network designs, e.g., deformable layers and larger receptive fields. This work proposes an orthogonal direction, improving the camera-only 3D detection by introducing multi-agent collaborations. Our proposed collaborative camera-only 3D detection (CoCa3D) enables agents to share complementary information with each other through communication. Meanwhile, we optimize communication efficiency by selecting the most informative cues. The shared messages from multiple viewpoints disambiguate the single-agent estimated depth and complement the occluded and long-range regions in the single-agent view. We evaluate CoCa3D in one real-world dataset and two new simulation datasets. Results show that CoCa3D improves previous SOTA performances by 44.21% on DAIR-V2X, 30.60% on OPV2V+, 12.59% on CoPerception-UAVs+ for AP@70. Our preliminary results show a potential that with sufficient collaboration, the camera might overtake LiDAR in some practical scenarios. We released the dataset and code at https://siheng-chen.github.io/dataset/CoPerception+ and https://github.com/MediaBrain-SJTU/CoCa3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2230.RealImpact: A Dataset of Impact Sound Fields for Real Objects</span><br>
                <span class="as">Clarke, SamuelandGao, RuohanandWang, MasonandRau, MarkandXu, JuliaandWang, Jui-HsienandJames, DougL.andWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Clarke_RealImpact_A_Dataset_of_Impact_Sound_Fields_for_Real_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1516-1525.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前缺乏真实物体冲击声音场的标准数据集，用于音频-视觉学习和模拟与现实的校准差距。<br>
                    动机：我们的目标是填补这个空白，提供一个大规模的真实物体冲击声音数据集，以帮助改进音频-视觉学习和校准现实差距的模拟方法。<br>
                    方法：我们创建了一个名为RealImpact的大型数据集，包含150,000个日常物品的冲击声音记录，这些记录是在受控条件下进行的，并带有详细的注释，包括冲击位置、麦克风位置、接触力分布、材料标签和RGBD图像。<br>
                    效果：初步实验显示，我们的数据集可以作为参考来评估当前模拟方法对真实世界物体冲击声音的估计。此外，通过两个基准任务（听者定位分类和视觉声学匹配）的评估，我们证明了该数据集在声学和视听学习方面的实用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Objects make unique sounds under different perturbations, environment conditions, and poses relative to the listener. While prior works have modeled impact sounds and sound propagation in simulation, we lack a standard dataset of impact sound fields of real objects for audio-visual learning and calibration of the sim-to-real gap. We present RealImpact, a large-scale dataset of real object impact sounds recorded under controlled conditions. RealImpact contains 150,000 recordings of impact sounds of 50 everyday objects with detailed annotations, including their impact locations, microphone locations, contact force profiles, material labels, and RGBD images. We make preliminary attempts to use our dataset as a reference to current simulation methods for estimating object impact sounds that match the real world. Moreover, we demonstrate the usefulness of our dataset as a testbed for acoustic and audio-visual learning via the evaluation of two benchmark tasks, including listener location classification and visual acoustic matching.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2231.Affection: Learning Affective Explanations for Real-World Visual Data</span><br>
                <span class="as">Achlioptas, PanosandOvsjanikov, MaksandGuibas, LeonidasandTulyakov, Sergey</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_Affection_Learning_Affective_Explanations_for_Real-World_Visual_Data_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6641-6651.png><br>
            
            <span class="tt"><span class="t0">研究问题：探索真实世界图像引发的情感反应空间。<br>
                    动机：通过大型数据集，分析公众情绪反应和自由形式的文本解释，以理解人们对特定图像的感受和原因。<br>
                    方法：开发神经网络，为用语言解释的真实世界视觉数据提供合理的情感反应。<br>
                    效果：为更人性化、情感感知的图像分析系统铺平道路，并公开了代码和数据集。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we explore the space of emotional reactions induced by real-world images. For this, we first introduce a large-scale dataset that contains both categorical emotional reactions and free-form textual explanations for 85,007 publicly available images, analyzed by 6,283 annotators who were asked to indicate and explain how and why they felt when observing a particular image, with a total of 526,749 responses. Although emotional reactions are subjective and sensitive to context (personal mood, social status, past experiences) -- we show that there is significant common ground to capture emotional responses with a large support in the subject population. In light of this observation, we ask the following questions: i) Can we develop neural networks that provide plausible affective responses to real-world visual data explained with language? ii) Can we steer such methods towards producing explanations with varying degrees of pragmatic language, justifying different emotional reactions by grounding them in the visual stimulus? Finally, iii) How to evaluate the performance of such methods for this novel task? In this work, we take the first steps in addressing all of these questions, paving the way for more human-centric and emotionally-aware image analysis systems. Our code and data are publicly available at https://affective-explanations.org.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2232.PIRLNav: Pretraining With Imitation and RL Finetuning for ObjectNav</span><br>
                <span class="as">Ramrakhya, RamandBatra, DhruvandWijmans, ErikandDas, Abhishek</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramrakhya_PIRLNav_Pretraining_With_Imitation_and_RL_Finetuning_for_ObjectNav_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17896-17906.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使虚拟机器人在新环境中导航到目标对象。<br>
                    动机：虽然模仿学习（IL）在人类示范数据集上使用行为克隆（BC）取得了良好的结果，但存在泛化能力差和收集示范数据成本高的问题。<br>
                    方法：提出了PIRLNav，一种两阶段学习方案，先进行基于人类示范的行为克隆预训练，然后进行强化学习微调。<br>
                    效果：这种BC->RL的策略在ObjectNav任务上达到了65.0%的成功率（比之前最先进的方法高出5.0%的绝对值）。同时，通过严格的实证分析，发现人类示范可以替代自动生成的示范源，如最短路径或任务无关的前沿探索轨迹；随着BC预训练数据集的增大，RL微调的效果会逐渐减弱；最后，分析了ObjectNav策略的失败模式，并提出了进一步改进的指导方针。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study ObjectGoal Navigation -- where a virtual robot situated in a new environment is asked to navigate to an object. Prior work has shown that imitation learning (IL) using behavior cloning (BC) on a dataset of human demonstrations achieves promising results. However, this has limitations -- 1) BC policies generalize poorly to new states, since the training mimics actions not their consequences, and 2) collecting demonstrations is expensive. On the other hand, reinforcement learning (RL) is trivially scalable, but requires careful reward engineering to achieve desirable behavior. We present PIRLNav, a two-stage learning scheme for BC pretraining on human demonstrations followed by RL-finetuning. This leads to a policy that achieves a success rate of 65.0% on ObjectNav (+5.0% absolute over previous state-of-the-art). Using this BC->RL training recipe, we present a rigorous empirical analysis of design choices. First, we investigate whether human demonstrations can be replaced with 'free' (automatically generated) sources of demonstrations, e.g. shortest paths (SP) or task-agnostic frontier exploration (FE) trajectories. We find that BC->RL on human demonstrations outperforms BC->RL on SP and FE trajectories, even when controlled for the same BC-pretraining success on train, and even on a subset of val episodes where BC-pretraining success favors the SP or FE policies. Next, we study how RL-finetuning performance scales with the size of the BC pretraining dataset. We find that as we increase the size of the BC-pretraining dataset and get to high BC accuracies, the improvements from RL-finetuning are smaller, and that 90% of the performance of our best BC->RL policy can be achieved with less than half the number of BC demonstrations. Finally, we analyze failure modes of our ObjectNav policies, and present guidelines for further improving them.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2233.EXCALIBUR: Encouraging and Evaluating Embodied Exploration</span><br>
                <span class="as">Zhu, HaoandKapoor, RaghavandMin, SoYeonandHan, WinsonandLi, JiataiandGeng, KaiwenandNeubig, GrahamandBisk, YonatanandKembhavi, AniruddhaandWeihs, Luca</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_EXCALIBUR_Encouraging_and_Evaluating_Embodied_Exploration_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14931-14942.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种名为EXCALIBUR的探索性交互式代理，以鼓励其长期探索环境并查询对物理世界的理解。<br>
                    动机：目前的机器学习模型主要通过静态和固定的数据集进行被动学习，或被教导完成特定的目标导向任务。EXCALIBUR的出现是为了鼓励发展具有探索性的交互式代理。<br>
                    方法：EXCALIBUR允许代理在其环境中进行长期探索，然后通过提问来查询他们对物理世界的理解，如“那个小而重的红色碗是玻璃做的吗？”或“有没有比鸡蛋还重的银勺？”等。一旦代理回答了一系列问题，他们可以重新进入场景以精炼知识、更新信念并提高问题解答的性能。<br>
                    效果：实验表明，EXCALIBUR数据集对当前最先进的嵌入式系统提出了挑战，并为开发新的创新方法提供了空间。此外，我们还展示了一个虚拟现实界面，使人类能够在模拟世界中无缝交互，并使用它来收集人类性能指标。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: "is the small heavy red bowl made from glass?" or "is there a silver spoon heavier than the egg?". This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to present-day benchmarks and represents the next frontier for embodied AI research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2234.A Bag-of-Prototypes Representation for Dataset-Level Applications</span><br>
                <span class="as">Tu, WeijieandDeng, WeijianandGedeon, TomandZheng, Liang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_A_Bag-of-Prototypes_Representation_for_Dataset-Level_Applications_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2881-2892.png><br>
            
            <span class="tt"><span class="t0">研究问题：本研究旨在解决两个数据集级别的任务：评估训练集的适用性和测试集的难度。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，在知识图谱中的有信息量的实体可以通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work investigates dataset vectorization for two dataset-level tasks: assessing training set suitability and test set difficulty. The former measures how suitable a training set is for a target domain, while the latter studies how challenging a test set is for a learned model. Central of the two tasks is measuring the underlying relationship between datasets. This needs a desirable dataset vectorization scheme, which should preserve as much discriminative dataset information as possible so that the distance between the resulting dataset vectors can reflect dataset-to-dataset similarity. To this end, we propose a bag-of-prototypes (BoP) dataset representation that extends the image level bag consisting of patch descriptors to dataset-level bag consisting of semantic prototypes. Specifically, we develop a codebook consisting of K prototypes clustered from a reference dataset. Given a dataset to be encoded, we quantize each of its image features to a certain prototype in the codebook and obtain a K-dimensional histogram feature. Without assuming access to dataset labels, the BoP representation provides rich characterization of dataset semantic distribution. Further, BoP representations cooperates well with Jensen-Shannon divergence for measuring dataset-to-dataset similarity. Albeit very simple, BoP consistently shows its advantage over existing representations on a series of benchmarks for two dataset-level tasks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2235.Leverage Interactive Affinity for Affordance Learning</span><br>
                <span class="as">Luo, HongchenandZhai, WeiandZhang, JingandCao, YangandTao, Dacheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Leverage_Interactive_Affinity_for_Affordance_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6809-6819.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从图像中感知潜在"行动可能性"（即功能区）并学习物体的互动功能，由于人类与物体互动的多样性，这是一个挑战。<br>
                    动机：现有的功能区学习方法通常采用标签分配范式，并假设功能区域和功能标签之间存在唯一的关系，当适应外观变化大且未见过的环境下时，表现不佳。<br>
                    方法：提出利用互动亲和力进行功能区学习，即从人与物体的互动中提取互动亲和力，并将其转移到非互动物体上。互动亲和力表示人体不同部位与目标物体局部区域的接触，可以提供人与物体之间内在连通性的固有线索，从而减少感知到的行动可能性的模糊性。具体来说，提出了一种基于姿态辅助的互动亲和力学习框架，利用人体姿态引导网络从人与物体的互动中学习互动亲和力。特别是设计了一种关键点启发式感知（KHP）方案，利用人体姿态的关键点关联来减轻由于互动多样性和接触遮挡引起的不确定性。此外，通过收集和标注了5000多张图片构建了一个接触驱动的功能区学习（CAL）数据集。<br>
                    效果：实验结果表明，我们的方法在客观指标和视觉质量方面优于代表性模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Perceiving potential "action possibilities" (i.e., affordance) regions of images and learning interactive functionalities of objects from human demonstration is a challenging task due to the diversity of human-object interactions. Prevailing affordance learning algorithms often adopt the label assignment paradigm and presume that there is a unique relationship between functional region and affordance label, yielding poor performance when adapting to unseen environments with large appearance variations. In this paper, we propose to leverage interactive affinity for affordance learning, i.e., extracting interactive affinity from human-object interaction and transferring it to non-interactive objects. Interactive affinity, which represents the contacts between different parts of the human body and local regions of the target object, can provide inherent cues of interconnectivity between humans and objects, thereby reducing the ambiguity of the perceived action possibilities. Specifically, we propose a pose-aided interactive affinity learning framework that exploits human pose to guide the network to learn the interactive affinity from human-object interactions. Particularly, a keypoint heuristic perception (KHP) scheme is devised to exploit the keypoint association of human pose to alleviate the uncertainties due to interaction diversities and contact occlusions. Besides, a contact-driven affordance learning (CAL) dataset is constructed by collecting and labeling over 5,000 images. Experimental results demonstrate that our method outperforms the representative models regarding objective metrics and visual quality. Code and dataset: github.com/lhc1224/PIAL-Net.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2236.Objaverse: A Universe of Annotated 3D Objects</span><br>
                <span class="as">Deitke, MattandSchwenk, DustinandSalvador, JordiandWeihs, LucaandMichel, OscarandVanderBilt, EliandSchmidt, LudwigandEhsani, KianaandKembhavi, AniruddhaandFarhadi, Ali</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13142-13153.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决当前大规模预训练语言模型对结构化知识利用不足的问题，以及3D数据集中对象类别多样性不足的问题。<br>
                    动机：现有的预训练语言模型和3D数据集在结构和知识表示上存在局限，限制了AI的发展和应用。<br>
                    方法：本文提出了一种增强的语言表示模型ERNIE，通过结合大规模文本语料库和知识图谱进行联合训练，以充分利用词汇、句法和知识信息。同时，构建了一个大规模的3D模型数据集Objaverse，包含80万+的3D模型和丰富的描述性标题、标签和动画。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并在其他常见的NLP任务上与最先进的BERT模型相媲美。Objaverse数据集的引入为AI领域的研究和新型应用打开了新的方向。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omission within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2237.BEVFormer v2: Adapting Modern Image Backbones to Bird&#x27;s-Eye-View Recognition via Perspective Supervision</span><br>
                <span class="as">Yang, ChenyuandChen, YuntaoandTian, HaoandTao, ChenxinandZhu, XizhouandZhang, ZhaoxiangandHuang, GaoandLi, HongyangandQiao, YuandLu, LeweiandZhou, JieandDai, Jifeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17830-17839.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的鸟瞰图（BEV）检测器通常与某些深度预训练的骨干网络（如VoVNet）紧密相关，限制了图像骨干网络和BEV检测器之间的协同作用。<br>
                    动机：为了解决这个限制，我们引入了透视空间监督，以简化BEV检测器的优化过程。<br>
                    方法：我们提出了一种两阶段BEV检测器，其中透视头部的提案被送入鸟瞰图头部进行最终预测。<br>
                    效果：通过广泛的消融研究，特别是在监督形式和所提出检测器的通用性方面，我们在传统和现代图像骨干网络上验证了该方法，并在大规模的nuScenes数据集上取得了新的最先进的结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones. Existing state-of-the-art BEV detectors are often tied to certain depth pre-trained backbones like VoVNet, hindering the synergy between booming image backbones and BEV detectors. To address this limitation, we prioritize easing the optimization of BEV detectors by introducing perspective space supervision. To this end, we propose a two-stage BEV detector, where proposals from the perspective head are fed into the bird's-eye-view head for final predictions. To evaluate the effectiveness of our model, we conduct extensive ablation studies focusing on the form of supervision and the generality of the proposed detector. The proposed method is verified with a wide spectrum of traditional and modern image backbones and achieves new SoTA results on the large-scale nuScenes dataset. The code shall be released soon.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2238.SlowLiDAR: Increasing the Latency of LiDAR-Based Detection Using Adversarial Examples</span><br>
                <span class="as">Liu, HanandWu, YuhaoandYu, ZhiyuanandVorobeychik, YevgeniyandZhang, Ning</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SlowLiDAR_Increasing_the_Latency_of_LiDAR-Based_Detection_Using_Adversarial_Examples_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5146-5155.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的LiDAR感知系统在对抗性扰动下的可用性（延迟）问题。<br>
                    动机：大部分研究关注对抗性扰动对预测的影响，但对实时网络物理系统来说，延迟是一个关键问题。<br>
                    方法：提出SlowLiDAR攻击，通过使用可微分代理和新的损耗函数来克服LiDAR检测管道中的不可微部分的技术挑战。<br>
                    效果：实验结果表明，SlowLiDAR可以显著增加六种最流行的LiDAR检测管道的延迟，同时保持其无法察觉。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR-based perception is a central component of autonomous driving, playing a key role in tasks such as vehicle localization and obstacle detection. Since the safety of LiDAR-based perceptual pipelines is critical to safe autonomous driving, a number of past efforts have investigated its vulnerability under adversarial perturbations of raw point cloud inputs. However, most such efforts have focused on investigating the impact of such perturbations on predictions (integrity), and little has been done to understand the impact on latency (availability), a critical concern for real-time cyber-physical systems. We present the first systematic investigation of the availability of LiDAR detection pipelines, and SlowLiDAR, an adversarial perturbation attack that maximizes LiDAR detection runtime. The attack overcomes the technical challenges posed by the non-differentiable parts of the LiDAR detection pipelines by using differentiable proxies and uses a novel loss function that effectively captures the impact of adversarial perturbations on the execution time of the pipeline. Extensive experimental results show that SlowLiDAR can significantly increase the latency of the six most popular LiDAR detection pipelines while maintaining imperceptibility.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2239.AeDet: Azimuth-Invariant Multi-View 3D Object Detection</span><br>
                <span class="as">Feng, ChengjianandJie, ZequnandZhong, YujieandChu, XiangxiangandMa, Lin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_AeDet_Azimuth-Invariant_Multi-View_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21580-21588.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何改进现有的基于LSS的多视角3D物体检测方法，使其更好地处理BEV特征并优化检测器。<br>
                    动机：当前的处理方法忽视了BEV特征的径向对称性，增加了检测器优化的难度。<br>
                    方法：提出一种方位等变卷积（AeConv）和方位等变锚点，以保留BEV特征的内在属性并简化优化过程。同时引入了与相机参数解耦的虚拟深度，统一不同相机内参数图像的深度预测。<br>
                    效果：在nuScenes上进行的大量实验表明，该方法所构建的检测器（AeDet）在NDS上达到了62.0%，大幅超过了最近的多视角3D物体检测器如PETRv2和BEVDepth。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent LSS-based multi-view 3D object detection has made tremendous progress, by processing the features in Brid-Eye-View (BEV) via the convolutional detector. However, the typical convolution ignores the radial symmetry of the BEV features and increases the difficulty of the detector optimization. To preserve the inherent property of the BEV features and ease the optimization, we propose an azimuth-equivariant convolution (AeConv) and an azimuth-equivariant anchor. The sampling grid of AeConv is always in the radial direction, thus it can learn azimuth-invariant BEV features. The proposed anchor enables the detection head to learn predicting azimuth-irrelevant targets. In addition, we introduce a camera-decoupled virtual depth to unify the depth prediction for the images with different camera intrinsic parameters. The resultant detector is dubbed Azimuth-equivariant Detector (AeDet). Extensive experiments are conducted on nuScenes, and AeDet achieves a 62.0% NDS, surpassing the recent multi-view 3D object detectors such as PETRv2 and BEVDepth by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2240.GFIE: A Dataset and Baseline for Gaze-Following From 2D to 3D in Indoor Environments</span><br>
                <span class="as">Hu, ZhengxiandYang, YuxueandZhai, XiaolinandYang, DingyeandZhou, BohanandLiu, Jingtai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_GFIE_A_Dataset_and_Baseline_for_Gaze-Following_From_2D_to_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8907-8916.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确自动地定位视线方向，以理解人类意图。<br>
                    动机：现有的视线跟踪数据集在收集视线标签时存在缺陷，手动标注可能引入主观偏差且劳动密集，而使用眼动仪进行自动标注会改变人的外观。<br>
                    方法：我们开发了一个新型的视线数据收集系统，包括一个Azure Kinect和一个激光测距仪，用于生成激光点引导被试者的注意力。我们还开发了一种算法，可以在图像中定位激光点，用于注释2D/3D视线目标并去除由激光点引入的地面真实值。整个收集视线行为的过程使我们能够在半自动的情况下在无约束的环境中获得无偏的标签。<br>
                    效果：我们在GFIE数据集上提出了一种基于立体视场感知的基线方法，建立了一个2D/3D视线跟踪基准。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Gaze-following is a kind of research that requires locating where the person in the scene is looking automatically under the topic of gaze estimation. It is an important clue for understanding human intention, such as identifying objects or regions of interest to humans. However, a survey of datasets used for gaze-following tasks reveals defects in the way they collect gaze point labels. Manual labeling may introduce subjective bias and is labor-intensive, while automatic labeling with an eye-tracking device would alter the person's appearance. In this work, we introduce GFIE, a novel dataset recorded by a gaze data collection system we developed. The system is constructed with two devices, an Azure Kinect and a laser rangefinder, which generate the laser spot to steer the subject's attention as they perform in front of the camera. And an algorithm is developed to locate laser spots in images for annotating 2D/3D gaze targets and removing ground truth introduced by the spots. The whole procedure of collecting gaze behavior allows us to obtain unbiased labels in unconstrained environments semi-automatically. We also propose a baseline method with stereo field-of-view (FoV) perception for establishing a 2D/3D gaze-following benchmark on the GFIE dataset. Project page: https://sites.google.com/view/gfie.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2241.Iterative Vision-and-Language Navigation</span><br>
                <span class="as">Krantz, JacobandBanerjee, ShurjoandZhu, WangandCorso, JasonandAnderson, PeterandLee, StefanandThomason, Jesse</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Krantz_Iterative_Vision-and-Language_Navigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14921-14930.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种评估语言引导的代理在持久环境中进行迭代视觉和语言导航的新范式。<br>
                    动机：现有的视觉和语言导航（VLN）基准测试在每个剧集开始时都会擦除代理的记忆，测试在没有任何先前信息的情况下进行冷启动导航的能力。然而，部署的机器人会在相同的环境中长时间停留。<br>
                    方法：通过训练和评估在场景中保持记忆的VLN代理，提出了迭代视觉和语言导航（IVLN）范式，这些场景由多达100个按顺序遵循的语言指令和目标路径组成。<br>
                    效果：我们发现，对于高性能的变换器VLN代理来说，扩展其隐含记忆并不足以进行IVLN，但能够构建地图的代理可以从环境的持久性中受益，这促使人们重新关注VLN中的地图构建代理。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for evaluating language-guided agents navigating in a persistent environment over time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the agent's memory at the beginning of every episode, testing the ability to perform cold-start navigation with no prior information. However, deployed robots occupy the same environment for long periods of time. The IVLN paradigm addresses this disparity by training and evaluating VLN agents that maintain memory across tours of scenes that consist of up to 100 ordered instruction-following Room-to-Room (R2R) episodes, each defined by an individual language instruction and a target path. We present discrete and continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours each in 80 indoor scenes. We find that extending the implicit memory of high-performing transformer VLN agents is not sufficient for IVLN, but agents that build maps can benefit from environment persistence, motivating a renewed focus on map-building agents in VLN.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2242.MaLP: Manipulation Localization Using a Proactive Scheme</span><br>
                <span class="as">Asnani, VishalandYin, XiandHassner, TalandLiu, Xiaoming</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Asnani_MaLP_Manipulation_Localization_Using_a_Proactive_Scheme_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12343-12352.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地检测和定位图像中的修改操作。<br>
                    动机：现有的被动篡改定位方法在未见过的生成模型和篡改属性上泛化性能差。<br>
                    方法：提出一种主动的篡改定位方案，称为MaLP。通过添加学习到的模板对真实图像进行加密，如果图像被任何生成模型篡改，这种来自模板的保护不仅有助于二进制检测，还能帮助识别被生成模型修改的像素。模板是通过利用双分支架构估计的局部和全局级别特征来学习的。<br>
                    效果：实验表明，MaLP的性能优于现有的被动方法。通过对22种不同的生成模型进行测试，证明了MaLP的泛化性，为未来的篡改定位研究提供了基准。最后，证明MaLP可以用作生成模型的判别器，以提高生成模型的质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Advancements in the generation quality of various Generative Models (GMs) has made it necessary to not only perform binary manipulation detection but also localize the modified pixels in an image. However, prior works termed as passive for manipulation localization exhibit poor generalization performance over unseen GMs and attribute modifications. To combat this issue, we propose a proactive scheme for manipulation localization, termed MaLP. We encrypt the real images by adding a learned template. If the image is manipulated by any GM, this added protection from the template not only aids binary detection but also helps in identifying the pixels modified by the GM. The template is learned by leveraging local and global-level features estimated by a two-branch architecture. We show that MaLP performs better than prior passive works. We also show the generalizability of MaLP by testing on 22 different GMs, providing a benchmark for future research on manipulation localization. Finally, we show that MaLP can be used as a discriminator for improving the generation quality of GMs. Our models/codes are available at www.github.com/vishal3477/pro_loc.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2243.Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection</span><br>
                <span class="as">Yu, YiandDa, Feipeng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13354-13363.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何准确预测物体的朝向，并解决由于旋转对称性引起的周期性模糊问题。<br>
                    动机：随着计算机视觉的发展，定向对象检测逐渐显现出其重要性。然而，由于旋转对称性，定向对象检测中存在各种周期性模糊问题。<br>
                    方法：提出一种名为相位移动编码器（PSC）的新型可微角度编码器，以及其双频版本（PSCD）。通过将不同周期的旋转周期性映射到不同频率的相位，为定向对象检测中由旋转对称性引起的各种周期性模糊问题提供了一个统一的框架。<br>
                    效果：在三个数据集上的视觉分析和实验证明了该方法的有效性和潜力。当面临需要高质量边界框的场景时，所提出的方法有望提供有竞争力的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the vigorous development of computer vision, oriented object detection has gradually been featured. In this paper, a novel differentiable angle coder named phase-shifting coder (PSC) is proposed to accurately predict the orientation of objects, along with a dual-frequency version (PSCD). By mapping the rotational periodicity of different cycles into the phase of different frequencies, we provide a unified framework for various periodic fuzzy problems caused by rotational symmetry in oriented object detection. Upon such a framework, common problems in oriented object detection such as boundary discontinuity and square-like problems are elegantly solved in a unified form. Visual analysis and experiments on three datasets prove the effectiveness and the potentiality of our approach. When facing scenarios requiring high-quality bounding boxes, the proposed methods are expected to give a competitive performance. The codes are publicly available at https://github.com/open-mmlab/mmrotate.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2244.Look, Radiate, and Learn: Self-Supervised Localisation via Radio-Visual Correspondence</span><br>
                <span class="as">Alloulah, MohammedandArnold, Maximilian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Alloulah_Look_Radiate_and_Learn_Self-Supervised_Localisation_via_Radio-Visual_Correspondence_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17430-17440.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何实现下一代蜂窝网络中的无线电感知功能，并提高其在全球户外的感知覆盖范围？<br>
                    动机：深度学习在计算机视觉领域取得了重大突破，但在无线电感知任务中的应用受到限制，主要原因是缺乏专门用于研究无线电感知性能和潜力的系统数据集和基准。<br>
                    方法：我们提出了MaxRay，一个合成的无线电视觉数据集和基准，用于精确定位无线电信号。我们还提出了一种无监督学习的方法，通过从无线电视觉对应中提取自我坐标来定位目标。<br>
                    效果：我们的实验结果表明，可以从配对的无线电视觉数据中自动学习准确的无线电目标定位，而无需标签，这对于实证数据非常重要。这为大规模数据扩展打开了大门，并可能证明是实现统一通信感知蜂窝基础设施的关键。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Next generation cellular networks will implement radio sensing functions alongside customary communications, thereby enabling unprecedented worldwide sensing coverage outdoors. Deep learning has revolutionised computer vision but has had limited application to radio perception tasks, in part due to lack of systematic datasets and benchmarks dedicated to the study of the performance and promise of radio sensing. To address this gap, we present MaxRay: a synthetic radio-visual dataset and benchmark that facilitate precise target localisation in radio. We further propose to learn to localise targets in radio without supervision by extracting self-coordinates from radio-visual correspondence. We use such self-supervised coordinates to train a radio localiser network. We characterise our performance against a number of state-of-the-art baselines. Our results indicate that accurate radio target localisation can be automatically learned from paired radio-visual data without labels, which is important for empirical data. This opens the door for vast data scalability and may prove key to realising the promise of robust radio sensing atop a unified communication-perception cellular infrastructure. Dataset will be hosted on IEEE DataPort.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2245.Indiscernible Object Counting in Underwater Scenes</span><br>
                <span class="as">Sun, GuoleiandAn, ZhaochongandLiu, YunandLiu, CeandSakaridis, ChristosandFan, Deng-PingandVanGool, Luc</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Indiscernible_Object_Counting_in_Underwater_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13791-13801.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在难以区分的场景中进行对象计数的问题，即难以区分的对象计数（IOC）。<br>
                    动机：由于缺乏适当的IOC数据集，我们提出了一个大规模的IOCfish5K数据集，以推动这个领域的研究。<br>
                    方法：我们创建了一个包含5637张高分辨率图像和659024个注释中心点的大规模数据集IOCfish5K。同时，我们还设计了一个新的强基线模型IOCFormer，该模型结合了密度和回归分支，可以有效地处理隐蔽场景下的对象计数。<br>
                    效果：实验表明，IOCFormer在IOCfish5K上取得了最先进的分数，证明了我们的方法的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, indiscernible scene understanding has attracted a lot of attention in the vision community. We further advance the frontier of this field by systematically studying a new challenge named indiscernible object counting (IOC), the goal of which is to count objects that are blended with respect to their surroundings. Due to a lack of appropriate IOC datasets, we present a large-scale dataset IOCfish5K which contains a total of 5,637 high-resolution images and 659,024 annotated center points. Our dataset consists of a large number of indiscernible objects (mainly fish) in underwater scenes, making the annotation process all the more challenging. IOCfish5K is superior to existing datasets with indiscernible scenes because of its larger scale, higher image resolutions, more annotations, and denser scenes. All these aspects make it the most challenging dataset for IOC so far, supporting progress in this area. For benchmarking purposes, we select 14 mainstream methods for object counting and carefully evaluate them on IOCfish5K. Furthermore, we propose IOCFormer, a new strong baseline that combines density and regression branches in a unified framework and can effectively tackle object counting under concealed scenes. Experiments show that IOCFormer achieves state-of-the-art scores on IOCfish5K.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2246.Relational Context Learning for Human-Object Interaction Detection</span><br>
                <span class="as">Kim, SanghyunandJung, DeunsolandCho, Minsu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Relational_Context_Learning_for_Human-Object_Interaction_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2925-2934.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高关系推理在发现HOI实例中的关键性。<br>
                    动机：现有的最新方法构建了两个解码器分支的变换器架构，但可能由于分支间上下文交换不足，导致关系推理缺乏上下文信息。<br>
                    方法：提出多重关系网络（MUREN），通过人、物和交互标记的一元、二元和三元关系，在三个解码器分支之间进行丰富的上下文交换。<br>
                    效果：该方法学习了发现HOI实例的全面关系上下文，并在HICO-DET和V-COCO两个标准基准上实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent state-of-the-art methods for HOI detection typically build on transformer architectures with two decoder branches, one for human-object pair detection and the other for interaction classification. Such disentangled transformers, however, may suffer from insufficient context exchange between the branches and lead to a lack of context information for relational reasoning, which is critical in discovering HOI instances. In this work, we propose the multiplex relation network (MUREN) that performs rich context exchange between three decoder branches using unary, pairwise, and ternary relations of human, object, and interaction tokens. The proposed method learns comprehensive relational contexts for discovering HOI instances, achieving state-of-the-art performance on two standard benchmarks for HOI detection, HICO-DET and V-COCO.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2247.FLAG3D: A 3D Fitness Activity Dataset With Language Instruction</span><br>
                <span class="as">Tang, YansongandLiu, JinpengandLiu, AoyangandYang, BinandDai, WenxunandRao, YongmingandLu, JiwenandZhou, JieandLi, Xiu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_FLAG3D_A_3D_Fitness_Activity_Dataset_With_Language_Instruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22106-22117.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决当前计算机视觉中健身活动分析领域对高质量数据、精细标签和多样化环境的需求。<br>
                    动机：随着全球健身活动的普及，健身活动分析成为计算机视觉领域的新兴研究课题。然而，现有的任务和算法需要大量的高质量数据资源。<br>
                    方法：本文提出了一个大规模的3D健身活动数据集FLAG3D，包含18万个60类别的动作序列。该数据集具有以下三个特点：1）通过先进的动作捕捉系统准确且密集地捕获3D人体姿态，以处理复杂的动作和大范围的运动；2）提供详细且专业的语言指令来描述如何执行特定的动作；3）从高科技的动作捕捉系统、渲染软件和价格合理的智能手机获取多样化的视频资源，在自然环境中进行拍摄。<br>
                    效果：广泛的实验和深入的分析表明，FLAG3D对于各种挑战（如跨领域人体动作识别、动态人体网格恢复和语言引导的人体动作生成）具有重要的研究价值。该数据集和源代码已公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>With the continuously thriving popularity around the world, fitness activity analytic has become an emerging research topic in computer vision. While a variety of new tasks and algorithms have been proposed recently, there are growing hunger for data resources involved in high-quality data, fine-grained labels, and diverse environments. In this paper, we present FLAG3D, a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D contributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action generation. Our dataset and source code are publicly available at https://andytang15.github.io/FLAG3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2248.PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation</span><br>
                <span class="as">Liu, QihaoandKortylewski, AdamandYuille, AlanL.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PoseExaminer_Automated_Testing_of_Out-of-Distribution_Robustness_in_Human_Pose_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/672-681.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前人体姿态和形状（HPS）估计方法在真实世界应用中，当观察到的数据与训练数据显著不同时，可能会面临分布外（OOD）的临界情况。<br>
                    动机：为了解决这个基本问题，开发了一种模拟器，通过可解释的参数以细粒度的方式控制，探索人体姿态的图像流形，例如通过改变姿态、形状和服装。<br>
                    方法：引入了一种学习型测试方法，称为PoseExaminer，通过搜索人体姿态图像的参数空间来自动诊断HPS算法的失败模式。<br>
                    效果：实验表明，PoseExaminer发现了当前最先进的模型在现实场景中的各种限制，这些限制在当前的基准测试中被忽略了。此外，通过利用PoseExaminer发现的失败模式对HPS方法进行微调，可以提高其鲁棒性，甚至在标准基准测试上的性能也有显著提高。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human pose and shape (HPS) estimation methods achieve remarkable results. However, current HPS benchmarks are mostly designed to test models in scenarios that are similar to the training data. This can lead to critical situations in real-world applications when the observed data differs significantly from the training data and hence is out-of-distribution (OOD). It is therefore important to test and improve the OOD robustness of HPS methods. To address this fundamental problem, we develop a simulator that can be controlled in a fine-grained manner using interpretable parameters to explore the manifold of images of human pose, e.g. by varying poses, shapes, and clothes. We introduce a learning-based testing method, termed PoseExaminer, that automatically diagnoses HPS algorithms by searching over the parameter space of human pose images to find the failure modes. Our strategy for exploring this high-dimensional parameter space is a multi-agent reinforcement learning system, in which the agents collaborate to explore different parts of the parameter space. We show that our PoseExaminer discovers a variety of limitations in current state-of-the-art models that are relevant in real-world scenarios but are missed by current benchmarks. For example, it finds large regions of realistic human poses that are not predicted correctly, as well as reduced performance for humans with skinny and corpulent body shapes. In addition, we show that fine-tuning HPS methods by exploiting the failure modes found by PoseExaminer improve their robustness and even their performance on standard benchmarks by a significant margin. The code are available for research purposes.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2249.Shepherding Slots to Objects: Towards Stable and Robust Object-Centric Learning</span><br>
                <span class="as">Kim, JinwooandChoi, JanghyukandChoi, Ho-JinandKim, SeonJoo</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Shepherding_Slots_to_Objects_Towards_Stable_and_Robust_Object-Centric_Learning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19198-19207.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何对单视图图像进行对象中心学习，以实现场景的通用和组合理解。<br>
                    动机：由于单视图图像的信息量较少，难以分离给定的场景，因此对单视图图像进行对象中心学习仍然具有挑战性。<br>
                    方法：提出了一种新的对象中心学习框架SLASH，该框架在Slot Attention的基础上增加了两个简单而有效的模块：Attention Refining Kernel（ARK）和Intermediate Point Predictor and Encoder（IPPE）。这两个模块分别防止插槽被背景噪音干扰，并指示插槽集中注意力的位置，以促进对象中心表示的学习。<br>
                    效果：实验表明，该方法能够一致地学习对象中心表示，并在四个数据集上取得了强大的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object-centric learning (OCL) aspires general and com- positional understanding of scenes by representing a scene as a collection of object-centric representations. OCL has also been extended to multi-view image and video datasets to apply various data-driven inductive biases by utilizing geometric or temporal information in the multi-image data. Single-view images carry less information about how to disentangle a given scene than videos or multi-view im- ages do. Hence, owing to the difficulty of applying induc- tive biases, OCL for single-view images still remains chal- lenging, resulting in inconsistent learning of object-centric representation. To this end, we introduce a novel OCL framework for single-view images, SLot Attention via SHep- herding (SLASH), which consists of two simple-yet-effective modules on top of Slot Attention. The new modules, At- tention Refining Kernel (ARK) and Intermediate Point Pre- dictor and Encoder (IPPE), respectively, prevent slots from being distracted by the background noise and indicate lo- cations for slots to focus on to facilitate learning of object- centric representation. We also propose a weak- and semi- supervision approach for OCL, whilst our proposed frame- work can be used without any assistant annotation during the inference. Experiments show that our proposed method enables consistent learning of object-centric representa- tion and achieves strong performance across four datasets. Code is available at https://github.com/object- understanding/SLASH.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2250.IPCC-TP: Utilizing Incremental Pearson Correlation Coefficient for Joint Multi-Agent Trajectory Prediction</span><br>
                <span class="as">Zhu, DekaiandZhai, GuangyaoandDi, YanandManhardt, FabianandBerkemeyer, HendrikandTran, TuanandNavab, NassirandTombari, FedericoandBusam, Benjamin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_IPCC-TP_Utilizing_Incremental_Pearson_Correlation_Coefficient_for_Joint_Multi-Agent_Trajectory_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5507-5516.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高多智能体轨迹预测的可靠性，以实现自主系统的安全规划和控制。<br>
                    动机：与单智能体情况相比，同时处理多个智能体的主要挑战在于模拟复杂的社会互动，这种互动由各种驾驶意图和道路条件引起。<br>
                    方法：本文提出了一种基于增量皮尔逊相关系数（IPCC）的新型相关性感知模块IPCC-TP，用于改进多智能体交互建模。IPCC-TP通过紧密耦合的均值和协方差的估计，根据交互式增量运动学习成对联合高斯分布。<br>
                    效果：在nuScenes和Argoverse 2数据集上的大量实验表明，IPCC-TP将基线的性能提高了很大一截。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Reliable multi-agent trajectory prediction is crucial for the safe planning and control of autonomous systems. Compared with single-agent cases, the major challenge in simultaneously processing multiple agents lies in modeling complex social interactions caused by various driving intentions and road conditions. Previous methods typically leverage graph-based message propagation or attention mechanism to encapsulate such interactions in the format of marginal probabilistic distributions. However, it is inherently sub-optimal. In this paper, we propose IPCC-TP, a novel relevance-aware module based on Incremental Pearson Correlation Coefficient to improve multi-agent interaction modeling. IPCC-TP learns pairwise joint Gaussian Distributions through the tightly-coupled estimation of the means and covariances according to interactive incremental movements. Our module can be conveniently embedded into existing multi-agent prediction methods to extend original motion distribution decoders. Extensive experiments on nuScenes and Argoverse 2 datasets demonstrate that IPCC-TP improves the performance of baselines by a large margin.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2251.BEV-Guided Multi-Modality Fusion for Driving Perception</span><br>
                <span class="as">Man, YunzeandGui, Liang-YanandWang, Yu-Xiong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21960-21969.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何整合多种传感器并解决自动驾驶中的多样化任务，同时在端到端算法中实现？<br>
                    动机：将各种传感器统一在一个端到端的Bird's Eye-View（BEV）指导下是自动驾驶中具有挑战性但至关重要的话题。<br>
                    方法：我们引入了BEVGuide，这是一个新的BEV表示学习框架，首次尝试以端到端的方式直接在BEV指导下统一广泛的传感器。我们的架构接受来自多样化的传感器池的输入，包括但不限于摄像头、激光雷达和雷达传感器，并使用通用的变压器主干提取BEV特征嵌入。我们设计了一个BEV引导的多传感器注意力块，从BEV嵌入中获取查询，并从特定于传感器的特征中学习BEV表示。<br>
                    效果：由于其轻量级主干设计和高度灵活性，BEVGuide非常高效，几乎支持任何输入传感器配置。大量实验证明，我们的框架在具有多样化传感器集的BEV感知任务中表现出色。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Integrating multiple sensors and addressing diverse tasks in an end-to-end algorithm are challenging yet critical topics for autonomous driving. To this end, we introduce BEVGuide, a novel Bird's Eye-View (BEV) representation learning framework, representing the first attempt to unify a wide range of sensors under direct BEV guidance in an end-to-end fashion. Our architecture accepts input from a diverse sensor pool, including but not limited to Camera, Lidar and Radar sensors, and extracts BEV feature embeddings using a versatile and general transformer backbone. We design a BEV-guided multi-sensor attention block to take queries from BEV embeddings and learn the BEV representation from sensor-specific features. BEVGuide is efficient due to its lightweight backbone design and highly flexible as it supports almost any input sensor configurations. Extensive experiments demonstrate that our framework achieves exceptional performance in BEV perception tasks with a diverse sensor set. Project page is at https://yunzeman.github.io/BEVGuide.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2252.Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding</span><br>
                <span class="as">Hwang, MinyoungandJeong, JaeyeonandKim, MinsooandOh, YoonseonandOh, Songhwai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hwang_Meta-Explore_Exploratory_Hierarchical_Vision-and-Language_Navigation_Using_Scene_Object_Spectrum_Grounding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6683-6693.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉-语言导航（VLN）的主要挑战在于如何在未见过的环境中理解自然语言指令。<br>
                    动机：传统的VLN算法的局限性在于，如果执行的动作错误，那么代理就无法遵循指令或探索不必要的区域，导致代理走向一条无法恢复的路径。<br>
                    方法：我们提出了Meta-Explore，一种分层导航方法，使用开发策略来纠正被误导的最近的动作。我们展示了一个开发策略，即在未访问但可观察的状态中选择一个精心选择的局部目标，优于将代理移动到以前访问过的状态的方法。我们还强调了需要用语义上有意义的线索想象遗憾的探索。<br>
                    效果：我们在R2R、SOON和REVERIE三个VLN基准测试中评估了我们的方法。Meta-Explore优于其他基线，并显示出显著的泛化性能。此外，使用提出的频域SOS特征进行局部目标搜索显著提高了成功率，在SOON基准测试中成功率提高了17.1%，SPL提高了20.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The main challenge in vision-and-language navigation (VLN) is how to understand natural-language instructions in an unseen environment. The main limitation of conventional VLN algorithms is that if an action is mistaken, the agent fails to follow the instructions or explores unnecessary regions, leading the agent to an irrecoverable path. To tackle this problem, we propose Meta-Explore, a hierarchical navigation method deploying an exploitation policy to correct misled recent actions. We show that an exploitation policy, which moves the agent toward a well-chosen local goal among unvisited but observable states, outperforms a method which moves the agent to a previously visited state. We also highlight the demand for imagining regretful explorations with semantically meaningful clues. The key to our approach is understanding the object placements around the agent in spectral-domain. Specifically, we present a novel visual representation, called scene object spectrum (SOS), which performs category-wise 2D Fourier transform of detected objects. Combining exploitation policy and SOS features, the agent can correct its path by choosing a promising local goal. We evaluate our method in three VLN benchmarks: R2R, SOON, and REVERIE. Meta-Explore outperforms other baselines and shows significant generalization performance. In addition, local goal search using the proposed spectral-domain SOS features significantly improves the success rate by 17.1% and SPL by 20.6% for the SOON benchmark.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2253.Query-Centric Trajectory Prediction</span><br>
                <span class="as">Zhou, ZikangandWang, JianpingandLi, Yung-HuiandHuang, Yu-Kai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17863-17873.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何预测周围代理的未来轨迹，以实现自动驾驶车辆的安全运行。<br>
                    动机：现有的方法在预测未来轨迹时存在计算冗余和无法捕获多模态行为的问题。<br>
                    方法：提出了一种名为QCNet的模型框架，该框架采用查询中心范式进行场景编码，实现了过去计算的重用，并引入了锚点无关的查询来生成轨迹提案，然后通过锚点基础的查询对轨迹提案进行进一步的优化。<br>
                    效果：在Argoverse 1和Argoverse 2运动预测基准测试中，该方法在所有主要指标上都优于所有其他方法，排名第一。同时，由于其查询中心的设计理念，该方法可以实现流式场景编码和并行多代理解码。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting the future trajectories of surrounding agents is essential for autonomous vehicles to operate safely. This paper presents QCNet, a modeling framework toward pushing the boundaries of trajectory prediction. First, we identify that the agent-centric modeling scheme used by existing approaches requires re-normalizing and re-encoding the input whenever the observation window slides forward, leading to redundant computations during online prediction. To overcome this limitation and achieve faster inference, we introduce a query-centric paradigm for scene encoding, which enables the reuse of past computations by learning representations independent of the global spacetime coordinate system. Sharing the invariant scene features among all target agents further allows the parallelism of multi-agent trajectory decoding. Second, even given rich encodings of the scene, existing decoding strategies struggle to capture the multimodality inherent in agents' future behavior, especially when the prediction horizon is long. To tackle this challenge, we first employ anchor-free queries to generate trajectory proposals in a recurrent fashion, which allows the model to utilize different scene contexts when decoding waypoints at different horizons. A refinement module then takes the trajectory proposals as anchors and leverages anchor-based queries to refine the trajectories further. By supplying adaptive and high-quality anchors to the refinement module, our query-based decoder can better deal with the multimodality in the output of trajectory prediction. Our approach ranks 1st on Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming all methods on all main metrics by a large margin. Meanwhile, our model can achieve streaming scene encoding and parallel multi-agent decoding thanks to the query-centric design ethos.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2254.Phone2Proc: Bringing Robust Robots Into Our Chaotic World</span><br>
                <span class="as">Deitke, MattandHendrix, RoseandFarhadi, AliandEhsani, KianaandKembhavi, Aniruddha</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Phone2Proc_Bringing_Robust_Robots_Into_Our_Chaotic_World_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9665-9675.png><br>
            
            <span class="tt"><span class="t0">研究问题：训练在模拟环境中的实体代理通常无法适应真实世界环境。<br>
                    动机：为了解决实体代理在真实世界中的表现不佳的问题，本文提出了一种新方法。<br>
                    方法：Phone2Proc方法利用10分钟的手机扫描和条件过程生成技术，创建与目标环境语义相似的训练场景分布。<br>
                    效果：通过使用Phone2Proc进行训练，实体代理在从模拟到真实的ObjectNav任务中的性能成功率从34.7%提高到70.7%，并且在包括家庭、办公室和RoboTHOR在内的多样化真实环境中的超过200次试验中表现出了对现实世界变化的显著鲁棒性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Training embodied agents in simulation has become mainstream for the embodied AI community. However, these agents often struggle when deployed in the physical world due to their inability to generalize to real-world environments. In this paper, we present Phone2Proc, a method that uses a 10-minute phone scan and conditional procedural generation to create a distribution of training scenes that are semantically similar to the target environment. The generated scenes are conditioned on the wall layout and arrangement of large objects from the scan, while also sampling lighting, clutter, surface textures, and instances of smaller objects with randomized placement and materials. Leveraging just a simple RGB camera, training with Phone2Proc shows massive improvements from 34.7% to 70.7% success rate in sim-to-real ObjectNav performance across a test suite of over 200 trials in diverse real-world environments, including homes, offices, and RoboTHOR. Furthermore, Phone2Proc's diverse distribution of generated scenes makes agents remarkably robust to changes in the real world, such as human movement, object rearrangement, lighting changes, or clutter.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2255.Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes</span><br>
                <span class="as">Ju, XuanandZeng, AilingandWang, JiananandXu, QiangandZhang, Lei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ju_Human-Art_A_Versatile_Human-Centric_Dataset_Bridging_Natural_and_Artificial_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/618-629.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前以人为中心的计算机视觉任务主要关注真实世界中的自然图像，而对雕塑、绘画和卡通等人造场景中的人类形象关注不足。<br>
                    动机：艺术作品作为生活的抽象，将人类同时融入自然和人造场景中，我们希望通过艺术来连接自然和人造场景的相关任务。<br>
                    方法：我们引入了Human-Art数据集，该数据集包含5万张高质量图片，超过123万个人体实例，来自5个自然场景和15个人造场景，为2D和3D的人体提供了边界框、关键点、自接触点和文本信息。<br>
                    效果：我们希望Human-Art数据集能为相关研究提供洞见，并开启新的研究问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans have long been recorded in a variety of forms since antiquity. For example, sculptures and paintings were the primary media for depicting human beings before the invention of cameras. However, most current human-centric computer vision tasks like human pose estimation and human image generation focus exclusively on natural images in the real world. Artificial humans, such as those in sculptures, paintings, and cartoons, are commonly neglected, making existing models fail in these scenarios. As an abstraction of life, art incorporates humans in both natural and artificial scenes. We take advantage of it and introduce the Human-Art dataset to bridge related tasks in natural and artificial scenarios. Specifically, Human-Art contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D. It is, therefore, comprehensive and versatile for various downstream tasks. We also provide a rich set of baseline results and detailed analyses for related tasks, including human detection, 2D and 3D human pose estimation, image generation, and motion transfer. As a challenging dataset, we hope Human-Art can provide insights for relevant research and open up new research questions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2256.Are We Ready for Vision-Centric Driving Streaming Perception? The ASAP Benchmark</span><br>
                <span class="as">Wang, XiaofengandZhu, ZhengandZhang, YunpengandHuang, GuanandYe, YunandXu, WenboandChen, ZiweiandWang, Xingang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Are_We_Ready_for_Vision-Centric_Driving_Streaming_Perception_The_ASAP_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9600-9610.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何量化自动驾驶中视觉感知的性能和效率之间的权衡，并解决传统评估方法忽视推理时间延迟的问题。<br>
                    动机：当前自动驾驶中的视觉感知技术在性能上有所提升，但延迟过高，无法满足实际应用需求。<br>
                    方法：提出自动驾驶流媒体感知（ASAP）基准，首次对自动驾驶中的视觉感知在线性能进行评估。基于2Hz标注的nuScenes数据集，设计了一个标注扩展流程，为12Hz原始图像生成高帧率标签。同时，构建了受限计算下的流感知评估协议（SPUR），在各种计算资源限制下使用12Hz输入进行流式评估。<br>
                    效果：ASAP基准的实验结果显示，在不同的约束条件下，模型排名会发生变化，说明在优化实际应用部署时，应考虑模型延迟和计算预算。此外，还为基于相机的流媒体3D检测建立了基线，显著提高了各种硬件的流式性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In recent years, vision-centric perception has flourished in various autonomous driving tasks, including 3D detection, semantic map construction, motion forecasting, and depth estimation. Nevertheless, the latency of vision-centric approaches is too high for practical deployment (e.g., most camera-based 3D detectors have a runtime greater than 300ms). To bridge the gap between ideal researches and real-world applications, it is necessary to quantify the trade-off between performance and efficiency. Traditionally, autonomous-driving perception benchmarks perform the online evaluation, neglecting the inference time delay. To mitigate the problem, we propose the Autonomous-driving StreAming Perception (ASAP) benchmark, which is the first benchmark to evaluate the online performance of vision-centric perception in autonomous driving. On the basis of the 2Hz annotated nuScenes dataset, we first propose an annotation-extending pipeline to generate high-frame-rate labels for the 12Hz raw images. Referring to the practical deployment, the Streaming Perception Under constRained-computation (SPUR) evaluation protocol is further constructed, where the 12Hz inputs are utilized for streaming evaluation under the constraints of different computational resources. In the ASAP benchmark, comprehensive experiment results reveal that the model rank alters under different constraints, suggesting that the model latency and computation budget should be considered as design choices to optimize the practical deployment. To facilitate further research, we establish baselines for camera-based streaming 3D detection, which consistently enhance the streaming performance across various hardware. The ASAP benchmark will be made publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2257.Azimuth Super-Resolution for FMCW Radar in Autonomous Driving</span><br>
                <span class="as">Li, Yu-JheandHunt, ShawnandPark, JinhyungandO{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Azimuth_Super-Resolution_for_FMCW_Radar_in_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17504-17513.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高调频连续波多输入多输出雷达的方位角分辨率。<br>
                    动机：由于硬件尺寸限制，调频连续波多输入多输出雷达通常具有较低的分辨率，这对自动驾驶中的物体定位和速度估计至关重要。<br>
                    方法：提出一种轻量且高效的模数转换超分辨率模型（ADC-SR），该模型仅使用少数接收器的信号预测或生成额外的雷达信号，以提高MIMO雷达的方位角分辨率。<br>
                    效果：实验证明，与处理后的距离-方位-多普勒（RAD）地图的基线模型相比，处理原始模数转换信号的ADC-SR方法在参数减少98%（50倍）的情况下，性能相当。同时，结合标准RAD超分辨率模型的混合超分辨率模型（Hybrid-SR）可以大幅提高性能。在城市雷达数据集和RADIal数据集上的实验验证了利用原始雷达模数转换信号的重要性。通过在我们的超分辨率模型结果上进行目标检测，发现我们的超分辨率模型可以提高约4%的mAP检测性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We tackle the task of Azimuth (angular dimension) super-resolution for Frequency Modulated Continuous Wave (FMCW) multiple-input multiple-output (MIMO) radar. FMCW MIMO radar is widely used in autonomous driving alongside Lidar and RGB cameras. However, compared to Lidar, MIMO radar is usually of low resolution due to hardware size restrictions. For example, achieving 1-degree azimuth resolution requires at least 100 receivers, but a single MIMO device usually supports at most 12 receivers. Having limitations on the number of receivers is problematic since a high-resolution measurement of azimuth angle is essential for estimating the location and velocity of objects. To improve the azimuth resolution of MIMO radar, we propose a light, yet efficient, Analog-to-Digital super-resolution model (ADC-SR) that predicts or hallucinates additional radar signals using signals from only a few receivers. Compared with the baseline models that are applied to processed radar Range-Azimuth-Doppler (RAD) maps, we show that our ADC-SR method that processes raw ADC signals achieves comparable performance with 98% (50 times) fewer parameters. We also propose a hybrid super-resolution model (Hybrid-SR) combining our ADC-SR with a standard RAD super-resolution model, and show that performance can be improved by a large margin. Experiments on our City-Radar dataset and the RADIal dataset validate the importance of leveraging raw radar ADC signals. To assess the value of our super-resolution model for autonomous driving, we also perform object detection on the results of our super-resolution model and find that our super-resolution model improves detection performance by around 4% in mAP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2258.UniHCP: A Unified Model for Human-Centric Perceptions</span><br>
                <span class="as">Ci, YuanzhengandWang, YizhouandChen, MeilinandTang, ShixiangandBai, LeiandZhu, FengandZhao, RuiandYu, FengweiandQi, DonglianandOuyang, Wanli</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17840-17852.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一个通用的人体感知模型，以解决各种以人为中心的视觉任务。<br>
                    动机：虽然特定的以人为中心的任务有其自身的相关语义重点，但它们也共享人体的基本语义结构。然而，很少有工作尝试利用这种同质性来设计一个通用的模型。<br>
                    方法：我们重新审视了广泛的以人为中心的任务，并以最简方式将它们统一起来。我们提出了UniHCP，这是一个统一的人体感知模型，它以简单的端到端方式，用普通的视觉转换器架构统一了广泛的以人为中心的任务。通过在33个人体中心数据集上的大规模联合训练，UniHCP可以在几个域内和下游任务上通过直接评估超过强大的基线。当适应特定任务时，UniHCP在广泛的以人为中心的任务上实现了新的最先进的性能。<br>
                    效果：例如，在CIHP上的人像解析达到了69.8 mIoU，在PA-100K上的属性预测达到了86.18 mA，在Market1501上的ReID达到了90.3 mAP，在CrowdHuman上的行人检测达到了85.8 JI，表现优于为每个任务量身定制的专用模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human-centric perceptions (e.g., pose estimation, human parsing, pedestrian detection, person re-identification, etc.) play a key role in industrial applications of visual models. While specific human-centric tasks have their own relevant semantic aspect to focus on, they also share the same underlying semantic structure of the human body. However, few works have attempted to exploit such homogeneity and design a general-propose model for human-centric tasks. In this work, we revisit a broad range of human-centric tasks and unify them in a minimalist manner. We propose UniHCP, a Unified Model for Human-Centric Perceptions, which unifies a wide range of human-centric tasks in a simplified end-to-end manner with the plain vision transformer architecture. With large-scale joint training on 33 humancentric datasets, UniHCP can outperform strong baselines on several in-domain and downstream tasks by direct evaluation. When adapted to a specific task, UniHCP achieves new SOTAs on a wide range of human-centric tasks, e.g., 69.8 mIoU on CIHP for human parsing, 86.18 mA on PA-100K for attribute prediction, 90.3 mAP on Market1501 for ReID, and 85.8 JI on CrowdHuman for pedestrian detection, performing better than specialized models tailored for each task. The code and pretrained model are available at https://github.com/OpenGVLab/UniHCP.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2259.Behavioral Analysis of Vision-and-Language Navigation Agents</span><br>
                <span class="as">Yang, ZijiaoandMajumdar, ArjunandLee, Stefan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Behavioral_Analysis_of_Vision-and-Language_Navigation_Agents_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2574-2582.png><br>
            
            <span class="tt"><span class="t0">研究问题：视觉-语言导航（VLN）代理如何根据环境将指令转化为行动。<br>
                    动机：为了成功，VLN代理必须能够基于周围环境将指令转化为行动。<br>
                    方法：我们开发了一种基于技能特定基础来研究代理行为的方法，通过生成技能特定的干预并测量代理预测的变化。<br>
                    效果：我们的分析表明，训练中的偏见对代理行为有持久影响，现有的模型能够将简单的指称表达式转化为行动。我们对多个模型的比较显示，技能特定分数与整体VLN任务性能的提高相关。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>To be successful, Vision-and-Language Navigation (VLN) agents must be able to ground instructions to actions based on their surroundings. In this work, we develop a methodology to study agent behavior on a skill-specific basis -- examining how well existing agents ground instructions about stopping, turning, and moving towards specified objects or rooms. Our approach is based on generating skill-specific interventions and measuring changes in agent predictions. We present a detailed case study analyzing the behavior of a recent agent and then compare multiple agents in terms of skill-specific competency scores. This analysis suggests that biases from training have lasting effects on agent behavior and that existing models are able to ground simple referring expressions. Our comparisons between models show that skill-specific scores correlate with improvements in overall VLN task performance.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2260.Distilling Focal Knowledge From Imperfect Expert for 3D Object Detection</span><br>
                <span class="as">Zeng, JiaandChen, LiandDeng, HanmingandLu, LeweiandYan, JunchiandQiao, YuandLi, Hongyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/992-1001.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从不完美的专家中提取知识进行模型压缩。<br>
                    动机：尽管现有的3D物体检测方法在性能上表现出色，但效率低下。<br>
                    方法：提出FD3D，一种针对3D物体检测的焦点蒸馏器。通过一系列查询来定位实例级别的区域以生成掩蔽特征，增强这些区域的特征表示能力。同时，这些查询找出了精细蒸馏的代表性位置。<br>
                    效果：在BEVFormer和DETR3D两种流行的检测模型上应用该方法，结果表明，该方法在nuScenes基准测试上的NDS指标分别提高了4.07和3.17点。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-camera 3D object detection blossoms in recent years and most of state-of-the-art methods are built up on the bird's-eye-view (BEV) representations. Albeit remarkable performance, these works suffer from low efficiency. Typically, knowledge distillation can be used for model compression. However, due to unclear 3D geometry reasoning, expert features usually contain some noisy and confusing areas. In this work, we investigate on how to distill the knowledge from an imperfect expert. We propose FD3D, a Focal Distiller for 3D object detection. Specifically, a set of queries are leveraged to locate the instance-level areas for masked feature generation, to intensify feature representation ability in these areas. Moreover, these queries search out the representative fine-grained positions for refined distillation. We verify the effectiveness of our method by applying it to two popular detection models, BEVFormer and DETR3D. The results demonstrate that our method achieves improvements of 4.07 and 3.17 points respectively in terms of NDS metric on nuScenes benchmark. Code is hosted at https://github.com/OpenPerceptionX/BEVPerception-Survey-Recipe.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2261.Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting</span><br>
                <span class="as">Khurana, TarashaandHu, PeiyunandHeld, DavidandRamanan, Deva</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khurana_Point_Cloud_Forecasting_as_a_Proxy_for_4D_Occupancy_Forecasting_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1116-1124.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何预测世界的未来变化，以实现自主系统的运动规划。<br>
                    动机：传统的运动规划方法依赖于昂贵的人工标注（如语义类别标签、边界框、轨迹或城市HD地图），难以扩展到大规模的未标注数据集。<br>
                    方法：提出一种基于未标注LiDAR序列的3D点云预测的自我监督任务，将此任务重新定义为空间时间（4D）占用预测，并通过给定传感器外参和内参从4D占用预测中"渲染"点云数据，以训练和测试未标注LiDAR序列的占用算法。<br>
                    效果：该方法使得自主系统能够对世界进行预测，而不是对其传感器进行预测，并且可以在不同的数据集、传感器和车辆上评估和比较点云预测算法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting how the world can evolve in the future is crucial for motion planning in autonomous systems. Classical methods are limited because they rely on costly human annotations in the form of semantic class labels, bounding boxes, and tracks or HD maps of cities to plan their motion -- and thus are difficult to scale to large unlabeled datasets. One promising self-supervised task is 3D point cloud forecasting from unannotated LiDAR sequences. We show that this task requires algorithms to implicitly capture (1) sensor extrinsics (i.e., the egomotion of the autonomous vehicle), (2) sensor intrinsics (i.e., the sampling pattern specific to the particular LiDAR sensor), and (3) the shape and motion of other objects in the scene. But autonomous systems should make predictions about the world and not their sensors! To this end, we factor out (1) and (2) by recasting the task as one of spacetime (4D) occupancy forecasting. But because it is expensive to obtain ground-truth 4D occupancy, we "render" point cloud data from 4D occupancy predictions given sensor extrinsics and intrinsics, allowing one to train and test occupancy algorithms with unannotated LiDAR sequences. This also allows one to evaluate and compare point cloud forecasting algorithms across diverse datasets, sensors, and vehicles.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2262.TopNet: Transformer-Based Object Placement Network for Image Compositing</span><br>
                <span class="as">Zhu, SijieandLin, ZheandCohen, ScottandKuen, JasonandZhang, ZhifeiandChen, Chen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TopNet_Transformer-Based_Object_Placement_Network_for_Image_Compositing_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1838-1847.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动将物体放置在背景图像中进行图像合成。<br>
                    动机：现有的方法无法充分利用背景图像中的局部信息，导致合成图像的质量受限。<br>
                    方法：提出一种利用转换器模块学习物体特征与所有局部背景特征之间相关性的方法，以提供所有可能的位置/缩放配置的详细信息。进一步提出稀疏对比损失来训练模型。<br>
                    效果：新方法在一次网络前向传递中生成一个3D热力图，表示所有位置/缩放组合的可信度，比之前的滑动窗口方法快10倍以上。同时支持用户定义位置或缩放的交互式搜索。该方法在真实世界图像上表现出色，具有广泛的适用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We investigate the problem of automatically placing an object into a background image for image compositing. Given a background image and a segmented object, the goal is to train a model to predict plausible placements (location and scale) of the object for compositing. The quality of the composite image highly depends on the predicted location/scale. Existing works either generate candidate bounding boxes or apply sliding-window search using global representations from background and object images, which fail to model local information in background images. However, local clues in background images are important to determine the compatibility of placing the objects with certain locations/scales. In this paper, we propose to learn the correlation between object features and all local background features with a transformer module so that detailed information can be provided on all possible location/scale configurations. A sparse contrastive loss is further proposed to train our model with sparse supervision. Our new formulation generates a 3D heatmap indicating the plausibility of all location/scale combinations in one network forward pass, which is >10x faster than the previous sliding-window method. It also supports interactive search when users provide a pre-defined location or scale. The proposed method can be trained with explicit annotation or in a self-supervised manner using an off-the-shelf inpainting model, and it outperforms state-of-the-art methods significantly. User study shows that the trained model generalizes well to real-world images with diverse challenging scenes and object categories.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2263.Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose Estimation From Image Sequence</span><br>
                <span class="as">Tian, YangandZhang, JiyaoandYin, ZekaiandDong, Hao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tian_Robot_Structure_Prior_Guided_Temporal_Attention_for_Camera-to-Robot_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8917-8926.png><br>
            
            <span class="tt"><span class="t0">研究问题：解决从单视图连续图像序列的在线相机到机器人位姿估计问题，这是机器人与世界互动的关键任务。<br>
                    动机：此任务的主要障碍是机器人的自我遮挡和单视图图像的模糊性。<br>
                    方法：我们的方法首次证明了时间信息和机器人结构先验在解决这些挑战中的有效性。给定连续帧和机器人关节配置，我们的方法学习精确地回归预定义的机器人关键点（如关节）的2D坐标。有了相机内参和机器人关节状态，我们使用透视n点（PnP）求解器得到相机到机器人的姿态。我们进一步利用机器人结构先验迭代改进相机到机器人的姿态。为了训练整个流程，我们构建了一个大规模的合成数据集，通过领域随机化弥合模拟与现实的差距。<br>
                    效果：我们在合成和真实世界的数据集上进行了广泛的实验，并在下游的机器人抓取任务中展示了我们的方法实现了新的最先进的性能，并且在实时（36 FPS）上超过了传统的手眼标定算法。代码和数据可在项目页面获取：https://sites.google.com/view/sgtapose。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we tackle the problem of online camera-to-robot pose estimation from single-view successive frames of an image sequence, a crucial task for robots to interact with the world. The primary obstacles of this task are the robot's self-occlusions and the ambiguity of single-view images. This work demonstrates, for the first time, the effectiveness of temporal information and the robot structure prior in addressing these challenges. Given the successive frames and the robot joint configuration, our method learns to accurately regress the 2D coordinates of the predefined robot's keypoints (e.g., joints). With the camera intrinsic and robotic joints status known, we get the camera-to-robot pose using a Perspective-n-point (PnP) solver. We further improve the camera-to-robot pose iteratively using the robot structure prior. To train the whole pipeline, we build a large-scale synthetic dataset generated with domain randomisation to bridge the sim-to-real gap. The extensive experiments on synthetic and real-world datasets and the downstream robotic grasping task demonstrate that our method achieves new state-of-the-art performances and outperforms traditional hand-eye calibration algorithms in real-time (36 FPS). Code and data are available at the project page: https://sites.google.com/view/sgtapose.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2264.Learning Human-to-Robot Handovers From Point Clouds</span><br>
                <span class="as">Christen, SammyandYang, WeiandP\&#x27;erez-D{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Christen_Learning_Human-to-Robot_Handovers_From_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9654-9664.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出第一个学习视觉基础的人类到机器人交接控制策略的框架。<br>
                    动机：尽管具身人工智能在模拟环境中训练机器人代理方面取得了重大进展，但由于模拟人类的困难，与人类进行交互仍然具有挑战性。<br>
                    方法：通过两阶段教师-学生框架利用运动和抓取规划、强化学习和自我监督进行人工辅助训练。<br>
                    效果：在模拟基准测试、模拟到模拟转移和模拟到真实转移方面，显著优于基线模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose the first framework to learn control policies for vision-based human-to-robot handovers, a critical task for human-robot interaction. While research in Embodied AI has made significant progress in training robot agents in simulated environments, interacting with humans remains challenging due to the difficulties of simulating humans. Fortunately, recent research has developed realistic simulated environments for human-to-robot handovers. Leveraging this result, we introduce a method that is trained with a human-in-the-loop via a two-stage teacher-student framework that uses motion and grasp planning, reinforcement learning, and self-supervision. We show significant performance gains over baselines on a simulation benchmark, sim-to-sim transfer and sim-to-real transfer.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2265.ProphNet: Efficient Agent-Centric Motion Forecasting With Anchor-Informed Proposals</span><br>
                <span class="as">Wang, XishunandSu, TongandDa, FangandYang, Xiaodong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProphNet_Efficient_Agent-Centric_Motion_Forecasting_With_Anchor-Informed_Proposals_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21995-22003.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动驾驶系统中的动态预测是一个关键模块，由于多源输入的异质性、代理行为的多模态性和车载部署所需的低延迟，这个任务极具挑战性。<br>
                    动机：为了应对这些困难，本文提出了一种新颖的以代理为中心的模型，该模型具有锚定信息的建议，用于有效的多模态动态预测。<br>
                    方法：我们设计了一种模态无关的策略，以简洁的方式统一编码复杂的输入。我们生成了与承载目标导向上下文的锚点融合的多样化建议，以引发覆盖广泛未来轨迹的多模态预测。网络架构高度统一且简洁，使得模型易于进行现实世界的部署。<br>
                    效果：实验表明，我们的以代理为中心的网络在预测精度上优于最先进的方法，同时实现了场景中心级别的推理延迟。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Motion forecasting is a key module in an autonomous driving system. Due to the heterogeneous nature of multi-sourced input, multimodality in agent behavior, and low latency required by onboard deployment, this task is notoriously challenging. To cope with these difficulties, this paper proposes a novel agent-centric model with anchor-informed proposals for efficient multimodal motion forecasting. We design a modality-agnostic strategy to concisely encode the complex input in a unified manner. We generate diverse proposals, fused with anchors bearing goal-oriented context, to induce multimodal prediction that covers a wide range of future trajectories. The network architecture is highly uniform and succinct, leading to an efficient model amenable for real-world deployment. Experiments reveal that our agent-centric network compares favorably with the state-of-the-art methods in prediction accuracy, while achieving scene-centric level inference latency.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2266.Learning and Aggregating Lane Graphs for Urban Automated Driving</span><br>
                <span class="as">B\&quot;uchner, MartinandZ\&quot;urn, JannikandTodoran, Ion-GeorgeandValada, AbhinavandBurgard, Wolfram</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Buchner_Learning_and_Aggregating_Lane_Graphs_for_Urban_Automated_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13415-13424.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决自动驾驶和高清地图学习中的一个重要且具有挑战性的任务，即车道图估计。<br>
                    动机：现有的使用车载或航空图像的方法在处理复杂的车道拓扑、分布外场景或图像空间中的显著遮挡等问题上存在困难。此外，合并重叠的车道图以获得一致的大型图形仍然具有挑战。<br>
                    方法：我们提出了一种新颖的自底向上的方法，从航空图像中进行车道图估计，该方法将多个重叠的图形聚合成一个单一的一致图形。由于其模块化设计，我们的方法可以解决两个互补的任务：使用图神经网络从任意车辆位置预测自我相关的后继车道图，并将这些预测聚合成一致的全局车道图。<br>
                    效果：我们在大规模的车道图数据集上进行了广泛的实验，证明我们的方法能够产生非常准确的车道图，即使在严重遮挡的区域也是如此。所提出的图形聚合方法证明可以消除不一致的预测，同时提高整体图形质量。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Lane graph estimation is an essential and highly challenging task in automated driving and HD map learning. Existing methods using either onboard or aerial imagery struggle with complex lane topologies, out-of-distribution scenarios, or significant occlusions in the image space. Moreover, merging overlapping lane graphs to obtain consistent largescale graphs remains difficult. To overcome these challenges, we propose a novel bottom-up approach to lane graph estimation from aerial imagery that aggregates multiple overlapping graphs into a single consistent graph. Due to its modular design, our method allows us to address two complementary tasks: predicting ego-respective successor lane graphs from arbitrary vehicle positions using a graph neural network and aggregating these predictions into a consistent global lane graph. Extensive experiments on a large-scale lane graph dataset demonstrate that our approach yields highly accurate lane graphs, even in regions with severe occlusions. The presented approach to graph aggregation proves to eliminate inconsistent predictions while increasing the overall graph quality. We make our large-scale urban lane graph dataset and code publicly available at http://urbanlanegraph.cs.uni-freiburg.de.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2267.Habitat-Matterport 3D Semantics Dataset</span><br>
                <span class="as">Yadav, KarmeshandRamrakhya, RamandRamakrishnan, SanthoshKumarandGervet, TheoandTurner, JohnandGokaslan, AaronandMaestre, NoahandChang, AngelXuanandBatra, DhruvandSavva, ManolisandClegg, AlexanderWilliamandChaplot, DevendraSingh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4927-4936.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种可以充分利用词汇、句法和知识信息的语言表示模型。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，需要通过外部知识来增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，开发出ERNIE模型。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present the Habitat-Matterport 3D Semantics (HM3DSEM) dataset. HM3DSEM is the largest dataset of 3D real-world spaces with densely annotated semantics that is currently available to the academic community. It consists of 142,646 object instance annotations across 216 3D spaces and 3,100 rooms within those spaces. The scale, quality, and diversity of object annotations far exceed those of prior datasets. A key difference setting apart HM3DSEM from other datasets is the use of texture information to annotate pixel-accurate object boundaries. We demonstrate the effectiveness of HM3DSEM dataset for the Object Goal Navigation task using different methods. Policies trained using HM3DSEM perform outperform those trained on prior datasets. Introduction of HM3DSEM in the Habitat ObjectNav Challenge lead to an increase in participation from 400 submissions in 2021 to 1022 submissions in 2022. Project page: https://aihabitat.org/datasets/hm3d-semantics/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2268.Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation</span><br>
                <span class="as">Gao, ChenandPeng, XingyuandYan, MiandWang, HeandYang, LirongandRen, HaibingandLi, HongshengandLiu, Si</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14911-14920.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉-语言导航（VLN）任务中，现有的单步规划方案不适用于分层导航过程的问题。<br>
                    动机：在VLN任务中，导航过程需要自适应地设置和实现一系列子目标，这是一个自然的分层导航过程。然而，现有的方法采用单步规划方案，即在每一步都直接执行导航动作，这不适合这种分层导航过程。<br>
                    方法：本文提出了一种自适应区域感知的分层规划器（AZHP），将导航过程明确分为两个异构阶段，即通过区域划分/选择设置子目标（高层动作）和执行子目标（低层动作）。具体来说，AZHP通过设计的状态切换器模块（SSM）异步执行两个级别的动作。对于高层动作，我们设计了一种场景感知的自适应区域划分（SZP）方法，能够实时地将整个导航区域划分为不同的区域。然后，我们提出了一种面向目标的区域选择（GZS）方法，用于为当前子目标选择一个合适的区域。对于低层动作，代理在选定的区域中进行多步导航决策。此外，我们还设计了一种分层强化学习（HRL）策略和辅助损失函数，结合课程学习来训练AZHP框架，为每个阶段提供有效的监督信号。<br>
                    效果：大量的实验表明，我们提出的方法具有优越性，在三个视觉-语言导航基准测试（REVERIE、SOON、R2R）上实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The task of Vision-Language Navigation (VLN) is for an embodied agent to reach the global goal according to the instruction. Essentially, during navigation, a series of sub-goals need to be adaptively set and achieved, which is naturally a hierarchical navigation process. However, previous methods leverage a single-step planning scheme, i.e., directly performing navigation action at each step, which is unsuitable for such a hierarchical navigation process. In this paper, we propose an Adaptive Zone-aware Hierarchical Planner (AZHP) to explicitly divides the navigation process into two heterogeneous phases, i.e., sub-goal setting via zone partition/selection (high-level action) and sub-goal executing (low-level action), for hierarchical planning. Specifically, AZHP asynchronously performs two levels of action via the designed State-Switcher Module (SSM). For high-level action, we devise a Scene-aware adaptive Zone Partition (SZP) method to adaptively divide the whole navigation area into different zones on-the-fly. Then the Goal-oriented Zone Selection (GZS) method is proposed to select a proper zone for the current sub-goal. For low-level action, the agent conducts navigation-decision multi-steps in the selected zone. Moreover, we design a Hierarchical RL (HRL) strategy and auxiliary losses with curriculum learning to train the AZHP framework, which provides effective supervision signals for each stage. Extensive experiments demonstrate the superiority of our proposed method, which achieves state-of-the-art performance on three VLN benchmarks (REVERIE, SOON, R2R).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2269.GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</span><br>
                <span class="as">Geng, HaoranandXu, HelinandZhao, ChengyangandXu, ChaoandYi, LiandHuang, SiyuanandWang, He</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7081-7091.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过学习跨类别技能来提高物体感知和操作的泛化能力？<br>
                    动机：目前对于可泛化的物体感知和操作的研究还处于初级阶段，而跨类别的泛化能力是研究者所追求但尚未充分探索的。<br>
                    方法：提出通过“可泛化且可操作的部分”（GAParts）来学习这种跨类别技能。在27个物体类别中识别并定义了9种GAParts类（如盖子、手柄等），构建了一个大规模的以部分为中心的交互式数据集GAPartNet，并对其中的8489个部分实例进行了丰富的部分级标注（语义、姿态）。<br>
                    效果：基于GAPartNet，我们研究了三个跨类别任务：部分分割、部分姿态估计和基于部分的对象操作。由于已见和未见的物体类别之间存在显著的领域差距，我们提出了一种从领域泛化的角度出发的鲁棒3D分割方法，该方法整合了对抗性学习技术，无论在已见还是未见的类别上，都大大超过了所有现有方法的性能。此外，我们还利用部分分割和姿态估计的结果，借助GAPart的姿态定义设计出了能够良好泛化到未见物体类别的部分基操控策略，无论是在模拟器还是在真实世界中都表现出良好的效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For years, researchers have been devoted to generalizable object perception and manipulation, where cross-category generalizability is highly desired yet underexplored. In this work, we propose to learn such cross-category skills via Generalizable and Actionable Parts (GAParts). By identifying and defining 9 GAPart classes (lids, handles, etc.) in 27 object categories, we construct a large-scale part-centric interactive dataset, GAPartNet, where we provide rich, part-level annotations (semantics, poses) for 8,489 part instances on 1,166 objects. Based on GAPartNet, we investigate three cross-category tasks: part segmentation, part pose estimation, and part-based object manipulation. Given the significant domain gaps between seen and unseen object categories, we propose a robust 3D segmentation method from the perspective of domain generalization by integrating adversarial learning techniques. Our method outperforms all existing methods by a large margin, no matter on seen or unseen categories. Furthermore, with part segmentation and pose estimation results, we leverage the GAPart pose definition to design part-based manipulation heuristics that can generalize well to unseen object categories in both the simulator and the real world.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2270.OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation</span><br>
                <span class="as">Wu, TongandZhang, JiaruiandFu, XiaoandWang, YuxinandRen, JiaweiandPan, LiangandWu, WayneandYang, LeiandWang, JiaqiandQian, ChenandLin, DahuaandLiu, Ziwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/803-814.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决目前3D对象建模主要依赖合成数据集的问题，以促进真实世界中的3D感知、重建和生成的发展。<br>
                    动机：由于缺乏大规模的真实扫描3D数据库，现有的3D对象建模方法大多依赖于合成数据集。为了解决这个问题，我们提出了OmniObject3D，这是一个大型的、高质量的真实扫描3D对象数据集。<br>
                    方法：我们使用专业的扫描仪对6000个物体进行扫描，每个物体都提供了纹理网格、点云、多视角渲染图像和多个真实捕获的视频。我们还设置了四个评估轨道：a) 鲁棒的3D感知，b) 新视图合成，c) 神经表面重建，d) 3D对象生成。<br>
                    效果：实验结果表明，OmniObject3D在各种基准测试中表现出色，为未来的现实3D视觉研究提供了新的观察、挑战和机会。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in modeling 3D objects mostly rely on synthetic datasets due to the lack of large-scale real-scanned 3D databases. To facilitate the development of 3D perception, reconstruction, and generation in the real world, we propose OmniObject3D, a large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects. OmniObject3D has several appealing properties: 1) Large Vocabulary: It comprises 6,000 scanned objects in 190 daily categories, sharing common classes with popular 2D datasets (e.g., ImageNet and LVIS), benefiting the pursuit of generalizable 3D representations. 2) Rich Annotations: Each 3D object is captured with both 2D and 3D sensors, providing textured meshes, point clouds, multiview rendered images, and multiple real-captured videos. 3) Realistic Scans: The professional scanners support high-quality object scans with precise shapes and realistic appearances. With the vast exploration space offered by OmniObject3D, we carefully set up four evaluation tracks: a) robust 3D perception, b) novel-view synthesis, c) neural surface reconstruction, and d) 3D object generation. Extensive studies are performed on these four benchmarks, revealing new observations, challenges, and opportunities for future research in realistic 3D vision.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2271.Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking</span><br>
                <span class="as">Pang, ZiqiandLi, JieandTokmakov, PavelandChen, DianandZagoruyko, SergeyandWang, Yu-Xiong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Standing_Between_Past_and_Future_Spatio-Temporal_Modeling_for_Multi-Camera_3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17928-17938.png><br>
            
            <span class="tt"><span class="t0">研究问题：提出一种端到端的多摄像头3D多目标跟踪（MOT）框架。<br>
                    动机：强调空间-时间连续性，整合过去和未来的推理对被追踪的对象。<br>
                    方法：采用"注意力跟踪"框架，通过对象查询来连贯地表示被追踪的实例在时间上。"过去推理"模块通过交叉关注前几帧和其他对象的查询来精炼轨迹和增强对象特征；"未来推理"模块消化历史信息并预测稳健的未来轨迹。<br>
                    效果：在nuScenes数据集上，该方法大幅度提高了AMOTA，并且与先前的方法相比，显著减少了90%的ID切换，这是一个数量级的差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an order of magnitude less. The code and models are made available at https://github.com/TRI-ML/PF-Track.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2272.Tracking Through Containers and Occluders in the Wild</span><br>
                <span class="as">VanHoorick, BasileandTokmakov, PavelandStent, SimonandLi, JieandVondrick, Carl</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Van_Hoorick_Tracking_Through_Containers_and_Occluders_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13802-13812.png><br>
            
            <span class="tt"><span class="t0">研究问题：在杂乱和动态的环境中，通过重度遮挡和包含进行视觉跟踪仍然是一个困难的计算机视觉系统挑战。<br>
                    动机：为了解决这一问题，我们引入了TCOW，这是一个新的基准和模型，用于通过重度遮挡和包含进行视觉跟踪。<br>
                    方法：我们设置了一个任务，目标是给定一个视频序列，对目标对象的投影范围以及存在的任何包围器或遮挡物进行分割。为此，我们创建了一个混合的合成和注释的真实数据集，以支持各种形式的任务变化下的有监督学习和模型性能的结构评估。<br>
                    效果：我们对两种最新的基于变压器的视频模型进行了评估，发现尽管它们在某些任务变化设置下能够出色地跟踪目标，但在我们可以宣称跟踪模型已经获得了对象持久性的真实概念之前，仍然存在相当大的性能差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce TCOW, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2273.LANA: A Language-Capable Navigator for Instruction Following and Generation</span><br>
                <span class="as">Wang, XiaohanandWang, WenguanandShao, JiayiandYang, Yi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_LANA_A_Language-Capable_Navigator_for_Instruction_Following_and_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19048-19058.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何使机器人代理不仅能够执行人类编写的导航指令，还能为人类提供路线描述。<br>
                    动机：现有的VLN研究主要关注将指令解释为行动，只提供了“哑巴”的寻路代理。<br>
                    方法：设计LANA，一种具有语言能力的导航代理，通过一个模型同时学习指令跟随和生成。具体来说，建立了两个共享的编码器（分别用于路线和语言编码）和两个解码器（分别用于动作预测和指令生成），以利用跨任务知识和捕获特定任务的特性。在预训练和微调过程中，将指令跟随和生成都设置为优化目标。<br>
                    效果：实验证明，与最新的专用解决方案相比，LANA在指令跟随和路线描述方面都取得了更好的性能，且复杂度降低了近一半。此外，由于具备语言生成能力，LANA可以向人类解释其行为并协助人类的寻路。这项工作有望推动未来构建更可信、更具社会智能的导航机器人的努力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, visual-language navigation (VLN) -- entailing robot agents to follow navigation instructions -- has shown great advance. However, existing literature put most emphasis on interpreting instructions into actions, only delivering "dumb" wayfinding agents. In this article, we devise LANA, a language-capable navigation agent which is able to not only execute human-written navigation commands, but also provide route descriptions to humans. This is achieved by simultaneously learning instruction following and generation with only one single model. More specifically, two encoders, respectively for route and language encoding, are built and shared by two decoders, respectively, for action prediction and instruction generation, so as to exploit cross-task knowledge and capture task-specific characteristics. Throughout pretraining and fine-tuning, both instruction following and generation are set as optimization objectives. We empirically verify that, compared with recent advanced task-specific solutions, LANA attains better performances on both instruction following and route description, with nearly half complexity. In addition, endowed with language generation capability, LANA can explain to humans its behaviors and assist human's wayfinding. This work is expected to foster future efforts towards building more trustworthy and socially-intelligent navigation robots. Our code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2274.StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments</span><br>
                <span class="as">Kulinski, SeanandWaytowich, NicholasR.andHare, JamesZ.andInouye, DavidI.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kulinski_StarCraftImage_A_Dataset_for_Prototyping_Spatial_Reasoning_Methods_for_Multi-Agent_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22004-22013.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在多智能体环境中进行空间推理任务，如事件预测、代理类型识别或缺失数据填充。<br>
                    动机：在多智能体环境（如星际争霸II）中的空间推理任务对许多应用（如自主监视传感器网络和强化学习子任务）至关重要，但提取简单的标准表示形式以原型化这些任务既费力又阻碍了可重复性。<br>
                    方法：研究人员从60,000个游戏回放中精心总结了一个255个连续游戏状态的窗口，创建了360万个摘要图像，包括所有相关元数据，如游戏结果和玩家种族。他们开发了三种复杂度递减的格式：类似于多光谱地理空间图像的超光谱图像、模仿CIFAR10的RGB图像和模仿MNIST的灰度图像。<br>
                    效果：这个数据集可以用于原型化空间推理方法，为多智能体环境中的空间推理任务提供了一个易于使用且具有挑战性的基准。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Spatial reasoning tasks in multi-agent environments such as event prediction, agent type identification, or missing data imputation are important for multiple applications (e.g., autonomous surveillance over sensor networks and subtasks for reinforcement learning (RL)). StarCraft II game replays encode intelligent (and adversarial) multi-agent behavior and could provide a testbed for these tasks; however, extracting simple and standardized representations for prototyping these tasks is laborious and hinders reproducibility. In contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled rapid prototyping and reproducibility of ML methods. Following the simplicity of these datasets, we construct a benchmark spatial reasoning dataset based on StarCraft II replays that exhibit complex multi-agent behaviors, while still being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize a window of 255 consecutive game states to create 3.6 million summary images from 60,000 replays, including all relevant metadata such as game outcome and player races. We develop three formats of decreasing complexity: Hyperspectral images that include one channel for every unit type (similar to multispectral geospatial images), RGB images that mimic CIFAR10, and grayscale images that mimic MNIST. We show how this dataset can be used for prototyping spatial reasoning methods. All datasets, code for extraction, and code for dataset loading can be found at https://starcraftdata.davidinouye.com/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2275.Bi-LRFusion: Bi-Directional LiDAR-Radar Fusion for 3D Dynamic Object Detection</span><br>
                <span class="as">Wang, YingjieandDeng, JiajunandLi, YaoandHu, JinshuiandLiu, CongandZhang, YuandJi, JianminandOuyang, WanliandZhang, Yanyong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Bi-LRFusion_Bi-Directional_LiDAR-Radar_Fusion_for_3D_Dynamic_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13394-13403.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地结合激光雷达（LiDAR）和雷达（Radar）的感知方式，以改进特征表示。<br>
                    动机：尽管激光雷达和雷达是两种互补的传感方法，但如何将它们有效结合以提高特征表示仍然不清楚。主要挑战在于雷达数据极度稀疏且缺乏高度信息。<br>
                    方法：本文提出了一种双向激光雷达-雷达融合框架（Bi-LRFusion），通过两步来解决挑战并提高动态物体的3D检测性能。首先，通过从激光雷达分支学习重要细节来丰富雷达的局部特征，以缓解由于缺乏高度信息和极度稀疏性导致的问题；其次，在统一的鸟瞰视角表示中将激光雷达特征与增强的雷达特征相结合。<br>
                    效果：在nuScenes和ORR数据集上进行了大量实验，结果显示我们的Bi-LRFusion在检测动态物体方面取得了最先进的性能。值得注意的是，这两个数据集中的雷达数据格式不同，这表明了我们的方法具有通用性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR and Radar are two complementary sensing approaches in that LiDAR specializes in capturing an object's 3D shape while Radar provides longer detection ranges as well as velocity hints. Though seemingly natural, how to efficiently combine them for improved feature representation is still unclear. The main challenge arises from that Radar data are extremely sparse and lack height information. Therefore, directly integrating Radar features into LiDAR-centric detection networks is not optimal. In this work, we introduce a bi-directional LiDAR-Radar fusion framework, termed Bi-LRFusion, to tackle the challenges and improve 3D detection for dynamic objects. Technically, Bi-LRFusion involves two steps: first, it enriches Radar's local features by learning important details from the LiDAR branch to alleviate the problems caused by the absence of height information and extreme sparsity; second, it combines LiDAR features with the enhanced Radar features in a unified bird's-eye-view representation. We conduct extensive experiments on nuScenes and ORR datasets, and show that our Bi-LRFusion achieves state-of-the-art performance for detecting dynamic objects. Notably, Radar data in these two datasets have different formats, which demonstrates the generalizability of our method. Codes will be published.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2276.BioNet: A Biologically-Inspired Network for Face Recognition</span><br>
                <span class="as">Li, Pengyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BioNet_A_Biologically-Inspired_Network_for_Face_Recognition_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10344-10354.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用最新的神经科学发现来提升人脸识别的性能。<br>
                    动机：尽管已有一些计算机视觉的研究试图通过增强人脸属性来提高人脸识别性能，但这些方法并未受到人类面部识别机制的启发，也没有显著提高性能。<br>
                    方法：我们设计了一个名为BioNet的生物启发网络，该网络由视觉皮层网络（VCN）和颞下皮质网络（ICN）两个级联子网络组成。VCN采用经典的卷积神经网络作为主干，而ICN则包含三个生物启发模块：皮层功能区隔化、区隔反应转换和反应强度调制。<br>
                    效果：实验证明，1) 最新的关于人类面部识别系统的研究发现可以进一步推动基于CNN的人脸识别网络的发展；2) 利用生物机制，与身份相关的属性（如性别）和与身份无关的属性（如表情）都可以使深度人脸识别模型受益，其中与身份无关的属性的贡献甚至更大；3) 我们提出的BioNet在标准的人脸识别基准数据集上显著提高了最先进的技术水平。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recently, whether and how cutting-edge Neuroscience findings can inspire Artificial Intelligence (AI) confuse both communities and draw much discussion. As one of the most critical fields in AI, Computer Vision (CV) also pays much attention to the discussion. To show our ideas and experimental evidence to the discussion, we focus on one of the most broadly researched topics both in Neuroscience and CV fields, i.e., Face Recognition (FR). Neuroscience studies show that face attributes are essential to the human face-recognizing system. How the attributes contribute also be explained by the Neuroscience community. Even though a few CV works improved the FR performance with attribute enhancement, none of them are inspired by the human face-recognizing mechanism nor boosted performance significantly. To show our idea experimentally, we model the biological characteristics of the human face-recognizing system with classical Convolutional Neural Network Operators (CNN Ops) purposely. We name the proposed Biologically-inspired Network as BioNet. Our BioNet consists of two cascade sub-networks, i.e., the Visual Cortex Network (VCN) and the Inferotemporal Cortex Network (ICN). The VCN is modeled with a classical CNN backbone. The proposed ICN comprises three biologically-inspired modules, i.e., the Cortex Functional Compartmentalization, the Compartment Response Transform, and the Response Intensity Modulation. The experiments prove that: 1) The cutting-edge findings about the human face-recognizing system can further boost the CNN-based FR network. 2) With the biological mechanism, both identity-related attributes (e.g., gender) and identity-unrelated attributes (e.g., expression) can benefit the deep FR models. Surprisingly, the identity-unrelated ones contribute even more than the identity-related ones. 3) The proposed BioNet significantly boosts state-of-the-art on standard FR benchmark datasets. For example, BioNet boosts IJB-B@1e-6 from 52.12% to 68.28% and MegaFace from 98.74% to 99.19%. The source code will be released.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2277.Visual-Tactile Sensing for In-Hand Object Reconstruction</span><br>
                <span class="as">Xu, WenqiangandYu, ZhenjunandXue, HanandYe, RuolinandYao, SiqiongandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8803-8812.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用触感传感器进行视觉-触觉学习，实现手部和物体的重建？<br>
                    动机：触感是人类感知世界的重要方式之一，结合视觉可以精细化局部几何结构，测量接触区域的形变，并指示手-物体接触状态。开源触感传感器如DIGIT的出现，使得视觉-触觉学习的研究变得更易获取和复制。<br>
                    方法：我们提出了一种新的视觉-触觉手持物体重建框架VTacO，并将其扩展到VTacOH以进行手部和物体的重建。由于我们的方法可以支持刚性和可变形物体的重建，并且没有现有的基准适合这个目标，因此我们提出了一个模拟环境VT-Sim，它可以生成刚性和可变形物体的手部-物体交互。<br>
                    效果：大量的实验表明，我们提出的方法在定性和定量上都能超越先前的基线方法。最后，我们将在模拟中训练的模型直接应用于各种真实世界的测试案例，展示了定性的结果。代码、模型、模拟环境和数据集将公开发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Tactile sensing is one of the modalities human rely on heavily to perceive the world. Working with vision, this modality refines local geometry structure, measures deformation at contact area, and indicates hand-object contact state. With the availability of open-source tactile sensors such as DIGIT, research on visual-tactile learning is becoming more accessible and reproducible. Leveraging this tactile sensor, we propose a novel visual-tactile in-hand object reconstruction framework VTacO, and extend it to VTacOH for hand-object reconstruction. Since our method can support both rigid and deformable object reconstruction, and no existing benchmark are proper for the goal. We propose a simulation environment, VT-Sim, which supports to generate hand-object interaction for both rigid and deformable objects. With VT-Sim, we generate a large-scale training dataset, and evaluate our method on it. Extensive experiments demonstrate that our proposed method can outperform the previous baseline methods qualitatively and quantitatively. Finally, we directly apply our model trained in simulation to various real-world test cases, which display qualitative results. Codes, models, simulation environment, datasets will be publicly available.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2278.FJMP: Factorized Joint Multi-Agent Motion Prediction Over Learned Directed Acyclic Interaction Graphs</span><br>
                <span class="as">Rowe, LukeandEthier, MartinandDykhne, Eli-HenryandCzarnecki, Krzysztof</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rowe_FJMP_Factorized_Joint_Multi-Agent_Motion_Prediction_Over_Learned_Directed_Acyclic_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13745-13755.png><br>
            
            <span class="tt"><span class="t0">研究问题：预测多智能体驾驶场景中道路参与者的未来运动是一项关键任务。<br>
                    动机：在多智能体交互驾驶场景中，生成一组场景级别的未来轨迹预测。<br>
                    方法：提出FJMP，一种因子化联合运动预测框架，将未来场景互动动态建模为稀疏有向交互图，然后将其剪枝成有向无环图，并根据有向无环图的偏序关系将联合预测任务分解为一系列边际和条件预测。<br>
                    效果：在INTERACTION和Argoverse 2数据集上进行实验，证明FJMP比非因子化方法产生更准确、更一致的场景联合轨迹预测，特别是在最具交互性和运动学的代理上。FJMP在INTERACTION数据集的多代理测试排行榜上排名第一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting the future motion of road agents is a critical task in an autonomous driving pipeline. In this work, we address the problem of generating a set of scene-level, or joint, future trajectory predictions in multi-agent driving scenarios. To this end, we propose FJMP, a Factorized Joint Motion Prediction framework for multi-agent interactive driving scenarios. FJMP models the future scene interaction dynamics as a sparse directed interaction graph, where edges denote explicit interactions between agents. We then prune the graph into a directed acyclic graph (DAG) and decompose the joint prediction task into a sequence of marginal and conditional predictions according to the partial ordering of the DAG, where joint future trajectories are decoded using a directed acyclic graph neural network (DAGNN). We conduct experiments on the INTERACTION and Argoverse 2 datasets and demonstrate that FJMP produces more accurate and scene-consistent joint trajectory predictions than non-factorized approaches, especially on the most interactive and kinematically interesting agents. FJMP ranks 1st on the multi-agent test leaderboard of the INTERACTION dataset.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2279.Probing Neural Representations of Scene Perception in a Hippocampally Dependent Task Using Artificial Neural Networks</span><br>
                <span class="as">Frey, MarkusandDoeller, ChristianF.andBarry, Caswell</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Frey_Probing_Neural_Representations_of_Scene_Perception_in_a_Hippocampally_Dependent_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2113-2121.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的深度神经网络在解释高级皮层区域表示的能力相对较弱，特别是在从自我中心到他者中心的转换上。<br>
                    动机：为了解决这一问题，研究人员设计了一种新场景感知基准测试，以探索深度神经网络将不同自我中心视角的场景进行转换的能力。<br>
                    方法：研究人员使用了一种受海马体和颞叶结构之间连接启发的网络架构，并使用三元组损失进行训练，同时通过强制分解潜在空间，将信息传播分为“什么”和“哪里”的路径，用于重建输入。<br>
                    效果：实验结果表明，这种方法在无监督物体分割任务上超越了现有技术，并在CATER和MOVi-A,B,C基准测试上取得了显著改进。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep artificial neural networks (DNNs) trained through backpropagation provide effective models of the mammalian visual system, accurately capturing the hierarchy of neural responses through primary visual cortex to inferior temporal cortex (IT). However, the ability of these networks to explain representations in higher cortical areas is relatively lacking and considerably less well researched. For example, DNNs have been less successful as a model of the egocentric to allocentric transformation embodied by circuits in retrosplenial and posterior parietal cortex. We describe a novel scene perception benchmark inspired by a hippocampal dependent task, designed to probe the ability of DNNs to transform scenes viewed from different egocentric perspectives. Using a network architecture inspired by the connectivity between temporal lobe structures and the hippocampus, we demonstrate that DNNs trained using a triplet loss can learn this task. Moreover, by enforcing a factorized latent space, we can split information propagation into "what" and "where" pathways, which we use to reconstruct the input. This allows us to beat the state-of-the-art for unsupervised object segmentation on the CATER and MOVi-A,B,C benchmarks.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2280.V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting</span><br>
                <span class="as">Yu, HaibaoandYang, WenxianandRuan, HongzhiandYang, ZhenweiandTang, YingjuanandGao, XuandHao, XinandShi, YifengandPan, YifengandSun, NingandSong, JuanandYuan, JiruiandLuo, PingandNie, Zaiqing</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_V2X-Seq_A_Large-Scale_Sequential_Dataset_for_Vehicle-Infrastructure_Cooperative_Perception_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5486-5495.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用基础设施和车辆侧信息跟踪和预测周围交通参与者的行为，以提高自动驾驶的决策和安全性。<br>
                    动机：缺乏真实世界的序列数据集限制了这一领域的研究。<br>
                    方法：引入V2X-Seq，首个大规模的序列V2X数据集，包括从自然场景中捕获的数据帧、轨迹、矢量地图和交通灯。V2X-Seq包含两部分：顺序感知数据集（包含从95个场景中捕获的15000多个帧）和轨迹预测数据集（包含从28个交叉口区域捕获的约8万个基础设施视角、8万个车辆视角和5万个合作视角的场景，覆盖672小时的数据）。基于V2X-Seq，提出了三个新的车辆基础设施协同（VIC）自动驾驶任务：VIC3D跟踪、在线VIC预测和离线VIC预测。同时提供了这些任务的基准测试。<br>
                    效果：实验结果表明，新提出的任务在提高自动驾驶决策和安全性方面具有显著效果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traffic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios captured from 28 intersections' areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driving: VIC3D Tracking, Online-VIC Forecasting, and Offline-VIC Forecasting. We also provide benchmarks for the introduced tasks. Find data, code, and more up-to-date information at https://github.com/AIR-THU/DAIR-V2X-Seq.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2281.3D Video Object Detection With Learnable Object-Centric Global Optimization</span><br>
                <span class="as">He, JiaweiandChen, YuntaoandWang, NaiyanandZhang, Zhaoxiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_3D_Video_Object_Detection_With_Learnable_Object-Centric_Global_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5106-5115.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索长期时间视觉对应关系优化在3D视频目标检测中的应用。<br>
                    动机：现有的3D视频目标检测方法主要依赖于2D图像信息，忽略了物体在时间和空间上的连续性。而视觉对应关系可以建立多个图像之间的像素级一对一映射，是3D场景重建的基础。<br>
                    方法：本文提出了BA-Det模型，通过对象为中心的时间对应关系学习和特征度量对象束调整进行端到端优化的目标检测器。<br>
                    效果：实验结果表明，BA-Det在多种基准3D检测器上在不同设置下均表现出了高效和有效，并在大规模Waymo开放数据集（WOD）上取得了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We explore long-term temporal visual correspondence-based optimization for 3D video object detection in this work. Visual correspondence refers to one-to-one mappings for pixels across multiple images. Correspondence-based optimization is the cornerstone for 3D scene reconstruction but is less studied in 3D video object detection, because moving objects violate multi-view geometry constraints and are treated as outliers during scene reconstruction. We address this issue by treating objects as first-class citizens during correspondence-based optimization. In this work, we propose BA-Det, an end-to-end optimizable object detector with object-centric temporal correspondence learning and featuremetric object bundle adjustment. Empirically, we verify the effectiveness and efficiency of BA-Det for multiple baseline 3D detectors under various setups. Our BA-Det achieves SOTA performance on the large-scale Waymo Open Dataset (WOD) with only marginal computation cost. Our code is available at https://github.com/jiaweihe1996/BA-Det.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2282.Imitation Learning As State Matching via Differentiable Physics</span><br>
                <span class="as">Chen, SiweiandMa, XiaoandXu, Zhongwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Imitation_Learning_As_State_Matching_via_Differentiable_Physics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/7846-7855.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的模仿学习（IL）方法，如逆强化学习（IRL），通常具有双循环训练过程，交替学习奖励函数和策略，往往导致训练时间长、方差大的问题。<br>
                    动机：本文提出了一种新的模仿学习方法，即通过可微分物理（ILD）进行模仿学习，该方法消除了双循环设计，并在最终性能、收敛速度和稳定性方面取得了显著改进。<br>
                    方法：ILD将可微分物理模拟器作为物理先验纳入其计算图中进行策略学习。它通过从参数化策略中采样动作来展开动力学，简单地最小化专家轨迹和代理轨迹之间的距离，并通过时间物理运算符向后传播梯度到策略中。有了物理先验，ILD策略不仅可以转移到未见过的环境规范，而且在各种任务上也能产生更高的最终性能。此外，ILD自然地形成了单循环结构，显著提高了稳定性和训练速度。为了简化由时间物理运算符引起的复杂优化景观，ILD在优化过程中为每个状态动态选择学习目标。<br>
                    效果：实验表明，ILD在一系列连续控制任务中优于最先进的方法，只需要一个专家演示。此外，ILD可以应用于具有挑战性的变形物体操纵任务，并可以推广到未见过的配置。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing imitation learning (IL) methods such as inverse reinforcement learning (IRL) usually have a double-loop training process, alternating between learning a reward function and a policy and tend to suffer long training time and high variance. In this work, we identify the benefits of differentiable physics simulators and propose a new IL method, i.e., Imitation Learning via Differentiable Physics (ILD), which gets rid of the double-loop design and achieves significant improvements in final performance, convergence speed, and stability. The proposed ILD incorporates the differentiable physics simulator as a physics prior into its computational graph for policy learning. It unrolls the dynamics by sampling actions from a parameterized policy, simply minimizing the distance between the expert trajectory and the agent trajectory, and back-propagating the gradient into the policy via temporal physics operators. With the physics prior, ILD policies can not only be transferable to unseen environment specifications but also yield higher final performance on a variety of tasks. In addition, ILD naturally forms a single-loop structure, which significantly improves the stability and training speed. To simplify the complex optimization landscape induced by temporal physics operations, ILD dynamically selects the learning objectives for each state during optimization. In our experiments, we show that ILD outperforms state-of-the-art methods in a variety of continuous control tasks with Brax, requiring only one expert demonstration. In addition, ILD can be applied to challenging deformable object manipulation tasks and can be generalized to unseen configurations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2283.Critical Learning Periods for Multisensory Integration in Deep Networks</span><br>
                <span class="as">Kleinman, MichaelandAchille, AlessandroandSoatto, Stefano</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kleinman_Critical_Learning_Periods_for_Multisensory_Integration_in_Deep_Networks_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/24296-24305.png><br>
            
            <span class="tt"><span class="t0">研究问题：神经网络整合多元信息的能力在训练早期阶段是否受到适当关联信号的影响。<br>
                    动机：干扰学习过程的初始阶段可能会永久损害技能的发展，这种现象在人工和生物系统中被称为关键学习期。<br>
                    方法：通过分析广泛和浅层的网络，研究了深度线性网络在多源整合中的关键学习期，同时比较了深层和浅层网络的差异。<br>
                    效果：研究发现，引入跨传感器重建目标的训练架构对关键学习期的抗性显著提高。这可能部分解释了最近自我监督多模态训练相对于以前有监督努力的成功。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We show that the ability of a neural network to integrate information from diverse sources hinges critically on being exposed to properly correlated signals during the early phases of training. Interfering with the learning process during this initial stage can permanently impair the development of a skill, both in artificial and biological systems where the phenomenon is known as a critical learning period. We show that critical periods arise from the complex and unstable early transient dynamics, which are decisive of final performance of the trained system and their learned representations. This evidence challenges the view, engendered by analysis of wide and shallow networks, that early learning dynamics of neural networks are simple, akin to those of a linear model. Indeed, we show that even deep linear networks exhibit critical learning periods for multi-source integration, while shallow networks do not. To better understand how the internal representations change according to disturbances or sensory deficits, we introduce a new measure of source sensitivity, which allows us to track the inhibition and integration of sources during training. Our analysis of inhibition suggests cross-source reconstruction as a natural auxiliary training objective, and indeed we show that architectures trained with cross-sensor reconstruction objectives are remarkably more resilient to critical periods. Our findings suggest that the recent success in self-supervised multi-modal training compared to previous supervised efforts may be in part due to more robust learning dynamics and not solely due to better architectures and/or more data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2284.GarmentTracking: Category-Level Garment Pose Tracking</span><br>
                <span class="as">Xue, HanandXu, WenqiangandZhang, JieyiandTang, TutianandLi, YutongandDu, WenxinandYe, RuolinandLu, Cewu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_GarmentTracking_Category-Level_Garment_Pose_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21233-21242.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种能够估计和跟踪完整服装姿态的视觉系统，以解决各种下游任务和实际应用。<br>
                    动机：由于服装对人类的重要性，一个可以准确估计和跟踪服装姿态的系统具有广泛的应用前景。<br>
                    方法：我们提出了一个完整的解决方案，包括一个记录系统VR-Garment，一个大规模的数据集VR-Folding，以及一个端到端的在线跟踪框架GarmentTracking。<br>
                    效果：实验表明，我们提出的GarmentTracking在预测服装姿态方面表现出色，即使在服装有大的非刚性变形时也能保持高速度和高精度，优于基线方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Garments are important to humans. A visual system that can estimate and track the complete garment pose can be useful for many downstream tasks and real-world applications. In this work, we present a complete package to address the category-level garment pose tracking task: (1) A recording system VR-Garment, with which users can manipulate virtual garment models in simulation through a VR interface. (2) A large-scale dataset VR-Folding, with complex garment pose configurations in manipulation like flattening and folding. (3) An end-to-end online tracking framework GarmentTracking, which predicts complete garment pose both in canonical space and task space given a point cloud sequence. Extensive experiments demonstrate that the proposed GarmentTracking achieves great performance even when the garment has large non-rigid deformation. It outperforms the baseline approach on both speed and accuracy. We hope our proposed solution can serve as a platform for future research. Codes and datasets are available in https://garment-tracking.robotflow.ai.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2285.TBP-Former: Learning Temporal Bird&#x27;s-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving</span><br>
                <span class="as">Fang, ShaohengandWang, ZiandZhong, YiqiandGe, JunhaoandChen, Siheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_TBP-Former_Learning_Temporal_Birds-Eye-View_Pyramid_for_Joint_Perception_and_Prediction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1368-1378.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何同步多视角和时间戳的特征，并进一步利用这些空间-时间特征进行视觉为中心的联合感知和预测。<br>
                    动机：由于几何畸变的存在，同步多视角和时间戳的特征以及进一步利用这些空间-时间特征是自动驾驶研究中的一个重要挑战。<br>
                    方法：提出一种基于鸟瞰图金字塔变压器的时间鸟瞰图金字塔变压器（TBP-Former）方法，包括两个创新设计。首先，提出一个姿态同步的BEV编码器，将任意相机姿态和任意时间的原始图像输入映射到一个共享且同步的BEV空间，以实现更好的空间-时间同步。其次，引入一个空间-时间金字塔变压器，全面提取多尺度BEV特征，并在空间先验的支持下预测未来的BEV状态。<br>
                    效果：在nuScenes数据集上的大量实验表明，我们提出的框架总体上优于所有最先进的基于视觉的预测方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-centric joint perception and prediction (PnP) has become an emerging trend in autonomous driving research. It predicts the future states of the traffic participants in the surrounding environment from raw RGB images. However, it is still a critical challenge to synchronize features obtained at multiple camera views and timestamps due to inevitable geometric distortions and further exploit those spatial-temporal features. To address this issue, we propose a temporal bird's-eye-view pyramid transformer (TBP-Former) for vision-centric PnP, which includes two novel designs. First, a pose-synchronized BEV encoder is proposed to map raw image inputs with any camera pose at any time to a shared and synchronized BEV space for better spatial-temporal synchronization. Second, a spatial-temporal pyramid transformer is introduced to comprehensively extract multi-scale BEV features and predict future BEV states with the support of spatial priors. Extensive experiments on nuScenes dataset show that our proposed framework overall outperforms all state-of-the-art vision-based prediction methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2286.Seeing With Sound: Long-range Acoustic Beamforming for Multimodal Scene Understanding</span><br>
                <span class="as">Chakravarthula, PraneethandD{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chakravarthula_Seeing_With_Sound_Long-range_Acoustic_Beamforming_for_Multimodal_Scene_Understanding_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/982-991.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自动驾驶车辆主要依赖电磁波传感器，但在环境恶劣的情况下可能会受到影响，而且只能检测到直接视线内的对象。<br>
                    动机：为了解决这些问题，研究人员提出了一种新的声波压力波束形成方法，作为传统光学传感器的补充，用于检测动态交通环境中的对象。<br>
                    方法：研究人员引入了长距离声学压力波束形成技术，通过汽车在自然环境中产生的噪声直接进行检测。他们创建了第一个多模态长距离声学压力波束形成数据集，并提出了一种新的神经孔径扩展方法进行波束形成。<br>
                    效果：实验结果表明，这种方法在具有挑战性的汽车场景中，可以有效地补充现有的RGB摄像头，提高对象检测的准确性和速度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Existing autonomous vehicles primarily use sensors that rely on electromagnetic waves which are undisturbed in good environmental conditions but can suffer in adverse scenarios, such as low light or for objects with low reflectance. Moreover, only objects in direct line-of-sight are typically detected by these existing methods. Acoustic pressure waves emanating from road users do not share these limitations. However, such signals are typically ignored in automotive perception because they suffer from low spatial resolution and lack directional information. In this work, we introduce long-range acoustic beamforming of pressure waves from noise directly produced by automotive vehicles in-the-wild as a  complementary sensing modality  to traditional optical sensor approaches for detection of objects in dynamic traffic environments. To this end, we introduce the first multimodal long-range acoustic beamforming dataset. We propose a neural aperture expansion method for beamforming and we validate its utility for multimodal automotive object detection. We validate the benefit of adding sound detections to existing RGB cameras in challenging automotive scenarios, where camera-only approaches fail or do not deliver the ultra-fast rates of pressure sensors.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2287.Neural Map Prior for Autonomous Driving</span><br>
                <span class="as">Xiong, XuanandLiu, YichengandYuan, TianyuanandWang, YueandWang, YilunandZhao, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17535-17544.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用神经网络表示全球地图，实现自动全局地图更新并提高局部地图推断性能。<br>
                    动机：传统的离线高清地图创建过程劳动密集且成本高昂，无法及时更新。而在线传感器观察推断的地图范围受限，易受遮挡影响。<br>
                    方法：提出神经地图先验（NMP），这是一种神经网络表示的全球地图，能自动更新全局地图并提升局部地图推断性能。通过利用交叉注意力动态捕捉当前特征和先前特征之间的关联性，将强大的地图先验融入局部地图推断中。使用学习型融合模块指导网络融合之前遍历的特征，以进行全局神经地图先验的更新。<br>
                    效果：在nuScenes数据集上的实验结果表明，该框架与大多数地图分割/检测方法兼容，并在具有挑战性的天气条件和延长的时间范围内提高了地图预测性能。据我们所知，这是第一个用于构建全局地图先验的学习系统。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>High-definition (HD) semantic maps are a crucial component for autonomous driving on urban streets. Traditional offline HD maps are created through labor-intensive manual annotation processes, which are costly and do not accommodate timely updates. Recently, researchers have proposed to infer local maps based on online sensor observations. However, the range of online map inference is constrained by sensor perception range and is easily affected by occlusions. In this work, we propose Neural Map Prior (NMP), a neural representation of global maps that enables automatic global map updates and enhances local map inference performance. To incorporate the strong map prior into local map inference, we leverage cross-attention to dynamically capture the correlations between current features and prior features. For updating the global neural map prior, we use a learning-based fusion module to guide the network in fusing features from previous traversals. This design allows the network to capture a global neural map prior while making sequential online map predictions. Experimental results on the nuScenes dataset demonstrate that our framework is compatible with most map segmentation/detection methods, improving map prediction performance in challenging weather conditions and over an extended horizon. To the best of our knowledge, this represents the first learning-based system for constructing a global map prior.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2288.PartManip: Learning Cross-Category Generalizable Part Manipulation Policy From Point Cloud Observations</span><br>
                <span class="as">Geng, HaoranandLi, ZimingandGeng, YiranandChen, JiayiandDong, HaoandWang, He</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_PartManip_Learning_Cross-Category_Generalizable_Part_Manipulation_Policy_From_Point_Cloud_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2978-2988.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让实体代理在复杂的真实世界场景中学习到可泛化的对象操作策略。<br>
                    动机：部件作为不同对象类别的共享组件，有可能提高操作策略的泛化能力，实现跨类别的对象操作。<br>
                    方法：构建了首个大规模的基于部件的跨类别对象操作基准PartManip，包含11个对象类别、494个对象和6个任务类别中的1432个任务。通过训练基于状态的专家和使用提出的基于部件的规范化和部件感知奖励，将知识提炼给学生，以解决基于视觉的策略学习难题。同时引入领域对抗学习进行领域不变特征提取，以提高跨类别的泛化能力。<br>
                    效果：实验表明，我们学习的策略在模拟环境中的表现优于其他方法，尤其是在未见过的类别上。同时，该方法也能在真实世界中成功操作新的对象。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Learning a generalizable object manipulation policy is vital for an embodied agent to work in complex real-world scenes. Parts, as the shared components in different object categories, have the potential to increase the generalization ability of the manipulation policy and achieve cross-category object manipulation. In this work, we build the first large-scale, part-based cross-category object manipulation benchmark, PartManip, which is composed of 11 object categories, 494 objects, and 1432 tasks in 6 task classes. Compared to previous work, our benchmark is also more diverse and realistic, i.e., having more objects and using sparse-view point cloud as input without oracle information like part segmentation. To tackle the difficulties of vision-based policy learning, we first train a state-based expert with our proposed part-based canonicalization and part-aware rewards, and then distill the knowledge to a vision-based student. We also find an expressive backbone is essential to overcome the large diversity of different objects. For cross-category generalization, we introduce domain adversarial learning for domain-invariant feature extraction. Extensive experiments in simulation show that our learned policy can outperform other methods by a large margin, especially on unseen object categories. We also demonstrate our method can successfully manipulate novel objects in the real world.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2289.Towards Unsupervised Object Detection From LiDAR Point Clouds</span><br>
                <span class="as">Zhang, LunjunandYang, AnqiJoyceandXiong, YuwenandCasas, SergioandYang, BinandRen, MengyeandUrtasun, Raquel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unsupervised_Object_Detection_From_LiDAR_Point_Clouds_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9317-9328.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文研究了在自动驾驶场景中，如何从3D点云中进行无监督物体检测的问题。<br>
                    动机：目前的无监督物体检测方法存在一些问题，如需要重复遍历同一地点、无法在稀疏和远距离区域进行零样本检测等。<br>
                    方法：本文提出了一种名为OYSTER的方法，该方法利用了（i）点云密集区域的点聚类，（ii）时间一致性来过滤噪声的无监督检测，（iii）CNN的平移等变性来将自动标签扩展到远距离，以及（iv）自我监督以提高自身性能。<br>
                    效果：实验结果表明，OYSTER方法在PandaSet和Argoverse 2 Sensor数据集上显著优于无监督基线，显示出自我监督结合物体先验能够在野外进行物体发现的可能性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we study the problem of unsupervised object detection from 3D point clouds in self-driving scenes. We present a simple yet effective method that exploits (i) point clustering in near-range areas where the point clouds are dense, (ii) temporal consistency to filter out noisy unsupervised detections, (iii) translation equivariance of CNNs to extend the auto-labels to long range, and (iv) self-supervision for improving on its own. Our approach, OYSTER (Object Discovery via Spatio-Temporal Refinement), does not impose constraints on data collection (such as repeated traversals of the same location), is able to detect objects in a zero-shot manner without supervised finetuning (even in sparse, distant regions), and continues to self-improve given more rounds of iterative self-training. To better measure model performance in self-driving scenarios, we propose a new planning-centric perception metric based on distance-to-collision. We demonstrate that our unsupervised object detector significantly outperforms unsupervised baselines on PandaSet and Argoverse 2 Sensor dataset, showing promise that self-supervision combined with object priors can enable object discovery in the wild. For more information, visit the project website: https://waabi.ai/research/oyster.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2290.M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis</span><br>
                <span class="as">Cheng, HiuyiandZhang, PeirongandWu, SihangandZhang, JiaxinandZhu, QiyuanandXie, ZechengandLi, JingandDing, KaiandJin, Lianwen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_Dataset_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15138-15147.png><br>
            
            <span class="tt"><span class="t0">研究问题：当前公开的文档布局分析数据集大多只包含PDF文档，缺乏真实的文档，这可能导致训练的模型无法很好地泛化到真实世界的场景。<br>
                    动机：为了解决这个问题，本文提出了一个名为M^6-Doc的大型多样化文档布局分析数据集，并设计了一种基于transformer的文档布局分析方法TransDLANet。<br>
                    方法：M^6-Doc具有多格式、多类型、多布局、多语言、多注释类别和现代文档等六种特性。TransDLANet利用自适应元素匹配机制优化查询嵌入，提高召回率，并通过构建分割分支进行更精确的文档图像实例分割。<br>
                    效果：通过在M^6-Doc数据集上与各种布局分析方法进行全面评估，实验结果表明TransDLANet取得了最先进的性能，mAP达到了64.5%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Document layout analysis is a crucial prerequisite for document understanding, including document retrieval and conversion. Most public datasets currently contain only PDF documents and lack realistic documents. Models trained on these datasets may not generalize well to real-world scenarios. Therefore, this paper introduces a large and diverse document layout analysis dataset called M^6-Doc. The M^6 designation represents six properties: (1) Multi-Format (including scanned, photographed, and PDF documents); (2) Multi-Type (such as scientific articles, textbooks, books, test papers, magazines, newspapers, and notes); (3) Multi-Layout (rectangular, Manhattan, non-Manhattan, and multi-column Manhattan); (4) Multi-Language (Chinese and English); (5) Multi-Annotation Category (74 types of annotation labels with 237,116 annotation instances in 9,080 manually annotated pages); and (6) Modern documents. Additionally, we propose a transformer-based document layout analysis method called TransDLANet, which leverages an adaptive element matching mechanism that enables query embedding to better match ground truth to improve recall, and constructs a segmentation branch for more precise document image instance segmentation. We conduct a comprehensive evaluation of M^6-Doc with various layout analysis methods and demonstrate its effectiveness. TransDLANet achieves state-of-the-art performance on M^6-Doc with 64.5% mAP. The M^6-Doc dataset will be available at https://github.com/HCIILAB/M6Doc.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2291.Object-Goal Visual Navigation via Effective Exploration of Relations Among Historical Navigation States</span><br>
                <span class="as">Du, HemingandLi, LinchengandHuang, ZiandYu, Xin</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Object-Goal_Visual_Navigation_via_Effective_Exploration_of_Relations_Among_Historical_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2563-2573.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决现有目标导向视觉导航方法中，导航状态的相关性对导航效率和成功率的影响。<br>
                    动机：现有的目标导向视觉导航方法主要关注学习有信息量的视觉表示，但忽视了导航状态对导航效果和效率的影响。<br>
                    方法：本文提出了一种历史启发的导航策略学习（HiNL）框架，通过探索历史导航状态之间的关系来有效估计导航状态。在HiNL中，我们设计了一个历史感知状态估计（HaSE）模块，以减轻主导历史状态对当前状态估计的影响，并鼓励代理对当前观察变化保持警觉，从而做出有效的行动。此外，我们还设计了一种基于历史的状态正则化（HbSR），以在训练过程中明确抑制导航状态之间的相关性。<br>
                    效果：在人工平台AI2-THOR上的实验表明，HiNL在未见过的环境测试中，无论是成功率还是SPL，都显著优于最先进的方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Object-goal visual navigation aims at steering an agent toward an object via a series of moving steps. Previous works mainly focus on learning informative visual representations for navigation, but overlook the impacts of navigation states on the effectiveness and efficiency of navigation. We observe that high relevance among navigation states will cause navigation inefficiency or failure for existing methods. In this paper, we present a History-inspired Navigation Policy Learning (HiNL) framework to estimate navigation states effectively by exploring relationships among historical navigation states. In HiNL, we propose a History-aware State Estimation (HaSE) module to alleviate the impacts of dominant historical states on the current state estimation. Meanwhile, HaSE also encourages an agent to be alert to the current observation changes, thus enabling the agent to make valid actions. Furthermore, we design a History-based State Regularization (HbSR) to explicitly suppress the correlation among navigation states in training. As a result, our agent can update states more effectively while reducing the correlations among navigation states. Experiments on the artificial platform AI2-THOR (i.e.,, iTHOR and RoboTHOR) demonstrate that HiNL significantly outperforms state-of-the-art methods on both Success Rate and SPL in unseen testing environments.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2292.Detecting and Grounding Multi-Modal Media Manipulation</span><br>
                <span class="as">Shao, RuiandWu, TianxingandLiu, Ziwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/6904-6913.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文提出了一种新的多模态假媒体检测和定位问题，即检测和定位多模态媒体操纵（DGM^4）。<br>
                    动机：虚假信息已成为一个紧迫的问题，而现有的深度伪造检测和文本假新闻检测方法只能处理基于二元分类的单一模态伪造，无法分析不同模态间的微妙伪造痕迹。<br>
                    方法：构建了首个DGM^4数据集，并设计了一种新型的分层多模态操纵推理变压器（HAMMER），通过对比学习进行浅层操纵推理，并通过多模态聚合器进行深层操纵推理。<br>
                    效果：实验结果表明，HAMMER模型在多模态假媒体检测和定位问题上表现出优越性，为未来的多模态媒体操纵研究提供了有价值的观察结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Misinformation has become a pressing issue. Fake media, in both visual and textual forms, is widespread on the web. While various deepfake detection and text fake news detection methods have been proposed, they are only designed for single-modality forgery based on binary classification, let alone analyzing and reasoning subtle forgery traces across different modalities. In this paper, we highlight a new research problem for multi-modal fake media, namely Detecting and Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not only detect the authenticity of multi-modal media, but also ground the manipulated content (i.e., image bounding boxes and text tokens), which requires deeper reasoning of multi-modal media manipulation. To support a large-scale investigation, we construct the first DGM^4 dataset, where image-text pairs are manipulated by various approaches, with rich annotation of diverse manipulations. Moreover, we propose a novel HierArchical Multi-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture the fine-grained interaction between different modalities. HAMMER performs 1) manipulation-aware contrastive learning between two uni-modal encoders as shallow manipulation reasoning, and 2) modality-aware cross-attention by multi-modal aggregator as deep manipulation reasoning. Dedicated manipulation detection and grounding heads are integrated from shallow to deep levels based on the interacted multi-modal information. Finally, we build an extensive benchmark and set up rigorous evaluation metrics for this new research problem. Comprehensive experiments demonstrate the superiority of our model; several valuable observations are also revealed to facilitate future research in multi-modal media manipulation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2293.Boosting Detection in Crowd Analysis via Underutilized Output Features</span><br>
                <span class="as">Wu, ShaokaiandYang, Fengyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Boosting_Detection_in_Crowd_Analysis_via_Underutilized_Output_Features_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15609-15618.png><br>
            
            <span class="tt"><span class="t0">研究问题：尽管基于检测的方法在密集人群中的表现不佳，但我们认为这些方法的潜力被低估了，因为它们提供了常被忽视的人群分析的关键信息。<br>
                    动机：我们主张，输出建议和边界框的区域大小和置信度分数为人群分析提供了关于人群规模和密度的见解。为了利用这些未充分利用的特征，我们提出了Crowd Hat，这是一个可以轻易集成到现有检测模型中的即插即用模块。<br>
                    方法：该模块使用混合的2D-1D压缩技术来精炼输出特征并获取人群特定信息的 spatial 和 numerical 分布。基于这些特征，我们进一步提出了区域自适应NMS阈值和一个解耦然后对齐的范例，以解决基于检测的方法的主要限制。<br>
                    效果：我们在各种人群分析任务上进行了广泛的评估，包括人群计数、定位和检测，结果证明了利用输出特征和使用基于检测的方法在人群分析中的有效性。我们的代码可以在 https://github.com/wskingdom/Crowd-Hat 找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Detection-based methods have been viewed unfavorably in crowd analysis due to their poor performance in dense crowds. However, we argue that the potential of these methods has been underestimated, as they offer crucial information for crowd analysis that is often ignored. Specifically, the area size and confidence score of output proposals and bounding boxes provide insight into the scale and density of the crowd. To leverage these underutilized features, we propose Crowd Hat, a plug-and-play module that can be easily integrated with existing detection models. This module uses a mixed 2D-1D compression technique to refine the output features and obtain the spatial and numerical distribution of crowd-specific information. Based on these features, we further propose region-adaptive NMS thresholds and a decouple-then-align paradigm that address the major limitations of detection-based methods. Our extensive evaluations on various crowd analysis tasks, including crowd counting, localization, and detection, demonstrate the effectiveness of utilizing output features and the potential of detection-based methods in crowd analysis. Our code is available at https://github.com/wskingdom/Crowd-Hat.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2294.MixSim: A Hierarchical Framework for Mixed Reality Traffic Simulation</span><br>
                <span class="as">Suo, SimonandWong, KelvinandXu, JustinandTu, JamesandCui, AlexanderandCasas, SergioandUrtasun, Raquel</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Suo_MixSim_A_Hierarchical_Framework_for_Mixed_Reality_Traffic_Simulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9622-9631.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何安全地将自动驾驶车辆部署到现实世界中？<br>
                    动机：目前的自动驾驶车辆测试主要在模拟环境中进行，但为了确保其在现实世界中的安全运行，需要对其进行闭环测试。<br>
                    方法：提出了一种混合现实交通模拟框架MixSim，通过学习反应性的路线条件策略，使模拟环境能够对真实世界中的情况进行反应和控制。<br>
                    效果：实验证明，MixSim可以作为真实世界情况的、反应性的、可控的数字双胞胎，为自动驾驶车辆的闭环测试提供了可能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The prevailing way to test a self-driving vehicle (SDV) in simulation involves non-reactive open-loop replay of real world scenarios. However, in order to safely deploy SDVs to the real world, we need to evaluate them in closed-loop. Towards this goal, we propose to leverage the wealth of interesting scenarios captured in the real world and make them reactive and controllable to enable closed-loop SDV evaluation in what-if situations. In particular, we present MixSim, a hierarchical framework for mixed reality traffic simulation. MixSim explicitly models agent goals as routes along the road network and learns a reactive route-conditional policy. By inferring each agent's route from the original scenario, MixSim can reactively re-simulate the scenario and enable testing different autonomy systems under the same conditions. Furthermore, by varying each agent's route, we can expand the scope of testing to what-if situations with realistic variations in agent behaviors or even safety-critical interactions. Our experiments show that MixSim can serve as a realistic, reactive, and controllable digital twin of real world scenarios. For more information, please visit the project website: https://waabi.ai/research/mixsim/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2295.The ObjectFolder Benchmark: Multisensory Learning With Neural and Real Objects</span><br>
                <span class="as">Gao, RuohanandDou, YimingandLi, HaoandAgarwal, TanmayandBohg, JeannetteandLi, YunzhuandFei-Fei, LiandWu, Jiajun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_The_ObjectFolder_Benchmark_Multisensory_Learning_With_Neural_and_Real_Objects_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17276-17286.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一个以物体为中心的多感官学习基准测试套件，并创建包含100个真实世界家用物品的多感官测量数据的ObjectFolder Real数据集。<br>
                    动机：现有的研究主要关注单一感官的学习，而现实生活中的物体识别、重建和操作需要结合视觉、听觉和触觉等多种感官信息。<br>
                    方法：作者设计了一个新的数据收集管道，用于收集现实世界物体的3D网格、视频、冲击声音和触觉读数，创建了包含100个真实世界家用物品的ObjectFolder Real数据集。同时，作者还开发了一个包含10个任务的多感官物体中心学习基准测试套件ObjectFolder Benchmark。<br>
                    效果：通过在ObjectFolder的1000个多感官神经对象和ObjectFolder Real的真实多感官数据上进行系统基准测试，结果证明了多感官知觉的重要性，揭示了视觉、音频和触觉在不同物体中心学习任务中各自的作用。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce the ObjectFolder Benchmark, a benchmark suite of 10 tasks for multisensory object-centric learning, centered around object recognition, reconstruction, and manipulation with sight, sound, and touch. We also introduce the ObjectFolder Real dataset, including the multisensory measurements for 100 real-world household objects, building upon a newly designed pipeline for collecting the 3D meshes, videos, impact sounds, and tactile readings of real-world objects. For each task in the ObjectFolder Benchmark, we conduct systematic benchmarking on both the 1,000 multisensory neural objects from ObjectFolder, and the real multisensory data from ObjectFolder Real. Our results demonstrate the importance of multisensory perception and reveal the respective roles of vision, audio, and touch for different object-centric learning tasks. By publicly releasing our dataset and benchmark suite, we hope to catalyze and enable new research in multisensory object-centric learning in computer vision, robotics, and beyond. Project page: https://objectfolder.stanford.edu</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2296.NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis</span><br>
                <span class="as">Zhou, AllanandKim, MooJinandWang, LiruiandFlorence, PeteandFinn, Chelsea</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_NeRF_in_the_Palm_of_Your_Hand_Corrective_Augmentation_for_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17907-17917.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过专家演示有效地训练视觉机器人操作策略，同时减少对大量演示或昂贵在线专家监督的依赖。<br>
                    动机：目前的模仿学习方法通常需要大量的演示或昂贵的在线专家监督来学习反应性的闭环行为。<br>
                    方法：提出了一种名为SPARTN（通过NeRF增强机器人轨迹的合成扰动）的全离线数据增强方案，用于改进使用手持摄像头的机器人策略。该方法利用神经辐射场（NeRFs）在视觉演示中合成注入纠正性噪声：使用NeRFs生成扰动的视角，同时计算纠正性动作。<br>
                    效果：在模拟的6自由度视觉抓取基准测试中，SPARTN比没有纠正性增强的模仿学习方法提高了2.8倍的离线成功率，甚至超过了一些使用在线监督的方法。此外，它缩小了RGB-only和RGB-D成功率之间的差距，消除了以前对深度传感器的需求。在实际的6自由度机器人抓取实验中，该方法平均提高了22.5%的绝对成功率，包括那些传统上对深度基于方法具有挑战性的对象。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Expert demonstrations are a rich source of supervision for training visual robotic manipulation policies, but imitation learning methods often require either a large number of demonstrations or expensive online expert supervision to learn reactive closed-loop behaviors. In this work, we introduce SPARTN (Synthetic Perturbations for Augmenting Robot Trajectories via NeRF): a fully-offline data augmentation scheme for improving robot policies that use eye-in-hand cameras. Our approach leverages neural radiance fields (NeRFs) to synthetically inject corrective noise into visual demonstrations: using NeRFs to generate perturbed viewpoints while simultaneously calculating the corrective actions. This requires no additional expert supervision or environment interaction, and distills the geometric information in NeRFs into a real-time reactive RGB-only policy. In a simulated 6-DoF visual grasping benchmark, SPARTN improves offline success rates by 2.8x over imitation learning without the corrective augmentations and even outperforms some methods that use online supervision. It additionally closes the gap between RGB-only and RGB-D success rates, eliminating the previous need for depth sensors. In real-world 6-DoF robotic grasping experiments from limited human demonstrations, our method improves absolute success rates by 22.5% on average, including objects that are traditionally challenging for depth-based methods.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2297.Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph</span><br>
                <span class="as">Zhou, RixinandWei, JiafuandZhang, QianandQi, RuihuaandYang, XiandLi, Chuntao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Multi-Granularity_Archaeological_Dating_of_Chinese_Bronze_Dings_Based_on_a_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/3103-3113.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用深度学习技术进行青铜鼎的考古年代鉴定。<br>
                    动机：目前的青铜鼎考古年代鉴定依赖于训练有素的专家，耗时耗力。<br>
                    方法：收集大规模的青铜鼎图像数据集，引入多头分类器和知识引导的关系图挖掘属性与鼎的时代之间的关系。<br>
                    效果：实验结果表明，该方法在青铜鼎考古年代鉴定上达到了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The archaeological dating of bronze dings has played a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose a learning-based approach to integrate advanced deep learning techniques and archaeological knowledge. To achieve this, we first collect a large-scale image dataset of bronze dings, which contains richer attribute information than other existing fine-grained datasets. Second, we introduce a multihead classifier and a knowledge-guided relation graph to mine the relationship between attributes and the ding era. Third, we conduct comparison experiments with various existing methods, the results of which show that our dating method achieves a state-of-the-art performance. We hope that our data and applied networks will enrich fine-grained classification research relevant to other interdisciplinary areas of expertise. The dataset and source code used are included in our supplementary materials, and will be open after submission owing to the anonymity policy. Source codes and data are available at: https://github.com/zhourixin/bronze-Ding.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2298.What Happened 3 Seconds Ago? Inferring the Past With Thermal Imaging</span><br>
                <span class="as">Tang, ZitianandYe, WenjieandMa, Wei-ChiuandZhao, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_What_Happened_3_Seconds_Ago_Inferring_the_Past_With_Thermal_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17111-17120.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从RGB图像中推断过去的人体运动？<br>
                    动机：由于预测问题的固有不确定性，从RGB图像中推断过去的人体运动具有挑战性。而热成像则通过测量热辐射，记录了环境中过去人与物体交互的痕迹。<br>
                    方法：我们收集了首个用于人体运动分析的RGB-Thermal数据集，命名为Thermal-IM。然后，我们开发了一个三阶段的神经网络模型，用于精确估计过去的人体姿态。<br>
                    效果：实验表明，热线索显著降低了此任务的模糊性，所提出的模型取得了显著的性能。该数据集可在https://github.com/ZitianTang/Thermal-IM获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Inferring past human motion from RGB images is challenging due to the inherent uncertainty of the prediction problem. Thermal images, on the other hand, encode traces of past human-object interactions left in the environment via thermal radiation measurement. Based on this observation, we collect the first RGB-Thermal dataset for human motion analysis, dubbed Thermal-IM. Then we develop a three-stage neural network model for accurate past human pose estimation. Comprehensive experiments show that thermal cues significantly reduce the ambiguities of this task, and the proposed model achieves remarkable performance. The dataset is available at https://github.com/ZitianTang/Thermal-IM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2299.MIME: Human-Aware 3D Scene Generation</span><br>
                <span class="as">Yi, HongweiandHuang, Chun-HaoP.andTripathi, ShashankandHering, LeaandThies, JustusandBlack, MichaelJ.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12965-12976.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地生成3D室内场景，考虑到人类运动和互动？<br>
                    动机：现有的3D场景生成方法成本高且劳动密集，而通过人类运动和互动可以更有效地生成3D室内场景。<br>
                    方法：提出MIME模型，利用自回归转换器架构，根据已生成的场景物体和人类运动输入，输出下一个可能的物体。<br>
                    效果：实验表明，MIME生成的3D场景比不考虑人类运动的现有生成场景方法更具多样性和可信度。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a "scanner" of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2300.A New Path: Scaling Vision-and-Language Navigation With Synthetic Instructions and Imitation Learning</span><br>
                <span class="as">Kamath, AishwaryaandAnderson, PeterandWang, SuandKoh, JingYuandKu, AlexanderandWaters, AustinandYang, YinfeiandBaldridge, JasonandParekh, Zarana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10813-10823.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的视觉-语言导航（VLN）研究在处理复杂语言基础和空间语言理解上存在困难。<br>
                    动机：由于人类指令数据的稀缺性和训练环境的有限多样性，现有的VLN研究无法很好地执行自然语言导航指令。<br>
                    方法：通过大规模的网络文本和图像-文本数据集进行预训练，并使用高质量的多语言导航指令生成器Marky生成视觉基础的指令。同时，利用图像到图像的GAN从新的视角合成图像观察。<br>
                    效果：通过这种方法，我们创建了一个比现有人工注释数据集大两个数量级的数据集，包含更广泛的环境和视角。我们的简单转换器代理在具有挑战性的RxR数据集上的表现超过了所有现有的RL代理，将最先进的NDTW从71.1提高到79.1（在可见环境中），从64.6提高到66.8（在未见过的环境测试中）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pre-training on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting dataset of 4.2M instruction-trajectory pairs is two orders of magnitude larger than existing human-annotated datasets, and contains a wider variety of environments and viewpoints. To efficiently leverage data at this scale, we train a simple transformer agent with imitation learning. On the challenging RxR dataset, our approach outperforms all existing RL agents, improving the state-of-the-art NDTW from 71.1 to 79.1 in seen environments, and from 64.6 to 66.8 in unseen test environments. Our work points to a new path to improving instruction-following agents, emphasizing large-scale training on near-human quality synthetic instructions.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2301.Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration</span><br>
                <span class="as">Oksuz, KemalandJoy, TomandDokania, PuneetK.</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9263-9274.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的物体检测器鲁棒性测试方法存在缺陷，如执行分布外检测的方法不当和使用不考虑定位和分类质量的校准度量标准。<br>
                    动机：为了解决这些问题，我们提出了自我感知物体检测（SAOD）任务，这是一个统一的测试框架，尊重并符合物体检测器在自动驾驶等安全关键环境中面临的挑战。<br>
                    方法：SAOD任务要求物体检测器能够：对领域偏移具有鲁棒性；为整个场景获得可靠的不确定性估计；并为检测结果提供校准的信心分数。我们广泛使用我们的框架，引入新的度量标准和大规模的测试数据集，来测试许多物体检测器在两种不同的用例中，以突出其鲁棒性能的关键见解。<br>
                    效果：最后，我们为SAOD任务引入了一个简单的基线，使研究人员能够为未来提出的方法进行基准测试，并朝着适合目的的鲁棒物体检测器迈进。代码可在以下网址获取：https://github.com/fiveai/saod</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The current approach for testing the robustness of object detectors suffers from serious deficiencies such as improper methods of performing out-of-distribution detection and using calibration metrics which do not consider both localisation and classification quality. In this work, we address these issues, and introduce the Self Aware Object Detection (SAOD) task, a unified testing framework which respects and adheres to the challenges that object detectors face in safety-critical environments such as autonomous driving. Specifically, the SAOD task requires an object detector to be: robust to domain shift; obtain reliable uncertainty estimates for the entire scene; and provide calibrated confidence scores for the detections. We extensively use our framework, which introduces novel metrics and large scale test datasets, to test numerous object detectors in two different use-cases, allowing us to highlight critical insights into their robustness performance. Finally, we introduce a simple baseline for the SAOD task, enabling researchers to benchmark future proposed methods and move towards robust object detectors which are fit for purpose. Code is available at: https://github.com/fiveai/saod</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2302.CIRCLE: Capture in Rich Contextual Environments</span><br>
                <span class="as">Ara\&#x27;ujo, Jo\~aoPedroandLi, JiamanandVetrivel, KarthikandAgarwal, RishiandWu, JiajunandGopinath, DeepakandClegg, AlexanderWilliamandLiu, Karen</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21211-21221.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地在具有丰富上下文的生态环境中合成3D人体运动，以模拟人们在现实世界中执行的真实活动。<br>
                    动机：传统的基于光学的运动捕捉系统无法同时捕捉人类运动和复杂场景，且缺乏丰富的上下文3D人体运动数据集，这对创建高质量的生成性人体运动模型构成了障碍。<br>
                    方法：我们提出了一种新的运动捕捉系统，演员在这个高度情境化的虚拟世界中感知和操作，同时在真实世界中进行动作捕捉。我们的系统能够在高度多样化的场景中快速收集高质量的人体运动，无需担心遮挡或在真实世界中需要物理场景构建的问题。<br>
                    效果：我们提出了CIRCLE数据集，包含5个主题在9个场景中的10小时全身伸手运动，以及以各种形式（如RGBD视频）表示的环境自我中心信息。我们使用此数据集训练了一个根据场景信息生成人体运动模型。利用我们的数据集，该模型学习使用自我中心场景信息在复杂3D场景中完成非平凡的伸手任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Synthesizing 3D human motion in a contextual, ecological environment is important for simulating realistic activities people perform in the real world. However, conventional optics-based motion capture systems are not suited for simultaneously capturing human movements and complex scenes. The lack of rich contextual 3D human motion datasets presents a roadblock to creating high-quality generative human motion models. We propose a novel motion acquisition system in which the actor perceives and operates in a highly contextual virtual world while being motion captured in the real world. Our system enables rapid collection of high-quality human motion in highly diverse scenes, without the concern of occlusion or the need for physical scene construction in the real world. We present CIRCLE, a dataset containing 10 hours of full-body reaching motion from 5 subjects across nine scenes, paired with ego-centric information of the environment represented in various forms, such as RGBD videos. We use this dataset to train a model that generates human motion conditioned on scene information. Leveraging our dataset, the model learns to use ego-centric scene information to achieve nontrivial reaching tasks in the context of complex 3D scenes. To download the data please visit our website (https://stanford-tml.github.io/circle_dataset/).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2303.PyPose: A Library for Robot Learning With Physics-Based Optimization</span><br>
                <span class="as">Wang, ChenandGao, DasongandXu, KuanandGeng, JunyiandHu, YaoyuandQiu, YuhengandLi, BowenandYang, FanandMoon, BradyandPandey, AbhinavandAryanandXu, JiaheandWu, TianhaoandHe, HaonanandHuang, DaningandRen, ZhongqiangandZhao, ShiboandFu, TaimengandReddy, PranayandLin, XiaoandWang, WenshanandShi, JingnanandTalak, RajatandCao, KunandDu, YiandWang, HanandYu, HuaiandWang, ShanzhaoandChen, SiyuandKashyap, AnanthandBandaru, RohanandDantu, KarthikandWu, JiajunandXie, LihuaandCarlone, LucaandHutter, MarcoandScherer, Sebastian</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PyPose_A_Library_for_Robot_Learning_With_Physics-Based_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22024-22034.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何结合深度学习和物理优化，以适应不断变化的环境并处理复杂任务？<br>
                    动机：深度学习在机器人感知方面取得了显著的成功，但在应对不断变化的环境中表现不佳；而物理优化虽然泛化能力更强，但在复杂任务中表现不佳，且需要手动调整参数。<br>
                    方法：提出了PyPose，一个面向机器人的基于PyTorch的库，将深度感知模型与物理优化相结合。<br>
                    效果：实验表明，PyPose比现有最先进的库快10倍以上，为未来研究提供了具体示例，包括SLAM、规划、控制和惯性导航等领域。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning has had remarkable success in robotic perception, but its data-centric nature suffers when it comes to generalizing to ever-changing environments. By contrast, physics-based optimization generalizes better, but it does not perform as well in complicated tasks due to the lack of high-level semantic information and reliance on manual parametric tuning. To take advantage of these two complementary worlds, we present PyPose: a robotics-oriented, PyTorch-based library that combines deep perceptual models with physics-based optimization. PyPose's architecture is tidy and well-organized, it has an imperative style interface and is efficient and user-friendly, making it easy to integrate into real-world robotic applications. Besides, it supports parallel computing of any order gradients of Lie groups and Lie algebras and 2nd-order optimizers, such as trust region methods. Experiments show that PyPose achieves more than 10x speedup in computation compared to the state-of-the-art libraries. To boost future research, we provide concrete examples for several fields of robot learning, including SLAM, planning, control, and inertial navigation.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2304.Multi-Sensor Large-Scale Dataset for Multi-View 3D Reconstruction</span><br>
                <span class="as">Voynov, OlegandBobrovskikh, GlebandKarpyshev, PavelandGalochkin, SaveliyandArdelean, Andrei-TimoteiandBozhenko, ArseniyandKarmanova, EkaterinaandKopanev, PavelandLabutin-Rymsho, YaroslavandRakhimov, RuslanandSafin, AleksandrandSerpiva, ValeriiandArtemov, AlexeyandBurnaev, EvgenyandTsetserukou, DzmitryandZorin, Denis</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Voynov_Multi-Sensor_Large-Scale_Dataset_for_Multi-View_3D_Reconstruction_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21392-21403.png><br>
            
            <span class="tt"><span class="t0">研究问题：开发一种新的多传感器数据集，用于多视角3D表面重建。<br>
                    动机：现有的算法在处理具有挑战性的材料属性时表现不佳，需要一个新的、多样化的数据集进行评估和训练。<br>
                    方法：收集了来自不同分辨率和模态的传感器（如智能手机、Intel RealSense、Microsoft Kinect、工业相机和结构光扫描仪）的已注册RGB和深度数据，并从100个视角在14种光照条件下获取了约140万张图像。<br>
                    效果：创建了一个包含107个场景、1.4百万张图像的数据集，可用于评估和训练3D重建算法及相关任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2305.Privacy-Preserving Representations Are Not Enough: Recovering Scene Content From Camera Poses</span><br>
                <span class="as">Chelani, KunalandSattler, TorstenandKahl, FredrikandKukelova, Zuzana</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chelani_Privacy-Preserving_Representations_Are_Not_Enough_Recovering_Scene_Content_From_Camera_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13132-13141.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何保护视觉定位过程中的隐私，防止攻击者通过查询本地化服务获取场景细节。<br>
                    动机：随着AR/VR/MR设备和基于云的应用的普及，隐私问题在定位过程中变得越来越重要。<br>
                    方法：本文提出了一种攻击方法，攻击者可以通过查询本地化服务来学习场景的细节，而无需任何访问权限。这种攻击基于现代视觉定位算法对外观和几何变化的鲁棒性。<br>
                    效果：本文开发了一种概念验证版本的攻击，并证明了其实际可行性。该攻击不要求使用特定的本地化算法，因此也适用于隐私保护表示。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual localization is the task of estimating the camera pose from which a given image was taken and is central to several 3D computer vision applications. With the rapid growth in the popularity of AR/VR/MR devices and cloud-based applications, privacy issues are becoming a very important aspect of the localization process. Existing work on privacy-preserving localization aims to defend against an attacker who has access to a cloud-based service. In this paper, we show that an attacker can learn about details of a scene without any access by simply querying a localization service. The attack is based on the observation that modern visual localization algorithms are robust to variations in appearance and geometry. While this is in general a desired property, it also leads to algorithms localizing objects that are similar enough to those present in a scene. An attacker can thus query a server with a large enough set of images of objects, e.g., obtained from the Internet, and some of them will be localized. The attacker can thus learn about object placements from the camera poses returned by the service (which is the minimal information returned by such a service). In this paper, we develop a proof-of-concept version of this attack and demonstrate its practical feasibility. The attack does not place any requirements on the localization algorithm used, and thus also applies to privacy-preserving representations. Current work on privacy-preserving representations alone is thus insufficient.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2306.A New Dataset Based on Images Taken by Blind People for Testing the Robustness of Image Classification Models Trained for ImageNet Categories</span><br>
                <span class="as">Bafghi, RezaAkbarianandGurari, Danna</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bafghi_A_New_Dataset_Based_on_Images_Taken_by_Blind_People_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16261-16270.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高在一个领域训练的图像分类模型在另一个领域图像上的性能。<br>
                    动机：现有的图像分类模型在跨领域应用时性能下降，且缺乏针对这一问题的公开数据集。<br>
                    方法：构建了一个新数据集VizWiz-Classification，包含8900张由盲人拍摄的图片，每张图片都标注了200个ImageNet物体类别的存在与否。<br>
                    效果：通过分析100个ImageNet图像分类模型在该数据集上的表现，发现这些模型在质量有问题的图像上表现不佳。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Our goal is to improve upon the status quo for designing image classification models trained in one domain that perform well on images from another domain. Complementing existing work in robustness testing, we introduce the first dataset for this purpose which comes from an authentic use case where photographers wanted to learn about the content in their images. We built a new test set using 8,900 images taken by people who are blind for which we collected metadata to indicate the presence versus absence of 200 ImageNet object categories. We call this dataset VizWiz-Classification. We characterize this dataset and how it compares to the mainstream datasets for evaluating how well ImageNet-trained classification models generalize. Finally, we analyze the performance of 100 ImageNet classification models on our new test dataset. Our fine-grained analysis demonstrates that these models struggle on images with quality issues. To enable future extensions to this work, we share our new dataset with evaluation server at: https://vizwiz.org/tasks-and-datasets/image-classification</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2307.Renderable Neural Radiance Map for Visual Navigation</span><br>
                <span class="as">Kwon, ObinandPark, JeonghoandOh, Songhwai</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Renderable_Neural_Radiance_Map_for_Visual_Navigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9099-9108.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一种可以包含3D环境全局视觉信息的新型地图，用于视觉导航？<br>
                    动机：现有的地图在视觉导航中存在信息不全的问题，需要一种新的地图来更好地描述和指导视觉定位和导航。<br>
                    方法：提出一种新型的可渲染神经辐射度图（RNR-Map），以网格形式包含每个像素的隐藏码，这些隐藏码从图像观察中嵌入，并可以转换为给定摄像头位姿的神经辐射度场，从而实现图像渲染。记录的隐藏码隐含地包含有关环境的视觉信息，使RNR-Map具有视觉描述性。开发了能有效利用RNR-Map的定位和导航框架。<br>
                    效果：实验结果表明，基于RNR-Map的定位框架可以在单一查询图像的基础上快速准确地找到目标位置，且对环境变化具有鲁棒性。提出的导航框架在困难的场景下优于现有的图像目标导航方法，在NRNS数据集的弯曲场景中表现出65.7%的成功率，比当前最先进的方法提高了18.6%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We propose a novel type of map for visual navigation, a renderable neural radiance map (RNR-Map), which is designed to contain the overall visual information of a 3D environment. The RNR-Map has a grid form and consists of latent codes at each pixel. These latent codes are embedded from image observations, and can be converted to the neural radiance field which enables image rendering given a camera pose. The recorded latent codes implicitly contain visual information about the environment, which makes the RNR-Map visually descriptive. This visual information in RNR-Map can be a useful guideline for visual localization and navigation. We develop localization and navigation frameworks that can effectively utilize the RNR-Map. We evaluate the proposed frameworks on camera tracking, visual localization, and image-goal navigation. Experimental results show that the RNR-Map-based localization framework can find the target location based on a single query image with fast speed and competitive accuracy compared to other baselines. Also, this localization framework is robust to environmental changes, and even finds the most visually similar places when a query image from a different environment is given. The proposed navigation framework outperforms the existing image-goal navigation methods in difficult scenarios, under odometry and actuation noises. The navigation framework shows 65.7% success rate in curved scenarios of the NRNS dataset, which is an improvement of 18.6% over the current state-of-the-art. Project page: https://rllab-snu.github.io/projects/RNR-Map/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2308.Diffusion-Based Generation, Optimization, and Planning in 3D Scenes</span><br>
                <span class="as">Huang, SiyuanandWang, ZanandLi, PuhaoandJia, BaoxiongandLiu, TengyuandZhu, YixinandLiang, WeiandZhu, Song-Chun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/16750-16761.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种用于3D场景理解的条件生成模型SceneDiffuser。<br>
                    动机：现有的3D场景理解模型存在模块间差异大、后验塌陷等问题，需要一种统一的场景感知、基于物理和目标导向的模型。<br>
                    方法：SceneDiffuser采用迭代采样策略，通过全可微的扩散去噪过程，联合进行场景感知生成、基于物理优化和目标导向规划。<br>
                    效果：在人体姿态和运动生成、灵巧抓取生成、3D导航路径规划、机器人手臂运动规划等任务上，SceneDiffuser相比现有模型有显著改进，显示出巨大的应用潜力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce SceneDiffuser, a conditional generative model for 3D scene understanding. SceneDiffuser provides a unified model for solving scene-conditioned generation, optimization, and planning. In contrast to prior works, SceneDiffuser is intrinsically scene-aware, physics-based, and goal-oriented. With an iterative sampling strategy, SceneDiffuser jointly formulates the scene-aware generation, physics-based optimization, and goal-oriented planning via a diffusion-based denoising process in a fully differentiable fashion. Such a design alleviates the discrepancies among different modules and the posterior collapse of previous scene-conditioned generative models. We evaluate SceneDiffuser with various 3D scene understanding tasks, including human pose and motion generation, dexterous grasp generation, path planning for 3D navigation, and motion planning for robot arms. The results show significant improvements compared with previous models, demonstrating the tremendous potential of SceneDiffuser for the broad community of 3D scene understanding.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2309.Planning-Oriented Autonomous Driving</span><br>
                <span class="as">Hu, YihanandYang, JiazhiandChen, LiandLi, KeyuandSima, ChonghaoandZhu, XizhouandChai, SiqiandDu, SenyaoandLin, TianweiandWang, WenhaiandLu, LeweiandJia, XiaosongandLiu, QiangandDai, JifengandQiao, YuandLi, Hongyang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17853-17862.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何设计一个优化的框架，以实现自动驾驶汽车的规划。<br>
                    动机：现有的自动驾驶系统要么采用独立模型处理单个任务，要么设计多任务模式，但这些方法可能会累积错误或缺乏任务协调。<br>
                    方法：我们提出了统一自主驾驶（UniAD）框架，将感知、预测和规划等全栈驾驶任务整合到一个网络中，通过统一的查询接口进行任务间的通信和协作。<br>
                    效果：我们在具有挑战性的nuScenes基准上实例化了UniAD，并通过广泛的消融实验证明，这种哲学的有效性在各方面都大大超过了先前最先进的技术。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern autonomous driving system is characterized as modular tasks in sequential order, i.e., perception, prediction, and planning. In order to perform a wide diversity of tasks and achieve advanced-level intelligence, contemporary approaches either deploy standalone models for individual tasks, or design a multi-task paradigm with separate heads. However, they might suffer from accumulative errors or deficient task coordination. Instead, we argue that a favorable framework should be devised and optimized in pursuit of the ultimate goal, i.e., planning of the self-driving car. Oriented at this, we revisit the key components within perception and prediction, and prioritize the tasks such that all these tasks contribute to planning. We introduce Unified Autonomous Driving (UniAD), a comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. It is exquisitely devised to leverage advantages of each module, and provide complementary feature abstractions for agent interaction from a global perspective. Tasks are communicated with unified query interfaces to facilitate each other toward planning. We instantiate UniAD on the challenging nuScenes benchmark. With extensive ablations, the effectiveness of using such a philosophy is proven by substantially outperforming previous state-of-the-arts in all aspects. Code and models are public.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2310.LEGO-Net: Learning Regular Rearrangements of Objects in Rooms</span><br>
                <span class="as">Wei, QiuhongAnnaandDing, SijieandPark, JeongJoonandSajnani, RahulandPoulenard, AdrienandSridhar, SrinathandGuibas, Leonidas</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19037-19047.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让机器理解并帮助人类整理杂乱的房间，使其符合人类的空间排列规则和审美标准。<br>
                    动机：由于人类普遍不喜欢清理杂乱的房间，如果机器能帮忙做这件事，就必须理解人类的空间排列规则和审美标准。<br>
                    方法：本文提出了一种基于数据驱动的迭代方法LEGO-Net，该方法借鉴了扩散模型的思想，通过迭代消除物体位置和方向的噪声，将杂乱的房间逐步整理成符合人类审美规则的状态。<br>
                    效果：实验结果表明，LEGO-Net能够可靠地重新排列房间场景，并在评估房间布局规则的指标上优于其他方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans universally dislike the task of cleaning up a messy room. If machines were to help us with this task, they must understand human criteria for regular arrangements, such as several types of symmetry, co-linearity or co-circularity, spacing uniformity in linear or circular patterns, and further inter-object relationships that relate to style and functionality. Previous approaches for this task relied on human input to explicitly specify goal state, or synthesized scenes from scratch--but such methods do not address the rearrangement of existing messy scenes without providing a goal state. In this paper, we present LEGO-Net, a data-driven transformer-based iterative method for LEarning reGular rearrangement of Objects in messy rooms. LEGO-Net is partly inspired by diffusion models--it starts with an initial messy state and iteratively "de-noises" the position and orientation of objects to a regular state while reducing distance traveled. Given randomly perturbed object positions and orientations in an existing dataset of professionally-arranged scenes, our method is trained to recover a regular re-arrangement. Results demonstrate that our method is able to reliably rearrange room scenes and outperform other methods. We additionally propose a metric for evaluating regularity in room arrangements using number-theoretic machinery.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2311.Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</span><br>
                <span class="as">Cao, JinkunandPang, JiangmiaoandWeng, XinshuoandKhirodkar, RawalandKitani, Kris</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9686-9696.png><br>
            
            <span class="tt"><span class="t0">研究问题：多目标跟踪（MOT）中，卡尔曼滤波器（KF）的线性运动假设在长时间遮挡下可能导致高度不准确的估计。<br>
                    动机：当没有测量来更新卡尔曼滤波器参数时，通常依赖先验状态估计进行后验更新，这会导致遮挡期间误差的累积。<br>
                    方法：我们提出一种基于观察的SORT方法（OC-SORT），使用物体观测（即物体检测器的测量）来计算遮挡期间的虚拟轨迹，以修复滤波器参数的误差累积。<br>
                    效果：OC-SORT方法简单、在线、实时，并在遮挡和非线性运动方面提高了鲁棒性。在多个数据集上，包括MOT17、MOT20、KITTI、头跟踪和舞蹈跟踪（其中物体运动高度非线性），OC-SORT实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at https://github.com/noahcao/OC_SORT.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2312.UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</span><br>
                <span class="as">Xu, YinzhenandWan, WeikangandZhang, JialiangandLiu, HaoranandShan, ZikangandShen, HaoandWang, RuichengandGeng, HaoranandWeng, YijiaandChen, JiayiandLiu, TengyuandYi, LiandWang, He</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_UniDexGrasp_Universal_Robotic_Dexterous_Grasping_via_Learning_Diverse_Proposal_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4737-4746.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决在桌面环境下，从点云观察中学习通用机器人灵巧抓取的问题。<br>
                    动机：为了实现对各种类别的物体进行高质量、多样化的抓取和提起，并推广到数百个类别甚至未见过的对象上。<br>
                    方法：受到并行夹爪抓取成功管道的启发，将任务分为两个阶段：1）抓取姿态生成；2）目标条件抓取执行。对于第一阶段，提出了一种新颖的概率模型，该模型根据点云观察来生成抓取姿态，并将旋转从平移和关节运动中分离出来。在大规模灵巧抓取数据集上训练后，该模型能够为物体点云生成多样化、高质量的灵巧抓取姿态。第二阶段，由于灵巧抓取执行的复杂性，提出用目标条件抓取策略取代并行夹爪抓取中使用的运动规划。<br>
                    效果：通过整合两个阶段，我们的最终流程首次实现了通用的灵巧抓取泛化，在数千个物体实例上的平均成功率超过60%，显著优于所有基线，同时展示出最小的泛化差距。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this work, we tackle the problem of learning universal robotic dexterous grasping from a point cloud observation under a table-top setting. The goal is to grasp and lift up objects in high-quality and diverse ways and generalize across hundreds of categories and even the unseen. Inspired by successful pipelines used in parallel gripper grasping, we split the task into two stages: 1) grasp proposal (pose) generation and 2) goal-conditioned grasp execution. For the first stage, we propose a novel probabilistic model of grasp pose conditioned on the point cloud observation that factorizes rotation from translation and articulation. Trained on our synthesized large-scale dexterous grasp dataset, this model enables us to sample diverse and high-quality dexterous grasp poses for the object point cloud. For the second stage, we propose to replace the motion planning used in parallel gripper grasping with a goal-conditioned grasp policy, due to the complexity involved in dexterous grasping execution. Note that it is very challenging to learn this highly generalizable grasp policy that only takes realistic inputs without oracle states. We thus propose several important innovations, including state canonicalization, object curriculum, and teacher-student distillation. Integrating the two stages, our final pipeline becomes the first to achieve universal generalization for dexterous grasping, demonstrating an average success rate of more than 60% on thousands of object instances, which significantly outperforms all baselines, meanwhile showing only a minimal generalization gap.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2313.Crowd3D: Towards Hundreds of People Reconstruction From a Single Image</span><br>
                <span class="as">Wen, HaoandHuang, JingandCui, HuiliandLin, HaozheandLai, Yu-KunandFang, LuandLi, Kun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_Crowd3D_Towards_Hundreds_of_People_Reconstruction_From_a_Single_Image_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8937-8946.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从大规模场景图像中重建数百人的3D姿态、形状和位置，以实现人群分析和安全警报。<br>
                    动机：现有的方法无法处理包含数百人的大型场景，面临人数众多、人体比例变化大和空间分布复杂等挑战。<br>
                    方法：提出Crowd3D，首个能从单张大型场景图像中全局一致地重建数百人3D姿态、形状和位置的框架。通过新定义的“人-场景虚拟交互点”（HVIP）将复杂的人群定位问题转化为像素定位问题，并设计了基于HVIP的渐进式重建网络以及适应不同人体尺寸的人本主义裁剪方案。<br>
                    效果：实验结果表明，该方法有效。建立了大型场景下的人群重建基准数据集LargeCrowd，代码和数据集可在http://cic.tju.edu.cn/faculty/likun/projects/Crowd3D获取。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Image-based multi-person reconstruction in wide-field large scenes is critical for crowd analysis and security alert. However, existing methods cannot deal with large scenes containing hundreds of people, which encounter the challenges of large number of people, large variations in human scale, and complex spatial distribution. In this paper, we propose Crowd3D, the first framework to reconstruct the 3D poses, shapes and locations of hundreds of people with global consistency from a single large-scene image. The core of our approach is to convert the problem of complex crowd localization into pixel localization with the help of our newly defined concept, Human-scene Virtual Interaction Point (HVIP). To reconstruct the crowd with global consistency, we propose a progressive reconstruction network based on HVIP by pre-estimating a scene-level camera and a ground plane. To deal with a large number of persons and various human sizes, we also design an adaptive human-centric cropping scheme. Besides, we contribute a benchmark dataset, LargeCrowd, for crowd reconstruction in a large scene. Experimental results demonstrate the effectiveness of the proposed method. The code and the dataset are available at http://cic.tju.edu.cn/faculty/likun/projects/Crowd3D.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2314.Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction</span><br>
                <span class="as">Cai, ShaofeiandWang, ZihaoandMa, XiaojianandLiu, AnjiandLiang, Yitao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_Open-World_Multi-Task_Control_Through_Goal-Aware_Representation_Learning_and_Adaptive_Horizon_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13734-13744.png><br>
            
            <span class="tt"><span class="t0">研究问题：在Minecraft中学习目标条件策略的问题。<br>
                    动机：由于场景的多样性和部分可观察性导致的环境动态非稳定性，使得在Minecraft中学习目标条件策略存在两个主要挑战。<br>
                    方法：提出了Goal-Sensitive Backbone（GSB）来鼓励目标相关视觉状态表示的出现，并通过自适应视野预测模块来解决第二个挑战。<br>
                    效果：在20个Minecraft任务上进行的实验表明，该方法显著优于目前最好的基线；在许多任务中，性能提高了一倍。此外，还发现该方法具有零样本泛化到新场景的能力。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We study the problem of learning goal-conditioned policies in Minecraft, a popular, widely accessible yet challenging open-ended environment for developing human-level multi-task agents. We first identify two main challenges of learning such policies: 1) the indistinguishability of tasks from the state distribution, due to the vast scene diversity, and 2) the non-stationary nature of environment dynamics caused by the partial observability. To tackle the first challenge, we propose Goal-Sensitive Backbone (GSB) for the policy to encourage the emergence of goal-relevant visual state representations. To tackle the second challenge, the policy is further fueled by an adaptive horizon prediction module that helps alleviate the learning uncertainty brought by the non-stationary dynamics. Experiments on 20 Minecraft tasks show that our method significantly outperforms the best baseline so far; in many of them, we double the performance. Our ablation and exploratory studies then explain how our approach beat the counterparts and also unveil the surprising bonus of zero-shot generalization to new scenes (biomes). We hope our agent could help shed some light on learning goal-conditioned, multi-task agents in challenging, open-ended environments like Minecraft.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2315.Visual Localization Using Imperfect 3D Models From the Internet</span><br>
                <span class="as">Panek, VojtechandKukelova, ZuzanaandSattler, Torsten</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Panek_Visual_Localization_Using_Imperfect_3D_Models_From_the_Internet_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13175-13186.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用互联网上的3D模型进行视觉定位，并研究这些模型的不完美性对定位精度的影响。<br>
                    动机：目前的视觉定位算法需要大量的数据和复杂的计算过程，而互联网上的3D模型可以作为现成的场景表示，避免了这些步骤。然而，这些模型往往存在不完美性，这对定位精度产生影响。<br>
                    方法：通过构建一个新的基准测试，对多种3D模型进行详细的实验评估，研究这些模型的不完美性对定位精度的影响。<br>
                    效果：研究发现，互联网上的3D模型具有潜力作为易获取的场景表示。同时，视觉定位流程还有很大的改进空间。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Visual localization is a core component in many applications, including augmented reality (AR). Localization algorithms compute the camera pose of a query image w.r.t. a scene representation, which is typically built from images. This often requires capturing and storing large amounts of data, followed by running Structure-from-Motion (SfM) algorithms. An interesting, and underexplored, source of data for building scene representations are 3D models that are readily available on the Internet, e.g., hand-drawn CAD models, 3D models generated from building footprints, or from aerial images. These models allow to perform visual localization right away without the time-consuming scene capturing and model building steps. Yet, it also comes with challenges as the available 3D models are often imperfect reflections of reality. E.g., the models might only have generic or no textures at all, might only provide a simple approximation of the scene geometry, or might be stretched. This paper studies how the imperfections of these models affect localization accuracy. We create a new benchmark for this task and provide a detailed experimental evaluation based on multiple 3D models per scene. We show that 3D models from the Internet show promise as an easy-to-obtain scene representation. At the same time, there is significant room for improvement for visual localization pipelines. To foster research on this interesting and challenging task, we release our benchmark at v-pnk.github.io/cadloc.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2316.Benchmarking Robustness of 3D Object Detection to Common Corruptions</span><br>
                <span class="as">Dong, YinpengandKang, CaixinandZhang, JinlaiandZhu, ZijianandWang, YikaiandYang, XiaoandSu, HangandWei, XingxingandZhu, Jun</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Benchmarking_Robustness_of_3D_Object_Detection_to_Common_Corruptions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1022-1032.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的3D物体检测器在面对不利天气和传感器噪声等现实世界的干扰时，缺乏稳健性，这引发了对自动驾驶系统的安全性和可靠性的关注。<br>
                    动机：为了全面严格地评估3D检测器的抗干扰能力，我们设计了27种常见的针对激光雷达和相机输入的干扰类型，以考虑真实的驾驶场景。<br>
                    方法：通过在公共数据集上合成这些干扰，我们建立了三个抗干扰基准测试——KITTI-C、nuScenes-C和Waymo-C。然后，我们在24个不同的3D物体检测模型上进行大规模的实验，以评估它们的抗干扰能力。<br>
                    效果：实验结果表明，运动级别的干扰是最具威胁性的，会导致所有模型的性能大幅下降；激光雷达和相机融合的模型显示出更好的稳健性；仅依赖相机的模型对图像干扰极其敏感，显示了LiDAR点云的必要性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection is an important task in autonomous driving to perceive the surroundings. Despite the excellent performance, the existing 3D detectors lack the robustness to real-world corruptions caused by adverse weathers, sensor noises, etc., provoking concerns about the safety and reliability of autonomous driving systems. To comprehensively and rigorously benchmark the corruption robustness of 3D detectors, in this paper we design 27 types of common corruptions for both LiDAR and camera inputs considering real-world driving scenarios. By synthesizing these corruptions on public datasets, we establish three corruption robustness benchmarks---KITTI-C, nuScenes-C, and Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object detection models to evaluate their corruption robustness. Based on the evaluation results, we draw several important findings, including: 1) motion-level corruptions are the most threatening ones that lead to significant performance drop of all models; 2) LiDAR-camera fusion models demonstrate better robustness; 3) camera-only models are extremely vulnerable to image corruptions, showing the indispensability of LiDAR point clouds. We release the benchmarks and codes at https://github.com/thu-ml/3D_Corruptions_AD to be helpful for future studies.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2317.OrienterNet: Visual Localization in 2D Public Maps With Neural Matching</span><br>
                <span class="as">Sarlin, Paul-EdouardandDeTone, DanielandYang, Tsun-YiandAvetisyan, ArmenandStraub, JulianandMalisiewicz, TomaszandBul\`o, SamuelRotaandNewcombe, RichardandKontschieder, PeterandBalntas, Vasileios</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sarlin_OrienterNet_Visual_Localization_in_2D_Public_Maps_With_Neural_Matching_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21632-21642.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让算法像人类一样，使用2D地图在3D环境中进行定位？<br>
                    动机：目前的视觉定位算法大多依赖昂贵的3D点云，而我们的目标是让算法能够使用人类使用的2D语义地图进行定位。<br>
                    方法：我们提出了OrienterNet，这是一个能够使用2D语义地图进行定位的深度神经网络。OrienterNet通过匹配神经鸟瞰图和全球可用的OpenStreetMap地图，来估计查询图像的位置和方向。<br>
                    效果：OrienterNet可以在12个城市的各种视角下进行图像捕获，并推广到新的数据集，将机器人和增强现实场景中的状态推向了前沿。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Humans can orient themselves in their 3D environments using simple 2D maps. Differently, algorithms for visual localization mostly rely on complex 3D point clouds that are expensive to build, store, and maintain over time. We bridge this gap by introducing OrienterNet, the first deep neural network that can localize an image with sub-meter accuracy using the same 2D semantic maps that humans use. OrienterNet estimates the location and orientation of a query image by matching a neural Bird's-Eye View with open and globally available maps from OpenStreetMap, enabling anyone to localize anywhere such maps are available. OrienterNet is supervised only by camera poses but learns to perform semantic matching with a wide range of map elements in an end-to-end manner. To enable this, we introduce a large crowd-sourced dataset of images captured across 12 cities from the diverse viewpoints of cars, bikes, and pedestrians. OrienterNet generalizes to new datasets and pushes the state of the art in both robotics and AR scenarios. The code is available at https://github.com/facebookresearch/OrienterNet</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2318.Pix2map: Cross-Modal Retrieval for Inferring Street Maps From Images</span><br>
                <span class="as">Wu, XindiandLau, KwunFungandFerroni, FrancescoandO\v{s</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Pix2map_Cross-Modal_Retrieval_for_Inferring_Street_Maps_From_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17514-17523.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何直接从自我视角图像中推断城市街道地图的拓扑结构，以持续更新和扩展现有地图。<br>
                    动机：现有的地图需要不断更新和扩展，而直接从原始图像数据中推断复杂的城市道路拓扑结构是一项具有挑战性的任务。<br>
                    方法：通过学习图像和现有地图（表示为编码视觉周围布局的离散图形）的联合、跨模态嵌入空间，将此问题表述为跨模态检索。<br>
                    效果：实验结果表明，仅使用图像数据就可以准确检索到对应已见和未见道路的街道地图。此外，我们的检索地图可以用于更新或扩展现有地图，甚至展示了从空间图进行视觉定位和图像检索的概念验证结果。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Self-driving vehicles rely on urban street maps for autonomous navigation. In this paper, we introduce Pix2Map, a method for inferring urban street map topology directly from ego-view images, as needed to continually update and expand existing maps. This is a challenging task, as we need to infer a complex urban road topology directly from raw image data. The main insight of this paper is that this problem can be posed as cross-modal retrieval by learning a joint, cross-modal embedding space for images and existing maps, represented as discrete graphs that encode the topological layout of the visual surroundings. We conduct our experimental evaluation using the Argoverse dataset and show that it is indeed possible to accurately retrieve street maps corresponding to both seen and unseen roads solely from image data. Moreover, we show that our retrieved maps can be used to update or expand existing maps and even show proof-of-concept results for visual localization and image retrieval from spatial graphs.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2319.Affordances From Human Videos as a Versatile Representation for Robotics</span><br>
                <span class="as">Bahl, ShikharandMendonca, RussellandChen, LiliandJain, UnnatandPathak, Deepak</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13778-13790.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将现有的模型直接应用于机器人上，使机器人通过观察人类进行理解和学习交互。<br>
                    动机：尽管在静态数据集上取得了一些成功的结果，但目前还不清楚如何以环境为中心的方式利用互联网上的视频来训练一个视觉适应性模型。<br>
                    方法：利用互联网上的视频对人类行为进行训练，建立一个视觉适应性模型，估计人类在场景中的互动位置和方式。<br>
                    效果：我们的方法被称为视觉机器人桥（VRB），在4个真实世界中的环境中，超过10个不同的任务和2个在野外操作的机器人平台上展示了其有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Building a robot that can understand and learn to interact by watching humans has inspired several vision problems. However, despite some successful results on static datasets, it remains unclear how current models can be used on a robot directly. In this paper, we aim to bridge this gap by leveraging videos of human interactions in an environment centric manner. Utilizing internet videos of human behavior, we train a visual affordance model that estimates where and how in the scene a human is likely to interact. The structure of these behavioral affordances directly enables the robot to perform many complex tasks. We show how to seamlessly integrate our affordance model with four robot learning paradigms including offline imitation learning, exploration, goal-conditioned learning, and action parameterization for reinforcement learning. We show the efficacy of our approach, which we call Vision-Robotics Bridge (VRB) across 4 real world environments, over 10 different tasks, and 2 robotic platforms operating in the wild.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2320.Toward RAW Object Detection: A New Benchmark and a New Model</span><br>
                <span class="as">Xu, RuikangandChen, ChangandPeng, JingyangandLi, ChengandHuang, YibinandSong, FenglongandYan, YouliangandXiong, Zhiwei</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_a_New_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13384-13393.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何在无需额外设备成本的情况下，让物体检测算法处理各种光照条件，如强光。<br>
                    动机：在许多计算机视觉应用中（例如机器人和自动驾驶），高动态范围（HDR）数据对于物体检测算法处理各种光照条件是必要的。<br>
                    方法：构建了一个名为ROD的新颖RAW传感器数据集，用于基于深度神经网络（DNNs）的物体检测算法应用于HDR数据。该数据集包含大量标注的日间和夜间驾驶场景的24位动态范围实例。<br>
                    效果：实验表明，RAW传感器数据上的检测性能在不同情况下明显优于标准动态范围（SDR）数据。此外，我们还分析了输入数据的纹理信息和像素分布对基于DNNs的检测器性能的影响。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In many computer vision applications (e.g., robotics and autonomous driving), high dynamic range (HDR) data is necessary for object detection algorithms to handle a variety of lighting conditions, such as strong glare. In this paper, we aim to achieve object detection on RAW sensor data, which naturally saves the HDR information from image sensors without extra equipment costs. We build a novel RAW sensor dataset, named ROD, for Deep Neural Networks (DNNs)-based object detection algorithms to be applied to HDR data. The ROD dataset contains a large amount of annotated instances of day and night driving scenes in 24-bit dynamic range. Based on the dataset, we first investigate the impact of dynamic range for DNNs-based detectors and demonstrate the importance of dynamic range adjustment for detection on RAW sensor data. Then, we propose a simple and effective adjustment method for object detection on HDR RAW sensor data, which is image adaptive and jointly optimized with the downstream detector in an end-to-end scheme. Extensive experiments demonstrate that the performance of detection on RAW sensor data is significantly superior to standard dynamic range (SDR) data in different situations. Moreover, we analyze the influence of texture information and pixel distribution of input data on the performance of the DNNs-based detector.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2321.Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving</span><br>
                <span class="as">Liang, XiwenandNiu, MinzheandHan, JianhuaandXu, HangandXu, ChunjingandLiang, Xiaodan</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Visual_Exemplar_Driven_Task-Prompting_for_Unified_Perception_in_Autonomous_Driving_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9611-9621.png><br>
            
            <span class="tt"><span class="t0">研究问题：目前多任务学习算法在自动驾驶领域的表现与单任务基准存在较大差距。<br>
                    动机：为了解决这一问题，我们提出了一种有效的多任务框架VE-Prompt，通过引入视觉范例来引导模型学习高质量的特定任务表示。<br>
                    方法：我们在BDD100K数据集上实施了VE-Prompt，该数据集覆盖了对象检测、语义分割、可驾驶区域分割和车道检测等四种常见感知任务。<br>
                    效果：实验结果表明，VE-Prompt不仅提高了多任务基线的性能，而且超过了单任务模型，显示出其在自动驾驶领域的有效性。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Multi-task learning has emerged as a powerful paradigm to solve a range of tasks simultaneously with good efficiency in both computation resources and inference time. However, these algorithms are designed for different tasks mostly not within the scope of autonomous driving, thus making it hard to compare multi-task methods in autonomous driving. Aiming to enable the comprehensive evaluation of present multi-task learning methods in autonomous driving, we extensively investigate the performance of popular multi-task methods on the large-scale driving dataset, which covers four common perception tasks, i.e., object detection, semantic segmentation, drivable area segmentation, and lane detection. We provide an in-depth analysis of current multi-task learning methods under different common settings and find out that the existing methods make progress but there is still a large performance gap compared with single-task baselines. To alleviate this dilemma in autonomous driving, we present an effective multi-task framework, VE-Prompt, which introduces visual exemplars via task-specific prompting to guide the model toward learning high-quality task-specific representations. Specifically, we generate visual exemplars based on bounding boxes and color-based markers, which provide accurate visual appearances of target categories and further mitigate the performance gap. Furthermore, we bridge transformer-based encoders and convolutional layers for efficient and accurate unified perception in autonomous driving. Comprehensive experimental results on the diverse self-driving dataset BDD100K show that the VE-Prompt improves the multi-task baseline and further surpasses single-task models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2322.Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation</span><br>
                <span class="as">Otani, MayuandTogashi, RikuandSawai, YuandIshigami, RyosukeandNakashima, YutaandRahtu, EsaandHeikkil\&quot;a, JanneandSatoh, Shin{\textquoteright</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/14277-14286.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何进行有效的文本到图像生成模型的人类评估，以验证其性能。<br>
                    动机：当前许多研究仅依赖自动测量或描述不清、不可靠的人工评估，这导致无法验证和重复的结果。<br>
                    方法：本文提出了一种标准化和明确定义的人工评估协议，以促进未来工作的可验证和可重复的人工评估。<br>
                    效果：实验结果显示，当前的自动测量与人类在评估文本到图像生成结果的性能方面的认知不兼容。同时，为设计可靠和决定性的人工评估实验提供了见解，并公开了一些资源供社区使用，以便于快速实施。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Human evaluation is critical for validating the performance of text-to-image generative models, as this highly cognitive process requires deep comprehension of text and images. However, our survey of 37 recent papers reveals that many works rely solely on automatic measures (e.g., FID) or perform poorly described human evaluations that are not reliable or repeatable. This paper proposes a standardized and well-defined human evaluation protocol to facilitate verifiable and reproducible human evaluation in future works. In our pilot data collection, we experimentally show that the current automatic measures are incompatible with human perception in evaluating the performance of the text-to-image generation results. Furthermore, we provide insights for designing human evaluation experiments reliably and conclusively. Finally, we make several resources publicly available to the community to facilitate easy and fast implementations.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2323.BAEFormer: Bi-Directional and Early Interaction Transformers for Bird&#x27;s Eye View Semantic Segmentation</span><br>
                <span class="as">Pan, CongandHe, YonghaoandPeng, JunranandZhang, QianandSui, WeiandZhang, Zhaoxiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_BAEFormer_Bi-Directional_and_Early_Interaction_Transformers_for_Birds_Eye_View_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9590-9599.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何将透视图转换为鸟瞰图语义分割，以解决自动驾驶中的关键任务。<br>
                    动机：现有的基于Transformer的方法在将透视图转换为鸟瞰图时面临困难，因为它们的单向和后向交互机制。<br>
                    方法：提出了一种名为BAEFormer的新型双向早期交互Transformers框架，包括（i）早期交互的透视图-鸟瞰图管道和（ii）双向交叉注意力机制。<br>
                    效果：在nuScenes数据集上，所提出的方法在实时推理速度方面达到了最先进的性能，即在单个A100 GPU上以45 FPS的速度达到38.9 mIoU。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Bird's Eye View (BEV) semantic segmentation is a critical task in autonomous driving. However, existing Transformer-based methods confront difficulties in transforming Perspective View (PV) to BEV due to their unidirectional and posterior interaction mechanisms. To address this issue, we propose a novel Bi-directional and Early Interaction Transformers framework named BAEFormer, consisting of (i) an early-interaction PV-BEV pipeline and (ii) a bi-directional cross-attention mechanism. Moreover, we find that the image feature maps' resolution in the cross-attention module has a limited effect on the final performance. Under this critical observation, we propose to enlarge the size of input images and downsample the multi-view image features for cross-interaction, further improving the accuracy while keeping the amount of computation controllable. Our proposed method for BEV semantic segmentation achieves state-of-the-art performance in real-time inference speed on the nuScenes dataset, i.e., 38.9 mIoU at 45 FPS on a single A100 GPU.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2324.3D-POP - An Automated Annotation Approach to Facilitate Markerless 2D-3D Tracking of Freely Moving Birds With Marker-Based Motion Capture</span><br>
                <span class="as">Naik, HemalandChan, AlexHoiHangandYang, JunranandDelacoux, MathildeandCouzin, IainD.andKano, FumihiroandNagy, M\&#x27;at\&#x27;e</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Naik_3D-POP_-_An_Automated_Annotation_Approach_to_Facilitate_Markerless_2D-3D_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21274-21284.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的动物行为跟踪技术需要标记，缺乏大数据集。<br>
                    动机：利用机器学习和计算机视觉的进步，开发一种无需标记即可追踪动物姿势和位置的方法。<br>
                    方法：提出一种使用运动捕捉系统获取动物运动和姿势（2D和3D）的半自动标注大量数据的方法，通过提取形态关键点相对于动物身上标记的位置的3D位置。<br>
                    效果：创建了一个新的数据集3D-POP，包含约30万个标注帧（400万个实例），以视频形式记录了在3.6m x 4.2m区域内从四个不同相机视角自由移动的一至十只鸟。这是首个具有精确关键点2D和3D注释以及边界框和个人身份识别的群集鸟类数据集，将有助于解决鸟类的2D到3D无标记姿态、轨迹跟踪和识别问题。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent advances in machine learning and computer vision are revolutionizing the field of animal behavior by enabling researchers to track the poses and locations of freely moving animals without any marker attachment. However, large datasets of annotated images of animals for markerless pose tracking, especially high-resolution images taken from multiple angles with accurate 3D annotations, are still scant. Here, we propose a method that uses a motion capture (mo-cap) system to obtain a large amount of annotated data on animal movement and posture (2D and 3D) in a semi-automatic manner. Our method is novel in that it extracts the 3D positions of morphological keypoints (e.g eyes, beak, tail) in reference to the positions of markers attached to the animals. Using this method, we obtained, and offer here, a new dataset - 3D-POP with approximately 300k annotated frames (4 million instances) in the form of videos having groups of one to ten freely moving birds from 4 different camera views in a 3.6m x 4.2m area. 3D-POP is the first dataset of flocking birds with accurate keypoint annotations in 2D and 3D along with bounding box and individual identities and will facilitate the development of solutions for problems of 2D to 3D markerless pose, trajectory tracking, and identification in birds.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2325.Policy Adaptation From Foundation Model Feedback</span><br>
                <span class="as">Ge, YuyingandMacaluso, AnnabellaandLi, LiErranandLuo, PingandWang, Xiaolong</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Policy_Adaptation_From_Foundation_Model_Feedback_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19059-19069.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高视觉-语言基础模型在未见过的任务或环境中的表现。<br>
                    动机：尽管现有的预训练模型在处理不同对象和任务时具有泛化能力，但在面对未见过的任务或环境时，其表现往往不佳。<br>
                    方法：提出一种从基础模型反馈中进行策略调整的方法（PAFF）。当将训练好的策略部署到新任务或新环境中时，首先让策略与随机生成的指令一起运行以记录演示。然后使用预训练的基础模型提供反馈来重新标记这些演示，从而自动为策略微调提供新的演示-指令数据对。<br>
                    效果：在广泛的实验中评估了该方法，重点关注在未见过的物体、任务、环境和模拟到现实的转换上的表现。结果显示，PAFF在所有情况下都能大幅提高基线性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unseen tasks, unseen environments, and sim-to-real transfer. We show PAFF improves baselines by a large margin in all cases.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2326.Infinite Photorealistic Worlds Using Procedural Generation</span><br>
                <span class="as">Raistrick, AlexanderandLipson, LahavandMa, ZeyuandMei, LingjieandWang, MingzheandZuo, YimingandKayan, KarhanandWen, HongyuandHan, BeiningandWang, YihanandNewell, AlejandroandLaw, HeiandGoyal, AnkitandYang, KaiyuandDeng, Jia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Raistrick_Infinite_Photorealistic_Worlds_Using_Procedural_Generation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12630-12641.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用程序生成器生成逼真的自然世界3D场景。<br>
                    动机：为了提供无限的、多样化的计算机视觉训练数据，以应对各种计算机视觉任务。<br>
                    方法：开发了一个名为Infinigen的程序生成器，通过随机化的数学规则从零开始生成所有的资产，包括形状和纹理，无需使用任何外部资源，允许无限的变化和组合。<br>
                    效果：Infinigen可以生成广泛的自然世界中的对象和场景，如植物、动物、地形以及火、云、雨、雪等自然现象。它可以用于生成无限的、多样化的训练数据，适用于对象检测、语义分割、光流和3D重建等多种计算机视觉任务。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We introduce Infinigen, a procedural generator of photorealistic 3D scenes of the natural world. Infinigen is entirely procedural: every asset, from shape to texture, is generated from scratch via randomized mathematical rules, using no external source and allowing infinite variation and composition. Infinigen offers broad coverage of objects and scenes in the natural world including plants, animals, terrains, and natural phenomena such as fire, cloud, rain, and snow. Infinigen can be used to generate unlimited, diverse training data for a wide range of computer vision tasks including object detection, semantic segmentation, optical flow, and 3D reconstruction. We expect Infinigen to be a useful resource for computer vision research and beyond. Please visit https://infinigen.org for videos, code and pre-generated data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2327.KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation</span><br>
                <span class="as">Li, XiangyangandWang, ZihanandYang, JiahaoandWang, YaoweiandJiang, Shuqiang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/2583-2592.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决视觉-语言导航任务中，如何利用知识提高代理的导航能力。<br>
                    动机：现有的方法主要使用整个特征或以物体为中心的特征来表示可导航的候选对象，但这些表示方式对代理执行到达目标位置的动作来说效率不够高。<br>
                    方法：本文提出了一种基于知识的增强推理模型（KERM），通过从构建的知识库中检索与导航视图相关的事实（即由语言描述的知识）来提升代理的导航能力。<br>
                    效果：实验结果表明，提出的KERM在REVERIE、R2R和SOON等数据集上取得了良好的效果，能够自动选择和收集关键和相关的线索，进行更准确的行动预测。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-and-language navigation (VLN) is the task to enable an embodied agent to navigate to a remote location following the natural language instruction in real scenes. Most of the previous approaches utilize the entire features or object-centric features to represent navigable candidates. However, these representations are not efficient enough for an agent to perform actions to arrive the target location. As knowledge provides crucial information which is complementary to visible content, in this paper, we propose a Knowledge Enhanced Reasoning Model (KERM) to leverage knowledge to improve agent navigation ability. Specifically, we first retrieve facts (i.e., knowledge described by language descriptions) for the navigation views based on local regions from the constructed knowledge base. The retrieved facts range from properties of a single object (e.g., color, shape) to relationships between objects (e.g., action, spatial position), providing crucial information for VLN. We further present the KERM which contains the purification, fact-aware interaction, and instruction-guided aggregation modules to integrate visual, history, instruction, and fact features. The proposed KERM can automatically select and gather crucial and relevant cues, obtaining more accurate action prediction. Experimental results on the REVERIE, R2R, and SOON datasets demonstrate the effectiveness of the proposed method. The source code is available at https://github.com/XiangyangLi20/KERM.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2328.LiDAR-in-the-Loop Hyperparameter Optimization</span><br>
                <span class="as">Goudreault, F\&#x27;elixandScheuble, DominikandBijelic, MarioandRobidoux, NicolasandHeide, Felix</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goudreault_LiDAR-in-the-Loop_Hyperparameter_Optimization_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13404-13414.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何优化激光雷达（LiDAR）系统参数以提高三维视觉任务的性能。<br>
                    动机：现有的LiDAR系统通常被视为黑箱，其参数调整主要依赖于人工专家经验，缺乏系统性的优化方法。<br>
                    方法：提出一种直接优化LiDAR传感和数字信号处理（DSP）参数的方法，通过非线性多目标优化问题求解，并使用0阶随机算法进行优化。<br>
                    效果：在自动驾驶三维物体检测模型上，该方法比人工专家调优提高了39.5%的平均精度（mAP）。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>LiDAR has become a cornerstone sensing modality for 3D vision. LiDAR systems emit pulses of light into the scene, take measurements of the returned signal, and rely on hardware digital signal processing (DSP) pipelines to construct 3D point clouds from these measurements. The resulting point clouds output by these DSPs are input to downstream 3D vision models -- both, in the form of training datasets or as input at inference time. Existing LiDAR DSPs are composed of cascades of parameterized operations; modifying configuration parameters results in significant changes in the point clouds and consequently the output of downstream methods. Existing methods treat LiDAR systems as fixed black boxes and construct downstream task networks more robust with respect to measurement fluctuations. Departing from this approach, the proposed method directly optimizes LiDAR sensing and DSP parameters for downstream tasks. To investigate the optimization of LiDAR system parameters, we devise a realistic LiDAR simulation method that generates raw waveforms as input to a LiDAR DSP pipeline. We optimize LiDAR parameters for both 3D object detection IoU losses and depth error metrics by solving a nonlinear multi-objective optimization problem with a 0th-order stochastic algorithm. For automotive 3D object detection models, the proposed method outperforms manual expert tuning by 39.5% mean Average Precision (mAP).</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2329.BEVHeight: A Robust Framework for Vision-Based Roadside 3D Object Detection</span><br>
                <span class="as">Yang, LeiandYu, KaichengandTang, TaoandLi, JunandYuan, KunandWang, LiandZhang, XinyuandChen, Peng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVHeight_A_Robust_Framework_for_Vision-Based_Roadside_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21611-21620.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用智能路旁摄像头来扩展感知能力，超越视觉范围。<br>
                    动机：目前自动驾驶系统主要关注车辆传感器的感知方法，而忽视了利用智能路旁摄像头作为另一种可能的感知方式。<br>
                    方法：提出一种名为BEVHeight的简单有效方法，通过预测地面高度而非像素级深度，实现距离无关的优化过程，以解决现有视觉中心的方法在远距离时深度差异快速缩小的问题。<br>
                    效果：在流行的路旁摄像头3D检测基准测试中，该方法大幅超越了所有先前的视觉中心方法。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>While most recent autonomous driving system focuses on developing perception methods on ego-vehicle sensors, people tend to overlook an alternative approach to leverage intelligent roadside cameras to extend the perception ability beyond the visual range. We discover that the state-of-the-art vision-centric bird's eye view detection methods have inferior performances on roadside cameras. This is because these methods mainly focus on recovering the depth regarding the camera center, where the depth difference between the car and the ground quickly shrinks while the distance increases. In this paper, we propose a simple yet effective approach, dubbed BEVHeight, to address this issue. In essence, instead of predicting the pixel-wise depth, we regress the height to the ground to achieve a distance-agnostic formulation to ease the optimization process of camera-only perception methods. On popular 3D detection benchmarks of roadside cameras, our method surpasses all previous vision-centric methods by a significant margin. The code is available at https://github.com/ADLab-AutoDrive/BEVHeight.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2330.MVImgNet: A Large-Scale Dataset of Multi-View Images</span><br>
                <span class="as">Yu, XianggangandXu, MutianandZhang, YidanandLiu, HaolinandYe, ChongjieandWu, YushuangandYan, ZizhengandZhu, ChenmingandXiong, ZhangyangandLiang, TianyouandChen, GuanyingandCui, ShuguangandHan, Xiaoguang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MVImgNet_A_Large-Scale_Dataset_of_Multi-View_Images_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/9150-9161.png><br>
            
            <span class="tt"><span class="t0">研究问题：由于真实世界3D数据的收集困难，目前还没有一个与ImageNet在2D视觉中的地位相当的通用数据集。<br>
                    动机：为了解决这个问题，我们引入了MVImgNet，这是一个大规模的多视角图像数据集，通过拍摄现实生活中物体的视频可以很容易地获取。<br>
                    方法：MVImgNet包含来自219,188个视频的650万个帧，涵盖了238个类别的对象，具有丰富的对象掩码、相机参数和点云注释。其多视角属性使我们的数据集具有3D感知信号，成为2D和3D视觉之间的软桥梁。<br>
                    效果：我们在多种3D和2D视觉任务上进行了初步研究，包括辐射场重建、多视图立体视觉和视图一致的图像理解，其中MVImgNet表现出了有希望的性能，为未来的探索留下了大量的可能性。此外，通过在MVImgNet上的密集重建，我们还得到了一个名为MVPNet的3D物体点云数据集，包含了来自150个类别的87,200个样本，每个点云都有类标签。实验表明，MVPNet可以促进真实世界的3D物体分类，同时对点云理解提出了新的挑战。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Being data-driven is one of the most iconic properties of deep learning algorithms. The birth of ImageNet drives a remarkable trend of "learning from large-scale data" in computer vision. Pretraining on ImageNet to obtain rich universal representations has been manifested to benefit various 2D visual tasks, and becomes a standard in 2D vision. However, due to the laborious collection of real-world 3D data, there is yet no generic dataset serving as a counterpart of ImageNet in 3D vision, thus how such a dataset can impact the 3D community is unraveled. To remedy this defect, we introduce MVImgNet, a large-scale dataset of multi-view images, which is highly convenient to gain by shooting videos of real-world objects in human daily life. It contains 6.5 million frames from 219,188 videos crossing objects from 238 classes, with rich annotations of object masks, camera parameters, and point clouds. The multi-view attribute endows our dataset with 3D-aware signals, making it a soft bridge between 2D and 3D vision. We conduct pilot studies for probing the potential of MVImgNet on a variety of 3D and 2D visual tasks, including radiance field reconstruction, multi-view stereo, and view-consistent image understanding, where MVImgNet demonstrates promising performance, remaining lots of possibilities for future explorations. Besides, via dense reconstruction on MVImgNet, a 3D object point cloud dataset is derived, called MVPNet, covering 87,200 samples from 150 categories, with the class label on each point cloud. Experiments show that MVPNet can benefit the real-world 3D object classification while posing new challenges to point cloud understanding. MVImgNet and MVPNet will be publicly available, hoping to inspire the broader vision community.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2331.A New Benchmark: On the Utility of Synthetic Data With Blender for Bare Supervised Learning and Downstream Domain Adaptation</span><br>
                <span class="as">Tang, HuiandJia, Kui</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_A_New_Benchmark_On_the_Utility_of_Synthetic_Data_With_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/15954-15964.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在解决深度学习在计算机视觉领域的问题，如大规模标注训练数据的获取困难、数据收集过程中的非独立同分布（IID）问题等。<br>
                    动机：由于高昂的人工成本和标签准确性无法保证，全面的数据标注对于所有感兴趣的领域来说都是不现实的。此外，不可控制的数据收集过程可能会产生非IID的训练和测试数据，其中可能存在不希望出现的重复。所有这些问题都可能阻碍对典型理论的验证和新发现的出现。<br>
                    方法：本文通过使用3D渲染生成合成数据并实施领域随机化，以规避这些问题。在可控的、由3D渲染实现的IID数据设置下，我们系统地验证了典型的、重要的学习见解，例如捷径学习，并发现了各种数据模式和网络架构在泛化中的新规律。<br>
                    效果：我们还研究了图像形成因素对泛化的影响，例如3D场景中的对象比例、材料纹理、照明、相机视角和背景。此外，我们将模拟到现实的适应作为下游任务，比较预训练时合成数据和真实数据的可转移性，结果表明，合成数据预训练也有望提高真实测试结果。最后，为了推动未来的研究，我们开发了一个名为S2RDA的新的大型合成到真实基准测试集，用于图像分类，为从模拟到现实的转换提供了更大的挑战。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Deep learning in computer vision has achieved great success with the price of large-scale labeled training data. However, exhaustive data annotation is impracticable for each task of all domains of interest, due to high labor costs and unguaranteed labeling accuracy. Besides, the uncontrollable data collection process produces non-IID training and test data, where undesired duplication may exist. All these nuisances may hinder the verification of typical theories and exposure to new findings. To circumvent them, an alternative is to generate synthetic data via 3D rendering with domain randomization. We in this work push forward along this line by doing profound and extensive research on bare supervised learning and downstream domain adaptation. Specifically, under the well-controlled, IID data setting enabled by 3D rendering, we systematically verify the typical, important learning insights, e.g., shortcut learning, and discover the new laws of various data regimes and network architectures in generalization. We further investigate the effect of image formation factors on generalization, e.g., object scale, material texture, illumination, camera viewpoint, and background in a 3D scene. Moreover, we use the simulation-to-reality adaptation as a downstream task for comparing the transferability between synthetic and real data when used for pre-training, which demonstrates that synthetic data pre-training is also promising to improve real test results. Lastly, to promote future research, we develop a new large-scale synthetic-to-real benchmark for image classification, termed S2RDA, which provides more significant challenges for transfer from simulation to reality. The code and datasets are available at https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2332.Temporal Consistent 3D LiDAR Representation Learning for Semantic Perception in Autonomous Driving</span><br>
                <span class="as">Nunes, LucasandWiesmann, LouisandMarcuzzi, RodrigoandChen, XieyuanliandBehley, JensandStachniss, Cyrill</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nunes_Temporal_Consistent_3D_LiDAR_Representation_Learning_for_Semantic_Perception_in_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5217-5228.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地利用大规模文本语料库和知识图谱训练语言表示模型，并充分利用词汇、句法和知识信息。<br>
                    动机：目前的预训练语言模型缺乏对丰富的结构化知识的利用，通过结合知识图谱中的有信息量的实体，可以增强语言表示。<br>
                    方法：采用大规模文本语料库和知识图谱进行联合训练，训练出ERNIE模型，该模型能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Semantic perception is a core building block in autonomous driving, since it provides information about the drivable space and location of other traffic participants. For learning-based perception, often a large amount of diverse training data is necessary to achieve high performance. Data labeling is usually a bottleneck for developing such methods, especially for dense prediction tasks, e.g., semantic segmentation or panoptic segmentation. For 3D LiDAR data, the annotation process demands even more effort than for images. Especially in autonomous driving, point clouds are sparse, and objects appearance depends on its distance from the sensor, making it harder to acquire large amounts of labeled training data. This paper aims at taking an alternative path proposing a self-supervised representation learning method for 3D LiDAR data. Our approach exploits the vehicle motion to match objects across time viewed in different scans. We then train a model to maximize the point-wise feature similarities from points of the associated object in different scans, which enables to learn a consistent representation across time. The experimental results show that our approach performs better than previous state-of-the-art self-supervised representation learning methods when fine-tuning to different downstream tasks. We furthermore show that with only 10% of labeled data, a network pre-trained with our approach can achieve better performance than the same network trained from scratch with all labels for semantic segmentation on SemanticKITTI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2333.Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention</span><br>
                <span class="as">Mondal, SounakandYang, ZhiboandAhn, SeoyoungandSamaras, DimitrisandZelinsky, GregoryandHoai, Minh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mondal_Gazeformer_Scalable_Effective_and_Fast_Prediction_of_Goal-Directed_Human_Attention_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/1441-1450.png><br>
            
            <span class="tt"><span class="t0">研究问题：预测人类视线在人机交互中的重要性，但现有的模型需要训练目标检测器和大量的人类视线数据，且速度慢、准确性不高。<br>
                    动机：为了解决这些问题，我们提出了一种新的任务——ZeroGaze，并开发了一种新的模型Gazeformer。<br>
                    方法：Gazeformer使用自然语言模型对目标进行编码，利用语义相似性进行扫描路径预测，采用基于变压器的编码-解码架构。<br>
                    效果：实验结果表明，Gazeformer在ZeroGaze设置上比其他模型有显著的改进（19% - 70%），并在标准视线预测的目标存在和缺失搜索任务上都优于现有的目标检测模型。此外，Gazeformer的速度比最先进的目标存在视觉搜索模型快五倍以上。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Predicting human gaze is important in Human-Computer Interaction (HCI). However, to practically serve HCI applications, gaze prediction models must be scalable, fast, and accurate in their spatial and temporal gaze predictions. Recent scanpath prediction models focus on goal-directed attention (search). Such models are limited in their application due to a common approach relying on trained target detectors for all possible objects, and the availability of human gaze data for their training (both not scalable). In response, we pose a new task called ZeroGaze, a new variant of zero-shot learning where gaze is predicted for never-before-searched objects, and we develop a novel model, Gazeformer, to solve the ZeroGaze problem. In contrast to existing methods using object detector modules, Gazeformer encodes the target using a natural language model, thus leveraging semantic similarities in scanpath prediction. We use a transformer-based encoder-decoder architecture because transformers are particularly useful for generating contextual representations. Gazeformer surpasses other models by a large margin (19% - 70%) on the ZeroGaze setting. It also outperforms existing target-detection models on standard gaze prediction for both target-present and target-absent search tasks. In addition to its improved performance, Gazeformer is more than five times faster than the state-of-the-art target-present visual search model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2334.MammalNet: A Large-Scale Video Benchmark for Mammal Recognition and Behavior Understanding</span><br>
                <span class="as">Chen, JunandHu, MingandCoker, DarrenJ.andBerumen, MichaelL.andCostelloe, BlairandBeery, SaraandRohrbach, AnnaandElhoseiny, Mohamed</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MammalNet_A_Large-Scale_Video_Benchmark_for_Mammal_Recognition_and_Behavior_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13052-13061.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何有效地监测动物行为以促进保护工作？<br>
                    动机：现有的视频数据集无法满足大规模动物行为识别的需求，缺乏适当的标签和分类。<br>
                    方法：开发了一种新的大型动物行为数据集MammalNet，包含超过18K的视频，覆盖了17个目、69个科和173种哺乳动物类别，以及12种常见的高级动物行为。<br>
                    效果：MammalNet比现有的任何动物行为数据集都要大十倍，可以用于进行大规模的动物行为识别研究。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monitoring animal behavior can facilitate conservation efforts by providing key insights into wildlife health, population status, and ecosystem function. Automatic recognition of animals and their behaviors is critical for capitalizing on the large unlabeled datasets generated by modern video devices and for accelerating monitoring efforts at scale. However, the development of automated recognition systems is currently hindered by a lack of appropriately labeled datasets. Existing video datasets 1) do not classify animals according to established biological taxonomies; 2) are too small to facilitate large-scale behavioral studies and are often limited to a single species; and 3) do not feature temporally localized annotations and therefore do not facilitate localization of targeted behaviors within longer video sequences. Thus, we propose MammalNet, a new large-scale animal behavior dataset with taxonomy-guided annotations of mammals and their common behaviors. MammalNet contains over 18K videos totaling 539 hours, which is  10 times larger than the largest existing animal behavior dataset. It covers 17 orders, 69 families, and 173 mammal categories for animal categorization and captures 12 high-level animal behaviors that received focus in previous animal behavior studies. We establish three benchmarks on MammalNet: standard animal and behavior recognition, compositional low-shot animal and behavior recognition, and behavior detection. Our dataset and code have been made available at: https://mammal-net.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2335.ReasonNet: End-to-End Driving With Temporal and Global Reasoning</span><br>
                <span class="as">Shao, HaoandWang, LetianandChen, RuobingandWaslander, StevenL.andLi, HongshengandLiu, Yu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_ReasonNet_End-to-End_Driving_With_Temporal_and_Global_Reasoning_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13723-13733.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何预测城市密集交通场景的未来演变和物体行为，以及处理罕见的负面事件，如突然出现的遮挡物体。<br>
                    动机：当前自动驾驶车辆的大规模部署尚未实现，其中主要的挑战之一在于城市密集交通场景。在这些情况下，预测场景的未来演变和物体的行为，以及处理罕见的负面事件，如突然出现的遮挡物体，仍然具有挑战性。<br>
                    方法：本文提出了一种名为ReasonNet的新型端到端驾驶框架，该框架广泛利用了驾驶场景的时空信息。通过推理物体的时间行为，该方法可以有效地处理不同帧中特征之间的交互和关系。对场景全局信息的推理也可以提高整体感知性能，并有利于负面事件的检测，特别是对遮挡物体潜在危险的预期。<br>
                    效果：在多个CARLA基准上进行了广泛的实验，其中我们的模型在所有先前的方法中表现最好，在公共CARLA排行榜的传感器轨迹上排名第一。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive experiments on multiple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public CARLA Leaderboard.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2336.SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments</span><br>
                <span class="as">Dai, YudiandLin, YitaiandLin, XipingandWen, ChengluandXu, LanandYi, HongweiandShen, SiqiandMa, YuexinandWang, Cheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/682-692.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在开发一种新的场景感知数据集SLOPER4D，以促进在野外进行全球人体姿态估计（GHPE）和人与场景互动的研究。<br>
                    动机：通过使用集成了激光雷达和相机的头戴式设备，记录了12个主题在10个不同的城市环境中的活动，从自我中心的视角出发，为研究提供了丰富的动态场景数据。<br>
                    方法：提出了一种联合优化方法，将局部SMPL网格拟合到场景中，并在动态运动的每一帧中微调相机校准，从而得到可信且符合场景的自然3D人体姿态。<br>
                    效果：SLOPER4D数据集包含15个人类运动序列，每个序列的轨迹长度超过200米（最长达到1300米），覆盖面积超过200平方米（最大达到3万平方米）。该数据集对现有的方法提出了重大挑战，并为研究提供了巨大的机会。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present SLOPER4D, a novel scene-aware dataset collected in large urban environments to facilitate the research of global human pose estimation (GHPE) with human-scene interaction in the wild. Employing a head-mounted device integrated with a LiDAR and camera, we record 12 human subjects' activities over 10 diverse urban scenes from an egocentric view. Frame-wise annotations for 2D key points, 3D pose parameters, and global translations are provided, together with reconstructed scene point clouds. To obtain accurate 3D ground truth in such large dynamic scenes, we propose a joint optimization method to fit local SMPL meshes to the scene and fine-tune the camera calibration during dynamic motions frame by frame, resulting in plausible and scene-natural 3D human poses. Eventually, SLOPER4D consists of 15 sequences of human motions, each of which has a trajectory length of more than 200 meters (up to 1,300 meters) and covers an area of more than 200 square meters (up to 30,000 square meters), including more than 100k LiDAR frames, 300k video frames, and 500K IMU-based motion frames. With SLOPER4D, we provide a detailed and thorough analysis of two critical tasks, including camera-based 3D HPE and LiDAR-based 3D HPE in urban environments, and benchmark a new task, GHPE. The in-depth analysis demonstrates SLOPER4D poses significant challenges to existing methods and produces great research opportunities. The dataset and code are released at http://www.lidarhumanmotion.net/sloper4d/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2337.SketchXAI: A First Look at Explainability for Human Sketches</span><br>
                <span class="as">Qu, ZhiyuandGryaditskaya, YuliaandLi, KeandPang, KaiyueandXiang, TaoandSong, Yi-Zhe</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qu_SketchXAI_A_First_Look_at_Explainability_for_Human_Sketches_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23327-23337.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文首次将人类草图引入可解释人工智能（XAI）领域，探讨如何通过草图这种"以人为本"的数据形式来研究可解释性。<br>
                    动机：草图作为一种具有灵活性的构建和操作对象的数据形式，可以作为研究可解释性的自然接口。<br>
                    方法：首先，将草图中的笔划识别为一个独特的构建块，然后设计了一个简单且易于解释的草图编码器，以适应笔划的形状、位置和顺序等内在属性。接着，定义了第一个针对草图的XAI任务——笔划位置反转（SLI），即要求网络恢复未见过草图的笔划位置。<br>
                    效果：实验结果表明，由于其针对草图的设计，该草图编码器不仅在草图识别准确性上取得了迄今为止最好的结果，而且参数数量也最少。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial Intelligence). We argue that sketch as a "human-centred" data form, represents a natural interface to study explainability. We focus on cultivating sketch-specific explainability designs. This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we design a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, location, and order. We then move on to define the first ever XAI task for sketch, that of stroke location inversion SLI. Just as we have heat maps for photos, and correlation matrices for text, SLI offers an explainability angle to sketch in terms of asking a network how well it can recover stroke locations of an unseen sketch. We offer qualitative results for readers to interpret as snapshots of the SLI process in the paper, and as GIFs on the project page. A minor but interesting note is that thanks to its sketch-specific design, our sketch encoder also yields the best sketch recognition accuracy to date while having the smallest number of parameters. The code is available at https://sketchxai.github.io.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2338.Omni3D: A Large Benchmark and Model for 3D Object Detection in the Wild</span><br>
                <span class="as">Brazil, GarrickandKumar, AbhinavandStraub, JulianandRavi, NikhilaandJohnson, JustinandGkioxari, Georgia</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brazil_Omni3D_A_Large_Benchmark_and_Model_for_3D_Object_Detection_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13154-13164.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何从单张图像中识别3D场景和物体，以实现计算机视觉在机器人和AR/VR中的应用。<br>
                    动机：尽管2D识别已经取得了显著的进步，但3D识别由于数据集小、方法专一等问题，其进展有限。<br>
                    方法：通过重新利用和组合现有的数据集，构建了一个大规模的3D物体检测基准Omni3D，包含234k张图片和超过300万个实例和98个类别的标注。同时，提出了Cube R-CNN模型，该模型能够统一处理不同相机和场景类型的问题。<br>
                    效果：实验证明，Cube R-CNN在更大的Omni3D上的表现优于现有方法，Omni3D是一个强大的3D物体识别数据集，可以提高单一数据集的性能，并通过预训练加速新小型数据集的学习。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Recognizing scenes and objects in 3D from a single image is a longstanding goal of computer vision with applications in robotics and AR/VR. For 2D recognition, large datasets and scalable solutions have led to unprecedented advances. In 3D, existing benchmarks are small in size and approaches specialize in few object categories and specific domains, e.g. urban driving scenes. Motivated by the success of 2D recognition, we revisit the task of 3D object detection by introducing a large benchmark, called Omni3D. Omni3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 98 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types. We propose a model, called Cube R-CNN, designed to generalize across camera and scene types with a unified approach. We show that Cube R-CNN outperforms prior works on the larger Omni3D and existing benchmarks. Finally, we prove that Omni3D is a powerful dataset for 3D object recognition and show that it improves single-dataset performance and can accelerate learning on new smaller datasets via pre-training.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2339.UniDistill: A Universal Cross-Modality Knowledge Distillation Framework for 3D Object Detection in Bird&#x27;s-Eye View</span><br>
                <span class="as">Zhou, ShengchaoandLiu, WeizhouandHu, ChenandZhou, ShuchangandMa, Chao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_UniDistill_A_Universal_Cross-Modality_Knowledge_Distillation_Framework_for_3D_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5116-5125.png><br>
            
            <span class="tt"><span class="t0">研究问题：在自动驾驶的3D物体检测领域，多模态和单模态的传感器组合多样且复杂。由于多模态方法系统复杂性高，而单模态方法的准确性相对较低，如何在这两者之间进行权衡是困难的。<br>
                    动机：为了提高单模态检测器的性能，我们提出了一种通用跨模态知识蒸馏框架（UniDistill）。<br>
                    方法：在训练过程中，UniDistill将教师和学生检测器的特征投影到鸟瞰图（BEV）中，这是一种对不同模态友好的表示。然后，计算三种蒸馏损失来稀疏地对齐前景特征，帮助学生在不引入额外推理成本的情况下从教师那里学习。<br>
                    效果：通过利用不同检测器在BEV中的相似检测范式，UniDistill可以轻松支持LiDAR到相机、相机到LiDAR、融合到LiDAR和融合到相机的蒸馏路径。此外，这三种蒸馏损失可以过滤误对准的背景信息的影响，并平衡不同大小的对象，从而提高蒸馏的效果。在nuScenes上的大量实验表明，UniDistill有效地提高了学生检测器的mAP和NDS 2.0% - 3.2%。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In the field of 3D object detection for autonomous driving, the sensor portfolio including multi-modality and single-modality is diverse and complex. Since the multi-modal methods have system complexity while the accuracy of single-modal ones is relatively low, how to make a tradeoff between them is difficult. In this work, we propose a universal cross-modality knowledge distillation framework (UniDistill) to improve the performance of single-modality detectors. Specifically, during training, UniDistill projects the features of both the teacher and the student detector into Bird's-Eye-View (BEV), which is a friendly representation for different modalities. Then, three distillation losses are calculated to sparsely align the foreground features, helping the student learn from the teacher without introducing additional cost during inference. Taking advantage of the similar detection paradigm of different detectors in BEV, UniDistill easily supports LiDAR-to-camera, camera-to-LiDAR, fusion-to-LiDAR and fusion-to-camera distillation paths. Furthermore, the three distillation losses can filter the effect of misaligned background information and balance between objects of different sizes, improving the distillation effectiveness. Extensive experiments on nuScenes demonstrate that UniDistill effectively improves the mAP and NDS of student detectors by 2.0% 3.2%.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2340.JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking</span><br>
                <span class="as">Vendrow, EdwardandLe, DuyThoandCai, JianfeiandRezatofighi, Hamid</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4811-4820.png><br>
            
            <span class="tt"><span class="t0">研究问题：在人类环境中运行的自主机器人系统必须理解周围环境，以做出准确和安全的决定。<br>
                    动机：现有的从机器人平台捕获的数据集要么没有提供姿势注释，要么没有反映社交机器人的场景分布。<br>
                    方法：介绍了JRDB-Pose，这是一个大规模的多人体姿势估计和跟踪数据集和基准。JRDB-Pose扩展了现有的JRDB，包括在大学校园环境中由社交导航机器人捕获的视频，包含具有挑战性的场景，如拥挤的室内和室外位置以及各种规模和遮挡类型。<br>
                    效果：我们在JRDB-Pose上对最先进的多人体姿势估计和跟踪方法进行了全面实验研究，表明我们的数据集对现有方法提出了新的挑战。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Autonomous robotic systems operating in human environments must understand their surroundings to make accurate and safe decisions. In crowded human scenes with close-up human-robot interaction and robot navigation, a deep understanding of surrounding people requires reasoning about human motion and body dynamics over time with human body pose estimation and tracking. However, existing datasets captured from robot platforms either do not provide pose annotations or do not reflect the scene distribution of social robots. In this paper, we introduce JRDB-Pose, a large-scale dataset and benchmark for multi-person pose estimation and tracking. JRDB-Pose extends the existing JRDB which includes videos captured from a social navigation robot in a university campus environment, containing challenging scenes with crowded indoor and outdoor locations and a diverse range of scales and occlusion types. JRDB-Pose provides human pose annotations with per-keypoint occlusion labels and track IDs consistent across the scene and with existing annotations in JRDB. We conduct a thorough experimental study of state-of-the-art multi-person pose estimation and tracking methods on JRDB-Pose, showing that our dataset imposes new challenges for the existing methods. JRDB-Pose is available at https://jrdb.erc.monash.edu/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2341.Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-per-Second</span><br>
                <span class="as">Berges, Vincent-PierreandSzot, AndrewandChaplot, DevendraSinghandGokaslan, AaronandMottaghi, RoozbehandBatra, DhruvandUndersander, Eric</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13767-13777.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高机器人在室内环境中的移动操作能力？<br>
                    动机：现有的模拟和强化学习框架在速度和效率上存在限制，无法满足大规模实验的需求。<br>
                    方法：开发了一个名为Galactic的大型模拟和强化学习框架，用于机器人在室内环境中的移动操作。该框架通过优化渲染、物理和强化学习之间的交互，实现了极高的模拟和学习速度。<br>
                    效果：Galactic在模拟速度和模拟+学习速度上都大大超过了现有的框架Habitat 2.0。利用Galactic，研究人员在短短的时间内训练出了高精度的移动拾取技能，并在大规模实验中实现了85%的成功率。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Galactic, a large-scale simulation and reinforcement-learning (RL) framework for robotic mobile manipulation in indoor environments. Specifically, a Fetch robot (equipped with a mobile base, 7DoF arm, RGBD camera, egomotion, and onboard sensing) is spawned in a home environment and asked to rearrange objects -- by navigating to an object, picking it up, navigating to a target location, and then placing the object at the target location. Galactic is fast. In terms of simulation speed (rendering + physics), Galactic achieves over 421,000 steps-per-second (SPS) on an 8-GPU node, which is 54x faster than Habitat 2.0 (7699 SPS). More importantly, Galactic was designed to optimize the entire rendering+physics+RL interplay since any bottleneck in the interplay slows down training. In terms of simulation+RL speed (rendering + physics + inference + learning), Galactic achieves over 108,000 SPS, which 88x faster than Habitat 2.0 (1243 SPS). These massive speed-ups not only drastically cut the wall-clock training time of existing experiments, but also unlock an unprecedented scale of new experiments. First, Galactic can train a mobile pick skill to >80% accuracy in under 16 minutes, a 100x speedup compared to the over 24 hours it takes to train the same skill in Habitat 2.0. Second, we use Galactic to perform the largest-scale experiment to date for rearrangement using 5B steps of experience in 46 hours, which is equivalent to 20 years of robot experience. This scaling results in a single neural network composed of task-agnostic components achieving 85% success in GeometricGoal rearrangement, compared to 0% success reported in Habitat 2.0 for the same approach. The code is available at github.com/facebookresearch/galactic.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2342.CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation</span><br>
                <span class="as">Gadre, SamirYitzhakandWortsman, MitchellandIlharco, GabrielandSchmidt, LudwigandSong, Shuran</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/23171-23181.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何让机器人在没有昂贵导航训练的情况下，通过人类的语言描述找到任意对象（即零射击推理）。<br>
                    动机：受到最近开放词汇模型在图像分类中成功的启发，研究人员探索了一种直接的框架——CLIP on Wheels (CoW)，以适应这种任务而无需微调。<br>
                    方法：采用大规模文本语料库和知识图谱来训练ERNIE模型，将KG中的知识与文本语料库进行联合训练，ERNIE能够更好地捕捉语义模式。<br>
                    效果：实验结果表明，ERNIE在各种知识驱动任务上取得了显著改进，并且在其他常见的NLP任务上与最先进的BERT模型相媲美。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language-driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasture benchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 22 CoW baselines across Habitat, RoboTHOR, and Pasture. In total we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions, but are surprisingly proficient at finding uncommon objects. (2) A simple CoW, with CLIP-based object localization and classical exploration---and no additional training---matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on Habitat MP3D data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art RoboTHOR ZSON model.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2343.GINA-3D: Learning To Generate Implicit Neural Assets in the Wild</span><br>
                <span class="as">Shen, BokuiandYan, XinchenandQi, CharlesR.andNajibi, MahyarandDeng, BoyangandGuibas, LeonidasandZhou, YinandAnguelov, Dragomir</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_GINA-3D_Learning_To_Generate_Implicit_Neural_Assets_in_the_Wild_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/4913-4926.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何利用传感器数据对3D世界进行建模，以模拟机器人学习问题（如自动驾驶）的测试和验证环境？<br>
                    动机：手动创建或重建类似真实世界的3D环境既困难又昂贵，且无法扩展。现有的生成模型技术通过仅使用丰富的2D图像来学习3D资产，但仍然受到限制，因为它们要么依赖于人类策划的图像数据集，要么依赖于手动创建的合成3D环境的渲染。<br>
                    方法：我们引入GINA-3D，一种使用来自相机和激光雷达传感器的真实世界驾驶数据的生成模型，用于创建多样化车辆和行人的逼真3D隐式神经资产。<br>
                    效果：与现有图像数据集相比，真实世界驾驶设置由于遮挡、光照变化和长尾分布而带来新的挑战。GINA-3D通过将表示学习和生成建模分为两个阶段并用学习的三平面潜在结构来解决这些挑战，这受到了最近图像生成建模进展的启发。通过构建包含超过520K张车辆和行人图像的大规模面向对象的数据集以及80K张诸如施工设备、垃圾车和缆车的长尾实例新图像集，我们的方法在生成的图像和几何形状的质量与多样性方面均实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create photo-realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling representation learning and generative modeling into two stages with a learned tri-plane latent structure, inspired by recent advances in generative modeling of images. To evaluate our approach, we construct a large-scale object-centric dataset containing over 520K images of vehicles and pedestrians from the Waymo Open Dataset, and a new set of 80K images of long-tail instances such as construction equipment, garbage trucks, and cable cars. We compare our model with existing approaches and demonstrate that it achieves state-of-the-art performance in quality and diversity for both generated images and geometries.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2344.Consistent Direct Time-of-Flight Video Depth Super-Resolution</span><br>
                <span class="as">Sun, ZhanghaoandYe, WeiandXiong, JinhuiandChoe, GyeongminandWang, JialiangandSu, ShuochenandRanjan, Rakesh</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Consistent_Direct_Time-of-Flight_Video_Depth_Super-Resolution_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5075-5085.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何提高直接飞行时间（dToF）传感器的空间分辨率。<br>
                    动机：由于制造能力的限制，dToF数据的空间分辨率较低，需要进行超分辨率处理才能用于后续任务。<br>
                    方法：通过将低分辨率的dToF数据与相应的高分辨率RGB指导信息进行融合，提出首个多帧融合方案以减轻由低分辨率dToF成像引起的空间模糊。同时，利用dToF传感器为每个局部区域提供独特的深度直方图信息，进一步缓解空间模糊。<br>
                    效果：在复杂的动态室内环境中评估模型性能，并引入DyDToF，第一个具有动态对象和现实dToF模拟器的合成RGB-dToF视频数据集。这些方法和数据集对广大社区有益，因为dToF深度传感正在成为移动设备的主流。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Direct time-of-flight (dToF) sensors are promising for next-generation on-device 3D sensing. However, limited by manufacturing capabilities in a compact module, the dToF data has low spatial resolution (e.g.,  20x30 for iPhone dToF), and it requires a super-resolution step before being passed to downstream tasks. In this paper, we solve this super-resolution problem by fusing the low-resolution dToF data with the corresponding high-resolution RGB guidance. Unlike the conventional RGB-guided depth enhancement approaches which perform the fusion in a per-frame manner, we propose the first multi-frame fusion scheme to mitigate the spatial ambiguity resulting from the low-resolution dToF imaging. In addition, dToF sensors provide unique depth histogram information for each local patch, and we incorporate this dToF-specific feature in our network design to further alleviate spatial ambiguity. To evaluate our models on complex dynamic indoor environments and to provide a large-scale dToF sensor dataset, we introduce DyDToF, the first synthetic RGB-dToF video dataset that features dynamic objects and a realistic dToF simulator following the physical imaging process. We believe the methods and dataset are beneficial to a broad community as dToF depth sensing is becoming mainstream on mobile devices. Our code and data are publicly available. https://github.com/facebookresearch/DVSR/</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2345.Understanding the Robustness of 3D Object Detection With Bird&#x27;s-Eye-View Representations in Autonomous Driving</span><br>
                <span class="as">Zhu, ZijianandZhang, YichiandChen, HaiandDong, YinpengandZhao, ShuandDing, WenboandZhong, JiachenandZheng, Shibao</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Understanding_the_Robustness_of_3D_Object_Detection_With_Birds-Eye-View_Representations_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/21600-21610.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在评估各种代表性的视觉依赖BEV模型在广泛设置下的自然和对抗鲁棒性，以全面了解它们与没有BEV的特征相比，受显式BEV特征影响的行为。<br>
                    动机：虽然鸟瞰图（BEV）表示法已经显著提高了基于相机输入的3D检测器在流行基准上的性能，但目前仍缺乏对这种视觉依赖的BEV模型稳健性的系统理解，这与自动驾驶系统的安全性密切相关。<br>
                    方法：通过在广泛的设置下评估各种代表性模型的自然和对抗鲁棒性，来充分理解它们受显式BEV特征影响的行为。此外，还提出了一种3D一致补丁攻击，通过在3D空间应用对抗性补丁来保证时空一致性，这对于自动驾驶场景来说更为现实。<br>
                    效果：实验结果表明，1) BEV模型在不同自然条件和常见损坏下比之前的方法更稳定，这是由于其富有表现力的空间表示；2) BEV模型更容易受到对抗性噪声的影响，这主要是由于冗余的BEV特征；3) 具有多模态输入的相机-激光雷达融合模型在不同的设置下表现出优越的性能，但BEV融合模型仍然容易受到点云和图像的对抗性噪声的影响。这些发现提醒了BEV检测器应用中的安全问题，并有助于开发更鲁棒的模型。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>3D object detection is an essential perception task in autonomous driving to understand the environments. The Bird's-Eye-View (BEV) representations have significantly improved the performance of 3D detectors with camera inputs on popular benchmarks. However, there still lacks a systematic understanding of the robustness of these vision-dependent BEV models, which is closely related to the safety of autonomous driving systems. In this paper, we evaluate the natural and adversarial robustness of various representative models under extensive settings, to fully understand their behaviors influenced by explicit BEV features compared with those without BEV. In addition to the classic settings, we propose a 3D consistent patch attack by applying adversarial patches in the 3D space to guarantee the spatiotemporal consistency, which is more realistic for the scenario of autonomous driving. With substantial experiments, we draw several findings: 1) BEV models tend to be more stable than previous methods under different natural conditions and common corruptions due to the expressive spatial representations; 2) BEV models are more vulnerable to adversarial noises, mainly caused by the redundant BEV features; 3) Camera-LiDAR fusion models have superior performance under different settings with multi-modal inputs, but BEV fusion model is still vulnerable to adversarial noises of both point cloud and image. These findings alert the safety issue in the applications of BEV detectors and could facilitate the development of more robust models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2346.Anchor3DLane: Learning To Regress 3D Anchors for Monocular 3D Lane Detection</span><br>
                <span class="as">Huang, ShaofeiandShen, ZhenweiandHuang, ZehaoandDing, Zi-hanandDai, JiaoandHan, JizhongandWang, NaiyanandLiu, Si</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Anchor3DLane_Learning_To_Regress_3D_Anchors_for_Monocular_3D_Lane_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/17451-17460.png><br>
            
            <span class="tt"><span class="t0">研究问题：单目三维车道检测由于缺乏深度信息，是一个具有挑战性的任务。<br>
                    动机：目前的前视图像或特征转换为鸟瞰图空间的方法依赖于平坦地面的假设和上下文信息的丢失，使得从鸟瞰图表示恢复3D信息不准确。<br>
                    方法：我们定义了三维车道锚点，并提出了一种名为Anchor3DLane的无需鸟瞰图的方法，直接从前视表示预测三维车道。三维车道锚点被投影到前视特征上以提取其包含良好结构和上下文信息的特征，以进行准确的预测。此外，我们还开发了一种全局优化方法，利用车道之间的等宽属性来减少预测的横向误差。<br>
                    效果：我们在三个流行的三维车道检测基准上进行了广泛的实验，结果显示我们的Anchor3DLane优于先前的基于鸟瞰图的方法，并实现了最先进的性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Monocular 3D lane detection is a challenging task due to its lack of depth information. A popular solution is to first transform the front-viewed (FV) images or features into the bird-eye-view (BEV) space with inverse perspective mapping (IPM) and detect lanes from BEV features. However, the reliance of IPM on flat ground assumption and loss of context information make it inaccurate to restore 3D information from BEV representations. An attempt has been made to get rid of BEV and predict 3D lanes from FV representations directly, while it still underperforms other BEV-based methods given its lack of structured representation for 3D lanes. In this paper, we define 3D lane anchors in the 3D space and propose a BEV-free method named Anchor3DLane to predict 3D lanes directly from FV representations. 3D lane anchors are projected to the FV features to extract their features which contain both good structural and context information to make accurate predictions. In addition, we also develop a global optimization method that makes use of the equal-width property between lanes to reduce the lateral error of predictions. Extensive experiments on three popular 3D lane detection benchmarks show that our Anchor3DLane outperforms previous BEV-based methods and achieves state-of-the-art performances. The code is available at: https://github.com/tusen-ai/Anchor3DLane.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2347.V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception</span><br>
                <span class="as">Xu, RunshengandXia, XinandLi, JinlongandLi, HanzhaoandZhang, ShuoandTu, ZhengzhongandMeng, ZonglinandXiang, HaoandDong, XiaoyuandSong, RuiandYu, HongkaiandZhou, BoleiandMa, Jiaqi</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_Perception_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/13712-13722.png><br>
            
            <span class="tt"><span class="t0">研究问题：自动驾驶车辆的现代感知系统对遮挡敏感，且缺乏长感知范围的能力，这是阻碍五级自动驾驶的关键瓶颈之一。<br>
                    动机：最近的研究表明，车对车（V2V）协同感知系统具有改变自动驾驶行业的潜力，但缺乏真实世界数据集阻碍了该领域的发展。<br>
                    方法：我们提出了V2V4Real，这是第一个用于V2V感知的大型真实世界多模态数据集。数据由两辆配备多模态传感器的车辆通过各种场景共同驾驶收集。我们的V2V4Real数据集覆盖了410公里的驾驶区域，包括20K个激光雷达帧，40K个RGB帧，24万个标注的5类3D边界框和覆盖所有驾驶路线的高分辨率地图。<br>
                    效果：我们在三个任务上对最新的协同感知算法进行了全面基准测试，包括协同3D目标检测、协同3D目标跟踪和协同感知的Sim2Real领域适应。V2V4Real数据集可以在research.seas.ucla.edu/mobility-lab/v2v4real/找到。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Modern perception systems of autonomous vehicles are known to be sensitive to occlusions and lack the capability of long perceiving range. It has been one of the key bottlenecks that prevents Level 5 autonomy. Recent research has demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system has great potential to revolutionize the autonomous driving industry. However, the lack of a real-world dataset hinders the progress of this field. To facilitate the development of cooperative perception, we present V2V4Real, the first large-scale real-world multi-modal dataset for V2V perception. The data is collected by two vehicles equipped with multi-modal sensors driving together through diverse scenarios. Our V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real introduces three perception tasks, including cooperative 3D object detection, cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative perception. We provide comprehensive benchmarks of recent cooperative perception algorithms on three tasks. The V2V4Real dataset can be found at research.seas.ucla.edu/mobility-lab/v2v4real/.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2348.ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries</span><br>
                <span class="as">Gu, JunruandHu, ChenxuandZhang, TianyuanandChen, XuanyaoandWang, YilunandWang, YueandZhao, Hang</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_ViP3D_End-to-End_Visual_Trajectory_Prediction_via_3D_Agent_Queries_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/5496-5506.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的自动驾驶系统中，感知和预测是两个独立的模块，通过手工挑选的特征如代理边界框和轨迹进行交互。由于这种分离，作为下游模块的预测只能从感知模块接收有限的信息，而且感知模块的错误可能会传播和累积，对预测结果产生负面影响。<br>
                    动机：为了解决上述问题，本文提出了ViP3D，一种基于查询的视频视觉轨迹预测管道，利用原始视频中的丰富信息直接预测场景中代理的未来轨迹。<br>
                    方法：ViP3D在整个管道中使用稀疏的代理查询进行检测、跟踪和预测，使其成为首个全可微的视觉轨迹预测方法。与使用历史特征图和轨迹不同，ViP3D将来自先前时间戳的有用信息编码在代理查询中，使其成为一种简洁的流预测方法。<br>
                    效果：在nuScenes数据集上的大量实验结果表明，ViP3D在传统管道和先前的端到端模型上表现出强大的视觉预测性能。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Perception and prediction are two separate modules in the existing autonomous driving systems. They interact with each other via hand-picked features such as agent bounding boxes and trajectories. Due to this separation, prediction, as a downstream module, only receives limited information from the perception module. To make matters worse, errors from the perception modules can propagate and accumulate, adversely affecting the prediction results. In this work, we propose ViP3D, a query-based visual trajectory prediction pipeline that exploits rich information from raw videos to directly predict future trajectories of agents in a scene. ViP3D employs sparse agent queries to detect, track, and predict throughout the pipeline, making it the first fully differentiable vision-based trajectory prediction approach. Instead of using historical feature maps and trajectories, useful information from previous timestamps is encoded in agent queries, which makes ViP3D a concise streaming prediction method. Furthermore, extensive experimental results on the nuScenes dataset show the strong vision-based prediction performance of ViP3D over traditional pipelines and previous end-to-end models.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2349.Command-Driven Articulated Object Understanding and Manipulation</span><br>
                <span class="as">Chu, RuihangandLiu, ZhengzheandYe, XiaoqingandTan, XiaoandQi, XiaojuanandFu, Chi-WingandJia, Jiaya</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chu_Command-Driven_Articulated_Object_Understanding_and_Manipulation_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/8813-8823.png><br>
            
            <span class="tt"><span class="t0">研究问题：如何通过人类命令进行铰接物体的操作。<br>
                    动机：现有的研究主要关注于推断铰接结构，我们进一步支持根据简单的命令模板操作铰接形状。<br>
                    方法：我们提出了一种新的方法Cart，该方法利用对象结构的预测将视觉观察与用户命令连接起来，以实现有效的操作。<br>
                    效果：对于各种丰富的对象类别，Cart可以准确地操作对象形状，并在理解内在铰接结构方面超越最先进的方法。此外，它还可以很好地推广到未见过的对象类别和真实世界的物体上。我们希望Cart能为指导机器操作铰接物体开辟新的方向。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>We present Cart, a new approach towards articulated-object manipulations by human commands. Beyond the existing work that focuses on inferring articulation structures, we further support manipulating articulated shapes to align them subject to simple command templates. The key of Cart is to utilize the prediction of object structures to connect visual observations with user commands for effective manipulations. It is achieved by encoding command messages for motion prediction and a test-time adaptation to adjust the amount of movement from only command supervision. For a rich variety of object categories, Cart can accurately manipulate object shapes and outperform the state-of-the-art approaches in understanding the inherent articulation structures. Also, it can well generalize to unseen object categories and real-world objects. We hope Cart could open new directions for instructing machines to operate articulated objects.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2350.Unicode Analogies: An Anti-Objectivist Visual Reasoning Challenge</span><br>
                <span class="as">Spratley, StevenandEhinger, KristaA.andMiller, Tim</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Spratley_Unicode_Analogies_An_Anti-Objectivist_Visual_Reasoning_Challenge_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/19082-19091.png><br>
            
            <span class="tt"><span class="t0">研究问题：现有的PMP方法在评估计算机视觉中的类比推理能力时，难以暴露出研究问题：现有的PMP方法在评估计算机视觉中的类比推理能力时，难以暴露出求解器缺乏有意义的泛化的问题，并且强化了客观主义的观点，即物体只能以一种方式被看到。<br>
                    动机：本文提出了Unicode类比挑战，通过多义、基于字符的PMP来评估视觉系统中流畅的概念化能力。<br>
                    方法：我们设计了一个框架，通过呈现更难以完成的任务来挑战模型，这些任务需要强大的特征提取才能完成，但对人类参与者来说仍然可以解决。<br>
                    效果：我们认为Unicode类比优雅地捕捉并测试了当前一代AI严重缺乏的人的视觉推理方面。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Analogical reasoning enables agents to extract relevant information from scenes, and efficiently navigate them in familiar ways. While progressive-matrix problems (PMPs) are becoming popular for the development and evaluation of analogical reasoning in computer vision, we argue that the dominant methodology in this area struggles to expose the lack of meaningful generalisation in solvers, and reinforces an objectivist stance on perception -- that objects can only be seen one way -- which we believe to be counter-productive. In this paper, we introduce the Unicode Analogies challenge, consisting of polysemic, character-based PMPs to benchmark fluid conceptualisation ability in vision systems. Writing systems have evolved characters at multiple levels of abstraction, from iconic through to symbolic representations, producing both visually interrelated yet exceptionally diverse images when compared to those exhibited by existing PMP datasets. Our framework has been designed to challenge models by presenting tasks much harder to complete without robust feature extraction, while remaining largely solvable by human participants. We therefore argue that Unicode Analogies elegantly captures and tests for a facet of human visual reasoning that is severely lacking in current-generation AI.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2351.MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors</span><br>
                <span class="as">Zhang, YuangandWang, TiancaiandZhang, Xiangyu</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/22056-22065.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在提出一种简单而有效的方法，通过预训练的目标检测器启动端到端的多目标跟踪。<br>
                    动机：现有的端到端方法，如MOTR和TrackFormer，由于其较差的检测性能，不如基于检测的跟踪方法。<br>
                    方法：我们首先采用查询的锚定公式，然后使用额外的目标检测器生成提案作为锚点，为MOTR提供检测前处理。这种简单的修改大大缓解了MOTR中联合学习检测和关联任务之间的冲突。<br>
                    效果：实验结果表明，MOTRv2在DanceTrack数据集上实现了所有现有方法中的最高性能（73.4% HOTA）。此外，MOTRv2在BDD100K数据集上达到了最先进的性能。我们希望这个简单而有效的管道能为端到端的MOT社区提供一些新的启示。代码将在不久的将来发布。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, e.g. MOTR and TrackFormer are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the end-to-end feature and scales well on large-scale benchmarks. MOTRv2 achieves the top performance (73.4% HOTA) among all existing methods on the DanceTrack dataset. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the end-to-end MOT community. The code will be released in the near future.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2352.Improving Vision-and-Language Navigation by Generating Future-View Image Semantics</span><br>
                <span class="as">Li, JialuandBansal, Mohit</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Improving_Vision-and-Language_Navigation_by_Generating_Future-View_Image_Semantics_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/10803-10812.png><br>
            
            <span class="tt"><span class="t0">研究问题：本文旨在探索在视觉-语言导航任务中，通过生成未来可能的视图，是否可以提高导航效果。<br>
                    动机：人类在理解自然语言指令和周围环境的基础上，会对未来环境有一个预期，这有助于正确的导航。因此，作者提出让代理模型也具备这种能力。<br>
                    方法：作者首先提出了三个代理模型在领域内预训练时的代理任务：Masked Panorama Modeling（MPM），Masked Trajectory Modeling（MTM）和Action Prediction with Image Generation（APIG）。然后，作者在视觉-语言导航任务上对代理模型进行微调，并使用一个辅助损失函数来最小化生成的视图语义与下一步的真实视图语义之间的差异。<br>
                    效果：实验结果表明，该方法在Room-to-Room数据集和CVDN数据集上都取得了新的最先进的成果。此外，代理模型还能预测出未来视图中的缺失部分，提高了预测动作的解释性，并在更长的路径上表现更好。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Vision-and-Language Navigation (VLN) is the task that requires an agent to navigate through the environment based on natural language instructions. At each step, the agent takes the next action by selecting from a set of navigable locations. In this paper, we aim to take one step further and explore whether the agent can benefit from generating the potential future view during navigation. Intuitively, humans will have an expectation of how the future environment will look like, based on the natural language instructions and surrounding views, which will aid correct navigation. Hence, to equip the agent with this ability to generate the semantics of future navigation views, we first propose three proxy tasks during the agent's in-domain pre-training: Masked Panorama Modeling (MPM), Masked Trajectory Modeling (MTM), and Action Prediction with Image Generation (APIG). These three objectives teach the model to predict missing views in a panorama (MPM), predict missing steps in the full trajectory (MTM), and generate the next view based on the full instruction and navigation history (APIG), respectively. We then fine-tune the agent on the VLN task with an auxiliary loss that minimizes the difference between the view semantics generated by the agent and the ground truth view semantics of the next step. Empirically, our VLN-SIG achieves the new state-of-the-art on both the Room-to-Room dataset and the CVDN dataset. We further show that our agent learns to fill in missing patches in future views qualitatively, which brings more interpretability over agents' predicted actions. Lastly, we demonstrate that learning to predict future view semantics also enables the agent to have better performance on longer paths.</p>
                </div>
        </div>
           

        

        <div class="apaper" id="pid1">
            <div class="paperdesc">
                <span class="ts">2353.CIMI4D: A Large Multimodal Climbing Motion Dataset Under Human-Scene Interactions</span><br>
                <span class="as">Yan, MingandWang, XinandDai, YudiandShen, SiqiandWen, ChengluandXu, LanandMa, YuexinandWang, Cheng</span><br>
            </div><br>
            
            <div class="dllinks">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_CIMI4D_A_Large_Multimodal_Climbing_Motion_Dataset_Under_Human-Scene_Interactions_CVPR_2023_paper.pdf">[pdf-url] </a><br>
            </div><br>
            
            <img src = image/12977-12988.png><br>
            
            <span class="tt"><span class="t0">研究问题：运动捕捉是一个长期存在的问题，尤其是对于地面以上的动作如攀岩等，由于其复杂的后部姿势、复杂的人与场景交互和困难的全局定位，目前的研究还非常有限。<br>
                    动机：攀岩动作是体育和消防领域的重要动作，但由于缺乏特定的数据集，研究社区对其理解不深。为了解决这个问题，我们收集了CIMI4D数据集。<br>
                    方法：我们从12个人在13个不同的攀岩墙上攀岩的录像中收集了大量的数据，包括约180,000帧的姿势惯性测量、激光雷达点云、RGB视频、高精度静态点云场景和重建的场景网格。我们还逐帧标注了接触岩石支撑物的位置，以便于详细探索人与场景的交互。<br>
                    效果：通过四个任务（包括有人/无人场景约束的人姿估计、姿态预测和姿态生成）的实验结果证明，CIMI4D对现有方法提出了巨大的挑战，并为进一步研究提供了广阔的机会。</span>  </span><br>
            
            <button type="button" class="collapsible">[Abstract]</button>
                <div class="content">
                    <p>Motion capture is a long-standing research problem. Although it has been studied for decades, the majority of research focus on ground-based movements such as walking, sitting, dancing, etc. Off-grounded actions such as climbing are largely overlooked. As an important type of action in sports and firefighting field, the climbing movements is challenging to capture because of its complex back poses, intricate human-scene interactions, and difficult global localization. The research community does not have an in-depth understanding of the climbing action due to the lack of specific datasets. To address this limitation, we collect CIMI4D, a large rock ClImbing MotIon on dataset from 12 persons climbing 13 different climbing walls. The dataset consists of around 180,000 frames of pose inertial measurements, LiDAR point clouds, RGB videos, high-precision static point cloud scenes, and reconstructed scene meshes. Moreover, we frame-wise annotate touch rock holds to facilitate a detailed exploration of human-scene interaction. The core of this dataset is a blending optimization process, which corrects for the pose as it drifts and is affected by the magnetic conditions. To evaluate the merit of CIMI4D, we perform four tasks which include human pose estimations (with/without scene constraints), pose prediction, and pose generation. The experimental results demonstrate that CIMI4D presents great challenges to existing methods and enables extensive research opportunities. We share the dataset with the research community in http://www.lidarhumanmotion.net/cimi4d/.</p>
                </div>
        </div>
           

        

    </p>
  </div>
  

  <script>
    

  </script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>